{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "d975284c-a069-40d8-9847-9465d0ac4f47",
    "_uuid": "b6bb54c3-ff29-436c-8f5e-b3a13b9f5153",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "TEST_MODE = 0 # Testing macro\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "if TEST_MODE:\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "3ae86968-b273-4857-a4fd-537b178d854b",
    "_uuid": "bcec77b5-0738-4d15-8aad-f6b59b3ac023",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utility_functions imports setup complete\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install python-docx\n",
    "# !pip install antropy\n",
    "\n",
    "MAGENTA = (202/255, 18/255, 125/255)\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import dataclasses\n",
    "import math as math\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import cross_validate, RandomizedSearchCV\n",
    "import statsmodels.stats.api as sms\n",
    "from tqdm.auto import tqdm\n",
    "from dataclasses import asdict\n",
    "from sklearn import svm\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import random\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_curve, accuracy_score, make_scorer, auc\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold # Feature selector\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import iqr\n",
    "from scipy.stats import median_abs_deviation\n",
    "from scipy.stats import mode\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.signal import peak_widths\n",
    "# from scipy.special import entr\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, MaxAbsScaler, RobustScaler, PowerTransformer\n",
    "get_new_scaler_dict = {\"StandardScaler\": StandardScaler, \"MinMaxScaler\": MinMaxScaler, \"Normalizer\": Normalizer, \n",
    "                       \"MaxAbsScaler\": MaxAbsScaler, \"RobustScaler\": RobustScaler, \"PowerTransformer\": PowerTransformer}\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import auc\n",
    "# import antropy as ant\n",
    "import time\n",
    "# import docx\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "# Global utitlity functions are in separate notebook\n",
    "print(\"utility_functions imports setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.10\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "d7a8c48d-ce60-4891-a613-8cac6bcad885",
    "_uuid": "74529ec9-2ddf-43e8-8ef3-d2bba209f31b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def matchAccelGyroData(accel, gyro):\n",
    "    # Match the numbers by merge_asof to the higher length vector\n",
    "    accel_count = accel.count().time_stamp\n",
    "    gyro_count = gyro.count().time_stamp\n",
    "    names =['sensor_id', 'time_stamp', 'x', 'y', 'z']\n",
    "    if accel_count > gyro_count:\n",
    "        df = pd.merge_asof(accel, gyro, on=\"time_stamp\", direction='nearest')\n",
    "        df = df.sort_values(by=['time_stamp'])\n",
    "        df = df.dropna()\n",
    "        accel = df[[\"sensor_id_x\", \"time_stamp\", \"x_x\", \"y_x\", \"z_x\"]]\n",
    "        gyro = df[[\"sensor_id_y\", \"time_stamp\", \"x_y\", \"y_y\", \"z_y\"]]\n",
    "    else:\n",
    "        df = pd.merge_asof(gyro, accel, on=\"time_stamp\", direction='nearest')\n",
    "        df = df.sort_values(by=['time_stamp'])\n",
    "        df = df.dropna()\n",
    "        gyro = df[[\"sensor_id_x\", \"time_stamp\", \"x_x\", \"y_x\", \"z_x\"]]\n",
    "        accel = df[[\"sensor_id_y\", \"time_stamp\", \"x_y\", \"y_y\", \"z_y\"]]\n",
    "\n",
    "    accel.columns = names\n",
    "    gyro.columns = names\n",
    "    \n",
    "    return {'accel': accel, 'gyro': gyro}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "d8b4d9b0-695e-4551-be65-38a6f1817241",
    "_uuid": "fb41768e-82f1-4fb4-ba68-e1dd6cfcfba3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getDataStats1(i, print_accel_gyro_array_size=1, print_na_df_array_size=0, begin_idx=500, end_idx=-500):\n",
    "    \n",
    "    #load the data\n",
    "    names =['sensor_id', 'time_stamp', 'x', 'y', 'z']\n",
    "#     if i!=8:\n",
    "#         data = pd.read_csv('../input/wearable-assisted-ca/user{}_1.csv'.format(i), error_bad_lines = False, header=None, usecols = range(len(names)))\n",
    "#     else:\n",
    "    \n",
    "    data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
    "    data.columns = names\n",
    "    data = data[(data.sensor_id == '10') | (data.sensor_id =='4')]\n",
    "    data.head(10)\n",
    "\n",
    "\n",
    "    types_dict = {'sensor_id': 'int32', 'time_stamp': 'float64', 'x': 'float64', 'y': 'float64', 'z': 'float64'}\n",
    "    for col, col_type in types_dict.items():\n",
    "        data[col] = data[col].astype(col_type)\n",
    "\n",
    "    # find how many NAN values in the data\n",
    "    data.isna().sum()\n",
    "\n",
    "    # since only 7 NAN is a very small amount, drop them\n",
    "    df = data.dropna()\n",
    "\n",
    "    # get rid of begin and end noise\n",
    "    #sort df and filter\n",
    "    df = df.sort_values(by=['time_stamp'])\n",
    "    df = df[begin_idx:end_idx]\n",
    "    \n",
    "    # cleaning extreme outliers\n",
    "    df = df[(df.x < 10.1) & ( -10.1 < df.x) & (df.y < 10.1) & ( -10.1 < df.y) & (df.z < 10.1) & ( -10.1 < df.z) ]\n",
    "\n",
    "    # Extract Accelerometer values and sort\n",
    "    accel = df[df.sensor_id == 10]\n",
    "    accel = accel.sort_values(by=['time_stamp'])\n",
    "\n",
    "    # Extract gyro values and sort\n",
    "    gyro = df[df.sensor_id == 4]\n",
    "    gyro = gyro.sort_values(by=['time_stamp'])\n",
    "    \n",
    "    if print_accel_gyro_array_size:\n",
    "        print(\"{}) accel_count: {}, gyro_count: {}\".format(i, accel.count().time_stamp, gyro.count().time_stamp))\n",
    "    \n",
    "    result = matchAccelGyroData(accel, gyro)\n",
    "    accel, gyro = result['accel'], result['gyro']\n",
    "    \n",
    "    accel['EMA_x_a'] = accel['x'].ewm(span=40,adjust=False).mean()\n",
    "    accel['EMA_y_a'] = accel['y'].ewm(span=40,adjust=False).mean()\n",
    "    accel['EMA_z_a'] = accel['z'].ewm(span=40,adjust=False).mean()\n",
    "\n",
    "    gyro['EMA_x_g'] = gyro['x'].ewm(span=40,adjust=False).mean()\n",
    "    gyro['EMA_y_g'] = gyro['y'].ewm(span=40,adjust=False).mean()\n",
    "    gyro['EMA_z_g'] = gyro['z'].ewm(span=40,adjust=False).mean()\n",
    "    \n",
    "    accel['EMA_x_a'] = accel['x']\n",
    "    accel['EMA_y_a'] = accel['y']\n",
    "    accel['EMA_z_a'] = accel['z']\n",
    "\n",
    "    gyro['EMA_x_g'] = gyro['x']\n",
    "    gyro['EMA_y_g'] = gyro['y']\n",
    "    gyro['EMA_z_g'] = gyro['z']\n",
    "    \n",
    "    left = accel[[\"time_stamp\", \"EMA_x_a\", \"EMA_y_a\", \"EMA_z_a\"]]\n",
    "    right = gyro[[\"time_stamp\", \"EMA_x_g\", \"EMA_y_g\", \"EMA_z_g\"]].set_index('time_stamp')\n",
    "    df = left.join(right, on='time_stamp')\n",
    "\n",
    "    if print_na_df_array_size:\n",
    "        print(\"{}) na_count: {}, df count: {}\".format(i, df.isna().sum().sum(), df.count().time_stamp))\n",
    "    \n",
    "    return {\"accel\":accel.count().time_stamp, \"gyro\": gyro.count().time_stamp, \"df\": df, \"userIdx\": i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "81b39c51-3e35-4210-a3ae-618543e164df",
    "_uuid": "287b6e21-fe46-4032-8e82-2ed1837e9f44",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getDataStats2(i, print_accel_gyro_array_size=1, print_na_df_array_size=0, begin_idx=500, end_idx=-500):\n",
    "    \n",
    "    #load the data\n",
    "    names =['sensor_id', 'time_stamp', 'x', 'y', 'z']\n",
    "#     if i!=8:\n",
    "#         data = pd.read_csv('../input/wearable-assisted-ca/user{}_1.csv'.format(i), error_bad_lines = False, header=None, usecols = range(len(names)))\n",
    "#     else:\n",
    "    \n",
    "    data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
    "    data.columns = names\n",
    "    data = data[(data.sensor_id == '10') | (data.sensor_id =='4')]\n",
    "    data.head(10)\n",
    "\n",
    "\n",
    "    types_dict = {'sensor_id': 'int32', 'time_stamp': 'float64', 'x': 'float64', 'y': 'float64', 'z': 'float64'}\n",
    "    for col, col_type in types_dict.items():\n",
    "        data[col] = data[col].astype(col_type)\n",
    "\n",
    "    # find how many NAN values in the data\n",
    "    data.isna().sum()\n",
    "\n",
    "    # since only 7 NAN is a very small amount, drop them\n",
    "    df = data.dropna()\n",
    "\n",
    "    # get rid of begin and end noise\n",
    "    #sort df and filter\n",
    "    df = df.sort_values(by=['time_stamp'])\n",
    "    df = df[begin_idx:end_idx]\n",
    "    \n",
    "    # cleaning extreme outliers\n",
    "    df = df[(df.x < 10.1) & ( -10.1 < df.x) & (df.y < 10.1) & ( -10.1 < df.y) & (df.z < 10.1) & ( -10.1 < df.z) ]\n",
    "\n",
    "    # Extract Accelerometer values and sort\n",
    "    accel = df[df.sensor_id == 10]\n",
    "    accel = accel.sort_values(by=['time_stamp'])\n",
    "\n",
    "    # Extract gyro values and sort\n",
    "    gyro = df[df.sensor_id == 4]\n",
    "    gyro = gyro.sort_values(by=['time_stamp'])\n",
    "    \n",
    "    if print_accel_gyro_array_size:\n",
    "        print(\"{}) accel_count: {}, gyro_count: {}\".format(i, accel.count().time_stamp, gyro.count().time_stamp))\n",
    "    \n",
    "    result = matchAccelGyroData(accel, gyro)\n",
    "    accel, gyro = result['accel'], result['gyro']\n",
    "    \n",
    "#     accel['EMA_x_a'] = accel['x'].ewm(span=40,adjust=False).mean()\n",
    "#     accel['EMA_y_a'] = accel['y'].ewm(span=40,adjust=False).mean()\n",
    "#     accel['EMA_z_a'] = accel['z'].ewm(span=40,adjust=False).mean()\n",
    "\n",
    "#     gyro['EMA_x_g'] = gyro['x'].ewm(span=40,adjust=False).mean()\n",
    "#     gyro['EMA_y_g'] = gyro['y'].ewm(span=40,adjust=False).mean()\n",
    "#     gyro['EMA_z_g'] = gyro['z'].ewm(span=40,adjust=False).mean()\n",
    "\n",
    "    accel['EMA_x_a'] = accel['x']\n",
    "    accel['EMA_y_a'] = accel['y']\n",
    "    accel['EMA_z_a'] = accel['z']\n",
    "\n",
    "    gyro['EMA_x_g'] = gyro['x']\n",
    "    gyro['EMA_y_g'] = gyro['y']\n",
    "    gyro['EMA_z_g'] = gyro['z']\n",
    "    \n",
    "    left = accel[[\"time_stamp\", \"EMA_x_a\", \"EMA_y_a\", \"EMA_z_a\"]]\n",
    "    right = gyro[[\"time_stamp\", \"EMA_x_g\", \"EMA_y_g\", \"EMA_z_g\"]].set_index('time_stamp')\n",
    "    df = left.join(right, on='time_stamp')\n",
    "\n",
    "    if print_na_df_array_size:\n",
    "        print(\"{}) na_count: {}, df count: {}\".format(i, df.isna().sum().sum(), df.count().time_stamp))\n",
    "    \n",
    "    return {\"accel\":accel.count().time_stamp, \"gyro\": gyro.count().time_stamp, \"df\": df, \"userIdx\": i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "852f6592-f958-4c9b-b8f4-73946030cfef",
    "_uuid": "52758db0-8d25-455e-91ec-b1bd76be8f7b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample_rate = 10 #Hz\n",
    "# #3352843.3\n",
    "# x = np.array([318.45,302.78,316.47,334.14,333.41,326.15,320.07,318.68,314.12,308.64,\n",
    "#               300.15,304.33,318.42,322.72,329.56,339.18,338.03,343.27,351.44,353.23,\n",
    "#               352.35,352.88,353.43,352.14,351.28,352.82,353.36,353.35,353.19,353.82])\n",
    "\n",
    "# mn=np.mean(x)\n",
    "# print(f' mean = {mn:.3f} unit')\n",
    "# print(f' sum x[i]**2  : {np.sum(x**2) :.1f} unit^2 ')\n",
    "\n",
    "\n",
    "# print(f' n *sum X[k]**2   : {spectral_energy(x) :.1f} unit^2 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4054a4f7-9d44-4c4d-865d-b30fc17e48db",
    "_uuid": "e31d6f97-ed7c-4146-bec0-ba773f823c37",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-07T19:01:25.942693Z",
     "iopub.status.busy": "2022-05-07T19:01:25.942065Z",
     "iopub.status.idle": "2022-05-07T19:01:25.952827Z",
     "shell.execute_reply": "2022-05-07T19:01:25.951688Z",
     "shell.execute_reply.started": "2022-05-07T19:01:25.942645Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def spectral_energy(x):\n",
    "    '''\n",
    "    spectral_energy according to Parseval's theorem\n",
    "    '''\n",
    "    return (1/len(x)) * np.sum(np.abs(np.fft.rfft(x))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "77192541-b078-4a38-9a98-29b1868f61b2",
    "_uuid": "d0eaad53-bd24-45f3-8261-1e1f4a244ce5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:30.925626Z",
     "iopub.status.busy": "2022-03-29T07:28:30.925005Z",
     "iopub.status.idle": "2022-03-29T07:28:30.971124Z",
     "shell.execute_reply": "2022-03-29T07:28:30.97013Z",
     "shell.execute_reply.started": "2022-03-29T07:28:30.92558Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def signal_to_encoding(signal_df):\n",
    "    dic = {}\n",
    "    \n",
    "#     print(\"mean calculation started\")\n",
    "    dic['mean_x_a'] = np.mean(signal_df['EMA_x_a'])\n",
    "    dic['mean_y_a'] = np.mean(signal_df['EMA_y_a'])\n",
    "    dic['mean_z_a'] = np.mean(signal_df['EMA_z_a'])\n",
    "    dic['mean_x_g'] = np.mean(signal_df['EMA_x_g'])\n",
    "    dic['mean_y_g'] = np.mean(signal_df['EMA_y_g'])\n",
    "    dic['mean_z_g'] = np.mean(signal_df['EMA_z_g'])\n",
    "#     print(\"mean calculation ended\")\n",
    "    \n",
    "#     print(\"median calculation started\")\n",
    "    dic['median_x_a'] = np.median(signal_df['EMA_x_a'])\n",
    "    dic['median_y_a'] = np.median(signal_df['EMA_y_a'])\n",
    "    dic['median_z_a'] = np.median(signal_df['EMA_z_a'])\n",
    "    dic['median_x_g'] = np.median(signal_df['EMA_x_g'])\n",
    "    dic['median_y_g'] = np.median(signal_df['EMA_y_g'])\n",
    "    dic['median_z_g'] = np.median(signal_df['EMA_z_g'])\n",
    "#     print(\"median calculation ended\")\n",
    "    \n",
    "#     print(\"var calculation started\")\n",
    "    dic['var_x_a'] = np.var(signal_df['EMA_x_a'])\n",
    "    dic['var_y_a'] = np.var(signal_df['EMA_y_a'])\n",
    "    dic['var_z_a'] = np.var(signal_df['EMA_z_a'])\n",
    "    dic['var_x_g'] = np.var(signal_df['EMA_x_g'])\n",
    "    dic['var_y_g'] = np.var(signal_df['EMA_y_g'])\n",
    "    dic['var_z_g'] = np.var(signal_df['EMA_z_g'])\n",
    "#     print(\"var calculation ended\")\n",
    "    \n",
    "#     print(\"avg absolute difference of peaks calculation started\")\n",
    "    peaks_x_a, _ = find_peaks(signal_df['EMA_x_a'])\n",
    "    peaks_y_a, _ = find_peaks(signal_df['EMA_y_a'])\n",
    "    peaks_z_a, _ = find_peaks(signal_df['EMA_z_a'])\n",
    "    peaks_x_g, _ = find_peaks(signal_df['EMA_x_g'])\n",
    "    peaks_y_g, _ = find_peaks(signal_df['EMA_y_g'])\n",
    "    peaks_z_g, _ = find_peaks(signal_df['EMA_z_g'])\n",
    "    \n",
    "#     print(type(peak_widths(peaks_x_a, signal_df['EMA_x_a'], rel_height=0.5)[0]))\n",
    "    dic['aadp_x_a'] = np.mean(peak_widths(signal_df['EMA_x_a'], peaks_x_a, rel_height=0.5)[0])\n",
    "    dic['aadp_y_a'] = np.mean(peak_widths(signal_df['EMA_y_a'], peaks_y_a, rel_height=0.5)[0])\n",
    "    dic['aadp_z_a'] = np.mean(peak_widths(signal_df['EMA_z_a'], peaks_z_a, rel_height=0.5)[0])\n",
    "    dic['aadp_x_g'] = np.mean(peak_widths(signal_df['EMA_x_g'], peaks_x_g, rel_height=0.5)[0])\n",
    "    dic['aadp_y_g'] = np.mean(peak_widths(signal_df['EMA_y_g'], peaks_y_g, rel_height=0.5)[0])\n",
    "    dic['aadp_z_g'] = np.mean(peak_widths(signal_df['EMA_z_g'], peaks_z_g, rel_height=0.5)[0])\n",
    "#     print(\"avg absolute difference of peaks calculation ended\")\n",
    "    \n",
    "#     print(\"range calculation started\")\n",
    "    dic['ptp_x_a'] = np.ptp(signal_df['EMA_x_a'])\n",
    "    dic['ptp_y_a'] = np.ptp(signal_df['EMA_y_a'])\n",
    "    dic['ptp_z_a'] = np.ptp(signal_df['EMA_z_a'])\n",
    "    dic['ptp_x_g'] = np.ptp(signal_df['EMA_x_g'])\n",
    "    dic['ptp_y_g'] = np.ptp(signal_df['EMA_y_g'])\n",
    "    dic['ptp_z_g'] = np.ptp(signal_df['EMA_z_g'])\n",
    "#     print(\"range calculation ended\")\n",
    "    \n",
    "#     print(\"mode calculation started\")\n",
    "    dic['mode_x_a'] = mode(signal_df['EMA_x_a'])[0][0]\n",
    "    dic['mode_y_a'] = mode(signal_df['EMA_y_a'])[0][0]\n",
    "    dic['mode_z_a'] = mode(signal_df['EMA_z_a'])[0][0]\n",
    "    dic['mode_x_g'] = mode(signal_df['EMA_x_g'])[0][0]\n",
    "    dic['mode_y_g'] = mode(signal_df['EMA_y_g'])[0][0]\n",
    "    dic['mode_z_g'] = mode(signal_df['EMA_z_g'])[0][0]\n",
    "#     print(\"mode calculation ended\")\n",
    "    \n",
    "#     print(\"cov calculation started\")\n",
    "    dic['cov_x_a'] = np.cov(signal_df['EMA_x_a']) * 1\n",
    "    dic['cov_y_a'] = np.cov(signal_df['EMA_y_a']) * 1\n",
    "    dic['cov_z_a'] = np.cov(signal_df['EMA_z_a']) * 1\n",
    "    dic['cov_x_g'] = np.cov(signal_df['EMA_x_g']) * 1\n",
    "    dic['cov_y_g'] = np.cov(signal_df['EMA_y_g']) * 1\n",
    "    dic['cov_z_g'] = np.cov(signal_df['EMA_z_g']) * 1\n",
    "#     print(\"cov calculation ended\")\n",
    "    \n",
    "#     print(\"mean absolute deviation calculation started\")\n",
    "    dic['mad_x_a'] = median_abs_deviation(signal_df['EMA_x_a'])\n",
    "    dic['mad_y_a'] = median_abs_deviation(signal_df['EMA_y_a'])\n",
    "    dic['mad_z_a'] = median_abs_deviation(signal_df['EMA_z_a'])\n",
    "    dic['mad_x_g'] = median_abs_deviation(signal_df['EMA_x_g'])\n",
    "    dic['mad_y_g'] = median_abs_deviation(signal_df['EMA_y_g'])\n",
    "    dic['mad_z_g'] = median_abs_deviation(signal_df['EMA_z_g'])\n",
    "#     print(\"mean absolute deviation calculation ended\")\n",
    "    \n",
    "#     print(\"inter-quartile range calculation started\")\n",
    "    dic['iqr_x_a'] = iqr(signal_df['EMA_x_a'])\n",
    "    dic['iqr_y_a'] = iqr(signal_df['EMA_y_a'])\n",
    "    dic['iqr_z_a'] = iqr(signal_df['EMA_z_a'])\n",
    "    dic['iqr_x_g'] = iqr(signal_df['EMA_x_g'])\n",
    "    dic['iqr_y_g'] = iqr(signal_df['EMA_y_g'])\n",
    "    dic['iqr_z_g'] = iqr(signal_df['EMA_z_g'])\n",
    "#     print(\"inter-quartile range calculation ended\")\n",
    "    \n",
    "#     print(\"correlation calculation started\")\n",
    "    dic['correlate_xy_a'] = np.correlate(signal_df['EMA_x_a'], signal_df['EMA_y_a'])[0]\n",
    "    dic['correlate_yz_a'] = np.correlate(signal_df['EMA_y_a'], signal_df['EMA_z_a'])[0]\n",
    "    dic['correlate_xz_a'] = np.correlate(signal_df['EMA_x_a'], signal_df['EMA_z_a'])[0]\n",
    "    dic['correlate_xy_g'] = np.correlate(signal_df['EMA_x_g'], signal_df['EMA_y_g'])[0]\n",
    "    dic['correlate_yz_g'] = np.correlate(signal_df['EMA_y_g'], signal_df['EMA_z_g'])[0]\n",
    "    dic['correlate_xz_g'] = np.correlate(signal_df['EMA_x_g'], signal_df['EMA_z_g'])[0]\n",
    "#     print(\"correlation calculation ended\")\n",
    "    \n",
    "#     print(\"skew calculation started\")\n",
    "    dic['skew_x_a'] = skew(signal_df['EMA_x_a'])\n",
    "    dic['skew_y_a'] = skew(signal_df['EMA_y_a'])\n",
    "    dic['skew_z_a'] = skew(signal_df['EMA_z_a'])\n",
    "    dic['skew_x_g'] = skew(signal_df['EMA_x_g'])\n",
    "    dic['skew_y_g'] = skew(signal_df['EMA_y_g'])\n",
    "    dic['skew_z_g'] = skew(signal_df['EMA_z_g'])\n",
    "#     print(\"skew calculation ended\")\n",
    "    \n",
    "#     print(\"kurtosis calculation started\")\n",
    "    dic['kurtosis_x_a'] = kurtosis(signal_df['EMA_x_a'])\n",
    "    dic['kurtosis_y_a'] = kurtosis(signal_df['EMA_y_a'])\n",
    "    dic['kurtosis_z_a'] = kurtosis(signal_df['EMA_z_a'])\n",
    "    dic['kurtosis_x_g'] = kurtosis(signal_df['EMA_x_g'])\n",
    "    dic['kurtosis_y_g'] = kurtosis(signal_df['EMA_y_g'])\n",
    "    dic['kurtosis_z_g'] = kurtosis(signal_df['EMA_z_g'])\n",
    "#     print(\"kurtosis calculation ended\")\n",
    "    \n",
    "    \n",
    "#     print(\"spectral energy calculation started\")\n",
    "    dic['spectral_energy_x_a'] = spectral_energy(signal_df['EMA_x_a'])\n",
    "    dic['spectral_energy_y_a'] = spectral_energy(signal_df['EMA_y_a'])\n",
    "    dic['spectral_energy_z_a'] = spectral_energy(signal_df['EMA_z_a'])\n",
    "    dic['spectral_energy_x_g'] = spectral_energy(signal_df['EMA_x_g'])\n",
    "    dic['spectral_energy_y_g'] = spectral_energy(signal_df['EMA_y_g'])\n",
    "    dic['spectral_energy_z_g'] = spectral_energy(signal_df['EMA_z_g'])\n",
    "#     print(\"spectral energy calculation ended\")\n",
    "\n",
    "\n",
    "#     print(\"spectral entropy calculation started\")\n",
    "#     method = 'fft'\n",
    "#     normalize = False\n",
    "# #     print(signal_df['EMA_x_a'])\n",
    "# #     print(signal_df['EMA_x_a'].shape)\n",
    "#     axis = -1\n",
    "#     dic['spectral_entropy_x_a'] = ant.spectral_entropy(signal_df['EMA_x_a'], sf=len(signal_df['EMA_x_a']), method=method, normalize=normalize, axis=axis)\n",
    "#     dic['spectral_entropy_y_a'] = ant.spectral_entropy(signal_df['EMA_y_a'], sf=len(signal_df['EMA_y_a']), method=method, normalize=normalize, axis=axis)\n",
    "#     dic['spectral_entropy_z_a'] = ant.spectral_entropy(signal_df['EMA_z_a'], sf=len(signal_df['EMA_z_a']), method=method, normalize=normalize, axis=axis)\n",
    "#     dic['spectral_entropy_x_g'] = ant.spectral_entropy(signal_df['EMA_x_g'], sf=len(signal_df['EMA_x_g']), method=method, normalize=normalize, axis=axis)\n",
    "#     dic['spectral_entropy_y_g'] = ant.spectral_entropy(signal_df['EMA_y_g'], sf=len(signal_df['EMA_y_g']), method=method, normalize=normalize, axis=axis)\n",
    "#     dic['spectral_entropy_z_g'] = ant.spectral_entropy(signal_df['EMA_z_g'], sf=len(signal_df['EMA_z_g']), method=method, normalize=normalize, axis=axis)\n",
    "    \n",
    "#     print(dic['spectral_entropy_x_a'],\n",
    "#               dic['spectral_entropy_y_a'],\n",
    "#               dic['spectral_entropy_z_a'],\n",
    "#               dic['spectral_entropy_x_g'],\n",
    "#               dic['spectral_entropy_y_g'],\n",
    "#               dic['spectral_entropy_z_g'])\n",
    "#     print(\"spectral entropy calculation ended\")\n",
    "\n",
    "\n",
    "#     print(\"entropy calculation started\")\n",
    "    \n",
    "#     cols = signal_df[[\"EMA_x_a\", \"EMA_y_a\", \"EMA_z_a\", \"EMA_x_g\", \"EMA_y_g\", \"EMA_z_g\"]]\n",
    "#     cols = normalize(cols, norm='l2', axis = 0)\n",
    "#     print(cols.sum(axis = 0))\n",
    "#     cols = StandardScaler().fit_transform(cols)\n",
    "#     p = cols/cols.sum(axis=0)\n",
    "#     print(p.sum(axis=0))\n",
    "#     print(p.shape)\n",
    "#     print(cols.sum(axis=0))\n",
    "#     entropy = entr(p).sum(axis=0)\n",
    "#     print(entropy.shape)\n",
    "#     dic['entropy_x_a'] = entropy[0]\n",
    "#     dic['entropy_y_a'] = entropy[1]\n",
    "#     dic['entropy_z_a'] = entropy[2]\n",
    "#     dic['entropy_x_g'] = entropy[3]\n",
    "#     dic['entropy_y_g'] = entropy[4]\n",
    "#     dic['entropy_z_g'] = entropy[5]\n",
    "#     print(\"entropy calculation ended\")\n",
    "    \n",
    "    vector = [dic['mean_x_a'], \n",
    "              dic['mean_y_a'],\n",
    "              dic['mean_z_a'],\n",
    "              dic['mean_x_g'],\n",
    "              dic['mean_y_g'],\n",
    "              dic['mean_z_g'],\n",
    "              \n",
    "              dic['median_x_a'],\n",
    "              dic['median_y_a'],\n",
    "              dic['median_z_a'],\n",
    "              dic['median_x_g'],\n",
    "              dic['median_y_g'],\n",
    "              dic['median_z_g'],\n",
    "              \n",
    "              dic['var_x_a'],\n",
    "              dic['var_y_a'],\n",
    "              dic['var_z_a'],\n",
    "              dic['var_x_g'],\n",
    "              dic['var_y_g'],\n",
    "              dic['var_z_g'],\n",
    "              \n",
    "              dic['aadp_x_a'],\n",
    "              dic['aadp_y_a'],\n",
    "              dic['aadp_z_a'],\n",
    "              dic['aadp_x_g'],\n",
    "              dic['aadp_y_g'],\n",
    "              dic['aadp_z_g'],\n",
    "              \n",
    "              dic['ptp_x_a'],\n",
    "              dic['ptp_y_a'],\n",
    "              dic['ptp_z_a'],\n",
    "              dic['ptp_x_g'],\n",
    "              dic['ptp_y_g'],\n",
    "              dic['ptp_z_g'],\n",
    "              \n",
    "              dic['mode_x_a'],\n",
    "              dic['mode_y_a'],\n",
    "              dic['mode_z_a'],\n",
    "              dic['mode_x_g'],\n",
    "              dic['mode_y_g'],\n",
    "              dic['mode_z_g'],\n",
    "              \n",
    "              dic['cov_x_a'],\n",
    "              dic['cov_y_a'],\n",
    "              dic['cov_z_a'],\n",
    "              dic['cov_x_g'],\n",
    "              dic['cov_y_g'],\n",
    "              dic['cov_z_g'],\n",
    "              \n",
    "              dic['mad_x_a'],\n",
    "              dic['mad_y_a'],\n",
    "              dic['mad_z_a'],\n",
    "              dic['mad_x_g'],\n",
    "              dic['mad_y_g'],\n",
    "              dic['mad_z_g'],\n",
    "              \n",
    "              dic['iqr_x_a'],\n",
    "              dic['iqr_y_a'],\n",
    "              dic['iqr_z_a'],\n",
    "              dic['iqr_x_g'],\n",
    "              dic['iqr_y_g'],\n",
    "              dic['iqr_z_g'],\n",
    "              \n",
    "              dic['correlate_xy_a'],\n",
    "              dic['correlate_yz_a'],\n",
    "              dic['correlate_xz_a'],\n",
    "              dic['correlate_xy_g'],\n",
    "              dic['correlate_yz_g'],\n",
    "              dic['correlate_xz_g'],\n",
    "              \n",
    "              dic['skew_x_a'],\n",
    "              dic['skew_y_a'],\n",
    "              dic['skew_z_a'],\n",
    "              dic['skew_x_g'],\n",
    "              dic['skew_y_g'],\n",
    "              dic['skew_z_g'],\n",
    "              \n",
    "              dic['kurtosis_x_a'],\n",
    "              dic['kurtosis_y_a'],\n",
    "              dic['kurtosis_z_a'],\n",
    "              dic['kurtosis_x_g'],\n",
    "              dic['kurtosis_y_g'],\n",
    "              dic['kurtosis_z_g'],\n",
    "              \n",
    "              dic['spectral_energy_x_a'],\n",
    "              dic['spectral_energy_y_a'],\n",
    "              dic['spectral_energy_z_a'],\n",
    "              dic['spectral_energy_x_g'],\n",
    "              dic['spectral_energy_y_g'],\n",
    "              dic['spectral_energy_z_g'],\n",
    "              \n",
    "#               dic['spectral_entropy_x_a'],\n",
    "#               dic['spectral_entropy_y_a'],\n",
    "#               dic['spectral_entropy_z_a'],\n",
    "#               dic['spectral_entropy_x_g'],\n",
    "#               dic['spectral_entropy_y_g'],\n",
    "#               dic['spectral_entropy_z_g']\n",
    "             ]\n",
    "    \n",
    "    \n",
    "    return dic, np.array(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "405969a0-3049-4c9a-a3b1-fd5f15905170",
    "_uuid": "86a331fc-b630-4411-ab56-0375c262e607",
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:31.353612Z",
     "iopub.status.busy": "2022-03-29T07:28:31.353275Z",
     "iopub.status.idle": "2022-03-29T07:28:31.39463Z",
     "shell.execute_reply": "2022-03-29T07:28:31.393714Z",
     "shell.execute_reply.started": "2022-03-29T07:28:31.35358Z"
    }
   },
   "outputs": [],
   "source": [
    "def rolling_window(a, window, stride):\n",
    "    shape = a.shape[:-1] + (int((a.shape[-1] - window)/stride + 1), window)\n",
    "    strides = (stride*a.strides[-1],) + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "\n",
    "\n",
    "\n",
    "def getIndices2(sampleSize=1000, step=1000, numSamplePoints=24000):\n",
    "    indices = np.arange(0, numSamplePoints, 1)\n",
    "    indices = rolling_window(indices, sampleSize, step)\n",
    "    \n",
    "    return indices\n",
    "\n",
    "\n",
    "def getEncodingArray(df, windows):\n",
    "    a = []\n",
    "    for i in range(len(windows)):\n",
    "        # replaced loc with iloc per documentation\n",
    "#         a.append(signal_to_encoding(df.loc[windows[i], :])[1])\n",
    "        a.append(signal_to_encoding(df.iloc[windows[i], :])[1])\n",
    "        \n",
    "    return np.array(a)\n",
    "\n",
    "\n",
    "def deleteDiagonal(array):\n",
    "    depth = array.shape[-1]\n",
    "    m = array.shape[1]\n",
    "    strided = np.lib.stride_tricks.as_strided\n",
    "    s0,s1,s2 = array.strides\n",
    "    return strided(array.ravel()[depth:], shape=(m-1, m, depth), strides=(s0+s1,s1, s2)).reshape(m, m-1, depth)\n",
    "\n",
    "\n",
    "def MinMaxTransformation(windows_features_array):\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(windows_features_array)\n",
    "#     scaled_array = scaler.transform(windows_features_array)\n",
    "    \n",
    "    return scaler\n",
    "    \n",
    "    \n",
    "def getDistFRR1(dfList, window_size = 1000, step = 1000, numSamplePoints= 18000):\n",
    "    \n",
    "    windows = getIndices2(sampleSize=window_size, step=step, numSamplePoints= numSamplePoints)\n",
    "\n",
    "    norm_dist = []\n",
    "    norm_distro_dict = {}\n",
    "    counter = 1\n",
    "    for m in range(len(dfList)):\n",
    "        \n",
    "        \n",
    "        encoding_array = getEncodingArray(dfList[m], windows)\n",
    "#         print(dfList[m].columns)\n",
    "#         print(encoding_array.shape)\n",
    "\n",
    "#         print(np.sum(encoding_array, axis = 1))\n",
    "\n",
    "        # Doesn't make sense to normalize this here\n",
    "#         scaler = MinMaxTransformation(encoding_array)\n",
    "#         encoding_array = scaler.transform(encoding_array)\n",
    "#         print(encoding_array.shape)\n",
    "        # Should this be put between braces before indexing??? No\n",
    "        encoding_array = encoding_array / np.linalg.norm(encoding_array, axis = 1)[:, None]\n",
    "#         print(np.linalg.norm(encoding_array, axis=1)[:, None].shape)\n",
    "#         print(encoding_array.shape)\n",
    "#         print(np.linalg.norm(encoding_array, axis=1))\n",
    "        # Is dist_array distance calculation done correctly??? It seems it does not square\n",
    "        # It appears so dist = numpy.linalg.norm(a-b)\n",
    "        # https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy\n",
    "        dist_array = (encoding_array[None, :] - encoding_array[:, None])\n",
    "\n",
    "        dist_array = deleteDiagonal(dist_array)\n",
    "\n",
    "        dist_array = np.linalg.norm(dist_array, axis = 2)\n",
    "        \n",
    "        norm_dist.append(dist_array)\n",
    "        \n",
    "        # for err dist\n",
    "        norm_distro_dict[m] = np.array(dist_array).ravel()\n",
    "        \n",
    "        counter += 1\n",
    "    \n",
    "    return {\"dist_array\": np.array(norm_dist).ravel(), \"dist_dict\": norm_distro_dict}\n",
    "\n",
    "\n",
    "def getDistFRRFinal(dfList_exp1, dfList_exp2, window_size = 1000, step = 1000, numSamplePoints= 18000):\n",
    "    '''\n",
    "    dfLists are of the same size.\n",
    "    '''\n",
    "    if len(dfList_exp1) != len(dfList_exp2): \n",
    "        raise Exception(\"dfLists are not of the same size.\")\n",
    "        \n",
    "    windows = getIndices2(sampleSize=window_size, step=step, numSamplePoints= numSamplePoints)\n",
    "\n",
    "    norm_dist = []\n",
    "    norm_distro_dict = {}\n",
    "    voting_dist_dict = {}\n",
    "    counter = 1\n",
    "    for m in range(len(dfList_exp1)):\n",
    "        \n",
    "        \n",
    "        encoding_array_exp1 = getEncodingArray(dfList_exp1[m], windows)\n",
    "        encoding_array_exp2 = getEncodingArray(dfList_exp2[m], windows)\n",
    "        \n",
    "        # Doesn't make sense to normalize this here\n",
    "        scaler = MinMaxTransformation(encoding_array_exp1)\n",
    "        encoding_array_exp1 = scaler.transform(encoding_array_exp1)\n",
    "#         encoding_array_exp2 = scaler.transform(encoding_array_exp2)\n",
    "        \n",
    "#         # approach 2: Not intuitive as you dont have access to all of user2s stream of 4 min, only windowsizes at a time\n",
    "#         scaler = MinMaxTransformation(encoding_array_exp2)\n",
    "\n",
    "        encoding_array_exp2 = scaler.transform(encoding_array_exp2)\n",
    "        \n",
    "#         print(dfList[m].columns)\n",
    "#         print(encoding_array.shape)\n",
    "\n",
    "#         print(np.sum(encoding_array, axis = 1))\n",
    "#         print((encoding_array_exp1 / np.linalg.norm(encoding_array_exp1, axis = 1)[:, None]).shape)\n",
    "#         print(encoding_array_exp1.shape)\n",
    "#         print(encoding_array.shape)\n",
    "        # Should this be put between braces before indexing??? No\n",
    "        encoding_array_exp1 = encoding_array_exp1 / np.linalg.norm(encoding_array_exp1, axis = 1)[:, None]\n",
    "        encoding_array_exp2 = encoding_array_exp2 / np.linalg.norm(encoding_array_exp2, axis = 1)[:, None]\n",
    "#         print(np.linalg.norm(encoding_array, axis=1)[:, None].shape)\n",
    "#         print(encoding_array.shape)\n",
    "#         print(np.linalg.norm(encoding_array, axis=1))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # Is dist_array distance calculation done correctly??? It seems it does not square\n",
    "        # It appears so dist = numpy.linalg.norm(a-b)\n",
    "        # https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy\n",
    "        dist_array = (encoding_array_exp1[None, :] - encoding_array_exp2[:, None])\n",
    "#         print(dist_array.shape)\n",
    "        \n",
    "        dist_array = np.linalg.norm(dist_array, axis = 2)\n",
    "        \n",
    "        norm_dist.append(dist_array)\n",
    "        \n",
    "#         print(dist_array.shape)\n",
    "        # for err dist\n",
    "        norm_distro_dict[m] = np.array(dist_array).ravel()\n",
    "        \n",
    "        # for voting dist\n",
    "        voting_dist_dict[m] = dist_array[None, :]\n",
    "        \n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "#         print(voting_dist_dict[0].shape)\n",
    "    \n",
    "    return {\"dist_array\": np.array(norm_dist).ravel(), \"dist_dict\": norm_distro_dict, \"voting_dist_dict\": voting_dist_dict}\n",
    "\n",
    "\n",
    "def getDistFARFinal(dfList, window_size = 1000, step = 1000, numSamplePoints= 18000):\n",
    "    \n",
    "    windows = getIndices2(sampleSize=window_size, step=step, numSamplePoints= numSamplePoints)\n",
    "\n",
    "    norm_dist = []\n",
    "    norm_distro_dict = {}\n",
    "    voting_dist_dict = {}\n",
    "    counter = 1\n",
    "    \n",
    "    encoding_array_dic = {}\n",
    "    for i in range(len(dfList)):\n",
    "        encoding_array_dic[i] = getEncodingArray(dfList[i], windows)\n",
    "        # Should this be put between braces before indexing???\n",
    "#         # Should this be done here? Not here this only makes the result vector small and should be at the end\n",
    "#         encoding_array_dic[i] = encoding_array_dic[i] / np.linalg.norm(encoding_array_dic[i], axis = 1)[:, None]\n",
    "        \n",
    "    for m in range(len(dfList)):\n",
    "        \n",
    "        cum_distro_array = []\n",
    "        encoding_array_m = encoding_array_dic[m]\n",
    "        \n",
    "        # Should I scale the new vector with the transform of the user profile?\n",
    "        scaler = MinMaxTransformation(encoding_array_m)\n",
    "        encoding_array_m = scaler.transform(encoding_array_m)\n",
    "        \n",
    "        encoding_array_m = encoding_array_m / np.linalg.norm(encoding_array_m, axis = 1)[:, None]\n",
    "        \n",
    "        for k in range(len(dfList)):\n",
    "            \n",
    "            if m != k:\n",
    "                \n",
    "                encoding_array_k = encoding_array_dic[k]\n",
    "                \n",
    "#                 # approach 2\n",
    "#                 scaler = MinMaxTransformation(encoding_array_k)\n",
    "                \n",
    "                # Scale array_k with array_m transform\n",
    "                encoding_array_k = scaler.transform(encoding_array_k)\n",
    "                \n",
    "                encoding_array_k = encoding_array_k / np.linalg.norm(encoding_array_k, axis = 1)[:, None]\n",
    "                \n",
    "                # Is dist_array distance calculation done correctly??? It seems it does not square. No, resolved\n",
    "                # It appears so dist = numpy.linalg.norm(a-b)\n",
    "                # https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy\n",
    "                dist_array = (encoding_array_m[None, :] - encoding_array_k[:, None])\n",
    "                \n",
    "                # print and check dimensions\n",
    "                \n",
    "                dist_array = np.linalg.norm(dist_array, axis = 2)\n",
    "\n",
    "                norm_dist.append(dist_array)\n",
    "                \n",
    "                # for err dist\n",
    "                cum_distro_array.append(dist_array)\n",
    "                \n",
    "\n",
    "        \n",
    "        norm_distro_dict[m] = np.array(cum_distro_array).ravel()\n",
    "        \n",
    "        # for voting dist\n",
    "        voting_dist_dict[m] = np.array(cum_distro_array)\n",
    "\n",
    "        counter += 1\n",
    "        \n",
    "#         print(voting_dist_dict[0].shape)\n",
    "    \n",
    "    return {\"dist_array\": np.array(norm_dist).ravel(), \"dist_dict\": norm_distro_dict, \"voting_dist_dict\": voting_dist_dict}\n",
    "\n",
    "\n",
    "def decision_confidence(dist_array, dist_threshold):\n",
    "    '''\n",
    "    input: dist_array: (N, # unknown_user windows, # genuine_user windows)\n",
    "    output: (N, de/auth decision percentage)\n",
    "    '''\n",
    "    vals = np.where(dist_array < dist_threshold, 1, 0)\n",
    "    windows_decision_confidence = np.mean(vals, axis = 2)\n",
    "\n",
    "    return windows_decision_confidence\n",
    "\n",
    "\n",
    "def decision_module(dist_array, dist_threshold, acceptance_threshold):\n",
    "    '''\n",
    "    input: dist_array: (N, # unknown_user windows, # genuine_user windows)\n",
    "    output: (N, de/auth boolean decision)\n",
    "    '''\n",
    "    \n",
    "    windows_decision_confidence = decision_confidence(dist_array, dist_threshold = dist_threshold)\n",
    "    windows_final_decision = np.where(windows_decision_confidence >= acceptance_threshold, 1, 0)\n",
    "    \n",
    "    return windows_final_decision\n",
    "    \n",
    "def FRR_vote_based(dist_array, dist_threshold = None, acceptance_threshold = None):\n",
    "    '''\n",
    "    input: dist_array: (N, # unknown_user windows, # genuine_user windows)\n",
    "    output: vote based FRR\n",
    "    '''\n",
    "    \n",
    "    #good note but not applicable here np.where((a==0)|(a==1), a^1, a)\n",
    "\n",
    "    windows_final_decision = decision_module(dist_array, dist_threshold, acceptance_threshold)\n",
    "    vals = windows_final_decision^1\n",
    "    \n",
    "    return np.mean(vals)\n",
    "\n",
    "\n",
    "def FAR_vote_based(dist_array, dist_threshold = None, acceptance_threshold = None):\n",
    "    '''\n",
    "    input: dist_array: (M*N, # unknown_user windows, # genuine_user windows)\n",
    "    output: vote based FAR\n",
    "    '''\n",
    "    windows_final_decision = decision_module(dist_array, dist_threshold, acceptance_threshold)\n",
    "    vals = windows_final_decision\n",
    "    \n",
    "    return np.mean(vals)\n",
    "\n",
    "\n",
    "def FRR(dist, threshold):\n",
    "    \n",
    "    vals = np.where(dist < threshold, 0, 1)\n",
    "    \n",
    "    return np.mean(vals)\n",
    "\n",
    "\n",
    "def FAR(dist, threshold):\n",
    "    \n",
    "    vals = np.where(dist < threshold, 1, 0)\n",
    "    \n",
    "    return np.mean(vals)\n",
    "\n",
    "\n",
    "def DistroFRR(dist_dict, threshold):\n",
    "    \n",
    "    distro = []\n",
    "    for i in range(len(dist_dict)):\n",
    "        vals = np.where(dist_dict[i] < threshold, 0, 1)\n",
    "        distro.append(sum(vals))\n",
    "        \n",
    "    return distro\n",
    "\n",
    "\n",
    "def DistroFAR(dist_dict, threshold):\n",
    "    \n",
    "    distro = []\n",
    "    for i in range(len(dist_dict)):\n",
    "        vals = np.where(dist_dict[i] < threshold, 1, 0)\n",
    "        distro.append(sum(vals))\n",
    "        \n",
    "    return distro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "22910d65-b4d1-4e4a-84d3-a25c50e209d4",
    "_uuid": "84284a3d-28ba-4306-b7a6-5f75fc60d6ef",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:31.771334Z",
     "iopub.status.busy": "2022-03-29T07:28:31.771015Z",
     "iopub.status.idle": "2022-03-29T07:28:31.776454Z",
     "shell.execute_reply": "2022-03-29T07:28:31.775517Z",
     "shell.execute_reply.started": "2022-03-29T07:28:31.771306Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# np.where(np.array([.5, 1]) < .6, 1, 0)\n",
    "# a = np.array(range(5, 11))\n",
    "# b = np.array(range(2, 6))\n",
    "\n",
    "# res = a[None, :] - b[:, None]\n",
    "# # print(res)\n",
    "# a = res % 2\n",
    "# print(a)\n",
    "# a^1\n",
    "# b = np.array([a, a-9])\n",
    "# print(b)\n",
    "# np.concatenate(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f65d8fe7-ec15-4c54-bf10-60be43248f67",
    "_uuid": "44582f38-8dfe-4b42-bb60-0b9fb69c7d03",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:32.146322Z",
     "iopub.status.busy": "2022-03-29T07:28:32.14601Z",
     "iopub.status.idle": "2022-03-29T07:28:32.149655Z",
     "shell.execute_reply": "2022-03-29T07:28:32.149003Z",
     "shell.execute_reply.started": "2022-03-29T07:28:32.146295Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# d = {}\n",
    "# d[0] = np.array([[[1,2]]])\n",
    "# d[1] = np.array([[[2,3]]])\n",
    "# a = np.array(list(d.values()))\n",
    "# print(a.shape)\n",
    "# a = np.concatenate(a)\n",
    "# print(a.shape)\n",
    "# a = np.concatenate(a)\n",
    "# print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0fb342ee-f860-4038-8e5d-e5f15e05a0b7",
    "_uuid": "789c9f3b-abe9-4d26-b9ce-1e2ef446819c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:32.539084Z",
     "iopub.status.busy": "2022-03-29T07:28:32.538726Z",
     "iopub.status.idle": "2022-03-29T07:28:32.547034Z",
     "shell.execute_reply": "2022-03-29T07:28:32.546336Z",
     "shell.execute_reply.started": "2022-03-29T07:28:32.53905Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def getEER(distFRR, distFAR, thresholdList=None):\n",
    "    \n",
    "    if thresholdList is None:\n",
    "        thresholdList = np.arange(0, 3, 0.001)\n",
    "    \n",
    "    farList = []\n",
    "    frrList = []\n",
    "    \n",
    "    eer = []\n",
    "    for t in thresholdList:\n",
    "        far = FAR(distFAR, threshold = t)\n",
    "        frr = FRR(distFRR, threshold = t)\n",
    "        farList.append(far)\n",
    "        frrList.append(frr)\n",
    "        eer.append(abs(far-frr))\n",
    "        \n",
    "    eer = np.array(eer)\n",
    "    eer[eer==0] = 99999\n",
    "    print(farList[np.argmin(eer)])\n",
    "    print(frrList[np.argmin(eer)])\n",
    "\n",
    "#     print(\"EER: {}\".format((frrList[np.argmin(eer)] + farList[np.argmin(eer)])/2))\n",
    "    return {\"EER\": (frrList[np.argmin(eer)] + farList[np.argmin(eer)])/2, \"farList\": farList, \"frrList\": frrList, \"EER_threshold\": thresholdList[np.argmin(eer)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f960835d-39be-43fa-8133-1ad0afd703b3",
    "_uuid": "0fe92d18-0abf-49c8-bee3-45757483d7f0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:32.976193Z",
     "iopub.status.busy": "2022-03-29T07:28:32.975578Z",
     "iopub.status.idle": "2022-03-29T07:28:32.984663Z",
     "shell.execute_reply": "2022-03-29T07:28:32.983803Z",
     "shell.execute_reply.started": "2022-03-29T07:28:32.976154Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def getEERVoteBased(dist_array_FRR, dist_array_FAR, thresholdList=None):\n",
    "    \n",
    "    if thresholdList is None:\n",
    "        thresholdList = np.arange(0, 3, 0.001)\n",
    "    \n",
    "    farList = []\n",
    "    frrList = []\n",
    "    \n",
    "    eer = []\n",
    "    acceptance_threshold = .6\n",
    "    \n",
    "    for t in thresholdList:\n",
    "        far = FAR_vote_based(dist_array_FAR, dist_threshold = t, acceptance_threshold = acceptance_threshold)\n",
    "        frr = FRR_vote_based(dist_array_FRR, dist_threshold = t, acceptance_threshold = acceptance_threshold)\n",
    "        farList.append(far)\n",
    "        frrList.append(frr)\n",
    "        eer.append(abs(far-frr))\n",
    "        \n",
    "    eer = np.array(eer)\n",
    "    eer[eer==0] = 99999\n",
    "    print(farList[np.argmin(eer)])\n",
    "    print(frrList[np.argmin(eer)])\n",
    "\n",
    "#     print(\"EER: {}\".format((frrList[np.argmin(eer)] + farList[np.argmin(eer)])/2))\n",
    "    return {\"EER\": (frrList[np.argmin(eer)] + farList[np.argmin(eer)])/2, \"farList\": farList, \"frrList\": frrList, \"EER_threshold\": thresholdList[np.argmin(eer)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c3d38626-be37-4339-9588-cbb6f973631f",
    "_uuid": "61a4c410-4d16-4105-b115-77a477c1a094",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:33.341998Z",
     "iopub.status.busy": "2022-03-29T07:28:33.341667Z",
     "iopub.status.idle": "2022-03-29T07:28:33.354483Z",
     "shell.execute_reply": "2022-03-29T07:28:33.353407Z",
     "shell.execute_reply.started": "2022-03-29T07:28:33.341969Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def getEERWindowsDict(dfList_exp1, start_window_size=250, end_window_size=3000, increment_step=250, numSamplePoints=22001, isEqualSampleSize = False, fixedSampleStep=3000, thresholdList=None, dfList_exp2=None):\n",
    "    \n",
    "    window_EER_dict = {}\n",
    "    window_EER_threshold_dict = {}\n",
    "    window_farList_dict = {}\n",
    "    window_frrList_dict = {}\n",
    "    window_farDistro_dict = {}\n",
    "    window_frrDistro_dict = {}\n",
    "    \n",
    "    lst = np.arange(start_window_size, end_window_size + 1, increment_step)\n",
    "    \n",
    "    for w in lst:\n",
    "        if isEqualSampleSize:\n",
    "            sampleStep = fixedSampleStep\n",
    "        else:\n",
    "            sampleStep = w\n",
    "        \n",
    "        if dfList_exp2 is None:\n",
    "            print(\"dfList_exp2 is None\")\n",
    "            distFRRDATA = getDistFRR1(dfList_exp1, window_size = w, step = sampleStep, numSamplePoints= numSamplePoints)\n",
    "        else:\n",
    "            print(\"dfList_exp2 is Not None\")\n",
    "            distFRRDATA = getDistFRRFinal(dfList_exp1, dfList_exp2, window_size = w, step = sampleStep, numSamplePoints= numSamplePoints)\n",
    "            \n",
    "        distFARDATA = getDistFARFinal(dfList_exp1, window_size = w, step = sampleStep, numSamplePoints= numSamplePoints)\n",
    "        \n",
    "#         print('--- start of voting based')\n",
    "#         print(distFRRDATA[\"voting_dist_dict\"][0].shape)\n",
    "#         print(distFARDATA[\"voting_dist_dict\"][0].shape)\n",
    "#         print(np.concatenate(list(distFRRDATA[\"voting_dist_dict\"].values())).shape)\n",
    "#         print(np.concatenate(list(distFARDATA[\"voting_dist_dict\"].values())).shape)\n",
    "        \n",
    "        \n",
    "#         voting_dist_FRR = np.concatenate(list(distFRRDATA[\"voting_dist_dict\"].values()))\n",
    "#         voting_dist_FAR = np.concatenate(list(distFARDATA[\"voting_dist_dict\"].values()))\n",
    "#         voting_EER_data = getEERVoteBased(voting_dist_FRR, voting_dist_FAR, thresholdList=thresholdList)\n",
    "        \n",
    "#         print(\"numParticipants: {}, windowSize: {}, isEqualSampleSize: {}, EER: {}\".format(len(dfList_exp1), w, isEqualSampleSize, voting_EER_data[\"EER\"]))\n",
    "        \n",
    "#         print(\"--- end of voting based\")\n",
    "        \n",
    "        distFRR = distFRRDATA[\"dist_array\"]\n",
    "        distFAR = distFARDATA[\"dist_array\"]\n",
    "        EER_data = getEER(distFRR, distFAR, thresholdList=thresholdList)\n",
    "        \n",
    "        window_EER_dict[w] = EER_data[\"EER\"]\n",
    "        window_EER_threshold_dict[w] = EER_data[\"EER_threshold\"]\n",
    "        window_farList_dict[w] = EER_data[\"farList\"]\n",
    "        window_frrList_dict[w] = EER_data[\"frrList\"]\n",
    "        window_farDistro_dict[w] = distFARDATA[\"dist_dict\"]\n",
    "        window_frrDistro_dict[w] = distFRRDATA[\"dist_dict\"]\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"numParticipants: {}, windowSize: {}, isEqualSampleSize: {}, EER: {}\".format(len(dfList_exp1), w, isEqualSampleSize, window_EER_dict[w]))\n",
    "        \n",
    "    return { \"window_EER_dict\": window_EER_dict, \"window_EER_threshold_dict\": window_EER_threshold_dict , \"window_farList_dict\": window_farList_dict, \"window_frrList_dict\": window_frrList_dict, \"window_farDistro_dict\": window_farDistro_dict, \"window_frrDistro_dict\": window_frrDistro_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c8a14ba8-0697-40f9-86cc-89421fb6a520",
    "_uuid": "a53170a9-9345-41a5-a95d-29ef35396180",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:33.696707Z",
     "iopub.status.busy": "2022-03-29T07:28:33.696138Z",
     "iopub.status.idle": "2022-03-29T07:28:33.701551Z",
     "shell.execute_reply": "2022-03-29T07:28:33.700645Z",
     "shell.execute_reply.started": "2022-03-29T07:28:33.696663Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def getErrFixedThreshold(distFRR, distFAR, threshold):\n",
    "        \n",
    "    far = FAR(distFAR, threshold = threshold)\n",
    "    frr = FRR(distFRR, threshold = threshold)\n",
    "    \n",
    "\n",
    "    return {\"FAR\": far, \"FRR\": frr, \"threshold\": threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1d0c061-c833-4051-88cd-b03cf6665d3a",
    "_uuid": "e709a3ce-9cac-4c5b-908a-5a9f0eaaf4ac",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:34.106091Z",
     "iopub.status.busy": "2022-03-29T07:28:34.105346Z",
     "iopub.status.idle": "2022-03-29T07:28:34.115418Z",
     "shell.execute_reply": "2022-03-29T07:28:34.114408Z",
     "shell.execute_reply.started": "2022-03-29T07:28:34.106052Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# To be done\n",
    "def getErrFixedThresholdWindowsDict(dfList, start_window_size=250, end_window_size=3000, increment_step=250, numSamplePoints=22001, isEqualSampleSize = False, fixedSampleStep=3000):\n",
    "    \n",
    "    window_EER_dict = {}\n",
    "    window_EER_threshold_dict = {}\n",
    "    window_farDistro_dict = {}\n",
    "    window_frrDistro_dict = {}\n",
    "    \n",
    "    lst = np.arange(start_window_size, end_window_size + 1, increment_step)\n",
    "    \n",
    "    for w in lst:\n",
    "        if isEqualSampleSize:\n",
    "            sampleStep = fixedSampleStep\n",
    "        else:\n",
    "            sampleStep = w\n",
    "            \n",
    "        distFRRDATA = getDistFRRFinal(dfList, window_size = w, step = sampleStep, numSamplePoints= numSamplePoints)\n",
    "        distFARDATA = getDistFARFinal(dfList, window_size = w, step = sampleStep, numSamplePoints= numSamplePoints)\n",
    "        \n",
    "        distFRR = distFRRDATA[\"dist_array\"]\n",
    "        distFAR = distFARDATA[\"dist_array\"]\n",
    "        EER_data = getEER(distFRR, distFAR)\n",
    "\n",
    "        \n",
    "        window_EER_dict[w] = EER_data[\"EER\"]\n",
    "        window_EER_threshold_dict[w] = EER_data[\"EER_threshold\"]\n",
    "        window_farDistro_dict[w] = distFARDATA[\"dist_dict\"]\n",
    "        window_frrDistro_dict[w] = distFRRDATA[\"dist_dict\"]\n",
    "        \n",
    "        print(\"numParticipants: {}, windowSize: {}, isEqualSampleSize: {}, EER: {}\".format(len(dfList), w, isEqualSampleSize, window_EER_dict[w]))\n",
    "        \n",
    "    return { \"window_EER_dict\": window_EER_dict, \"window_EER_threshold_dict\": window_EER_threshold_dict , \"window_farList_dict\": window_farList_dict, \"window_frrList_dict\": window_frrList_dict, \"window_farDistro_dict\": window_farDistro_dict, \"window_frrDistro_dict\": window_frrDistro_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "ae9b2983-8cec-4f02-922a-d7b8c0e36822",
    "_uuid": "6afdfc26-d91b-4d3a-8604-528d56426826",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cite the thesis paper i found\n",
    "def utils_ppp(P):\n",
    "    \"\"\"Pretty print parameters of an experiment.\"\"\"\n",
    "    df = pd.DataFrame([asdict(P)])\n",
    "    df = df.T\n",
    "    df.columns = [\"Value\"]\n",
    "    \n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "ef3ded46-6c27-44c6-8792-7c30cf5c0af3",
    "_uuid": "449eb557-cbf1-416c-b0ec-ddcc4f104119",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# source: https://github.com/dynobo/ContinAuth/blob/master/notebooks/utils.ipynb\n",
    "def utils_eer(y_true, y_pred, return_threshold=False):\n",
    "    \"\"\"Calculate the Equal Error Rate.\n",
    "\n",
    "    Based on https://stackoverflow.com/a/49555212, https://yangcha.github.io/EER-ROC/\n",
    "    and https://scikit-learn.org/stable/modules/model_evaluation.html#implementing-your-own-scoring-object\n",
    "\n",
    "    Arguments:\n",
    "        y_true {np.array}  -- Actual labels\n",
    "        y_pred {np.array}  -- Predicted labels or probability\n",
    "        \n",
    "    Returns:\n",
    "        float              -- Equal Error Rate        \n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred, pos_label=1)\n",
    "    eer = brentq(lambda x: 1.0 - x - interp1d(fpr, tpr)(x), 0.0, 1.0)\n",
    "    thresh = interp1d(fpr, thresholds)(eer)  # Calculated threshold, not needed for score\n",
    "    if return_threshold:\n",
    "        return eer, thresh\n",
    "    else:\n",
    "        return eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "8e188ef2-60ad-44e7-8c98-77d859e152ec",
    "_uuid": "c755e89d-f293-4da4-a418-85186fcfbe17",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER: 0.333, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.400, Threshold: 0.200 <-- Worse case\n"
     ]
    }
   ],
   "source": [
    "TEST_MODE = 1\n",
    "if TEST_MODE:\n",
    "    temp_eer, tres = utils_eer(\n",
    "        [-1, -1, -1, 1, 1, 1], [0, 0.9, 0.1, 0.74, 0.8, .6], return_threshold=True\n",
    "    )\n",
    "    print(f\"EER: {temp_eer:.3f}, Threshold: {tres:.3f} <-- Arbitrary case\")\n",
    "\n",
    "    temp_eer, tres = utils_eer(\n",
    "        [-1, -1, -1, 1, 1], [0.1, 0.2, 0.3, 1, 0.9], return_threshold=True\n",
    "    )\n",
    "    print(f\"EER: {temp_eer:.3f}, Threshold: {tres:.3f} <-- Best case\")\n",
    "\n",
    "    temp_eer, tres = utils_eer(\n",
    "        [1, 1, 1, -1, -1], [0.1, 0.2, 0.3, 1, 0.9], return_threshold=True\n",
    "    )\n",
    "    print(f\"EER: {temp_eer:.3f}, Threshold: {tres:.3f} <-- Worse case\")\n",
    "    \n",
    "#     new case does it make sense? I don't think so\n",
    "    temp_eer, tres = utils_eer(\n",
    "        [1, 1, 1, -1, -1], [-1, 1, -1, -1, -1], return_threshold=True\n",
    "    )\n",
    "    print(f\"EER: {temp_eer:.3f}, Threshold: {tres:.3f} <-- Worse case\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array([0.74, 0.8, .6]) > .6).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array([0, 0.9, 0.1])<.6).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "304dbab1-3cf2-4166-a521-178f5a3f55fe",
    "_uuid": "1f3464be-7fe5-4d37-bf84-e2f63e99b345",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:36.875497Z",
     "iopub.status.busy": "2022-03-29T07:28:36.874802Z",
     "iopub.status.idle": "2022-03-29T07:28:36.879647Z",
     "shell.execute_reply": "2022-03-29T07:28:36.878748Z",
     "shell.execute_reply.started": "2022-03-29T07:28:36.875453Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "utils_eer_scorer = make_scorer(utils_eer, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0092c407-5812-43bd-a3a9-f78bfadff3b7",
    "_uuid": "35621051-b220-428d-b505-85ef106f8ce6"
   },
   "source": [
    "# Split Dataset for Valid/Test  \n",
    "In two splits: one used during hyperparameter optimization, and one used during testing.\n",
    "\n",
    "The split is done along the subjects: All sessions of a single subject will either be in the validation split or in the testing split, never in both.\n",
    "\n",
    "They did a 30 60 split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "92d8caee-6b8e-45c7-b272-010046daec00",
    "_uuid": "0c7969f6-43b9-4d95-9253-76d5d040d30a"
   },
   "source": [
    "# Reshaping Raw Features.\n",
    "We have our own function of windows for this. Do this for both training and testing.\n",
    "\n",
    "# Extracting time and frequency based features.\n",
    "Again, we have a function for this. Do this for both training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "11bbc563-4576-4588-a73c-44936653c13b",
    "_uuid": "b9984fbd-a13f-4d5d-ad57-8b0c41aafe1d"
   },
   "source": [
    "# Hyperparameter Optimization \n",
    "\n",
    "I do not find any reaqsonable explaination how to use a cross-validation as we are talking about anomaly detection.\n",
    "\n",
    "I am using the experiment 1 data as train, and experiment 2 data as validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d09c240f-d000-4840-880c-260436115209",
    "_uuid": "aca85e4e-ed2f-4c54-bf0f-2f938ac45a07"
   },
   "source": [
    "# Using SVM in a real-world Scenario with multiple genuine users and intruders\n",
    "Source: https://datascience.stackexchange.com/questions/23623/what-is-the-best-way-to-classify-data-not-belonging-to-set-of-classes\n",
    "\n",
    "Stage 1: \n",
    "    Use one-class SVM to assign those images that do not belong to the set of predefined classes as the 9-th class.\n",
    "\n",
    "Stage 2:\n",
    "    For those images that passes through your filter, let the multi-class SVM assign them to one of the 8 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79b10ce2-42b2-4156-a1df-633bf52c05e6",
    "_uuid": "bf0780fa-46ed-4d13-90ee-3640fe7cc4aa"
   },
   "source": [
    "Loading data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "72d60ae5-be59-4add-868d-4ba29d842b9f",
    "_uuid": "126e56a7-4e62-4f9d-adf7-89308283fccd",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:39.619456Z",
     "iopub.status.busy": "2022-03-29T07:28:39.618793Z",
     "iopub.status.idle": "2022-03-29T07:28:39.626966Z",
     "shell.execute_reply": "2022-03-29T07:28:39.626146Z",
     "shell.execute_reply.started": "2022-03-29T07:28:39.619418Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def load_data_frames(user_ids, begin_idx, end_idx, min_len):\n",
    "    '''\n",
    "    input: \n",
    "        user_ids: list of approved user_ids after exploratory data analysis\n",
    "        begin_idx: the index before which data is discarded for user i\n",
    "        end_idx: the index after which data is discarded for user i\n",
    "        min_len: the minimum length that a dataframe has to be after cutting of both endings\n",
    "        \n",
    "    output:\n",
    "        {dfList_exp1, dfList_exp2}: return dfList for exp1 and exp2 of the selected user_ids\n",
    "    '''\n",
    "    print(\"Loading exp1 data:\")\n",
    "    dfList_exp1 = []\n",
    "    for i in user_ids:\n",
    "        dic = getDataStats1(i, begin_idx=begin_idx, end_idx=end_idx)\n",
    "\n",
    "        if(dic['accel']<min_len):\n",
    "            raise Exception(\"The Stream is shorter than {}\".format(min_len))\n",
    "\n",
    "        dfList_exp1 = dfList_exp1 + [dic['df'].reset_index(drop=True)]\n",
    "\n",
    "\n",
    "    print(\"Loading exp2 data:\")\n",
    "    dfList_exp2 = []\n",
    "    for i in user_ids:\n",
    "        dic = getDataStats2(i, begin_idx=begin_idx, end_idx=end_idx)\n",
    "\n",
    "        if(dic['accel']<min_len):\n",
    "            raise Exception(\"The Stream is shorter than {}\".format(min_len))\n",
    "\n",
    "        dfList_exp2 = dfList_exp2 + [dic['df'].reset_index(drop=True)]\n",
    "    #     dfList = dfList + [dic['df']]\n",
    "    \n",
    "    return {\"dfList_exp1\": dfList_exp1, \"dfList_exp2\": dfList_exp2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "adb215c0-9999-4e8f-a64f-1ce0bef9da1a",
    "_uuid": "696cb33f-f151-4447-8e46-7018e15cc5be",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:40.156606Z",
     "iopub.status.busy": "2022-03-29T07:28:40.15626Z",
     "iopub.status.idle": "2022-03-29T07:28:40.171091Z",
     "shell.execute_reply": "2022-03-29T07:28:40.170208Z",
     "shell.execute_reply.started": "2022-03-29T07:28:40.156575Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def MakeXExpDic(dfList_exp1, dfList_exp2, window_size = 1000, step = 1000, numSamplePoints= 18000):\n",
    "    '''\n",
    "    return \n",
    "    X_exp1_dic\n",
    "    X_exp2_dic\n",
    "    dfLists are of the same size.\n",
    "    '''\n",
    "    if len(dfList_exp1) != len(dfList_exp2): \n",
    "        raise Exception(\"dfLists are not of the same size.\")\n",
    "    \n",
    "    windows = getIndices2(sampleSize=window_size, step=step, numSamplePoints= numSamplePoints)\n",
    "\n",
    "    X_exp1_dic = {}\n",
    "    X_exp2_dic = {}\n",
    "    for i in range(len(dfList_exp1)):\n",
    "        \n",
    "        \n",
    "        encoding_array_exp1 = getEncodingArray(dfList_exp1[i], windows)\n",
    "        encoding_array_exp2 = getEncodingArray(dfList_exp2[i], windows)\n",
    "        \n",
    "        X_exp1_dic[i] = encoding_array_exp1\n",
    "        X_exp2_dic[i] = encoding_array_exp2\n",
    "        \n",
    "    return {\"X_exp1_dic\": X_exp1_dic, \"X_exp2_dic\": X_exp2_dic}\n",
    "\n",
    "\n",
    "def OneClassSVMSets(k, X_exp1_dic, X_exp2_dic, cv=5):\n",
    "    '''\n",
    "    return the required sets for an OCSVM trained on the user with key. \n",
    "    X_train: X data from X_exp1_dic[k]\n",
    "    X_test_regular: X data from X_exp2_dic[k]\n",
    "    X_test_anomalous: X data from X_exp2_dic[!k]\n",
    "    '''\n",
    "    \n",
    "    if k not in  X_exp1_dic:\n",
    "        raise Exception(\"invalid key for dic\")\n",
    "        \n",
    "    \n",
    "    X_pos = X_exp1_dic[k]\n",
    "#     X_neg = np.concatenate([X_exp1_dic[key] for key in X_exp1_dic.keys() if key != k], axis=0)\n",
    "    X_test_regular = X_exp2_dic[k]\n",
    "    X_test_anomalous = np.concatenate([X_exp2_dic[key] for key in X_exp2_dic.keys() if key != k], axis=0)\n",
    "    \n",
    "    \n",
    "#     n, m = len(Xpos), len(Xneg)\n",
    "    np.random.shuffle(X_neg)\n",
    "    print((X_neg.shape[0], X_pos.shape[0]))\n",
    "    X_neg = X_neg[np.random.choice(X_neg.shape[0], size=X_pos.shape[0], replace=False), :]\n",
    "    print(X_pos.shape, X_neg.shape)\n",
    "    # Creating (train, test) tuples of indices for k-folds cross-validation\n",
    "    # We split the positive class (normal data) as we only want the positive examples in the training set.\n",
    "    \n",
    "    train_splits = KFold(n_splits=cv, shuffle=True).split(X_pos)\n",
    "    anomalous_splits = KFold(n_splits=cv, shuffle=True).split(X_neg)\n",
    "\n",
    "#     print(len(train_splits), len(anomalous_splits))\n",
    "    # Negative examples (abnormal data) are added to the test set (see https://stackoverflow.com/a/58459322/3673842)\n",
    "    y_train = np.concatenate([np.repeat(1.0, len(X_pos)), np.repeat(-1.0, len(X_neg))])\n",
    "    X_train = np.concatenate([X_pos, X_neg], axis=0)\n",
    "    \n",
    "    # https://github.com/steppi/adeft/blob/anomaly_detection/adeft/modeling/find_anomalies.py#L170\n",
    "    cv_splits = ((train, np.concatenate((test, anom_test + X_pos.shape[0]), axis = 0))\n",
    "                  for (train, test), (_, anom_test)\n",
    "                  in zip(train_splits, anomalous_splits))\n",
    "    \n",
    "    return {\"X_train\": X_train, \"y_train\": y_train, \"X_test_regular\": X_test_regular, \"X_test_anomalous\": X_test_anomalous, \"cv_splits\": cv_splits}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From CNN file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRawDataChunks(df, windows, scale=True, scaler=\"MinMaxScaler\", user_idx=None, exp_num=None):\n",
    "    a = []\n",
    "    df = df.drop(columns=[\"time_stamp\"]).copy()\n",
    "    df_array = df.to_numpy()\n",
    "    \n",
    "    if scale:\n",
    "        print(f\"user_idx: {user_idx}, exp_num: {exp_num}, scale: {scale}, scaler: {scaler}\")\n",
    "        print(df_array.shape)\n",
    "        scaler = get_new_scaler_dict[scaler]\n",
    "        scaler = scaler().fit(df_array)\n",
    "        df_array = scaler.transform(df_array)\n",
    "        scaled_df = pd.DataFrame(data=df_array, columns = df.columns, dtype=df_array.dtype)\n",
    "        df = scaled_df\n",
    "    \n",
    "    for i in range(len(windows)):\n",
    "        # replaced loc with iloc per documentation\n",
    "#         a.append(df_array[windows[i], :]) #CNN\n",
    "        a.append(df.iloc[windows[i], :]) #waca\n",
    "    \n",
    "#     print(len(a))\n",
    "#     print(len(a))\n",
    "#     return np.array([a])\n",
    "#     return np.array(a), scaler #CNN\n",
    "    return a, scaler #waca\n",
    "\n",
    "def MakeRawXExpDic(dfList_exp1, dfList_exp2, window_size = 1000, step = 1000, numSamplePoints= 18000, scale_exp1=False, scale_exp2=True, scaler=\"MinMaxScaler\"):\n",
    "    '''\n",
    "    return \n",
    "    X_exp1_dic\n",
    "    X_exp2_dic\n",
    "    dfLists are of the same size.\n",
    "    '''\n",
    "\n",
    "    if len(dfList_exp1) != len(dfList_exp2): \n",
    "        raise Exception(\"dfLists are not of the same size.\")\n",
    "    \n",
    "    windows = getIndices2(sampleSize=window_size, step=step, numSamplePoints= numSamplePoints)\n",
    "\n",
    "    X_exp1_dic = {}\n",
    "    X_exp2_dic = {}\n",
    "    fitted_scaler_exp1_dic={}\n",
    "    fitted_scaler_exp2_dic={}\n",
    "    for i in range(len(dfList_exp1)):\n",
    "        \n",
    "        \n",
    "        encoding_array_exp1, fitted_scaler_exp1 = getRawDataChunks(dfList_exp1[i], windows, scale=scale_exp1, scaler=scaler, user_idx=i, exp_num=1)\n",
    "        encoding_array_exp2, fitted_scaler_exp2 = getRawDataChunks(dfList_exp2[i], windows, scale=scale_exp2, scaler=scaler, user_idx=i, exp_num=2)\n",
    "        \n",
    "        X_exp1_dic[i] = encoding_array_exp1\n",
    "        X_exp2_dic[i] = encoding_array_exp2\n",
    "        \n",
    "        fitted_scaler_exp1_dic[i]=fitted_scaler_exp1\n",
    "        fitted_scaler_exp2_dic[i]=fitted_scaler_exp2\n",
    "        \n",
    "    return {\"Raw_X_exp1_dic\": X_exp1_dic, \"Raw_X_exp2_dic\": X_exp2_dic, \"fitted_scaler_exp1_dic\": fitted_scaler_exp1_dic, \"fitted_scaler_exp2_dic\": fitted_scaler_exp2_dic}\n",
    "\n",
    "\n",
    "\n",
    "def MakeDeepXExpDic(dfList_exp, deep_feature_model, fitted_scaler_dic=None):\n",
    "    '''\n",
    "    ???\n",
    "    return \n",
    "    X_exp_dic\n",
    "    dfLists are of the same size.\n",
    "    '''\n",
    "\n",
    "    X_exp_dic = {}\n",
    "    for k in dfList_exp.keys():\n",
    "        if fitted_scaler_dic:\n",
    "            print(f\"scaling exp1 samples of user: {k}\")\n",
    "            X_exp_dic[k] = deep_feature_model.predict(transform_user_windows(dfList_exp[k], fitted_scaler_dic[k]))\n",
    "        else:\n",
    "            print(f\"not scaling exp2 samples of user: {k}\")\n",
    "            X_exp_dic[k] = deep_feature_model.predict(dfList_exp[k])\n",
    "        \n",
    "        \n",
    "    return X_exp_dic\n",
    "\n",
    "\n",
    "\n",
    "def MakeWACAXExpDicOwner(X_exp_reg_df_dict, scaler_clip=True, scaler_type=\"MinMaxScaler\"):\n",
    "    '''k\n",
    "    ???\n",
    "    return \n",
    "    X_exp_dic\n",
    "    dfLists are of the same size.\n",
    "    '''\n",
    "\n",
    "    X_exp_dic = {}\n",
    "    for owner in X_exp_reg_df_dict.keys():\n",
    "\n",
    "        print(f\"raw exp2 samples of owner: {owner} are scaled already so we extracte WACA features\")\n",
    "        X_exp_dic[owner] = {\"profile_windows\": ExtractWACAFeatures(X_exp_reg_df_dict[owner]), \"unknown_users_dict\": {}, \"fitted_feature_scaler\": None}\n",
    "\n",
    "        # do minmax scaling here\n",
    "        print(f\"fit_transform waca features using scaler of type: {scaler_type}, for profile_windows of owner: {owner}\")\n",
    "        print(f\"owner: {owner}, exp_num: {2}, scaler: {scaler_type}, scaler_clip: {scaler_clip}\")\n",
    "        scaler = get_new_scaler_dict[scaler_type]\n",
    "        scaler = scaler(clip=scaler_clip).fit(X_exp_dic[owner][\"profile_windows\"])\n",
    "        X_exp_dic[owner][\"fitted_feature_scaler\"] = scaler\n",
    "\n",
    "        X_exp_dic[owner][\"profile_windows\"] = transform_user_WACA_windows(X_exp_dic[owner][\"profile_windows\"], X_exp_dic[owner][\"fitted_feature_scaler\"])\n",
    "        \n",
    "        \n",
    "    return X_exp_dic\n",
    "    \n",
    "\n",
    "    \n",
    "def MakeWACAXExpDicUnknown(X_exp_unknown_df_dict, X_exp_dic, fitted_raw_scaler_dict):\n",
    "    '''k\n",
    "    ???\n",
    "    return \n",
    "    X_exp_dic\n",
    "    dfLists are of the same size.\n",
    "    '''\n",
    "\n",
    "    for owner in X_exp_unknown_df_dict.keys():\n",
    "        if fitted_raw_scaler_dict:\n",
    "            print(f\"scaling exp1 unknown windows for when user: {owner}, is owner\")\n",
    "            for unknown_user in X_exp_unknown_df_dict.keys():\n",
    "                X_exp_dic[owner][\"unknown_users_dict\"][unknown_user] = ExtractWACAFeatures(transform_user_windows(X_exp_unknown_df_dict[unknown_user], fitted_raw_scaler_dict[owner]))\n",
    "                fitted_feature_scaler = X_exp_dic[owner][\"fitted_feature_scaler\"]\n",
    "                X_exp_dic[owner][\"unknown_users_dict\"][unknown_user] = transform_user_WACA_windows(X_exp_dic[owner][\"unknown_users_dict\"][unknown_user], fitted_feature_scaler)\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    return X_exp_dic\n",
    "\n",
    "\n",
    "def ExtractWACAFeatures(X_exp):\n",
    "    a = []\n",
    "    for window in X_exp:\n",
    "        a.append(signal_to_encoding(window)[1])\n",
    "        \n",
    "    return np.array(a)\n",
    "\n",
    "\n",
    "# def MakeScaledXExpDic(df_exp_dict, fitted_scaler_dic):\n",
    "#     '''\n",
    "#     ???\n",
    "#     return \n",
    "#     X_exp_dic\n",
    "#     dfLists are of the same size.\n",
    "#     '''\n",
    "\n",
    "#     X_exp_dic = {}\n",
    "#     for k in df_exp_dict.keys():\n",
    "#         print(f\"scaling exp1 samples of user: {k}\")\n",
    "#         X_exp_dic[k] = transform_user_windows(df_exp_dict[k], fitted_scaler_dic[k])\n",
    "        \n",
    "        \n",
    "#     return X_exp_dic\n",
    "\n",
    "\n",
    "def scale_feature_windows(df_exp_dict, fitted_scaler_dic=None, scaler_type=None, scaler_clip=False):\n",
    "    '''\n",
    "    ???\n",
    "    return \n",
    "    X_exp_dic\n",
    "    dfLists are of the same size.\n",
    "    '''\n",
    "    if fitted_scaler_dic == None:\n",
    "        fitted_scaler_dic={}\n",
    "        \n",
    "    X_exp_dic = {}\n",
    "    for k in df_exp_dict.keys():\n",
    "        if k in fitted_scaler_dic:\n",
    "            print(f\"transform exp1 samples of user: {k}\")\n",
    "        else:\n",
    "            print(f\"fit_transform exp2 samples of user: {k}\")\n",
    "            print(f\"user_idx: {k}, exp_num: {2}, scaler: {scaler_type}, scaler_clip: {scaler_clip}\")\n",
    "            scaler = get_new_scaler_dict[scaler_type]\n",
    "            scaler = scaler(clip=scaler_clip).fit(df_exp_dict[k])\n",
    "            fitted_scaler_dic[k] = scaler\n",
    "\n",
    "#         print(df_exp_dict[k].shape)\n",
    "        X_exp_dic[k] = transform_user_WACA_windows(df_exp_dict[k], fitted_scaler_dic[k])\n",
    "        \n",
    "        \n",
    "    return {\"X_exp_dic\": X_exp_dic, \"fitted_scaler_dic\": fitted_scaler_dic}\n",
    "\n",
    "\n",
    "def transform_user_WACA_windows(X_exp, fitted_scaler):\n",
    "    \n",
    "    \n",
    "    transformed_X_exp = []\n",
    "    \n",
    "#     print(X_exp[0].shape)\n",
    "    for window in X_exp:\n",
    "        if len(window.shape) == 1:\n",
    "            window = window.reshape(1, -1)\n",
    "        scaled_array = fitted_scaler.transform(window)\n",
    "        transformed_X_exp.append(scaled_array.reshape(-1))\n",
    "        \n",
    "    return np.array(transformed_X_exp)\n",
    "\n",
    "\n",
    "def transform_user_windows(X_exp, fitted_scaler):\n",
    "    \n",
    "    \n",
    "    transformed_X_exp = []\n",
    "    \n",
    "#     print(X_exp[0].shape)\n",
    "    for window in X_exp:\n",
    "        scaled_array = fitted_scaler.transform(window)\n",
    "        scaled_window_df = pd.DataFrame(data=scaled_array, columns = window.columns, dtype=scaled_array.dtype)\n",
    "        transformed_X_exp.append(scaled_window_df)\n",
    "        \n",
    "    return transformed_X_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6db5413f-6f83-4172-a988-bbd6b2c80ed8",
    "_uuid": "b3e92b0e-b8f0-4521-bb00-99e6b15496d7",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:42.417444Z",
     "iopub.status.busy": "2022-03-29T07:28:42.416737Z",
     "iopub.status.idle": "2022-03-29T07:28:42.42771Z",
     "shell.execute_reply": "2022-03-29T07:28:42.427032Z",
     "shell.execute_reply.started": "2022-03-29T07:28:42.417386Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.choice(range(5), 5, replace = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils_plot_distance_hist() For CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T00:56:38.183267Z",
     "iopub.status.busy": "2022-03-31T00:56:38.182871Z",
     "iopub.status.idle": "2022-03-31T00:56:38.201973Z",
     "shell.execute_reply": "2022-03-31T00:56:38.200775Z",
     "shell.execute_reply.started": "2022-03-31T00:56:38.183231Z"
    }
   },
   "outputs": [],
   "source": [
    "def utils_plot_distance_hist(dist_positive, dist_negative, thres, desc, fig_size=(12, 4), margin=None):\n",
    "    \"\"\"Plot histogramm of Euclidean Distances for Positive & Negative Pairs.\"\"\"\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Plot Distributions\n",
    "    plt.figure(figsize=fig_size, dpi=180)\n",
    "    bins = np.linspace(\n",
    "        min(dist_positive.min(), dist_negative.min()),\n",
    "        max(dist_positive.max(), dist_negative.max()),\n",
    "        num=21,\n",
    "    )\n",
    "    g1 = sns.distplot(\n",
    "        dist_positive,\n",
    "        label=\"positive pairs\",\n",
    "        bins=bins,\n",
    "        axlabel=False,\n",
    "        hist_kws=dict(edgecolor=\"k\", lw=0.5),\n",
    "        kde_kws=dict(linewidth=0.8),\n",
    "        color=\"tab:blue\",\n",
    "    )\n",
    "    g2 = sns.distplot(\n",
    "        dist_negative,\n",
    "        label=\"negative pairs\",\n",
    "        bins=bins,\n",
    "        hist_kws=dict(edgecolor=\"k\", lw=0.5),\n",
    "        kde_kws=dict(linewidth=0.8),\n",
    "        color=\"tab:gray\",\n",
    "    )\n",
    "\n",
    "    # Plot vertical lines\n",
    "    if thres > 0:\n",
    "        max_y = max(g1.get_ylim()[1], g2.get_ylim()[1])\n",
    "        plt.axvline(x=thres, color=MAGENTA, linestyle=\"--\", lw=0.8, alpha=0.7)\n",
    "        plt.text(\n",
    "            x=thres + 0.001,\n",
    "            y=max_y * 0.65,\n",
    "            s=f\"EER Threshold\\n({thres:.2f})\",\n",
    "            color=MAGENTA,\n",
    "            weight=\"bold\",\n",
    "            fontsize=5,\n",
    "            alpha=1\n",
    "        )\n",
    "        if margin:\n",
    "            plt.axvline(x=margin, color=MAGENTA, linestyle=\"--\", lw=0.8, alpha=0.7)\n",
    "            plt.text(\n",
    "                x=margin + 0.001,\n",
    "                y=max_y * 0.15,\n",
    "                s=f\"Margin\\n({margin})\",\n",
    "                color=MAGENTA,\n",
    "                weight=\"bold\",\n",
    "                fontsize=5,\n",
    "                alpha=1\n",
    "            )\n",
    "\n",
    "    # Legend\n",
    "    plt.legend(\n",
    "        loc=\"upper right\",\n",
    "        title=f\"{desc} Distances\",\n",
    "        title_fontsize=5,\n",
    "        fontsize=6,\n",
    "    )\n",
    "\n",
    "    warnings.filterwarnings(\"default\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-03-31T00:56:39.056016Z",
     "iopub.status.busy": "2022-03-31T00:56:39.055672Z",
     "iopub.status.idle": "2022-03-31T00:56:39.557205Z",
     "shell.execute_reply": "2022-03-31T00:56:39.556323Z",
     "shell.execute_reply.started": "2022-03-31T00:56:39.055986Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    dist_pos = np.array([0.0, 0.1, 0.1, 0.2, 0.2, 0.3, 0.3, 0.3, 0.3, 0.3, 0.5, 0.8])\n",
    "    dist_neg = np.array([0.4, 0.5, 0.6, 0.6, 0.6, 0.7, 0.7, 0.7, 0.7, 0.8, 1, 1])\n",
    "    utils_plot_distance_hist(\n",
    "        dist_pos, dist_neg, thres=0.4, desc=\"Pair\", fig_size=(12, 4), margin=0.8\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "478e1e7e-68bd-4f32-94a8-e2aa98381323",
    "_uuid": "59ce837f-6626-42f9-bd8c-ac2fdf93c5c2"
   },
   "source": [
    "# utils_create_cv_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1cfaf4af-a653-485b-be5b-c28d2797798f",
    "_uuid": "38ee9619-b1c4-40d6-9980-1809d1775875",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:43.591762Z",
     "iopub.status.busy": "2022-03-29T07:28:43.591444Z",
     "iopub.status.idle": "2022-03-29T07:28:43.608086Z",
     "shell.execute_reply": "2022-03-29T07:28:43.605627Z",
     "shell.execute_reply.started": "2022-03-29T07:28:43.591735Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def utils_create_cv_splits(owner_key, train_dic, valid_test_dic, seed=0):\n",
    "    '''\n",
    "    return the required sets for an OCSVM trained on the user with key. \n",
    "    X_train: X data from train_dic[k], comes from exp2\n",
    "    X_test_regular: X data from valid_test_dic[k], comes from exp1\n",
    "    X_test_anomalous: X data from valid_test_dic[!k], comes from exp1\n",
    "    \n",
    "    Create cross-validation mask with train-valid pairs.\n",
    "    \n",
    "    See e.g. https://stackoverflow.com/a/37591377\n",
    "    \n",
    "    Arguments:\n",
    "        cv_mask {np.ndarray} --\n",
    "        \n",
    "    Return:\n",
    "        {list} -- List of tuple: (<train indices>, <valid indices>)\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    if owner_key not in  train_dic:\n",
    "        raise Exception(\"invalid key for dic\")\n",
    "    \n",
    "        \n",
    "    X_pos = train_dic[owner_key].copy()\n",
    "    X_test_regular = valid_test_dic[owner_key].copy()\n",
    "    X_test_anomalous = np.concatenate([valid_test_dic[key] for key in valid_test_dic.keys() if key != owner_key], axis=0).copy()\n",
    "    \n",
    "    train_idx_owner = np.arange(X_pos.shape[0])\n",
    "    valid_idx_owner = np.arange(X_test_regular.shape[0]) + train_idx_owner.shape[0]\n",
    "    \n",
    "    print(f\"owner: {owner_key} train_idx range: {train_idx_owner[0]}, {train_idx_owner[-1]}\")\n",
    "    print(f\"owner: {owner_key} valid_idx range: {valid_idx_owner[0]}, {valid_idx_owner[-1]}\")\n",
    "    np.random.seed(seed + owner_key)\n",
    "    np.random.shuffle(train_idx_owner)\n",
    "    np.random.shuffle(valid_idx_owner)\n",
    "\n",
    "    \n",
    "    cv_splits = []\n",
    "    base_idx = train_idx_owner.shape[0] + valid_idx_owner.shape[0]\n",
    "    for key in valid_test_dic.keys():\n",
    "        \n",
    "        if key != owner_key:\n",
    "            # Impostor validation indices\n",
    "            valid_idx_impostor = np.arange(valid_test_dic[key].shape[0]) + base_idx\n",
    "            print(f\"imposter: {key} valid_idx range: {valid_idx_impostor[0]}, {valid_idx_impostor[-1]}\")\n",
    "\n",
    "            # Balance classes\n",
    "            min_samples = min(valid_idx_owner.shape[0], valid_idx_impostor.shape[0])\n",
    "            np.random.seed(seed + key)\n",
    "            valid_idx_owner_samp = np.random.choice(\n",
    "                valid_idx_owner, size=min_samples, replace=False\n",
    "            )\n",
    "            np.random.seed(seed + key)\n",
    "            valid_idx_impostor_samp = np.random.choice(\n",
    "                valid_idx_impostor, size=min_samples, replace=False\n",
    "            )\n",
    "\n",
    "            # Concat owner & impostor validation indices\n",
    "            valid_idx_both = np.hstack([valid_idx_owner_samp, valid_idx_impostor_samp])\n",
    "\n",
    "            # Add train/valid pair to cv\n",
    "            cv_splits.append((list(train_idx_owner), list(valid_idx_both)))\n",
    "            \n",
    "            base_idx += valid_idx_impostor.shape[0]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    y_train = np.concatenate([np.repeat(1.0, X_pos.shape[0]), np.repeat(1.0, X_test_regular.shape[0]), np.repeat(-1.0, X_test_anomalous.shape[0])])\n",
    "    X_train = np.concatenate([X_pos, X_test_regular, X_test_anomalous], axis=0)\n",
    "    \n",
    "    \n",
    "    return {\"X_train\": X_train, \"y_train\": y_train, \"X_test_regular\": X_test_regular, \"X_test_anomalous\": X_test_anomalous, \"cv_splits\": cv_splits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6abc81ef-9648-4c12-95c7-d8af54bd1923",
    "_uuid": "03caeebf-831e-40bd-8724-6eeeb713e63b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:44.42086Z",
     "iopub.status.busy": "2022-03-29T07:28:44.420184Z",
     "iopub.status.idle": "2022-03-29T07:28:44.425311Z",
     "shell.execute_reply": "2022-03-29T07:28:44.423991Z",
     "shell.execute_reply.started": "2022-03-29T07:28:44.42081Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# if TEST_MODE:\n",
    "#     # Mask Explained:\n",
    "#     # -2 => Training data (owner)\n",
    "#     # -1 => Validation data (owner)\n",
    "#     # 0+ => Validation impostors\n",
    "#     #              Indices:    0   1   2   3   4   5  6  7  8  9  10 11 12 13 14 15\n",
    "#     dummy_cv_mask = np.array([-2, -2, -1, -1, -1, -1, 0, 0, 0, 1, 1, 1, 2, 2, 2, -2])\n",
    "\n",
    "#     # Generate tuples of training data and validation data, one tuple for each impostor (0, 1, 2).\n",
    "#     # Training data (1st list in tuple) contains only indices of owner training data (-2)\n",
    "#     # Validation data (2nd list in tuple) contains  indices of validation data from owner (-1) and\n",
    "#     # from a single impostor (0+), each 50 %\n",
    "#     splits = utils_create_cv_splits(dummy_cv_mask, seed=123)\n",
    "#     [print(s) for s in splits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9f0938b3-dc22-4b9b-9418-3be1a44595e3",
    "_uuid": "04c3c832-4264-44b4-b076-195e0d551630"
   },
   "source": [
    "# utils_cv_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T11:25:38.019826Z",
     "iopub.status.busy": "2022-03-29T11:25:38.019469Z",
     "iopub.status.idle": "2022-03-29T11:25:38.024614Z",
     "shell.execute_reply": "2022-03-29T11:25:38.023103Z",
     "shell.execute_reply.started": "2022-03-29T11:25:38.019796Z"
    }
   },
   "outputs": [],
   "source": [
    "TEST_MODE=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T11:26:11.290958Z",
     "iopub.status.busy": "2022-03-29T11:26:11.290567Z",
     "iopub.status.idle": "2022-03-29T11:26:11.304482Z",
     "shell.execute_reply": "2022-03-29T11:26:11.303355Z",
     "shell.execute_reply.started": "2022-03-29T11:26:11.290918Z"
    }
   },
   "outputs": [],
   "source": [
    "def utils_cv_report(random_search, owner, impostors):\n",
    "    \"\"\"Transform the random_search.cv_results_ into nice formatted dataframe.\"\"\"\n",
    "    # Create report\n",
    "    df_report = pd.DataFrame(random_search.cv_results_)\n",
    "\n",
    "    # Add owner information\n",
    "    df_report[\"owner\"] = owner\n",
    "\n",
    "    # Drop uninteressting columns\n",
    "    drop_columns = [col for col in df_report.columns if \"_train_\" in col]\n",
    "    drop_columns = drop_columns + [col for col in df_report.columns if col.startswith(\"split\") and (col.endswith(\"recall\") or col.endswith(\"precision\") or col.endswith(\"f1\") or col.endswith(\"roc_auc\"))]\n",
    "    drop_columns = drop_columns + [\"params\"]\n",
    "    df_report = df_report.drop(columns=drop_columns)\n",
    "\n",
    "    # Flip sign of eer (revert flip by sklearn scorer)\n",
    "    eer_columns = [col for col in df_report.columns if col.endswith(\"_eer\")]\n",
    "    df_report[eer_columns] = df_report[eer_columns].abs()\n",
    "    \n",
    "    # Rename split result columns with impostor-ids used in split\n",
    "    rename_cols = {}\n",
    "    for idx, impostor in enumerate(impostors):\n",
    "        print(f\"idx: {idx}, impostor: {impostor}\")\n",
    "        to_rename_cols = [col for col in df_report.columns if col.startswith(f\"split{idx}\")]\n",
    "        for col in to_rename_cols:\n",
    "            rename_cols[col] = str(impostor)+col[len(f\"split{idx}\"):]\n",
    "    df_report = df_report.rename(columns=rename_cols)      \n",
    "\n",
    "    return df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T11:02:53.336815Z",
     "iopub.status.busy": "2022-03-29T11:02:53.336501Z",
     "iopub.status.idle": "2022-03-29T11:02:53.425955Z",
     "shell.execute_reply": "2022-03-29T11:02:53.424527Z",
     "shell.execute_reply.started": "2022-03-29T11:02:53.33679Z"
    }
   },
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    print(\"Performing Dummy RandomSearch...\")\n",
    "    from sklearn import svm, datasets\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "    iris = datasets.load_iris()\n",
    "    parameters = {\"kernel\": (\"linear\", \"rbf\"), \"C\": [1, 2, 3, 4, 5, 6, 7, 10]}\n",
    "    svc = svm.SVC(gamma=\"scale\")\n",
    "    clf = RandomizedSearchCV(svc, parameters, cv=3, iid=False)\n",
    "    clf.fit(iris.data, iris.target)\n",
    "    print(\"Create report:\")\n",
    "    df_temp = utils_cv_report(clf, \"owner x\", [\"impo_1\", \"impo_2\", \"impo_3\"])\n",
    "    display(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_plot_randomsearch_results(df_results, n_top=1):\n",
    "    # Prepare data for plotting\n",
    "    df_plot = df_results[df_results[\"rank_test_eer\"] <= n_top].rename(\n",
    "        columns={\n",
    "            \"param_model__nu\": r\"$\\nu$\",\n",
    "            \"param_model__gamma\": r\"$\\gamma$\",\n",
    "            \"mean_test_accuracy\": \"Mean Test Acc.\",\n",
    "            \"mean_test_eer\": \"Mean Test EER\",\n",
    "        }\n",
    "    )\n",
    "    df_plot[\"Mean Test EER\"] = df_plot[\"Mean Test EER\"] * -1  # Because fewer is more\n",
    "\n",
    "    median_nu = df_plot[r\"$\\nu$\"].median()\n",
    "    median_gamma = df_plot[r\"$\\gamma$\"].median()\n",
    "\n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(5.473 / 1.3, 2), dpi=180)\n",
    "    g = sns.scatterplot(\n",
    "        x=r\"$\\nu$\",\n",
    "        y=r\"$\\gamma$\",\n",
    "        data=df_plot,\n",
    "        size=\"Mean Test EER\",\n",
    "        sizes=(7, 60),\n",
    "        hue=\"Mean Test EER\",\n",
    "        alpha=1,\n",
    "        #        palette=\"Blues\",\n",
    "        linewidth=0,\n",
    "    )\n",
    "\n",
    "    # Format Legend labels\n",
    "    leg = g.get_legend()\n",
    "    new_handles = [h for h in leg.legendHandles]\n",
    "    new_labels = []\n",
    "    for i, handle in enumerate(leg.legendHandles):\n",
    "        label = handle.get_label()\n",
    "        print(f'{i}, {label}')\n",
    "        if ord(label[0]) == 8722:\n",
    "            label = '-' + label[1:]\n",
    "            \n",
    "        if i != 0:\n",
    "            \n",
    "            try:\n",
    "                new_labels.append(f\"{abs(float(label)):.3f}\")\n",
    "\n",
    "            except ValueError:\n",
    "                new_labels.append(\"\")\n",
    "\n",
    "    # Plot mean values\n",
    "    plt.plot(\n",
    "        [-0.01, 0.31],\n",
    "        [median_gamma, median_gamma],\n",
    "        linestyle=\"dashed\",\n",
    "        linewidth=0.8,\n",
    "        alpha=0.7,\n",
    "        color=\"black\",\n",
    "    )\n",
    "    plt.text(\n",
    "        0.23,\n",
    "        median_gamma * 1.7 ** 2,\n",
    "        r\"median($\\gamma$)\",\n",
    "        fontsize=6,\n",
    "        color=\"black\",\n",
    "        alpha=0.9,\n",
    "    )\n",
    "    plt.text(\n",
    "        0.23,\n",
    "        median_gamma * 1.2 ** 2,\n",
    "        f\"{median_gamma:.3f}\",\n",
    "        fontsize=5,\n",
    "        color=\"black\",\n",
    "        alpha=0.9,\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        [median_nu, median_nu],\n",
    "        [0.0001, 1000],\n",
    "        linestyle=\"dashed\",\n",
    "        linewidth=0.8,\n",
    "        alpha=0.7,\n",
    "        color=\"black\",\n",
    "    )\n",
    "    plt.text(\n",
    "        median_nu + 0.005, 400, r\"median($\\nu$)\", fontsize=6, color=\"black\", alpha=0.9\n",
    "    )\n",
    "    plt.text(\n",
    "        median_nu + 0.005, 200, f\"{median_nu:.3f}\", fontsize=5, color=\"black\", alpha=0.9\n",
    "    )\n",
    "\n",
    "    # Adjust axes & legend\n",
    "    plt.yscale(\"log\")\n",
    "    plt.ylim(0.0001, 1000)\n",
    "    plt.xlim(0, 0.305)\n",
    "#     print(new_handles)\n",
    "    print(new_labels)\n",
    "    plt.legend(\n",
    "        new_handles,\n",
    "        new_labels,\n",
    "        bbox_to_anchor=(1.02, 1),\n",
    "        loc=2,\n",
    "        borderaxespad=0.0,\n",
    "        title=\"Mean EER per Owner\\n(Validation Results)\",\n",
    "        title_fontsize=5,\n",
    "    )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return median_nu, median_gamma, fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b7fd521d-f17e-4894-84c3-4d3ab3655c44",
    "_uuid": "081f655b-dd63-4dc9-8b08-9cb0f7982129",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T11:02:44.974756Z",
     "iopub.status.busy": "2022-03-29T11:02:44.974191Z",
     "iopub.status.idle": "2022-03-29T11:02:45.008486Z",
     "shell.execute_reply": "2022-03-29T11:02:45.007423Z",
     "shell.execute_reply.started": "2022-03-29T11:02:44.974714Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def utils_plot_acc_eer_dist(df_plot, y_col):\n",
    "    n_subject = len(df_plot[\"Owner\"].unique()) - 1\n",
    "    mean_col = df_plot[y_col].mean()\n",
    "\n",
    "    fig = plt.figure(figsize=(5.473, 2), dpi=180)\n",
    "    ax = sns.boxplot(x=\"Owner\", y=y_col, data=df_plot, **utils_boxplot_style)\n",
    "    ax.set_ylim((0, 1))\n",
    "\n",
    "    plt.plot(\n",
    "        [-0.6, n_subject + 0.6],\n",
    "        [mean_col, mean_col],\n",
    "        linestyle=\"dashed\",\n",
    "        linewidth=1,\n",
    "        color=MAGENTA,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    plt.text(n_subject + 0.6, mean_col, f\"mean\", fontsize=6, color=MAGENTA)\n",
    "    plt.text(\n",
    "        n_subject + 0.6, mean_col - 0.04, f\"{mean_col:.3f}\", fontsize=4.5, color=MAGENTA\n",
    "    )\n",
    "    plt.xticks(rotation=45)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    print(f\"Overall mean: {mean_col:.4f}\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "230b6da5-a942-4634-a5cc-6e3a07d81646",
    "_uuid": "df80a643-53d2-4d31-9dc2-437d83a306eb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T11:02:45.271051Z",
     "iopub.status.busy": "2022-03-29T11:02:45.270467Z",
     "iopub.status.idle": "2022-03-29T11:02:45.276097Z",
     "shell.execute_reply": "2022-03-29T11:02:45.275043Z",
     "shell.execute_reply.started": "2022-03-29T11:02:45.271009Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# if TEST_MODE:\n",
    "#     print(\"Performing Dummy RandomSearch...\")\n",
    "#     from sklearn import svm, datasets\n",
    "#     from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#     iris = datasets.load_iris()\n",
    "#     parameters = {\"kernel\": (\"linear\", \"rbf\"), \"C\": [1, 2, 3, 4, 5, 6, 7, 10]}\n",
    "#     svc = svm.SVC(gamma=\"scale\")\n",
    "#     clf = RandomizedSearchCV(svc, parameters, cv=3, iid=False)\n",
    "#     clf.fit(iris.data, iris.target)\n",
    "#     print(\"Create report:\")\n",
    "#     df_temp = utils_cv_report(clf, \"owner x\", [\"impo_1\", \"impo_2\", \"impo_3\"])\n",
    "#     display(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "61fb5ecc-ec29-4f86-8590-0db7735de365",
    "_uuid": "bd017a22-bbf8-4c0b-890b-07cc71a417ae",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T11:02:45.905429Z",
     "iopub.status.busy": "2022-03-29T11:02:45.904866Z",
     "iopub.status.idle": "2022-03-29T11:02:45.926361Z",
     "shell.execute_reply": "2022-03-29T11:02:45.925209Z",
     "shell.execute_reply.started": "2022-03-29T11:02:45.905388Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class pca_feature_selector:\n",
    "    def __init__(self, n_components):\n",
    "        self._pca_dict = {}\n",
    "        self.n_components = n_components\n",
    "        \n",
    "    def add_user_pca(self, owner_idx, user_pca):\n",
    "        if owner_idx in self._pca_dict:\n",
    "            raise Exception(f\"owner_idx: {owner_idx} alraedy exists!\")\n",
    "        \n",
    "        self._pca_dict[owner_idx] = user_pca\n",
    "        \n",
    "    def user_feature_ranking(self, owner_idx):\n",
    "        '''\n",
    "        these two are the same \n",
    "        np.matmul(pca.explained_variance_ratio_[np.newaxis], abs_components) == np.dot(pca.explained_variance_ratio_, abs_components)[np.newaxis]\n",
    "        '''\n",
    "        pca = self._pca_dict[owner_idx]\n",
    "        abs_components = np.abs(pca.components_)\n",
    "        feature_importance = np.dot(pca.explained_variance_ratio_, abs_components)[np.newaxis]\n",
    "        top_feature_indices = np.argsort(-1*feature_importance)\n",
    "        \n",
    "        return {\"top_feature_indices\": top_feature_indices, \"feature_importance\": feature_importance}\n",
    "    \n",
    "    def get_comparison_matrix(self):\n",
    "        \n",
    "        feature_importance_matrix = []\n",
    "        top_feature_matrix = []\n",
    "        for owner_idx in self._pca_dict:\n",
    "            user_feature_dict = self.user_feature_ranking(owner_idx) \n",
    "            feature_importance_matrix += [user_feature_dict[\"feature_importance\"]]\n",
    "            top_feature_matrix += [user_feature_dict[\"top_feature_indices\"]]\n",
    "            \n",
    "        self._feature_importance_matrix = np.concatenate(feature_importance_matrix, axis=0)\n",
    "        self._top_feature_matrix = np.concatenate(top_feature_matrix, axis=0)\n",
    "\n",
    "        return {\"feature_importance_matrix\": self._feature_importance_matrix, \"top_feature_matrix\" :self._top_feature_matrix}\n",
    "    \n",
    "    def find_top_n_features(self):\n",
    "        \n",
    "        best_feature_lst = []\n",
    "        for i in range(self.n_components):\n",
    "            best_feature_lst.append(self.find_next_best_feature(best_feature_lst))\n",
    "            \n",
    "        return best_feature_lst\n",
    "        \n",
    "    def find_next_best_feature(self, curr_feature_lst):\n",
    "        \n",
    "        curr_pc_idx = len(curr_feature_lst)\n",
    "        feature_column_count = np.bincount(self._top_feature_matrix[:, curr_pc_idx])\n",
    "        print(f\"top_f_m: {self._top_feature_matrix[:, curr_pc_idx]}\")\n",
    "        print(curr_feature_lst)\n",
    "        \n",
    "        i = 0\n",
    "        #probably need to use a tree type or heap structure\n",
    "        while i < len(feature_column_count):\n",
    "            top_feature_idx = np.argmax(feature_column_count[i:]) + i\n",
    "            if top_feature_idx not in curr_feature_lst:\n",
    "                return top_feature_idx\n",
    "            print('-------')\n",
    "            print(feature_column_count)\n",
    "            print(f\"i: {i}, top_feature_idx: {top_feature_idx}\")\n",
    "\n",
    "            i = top_feature_idx + 1\n",
    "            \n",
    "        raise Exception('could not find best feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_plot_training_loss(history):\n",
    "    \"\"\"Plot Train/Valid Loss during Epochs.\"\"\"\n",
    "    fig = plt.figure(figsize=(5.473, 2.7), dpi=180)\n",
    "    plt.plot(history[\"loss\"], label=\"train\", color=\"tab:blue\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"valid\", color=MAGENTA)\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    fig.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    fig.tight_layout()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    HistoryDummy = type(\"History\", (object,), {})\n",
    "    history = HistoryDummy()\n",
    "    history.history = {}\n",
    "    history.history[\"loss\"] = [0.6, 0.4, 0.3, 0.2, 0.21, 0.15]\n",
    "    history.history[\"val_loss\"] = [0.9, 0.7, 0.5, 0.4, 0.35, 0.3]\n",
    "    utils_plot_training_loss(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9a6a5c02-ace2-47be-bad3-10b646266720",
    "_uuid": "b03ec3d1-6ac2-4da7-9d90-eadad41f35b7",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:46.405019Z",
     "iopub.status.busy": "2022-03-29T07:28:46.404687Z",
     "iopub.status.idle": "2022-03-29T07:28:46.408929Z",
     "shell.execute_reply": "2022-03-29T07:28:46.407833Z",
     "shell.execute_reply.started": "2022-03-29T07:28:46.40499Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# randomized_data_idx = list(range(len(r)))\n",
    "# random.shuffle(randomized_data_idx)\n",
    "# split_idx = 2 * (len(randomized_data_idx)//3) + 1\n",
    "# train_set = randomized_data_idx[: split_idx]\n",
    "# test_set = randomized_data_idx[split_idx: ]\n",
    "# print(f\"train_set: {train_set}\\ntest_set: {test_set}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c8931e16-a4e2-42c8-b626-7dfd6e8f2ed4",
    "_uuid": "d73dc5fe-0599-458f-ba69-ee6405152e40",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:46.900568Z",
     "iopub.status.busy": "2022-03-29T07:28:46.900021Z",
     "iopub.status.idle": "2022-03-29T07:28:46.904243Z",
     "shell.execute_reply": "2022-03-29T07:28:46.903422Z",
     "shell.execute_reply.started": "2022-03-29T07:28:46.900532Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # preparing train data\n",
    "# # train_set = r\n",
    "# dfList_exp1_train, dfList_exp2_train = [dfList_exp1[i] for i in train_set], [dfList_exp2[i] for i in train_set]\n",
    "# print(f\"len(dfList_exp1_train): {len(dfList_exp1_train)}\")\n",
    "# print(f\"len(dfList_exp2_train): {len(dfList_exp2_train)}\")\n",
    "# XExpTrainDict = MakeXExpDic(dfList_exp1_train, dfList_exp2_train, window_size = 250, step = 251, numSamplePoints= 18000)\n",
    "# X_exp1_train_dic, X_exp2_train_dic = XExpTrainDict[\"X_exp1_dic\"], XExpTrainDict[\"X_exp2_dic\"]\n",
    "\n",
    "# # preparing test data\n",
    "# dfList_exp1_test, dfList_exp2_test = [dfList_exp1[i] for i in test_set], [dfList_exp2[i] for i in test_set]\n",
    "# print(f\"len(dfList_exp1_test): {len(dfList_exp1_test)}\")\n",
    "# print(f\"len(dfList_exp2_test): {len(dfList_exp2_test)}\")\n",
    "# XExpTestDict = MakeXExpDic(dfList_exp1_test, dfList_exp2_test, window_size = 250, step = 251, numSamplePoints= 18000)\n",
    "# X_exp1_test_dic, X_exp2_test_dic = XExpTestDict[\"X_exp1_dic\"], XExpTestDict[\"X_exp2_dic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "92b22de7-7efb-4cce-aa17-a941d9bc262d",
    "_uuid": "7900b26e-3f79-4af8-b0dd-3c66f8193ad6"
   },
   "source": [
    "**use the following to write tests for distro functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "34212531-d2e8-4ddf-85fb-9735252c8b1b",
    "_uuid": "18ad7845-afb9-46a8-b194-28b36a1acf3e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:47.441543Z",
     "iopub.status.busy": "2022-03-29T07:28:47.440969Z",
     "iopub.status.idle": "2022-03-29T07:28:47.452093Z",
     "shell.execute_reply": "2022-03-29T07:28:47.451079Z",
     "shell.execute_reply.started": "2022-03-29T07:28:47.441496Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# source: https://zhiyzuo.github.io/Plot-Lorenz/\n",
    "#  0 representing perfect equality, and 1 absolute inequality.\n",
    "def gini(arr):\n",
    "    ## first sort\n",
    "    sorted_arr = arr.copy()\n",
    "    sorted_arr.sort()\n",
    "    n = arr.size\n",
    "    coef_ = 2. / n\n",
    "    const_ = (n + 1.) / n\n",
    "    weighted_sum = sum([(i+1)*yi for i, yi in enumerate(sorted_arr)])\n",
    "    return coef_*weighted_sum/(sorted_arr.sum()) - const_\n",
    "\n",
    "def lorenz_curve(X):\n",
    "    ## first sort\n",
    "    X = X.copy()\n",
    "    X.sort()\n",
    "    X_lorenz = X.cumsum() / X.sum()\n",
    "    X_lorenz = np.insert(X_lorenz, 0, 0) \n",
    "    X_lorenz[0], X_lorenz[-1]\n",
    "    fig, ax = plt.subplots(figsize=[6,6])\n",
    "    ## scatter plot of Lorenz curve\n",
    "    ax.scatter(np.arange(X_lorenz.size)/(X_lorenz.size-1), X_lorenz, \n",
    "               marker='x', color='darkgreen', s=100)\n",
    "    ## line plot of equality\n",
    "    ax.plot([0,1], [0,1], color='k')\n",
    "    ax.set_xlabel('% of Population')\n",
    "    ax.set_ylabel('% of Errors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9fcf45a3-f8fc-4df5-8463-ae1ec5a02e70",
    "_uuid": "d9c1d4f4-21a7-40f7-af56-bd0f48ab254e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:47.654627Z",
     "iopub.status.busy": "2022-03-29T07:28:47.65404Z",
     "iopub.status.idle": "2022-03-29T07:28:47.659605Z",
     "shell.execute_reply": "2022-03-29T07:28:47.65826Z",
     "shell.execute_reply.started": "2022-03-29T07:28:47.654572Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# window = 1000\n",
    "# window_farDistro_array = DistroFAR(dic[\"window_farDistro_dict\"][window], threshold = dic[\"window_EER_threshold_dict\"][window])\n",
    "# window_frrDistro_array = DistroFRR(dic[\"window_frrDistro_dict\"][window], threshold = dic[\"window_EER_threshold_dict\"][window])\n",
    "# X = np.array(window_frrDistro_array)\n",
    "# # print(X)\n",
    "# print(gini(X))\n",
    "# lorenz_curve(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8c1a264f-679a-4cf6-ae79-0750956ba0d7",
    "_uuid": "3598674f-4484-4ffc-a276-045f6fdd71a2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:47.996757Z",
     "iopub.status.busy": "2022-03-29T07:28:47.996401Z",
     "iopub.status.idle": "2022-03-29T07:28:48.000378Z",
     "shell.execute_reply": "2022-03-29T07:28:47.999235Z",
     "shell.execute_reply.started": "2022-03-29T07:28:47.996725Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# X = np.array(window_farDistro_array)\n",
    "# # print(X)\n",
    "# print(gini(X))\n",
    "# lorenz_curve(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "84cef82a-53b1-46ef-b9ea-53b56e9dd7ae",
    "_uuid": "decd2faf-d977-4db9-a53e-e303a433e7f5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:48.679392Z",
     "iopub.status.busy": "2022-03-29T07:28:48.679043Z",
     "iopub.status.idle": "2022-03-29T07:28:48.683453Z",
     "shell.execute_reply": "2022-03-29T07:28:48.682467Z",
     "shell.execute_reply.started": "2022-03-29T07:28:48.679363Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# keys_list = r\n",
    "# values_list = window_frrDistro_array\n",
    "# #Get pairs of elements\n",
    "\n",
    "\n",
    "# zip_iterator = zip(keys_list, values_list)\n",
    "\n",
    "# Distro_dict = dict(zip_iterator)\n",
    "\n",
    "# Distro_dict = {k: v for k, v in sorted(Distro_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "# fig, ax =plt.subplots(1,1, figsize=(8,8))\n",
    "\n",
    "# ax.set_title('FRR Distrobution')\n",
    "# print(Distro_dict)\n",
    "# data = {\"User\": list(Distro_dict.keys()), \"False Rejects\": list(Distro_dict.values())}\n",
    "# g = sns.barplot(x=data[\"User\"], y=data[\"False Rejects\"], order=data[\"User\"],ax = ax)\n",
    "# #y=EER_dict.values()\n",
    "\n",
    "# # y_ticks = np.arange(0, .25 + 0.001, .05)\n",
    "\n",
    "# # g.set_yticks(y_ticks)\n",
    "# ax.axhline(np.mean(data[\"False Rejects\"]), ls='--')\n",
    "# ax.set_xlabel('User')\n",
    "# ax.set_ylabel('False Rejects')\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "aabe956d-b0cc-44b0-a65c-47058842dbd1",
    "_uuid": "b629854f-9aca-444f-9b87-2309ba0a5d44",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:49.07909Z",
     "iopub.status.busy": "2022-03-29T07:28:49.078475Z",
     "iopub.status.idle": "2022-03-29T07:28:49.082701Z",
     "shell.execute_reply": "2022-03-29T07:28:49.08185Z",
     "shell.execute_reply.started": "2022-03-29T07:28:49.079048Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# X = np.array(window_farDistro_array)\n",
    "# # print(X)\n",
    "# print(gini(X))\n",
    "# lorenz_curve(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5438d555-f070-4019-8536-c063e35b7e94",
    "_uuid": "5a18caf2-bb56-45b4-a085-da83f4b35404",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:49.533056Z",
     "iopub.status.busy": "2022-03-29T07:28:49.532561Z",
     "iopub.status.idle": "2022-03-29T07:28:49.536687Z",
     "shell.execute_reply": "2022-03-29T07:28:49.535806Z",
     "shell.execute_reply.started": "2022-03-29T07:28:49.53301Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# auc(frrList, farList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e00ca0fd-f3b5-40b4-8e6b-4813887fe906",
    "_uuid": "2ef37b96-5eae-466c-9d73-cf9ee3abb975",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:49.888236Z",
     "iopub.status.busy": "2022-03-29T07:28:49.887827Z",
     "iopub.status.idle": "2022-03-29T07:28:49.894702Z",
     "shell.execute_reply": "2022-03-29T07:28:49.893711Z",
     "shell.execute_reply.started": "2022-03-29T07:28:49.888198Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def getAUROCDist(window_frrList_dict, window_farList_dict, start_window_size=250, end_window_size=3000, increment_step=250):\n",
    "    \n",
    "    window_AUROC_dict = {}\n",
    "    lst = np.arange(start_window_size, end_window_size + 1, increment_step)\n",
    "    \n",
    "    for w in lst:\n",
    "        \n",
    "        frrList = dic[\"window_frrList_dict\"][w]\n",
    "        farList = dic[\"window_farList_dict\"][w]\n",
    "        window_AUROC_dict[w] = auc(frrList, farList)\n",
    "        \n",
    "    return window_AUROC_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "09da174d-f14b-4a97-be01-ea606e593339",
    "_uuid": "b2e8ce39-9be1-4e28-8439-2ed3a704d924",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:50.359726Z",
     "iopub.status.busy": "2022-03-29T07:28:50.359407Z",
     "iopub.status.idle": "2022-03-29T07:28:50.364Z",
     "shell.execute_reply": "2022-03-29T07:28:50.362938Z",
     "shell.execute_reply.started": "2022-03-29T07:28:50.359698Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# AUROC_dict = getAUROCDist(dic[\"window_frrList_dict\"], dic[\"window_farList_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "84bd2da7-ede2-4a35-8495-5bc96140ca01",
    "_uuid": "1bbd0ddf-1782-49f8-94d6-d72dafd3bd1c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:51.624382Z",
     "iopub.status.busy": "2022-03-29T07:28:51.624045Z",
     "iopub.status.idle": "2022-03-29T07:28:51.62855Z",
     "shell.execute_reply": "2022-03-29T07:28:51.627496Z",
     "shell.execute_reply.started": "2022-03-29T07:28:51.624353Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# data = pd.read_csv('../input/wearable-assisted-ca/user10_1.docx', error_bad_lines = False, header=None, dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f3569ab8-ad0e-4ddc-8e5f-231c149e7995",
    "_uuid": "50be2017-9348-4fad-9f17-901e13021645",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:52.058309Z",
     "iopub.status.busy": "2022-03-29T07:28:52.057563Z",
     "iopub.status.idle": "2022-03-29T07:28:52.065136Z",
     "shell.execute_reply": "2022-03-29T07:28:52.064354Z",
     "shell.execute_reply.started": "2022-03-29T07:28:52.058249Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# def extractTextFromDocx(path):\n",
    "#     try:\n",
    "#         doc = docx.Document(path)  # Creating word reader object.\n",
    "#         data = \"\"\n",
    "#         fullText = []\n",
    "#         for para in doc.paragraphs:\n",
    "#             fullText.append(para.text)\n",
    "#             data = '\\n'.join(fullText)\n",
    "\n",
    "#     except IOError:\n",
    "#         print('There was an error opening the file!')\n",
    "#         return\n",
    "#     return data\n",
    "\n",
    "# # %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-03-18T18:34:19.771958Z\",\"iopub.execute_input\":\"2022-03-18T18:34:19.772266Z\",\"iopub.status.idle\":\"2022-03-18T18:34:19.793851Z\",\"shell.execute_reply.started\":\"2022-03-18T18:34:19.772229Z\",\"shell.execute_reply\":\"2022-03-18T18:34:19.792569Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n",
    "# def numberOfWords(text):\n",
    "#     return len(text.strip().split())\n",
    "\n",
    "# def numberOfChars(text):\n",
    "#     return len(text)\n",
    "\n",
    "# def wordsPerMinute(text, mins):\n",
    "#     return numberOfWords(text)/mins\n",
    "\n",
    "# def charsPerMinute(text, mins):\n",
    "#     return numberOfChars(text)/mins\n",
    "\n",
    "# def classifyTypists(typistsSpeeds):\n",
    "#     '''\n",
    "#         WPM\n",
    "#     Beginner\t0 - 24\n",
    "#     Intermediate\t25 - 30\n",
    "#     Average\t31 - 41\n",
    "#     Pro\t42 - 54\n",
    "#     Typemaster\t55 - 79\n",
    "#     Megaracer\t80+\n",
    "#     '''\n",
    "#     exp2_typingspeeds = [29.96428571, 37.42857143, 44.89285714, 52.35714286, 59.82142857, 67.28571429]\n",
    "#     speedDict = {\"Beginner\": 24, \"Intermediate\": 30, \"Average\": 41, \"Pro\": 54, \"Typemaster\": 79, \"Megaracer\": 1000}\n",
    "    \n",
    "#     keys = list(speedDict.keys())\n",
    "#     for i in range(len(speedDict.keys())):\n",
    "#         speedDict[keys[i]] = exp2_typingspeeds[i]\n",
    "    \n",
    "#     speedStats = {\"Beginner\": 0, \"Intermediate\": 0, \"Average\": 0, \"Pro\": 0, \"Typemaster\": 0, \"Megaracer\": 0}\n",
    "#     typistsIDStats = {\"Beginner\": [], \"Intermediate\": [], \"Average\": [], \"Pro\": [], \"Typemaster\": [], \"Megaracer\": []}\n",
    "    \n",
    "#     for typist, speed in typistsSpeeds.items():\n",
    "#         if speed <= speedDict[\"Beginner\"]:\n",
    "#             speedStats[\"Beginner\"] += 1\n",
    "#             typistsIDStats[\"Beginner\"].append(typist)\n",
    "            \n",
    "#         elif speed <= speedDict[\"Intermediate\"]:\n",
    "#             speedStats[\"Intermediate\"] += 1\n",
    "#             typistsIDStats[\"Intermediate\"].append(typist)\n",
    "            \n",
    "#         elif speed <= speedDict[\"Average\"]:\n",
    "#             speedStats[\"Average\"] += 1\n",
    "#             typistsIDStats[\"Average\"].append(typist)\n",
    "            \n",
    "#         elif speed <= speedDict[\"Pro\"]:\n",
    "#             speedStats[\"Pro\"] += 1\n",
    "#             typistsIDStats[\"Pro\"].append(typist)\n",
    "            \n",
    "#         elif speed <= speedDict[\"Typemaster\"]:\n",
    "#             speedStats[\"Typemaster\"] += 1\n",
    "#             typistsIDStats[\"Typemaster\"].append(typist)\n",
    "            \n",
    "#         else:\n",
    "#             speedStats[\"Megaracer\"] += 1\n",
    "#             typistsIDStats[\"Megaracer\"].append(typist)\n",
    "            \n",
    "#     return {\"speedStats\": speedStats, \"typistsIDStats\": typistsIDStats}\n",
    "\n",
    "# # %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-03-18T18:34:19.795159Z\",\"iopub.execute_input\":\"2022-03-18T18:34:19.795733Z\",\"iopub.status.idle\":\"2022-03-18T18:34:19.814022Z\",\"shell.execute_reply.started\":\"2022-03-18T18:34:19.795693Z\",\"shell.execute_reply\":\"2022-03-18T18:34:19.812934Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n",
    "# r = [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 28, 29, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49]\n",
    "# #r = [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 28, 29, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
    "\n",
    "\n",
    "# def users_typing_speed(user_ids_lst):\n",
    "#     '''\n",
    "#     input: \n",
    "#         user_ids_lst: list of selected user ids\n",
    "#     Return {user_id: typing_speed in words per minute}\n",
    "#     '''\n",
    "#     typistsSpeeds = {}\n",
    "#     for i in user_ids_lst:\n",
    "#         user_text_data = extractTextFromDocx('../input/wearable-assisted-ca/user{0}_{1}.docx'.format(i, 2))\n",
    "\n",
    "#         typistsSpeeds[i] = wordsPerMinute(user_text_data, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "19063d95-fc9e-4094-8ab7-5999e611c04e",
    "_uuid": "d119855a-a09a-4c7e-b31b-b26a65900d93",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:52.36913Z",
     "iopub.status.busy": "2022-03-29T07:28:52.368551Z",
     "iopub.status.idle": "2022-03-29T07:28:52.373777Z",
     "shell.execute_reply": "2022-03-29T07:28:52.372688Z",
     "shell.execute_reply.started": "2022-03-29T07:28:52.36908Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# dic = classifyTypists(typistsSpeeds)\n",
    "# dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1215f780-6abe-4c99-9c42-de10cf6fbd57",
    "_uuid": "50d96fcf-e53b-4424-9b98-f58d5133e2d9"
   },
   "source": [
    "# Divide the users using histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a style I use a lot for boxplots:\n",
    "utils_boxplot_style = dict(\n",
    "    color=\"tab:blue\",\n",
    "    linewidth=0.5,\n",
    "    saturation=1,\n",
    "    width=0.7,\n",
    "    flierprops=dict(\n",
    "        marker=\"o\", markersize=2, markerfacecolor=\"none\", markeredgewidth=0.5\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Define a style I use a lot for lineplots:\n",
    "utils_lineplot_style = dict(\n",
    "    color=\"tab:blue\", linewidth=0.5, marker=\"o\", markersize=3, markeredgewidth=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T04:26:44.057446Z",
     "iopub.status.busy": "2022-03-31T04:26:44.057Z",
     "iopub.status.idle": "2022-03-31T04:26:44.063077Z",
     "shell.execute_reply": "2022-03-31T04:26:44.062158Z",
     "shell.execute_reply.started": "2022-03-31T04:26:44.057407Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"utility functions imported\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
