{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "d975284c-a069-40d8-9847-9465d0ac4f47",
    "_uuid": "b6bb54c3-ff29-436c-8f5e-b3a13b9f5153",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "TEST_MODE = 0 # Testing macro\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "CORES = -1\n",
    "SEED = 567\n",
    "\n",
    "import os\n",
    "\n",
    "if TEST_MODE:\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "3ae86968-b273-4857-a4fd-537b178d854b",
    "_uuid": "bcec77b5-0738-4d15-8aad-f6b59b3ac023",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.9/site-packages (22.0.3)\n",
      "Collecting pip\n",
      "  Using cached pip-23.0.1-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.0.3\n",
      "    Uninstalling pip-22.0.3:\n",
      "      Successfully uninstalled pip-22.0.3\n",
      "Successfully installed pip-23.0.1\n",
      "utility_functions imports setup complete\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "# !pip install python-docx\n",
    "# !pip install antropy\n",
    "\n",
    "MAGENTA = (202/255, 18/255, 125/255)\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.fft import fft, ifft\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy import signal\n",
    "import numpy as np # linear algebraf\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import dataclasses\n",
    "import math as math\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import cross_validate, RandomizedSearchCV\n",
    "import statsmodels.stats.api as sms\n",
    "from tqdm.auto import tqdm\n",
    "from dataclasses import asdict\n",
    "from sklearn import svm\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import random\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_curve, accuracy_score, make_scorer, auc\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold # Feature selector\n",
    "from sklearn.model_selection import KFold\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Input,\n",
    "    Conv1D,\n",
    "    MaxPooling1D,\n",
    "    Flatten,\n",
    "    Lambda,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    "    GlobalAveragePooling1D,\n",
    "    Activation\n",
    ")\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Ftrl, Nadam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import concatenate as keras_concat\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import iqr\n",
    "from scipy.stats import median_abs_deviation\n",
    "from scipy.stats import mode\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.signal import peak_widths\n",
    "from scipy.stats import entropy\n",
    "# from scipy.special import entr\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, MaxAbsScaler, RobustScaler, PowerTransformer\n",
    "get_new_scaler_dict = {\"StandardScaler\": StandardScaler, \"MinMaxScaler\": MinMaxScaler, \"Normalizer\": Normalizer, \n",
    "                       \"MaxAbsScaler\": MaxAbsScaler, \"RobustScaler\": RobustScaler, \"PowerTransformer\": PowerTransformer}\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import auc\n",
    "import time\n",
    "import json\n",
    "# import docx\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "sns.set(rc={'figure.figsize':(12,8), 'figure.dpi': 180, \"legend.fontsize\": 26, \"axes.labelsize\": 26, #\"xtick.fontsize\": 26, \"ytick.fontsize\": 20, \n",
    "            \"xtick.labelsize\": 20, \"ytick.labelsize\": 20})\n",
    "\n",
    "err_distro_rc={'figure.figsize':(20,10), 'figure.dpi': 180, \"legend.fontsize\": 30, \"axes.labelsize\": 30, #\"xtick.fontsize\": 26, \"ytick.fontsize\": 20, \n",
    "            \"xtick.labelsize\": 25, \"ytick.labelsize\": 25}\n",
    "\n",
    "# Global utitlity functions are in separate notebook\n",
    "print(\"utility_functions imports setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(12,8), 'figure.dpi': 180, \"legend.fontsize\": 9, \"axes.labelsize\": 11, #\"xtick.fontsize\": 26, \"ytick.fontsize\": 20, \n",
    "            \"xtick.labelsize\": 11, \"ytick.labelsize\": 11})\n",
    "# sns.set(rc={'figure.figsize':(12,8), 'figure.dpi': 180, \"legend.fontsize\":11, \"axes.labelsize\": 11, #\"xtick.fontsize\": 26, \"ytick.fontsize\": 20, \n",
    "#             \"xtick.labelsize\": 11, \"ytick.labelsize\": 11})\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"paper\")\n",
    "sns.set(font=\"sans\")\n",
    "sns.set_palette(\"tab10\")\n",
    "# for plain matplotlib:\n",
    "plt.style.use([\"seaborn-darkgrid\", \"seaborn-paper\"])\n",
    "plt.rc(\"font\", family=\"sans\", size=8)\n",
    "plt.rc(\"axes\", titlesize=6)\n",
    "plt.rc(\"axes\", labelsize=6)\n",
    "plt.rc(\"xtick\", labelsize=6)\n",
    "plt.rc(\"ytick\", labelsize=6)\n",
    "plt.rc(\"xtick.major\", pad=1)\n",
    "plt.rc(\"ytick.major\", pad=3)\n",
    "plt.rc(\"legend\", fontsize=6)\n",
    "plt.rc(\"figure\", titlesize=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_distro_rc[\"figure.figsize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TEST_MODE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124;03m\"\"\"Test function above using implementation with numpy instead tensors.\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_true \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msquare(dist) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m y_true) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msquare(np\u001b[38;5;241m.\u001b[39mmax(margin \u001b[38;5;241m-\u001b[39m dist, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mTEST_MODE\u001b[49m:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositive: class=1, distance=0,     loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, contrastive_loss_test(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositive: class=1, distance=0.01,  loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, contrastive_loss_test(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.01\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TEST_MODE' is not defined"
     ]
    }
   ],
   "source": [
    "def k_euclidean_dist(t):\n",
    "    x = t[0]\n",
    "    y = t[1]    \n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=-1, keepdims=True))\n",
    "\n",
    "\n",
    "def k_contrastive_loss(y_true, dist, margin):\n",
    "    \"\"\"Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "    return K.mean(y_true * K.square(dist) + (1 - y_true) * K.square(K.maximum(margin - dist, 0)))\n",
    "\n",
    "def contrastive_loss_test(y_true, dist, margin=1):\n",
    "    \"\"\"Test function above using implementation with numpy instead tensors.\"\"\"\n",
    "    return y_true * np.square(dist) + (1 - y_true) * np.square(np.max(margin - dist, 0))\n",
    "\n",
    "if TEST_MODE:\n",
    "    print(\"Positive: class=1, distance=0,     loss:\", contrastive_loss_test(1, 0))\n",
    "    print(\"Positive: class=1, distance=0.01,  loss:\", contrastive_loss_test(1, 0.01))\n",
    "    print(\"Positive: class=1, distance=0.3,   loss:\", contrastive_loss_test(1, 0.3))\n",
    "    print(\"Positive: class=1, distance=0.5,   loss:\", contrastive_loss_test(1, 0.5))\n",
    "    print(\"Positive: class=1, distance=1,     loss:\", contrastive_loss_test(1, 1))\n",
    "\n",
    "    print(\"Negative: class=0, distance=0,     loss:\", contrastive_loss_test(0, 0))\n",
    "    print(\"Negative: class=0, distance=0.01,  loss:\", contrastive_loss_test(0, 0.01))\n",
    "    print(\"Negative: class=0, distance=0.3,   loss:\", contrastive_loss_test(0, 0.3))\n",
    "    print(\"Negative: class=0, distance=0.5,   loss:\", contrastive_loss_test(0, 0.5))\n",
    "    print(\"Negative: class=0, distance=1,     loss:\", contrastive_loss_test(0, 1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Positive: class=1, distance=0,     loss:\", k_contrastive_loss(tf.constant(1, dtype=tf.float32), tf.constant(0, dtype=tf.float32), 1))\n",
    "    print(\"Positive: class=1, distance=0.01,  loss:\", k_contrastive_loss(tf.constant(1, dtype=tf.float32), tf.constant(0.01, dtype=tf.float32), 1))\n",
    "    print(\"Positive: class=1, distance=0.3,   loss:\", k_contrastive_loss(tf.constant(1, dtype=tf.float32), tf.constant(0.3, dtype=tf.float32), 1))\n",
    "    print(\"Positive: class=1, distance=0.5,   loss:\", k_contrastive_loss(tf.constant(1, dtype=tf.float32), tf.constant(0.5, dtype=tf.float32), 1))\n",
    "    print(\"Positive: class=1, distance=1,     loss:\", k_contrastive_loss(tf.constant(1, dtype=tf.float32), tf.constant(1, dtype=tf.float32), 1))\n",
    "\n",
    "    print(\"Negative: class=0, distance=0,     loss:\", k_contrastive_loss(tf.constant(0, dtype=tf.float32), tf.constant(0, dtype=tf.float32), 1))\n",
    "    print(\"Negative: class=0, distance=0.01,  loss:\", k_contrastive_loss(tf.constant(0, dtype=tf.float32), tf.constant(0.01, dtype=tf.float32), 1))\n",
    "    print(\"Negative: class=0, distance=0.3,   loss:\", k_contrastive_loss(tf.constant(0, dtype=tf.float32), tf.constant(0.3, dtype=tf.float32), 1))\n",
    "    print(\"Negative: class=0, distance=0.5,   loss:\", k_contrastive_loss(tf.constant(0, dtype=tf.float32), tf.constant(0.5, dtype=tf.float32), 1))\n",
    "    print(\"Negative: class=0, distance=1,     loss:\", k_contrastive_loss(tf.constant(0, dtype=tf.float32), tf.constant(1, dtype=tf.float32), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_optimizer(name, lr=None, decay=None):\n",
    "    if name == \"SGD\":\n",
    "        lr = lr if lr != None else 0.01\n",
    "        decay = decay if decay != None else 0\n",
    "        optimizer = SGD(learning_rate=lr, decay=decay)\n",
    "    elif name == \"Adam\":\n",
    "        lr = lr if lr != None else 0.001\n",
    "        decay = decay if decay != None else 0\n",
    "        optimizer = Adam(learning_rate=lr, decay=decay)\n",
    "    elif name == \"RMSprop\":\n",
    "        lr = lr if lr != None else 0.001\n",
    "        optimizer = RMSprop(learning_rate=lr)\n",
    "    elif name == \"Adadelta\":\n",
    "        lr = lr if lr != None else 0.001\n",
    "        optimizer = Adadelta(learning_rate=lr)\n",
    "    elif name == \"Adagrad\":\n",
    "        lr = lr if lr != None else 0.001\n",
    "        optimizer = Adagrad(learning_rate=lr)\n",
    "    elif name == \"Adamax\":\n",
    "        lr = lr if lr != None else 0.001\n",
    "        optimizer = Adamax(learning_rate=lr)\n",
    "    elif name == \"Ftrl\":\n",
    "        lr = lr if lr != None else 0.001\n",
    "        optimizer = Ftrl(learning_rate=lr)\n",
    "    elif name == \"Nadam\":\n",
    "        lr = lr if lr != None else 0.001\n",
    "        optimizer = Nadam(learning_rate=lr)\n",
    "    else:\n",
    "        print(name)\n",
    "        raise BaseException(\"Error: Not a valid model name: 1d or 2d.\")\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def get_loss_func(arg_dict):\n",
    "    name = arg_dict[\"loss_func_name\"]\n",
    "    if name == \"k_contrastive_loss\":\n",
    "        loss_func = lambda y_true, dist: k_contrastive_loss(y_true, dist, arg_dict[\"contrastive_loss_margin\"])\n",
    "    else:\n",
    "        print(name)\n",
    "        raise BaseException(\"Error: Not a valid loss func name\")\n",
    "    return loss_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Siamese Model with FCN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_multi_headed_model_fcn(input_shape, arg_dict, do_plot=True):\n",
    "    '''\n",
    "    input_dropout_streams = arg_dict[\"input_dropout_streams\"]\n",
    "    filters_streams = arg_dict[\"filters_streams\"]\n",
    "    kernels_streams = arg_dict[\"kernels_streams\"]\n",
    "    kernels_init_streams = arg_dict[\"kernels_init_streams\"] \n",
    "    kernels_constraint_streams = arg_dict[\"kernels_constraint_streams\"] \n",
    "    strides_streams = arg_dict[\"strides_streams\"] \n",
    "    paddings_streams = arg_dict[\"paddings_streams\"] \n",
    "    activations_streams = arg_dict[\"activations_streams\"]\n",
    "    dropouts_streams = arg_dict[\"dropouts_streams\"] \n",
    "    \n",
    "    dense_layers = arg_dict[\"dense_layers\"] \n",
    "    dense_kernel_constraints = arg_dict[\"dense_kernel_constraints\"]\n",
    "    dense_kernel_inits = arg_dict[\"dense_kernel_inits\"] \n",
    "    dense_dropouts = arg_dict[\"dense_dropouts\"] \n",
    "    dense_activations = arg_dict[\"dense_activations\"] \n",
    "    \n",
    "    loss_func = arg_dict[\"loss_func\"] \n",
    "    optimizer = arg_dict[\"optimizer\"] \n",
    "    \n",
    "    # Define the tensors for the two input images\n",
    "    left_inputs = Input(input_shape, name=\"left_inputs\")\n",
    "    right_inputs = Input(input_shape, name=\"right_inputs\")\n",
    "    \n",
    "    '''\n",
    "    # conv\n",
    "    input_dropout_streams = arg_dict[\"input_dropout_streams\"]\n",
    "    filters_streams = arg_dict[\"filters_streams\"]\n",
    "    kernels_streams = arg_dict[\"kernels_streams\"]\n",
    "    kernels_init_streams = arg_dict[\"kernels_init_streams\"] \n",
    "    kernels_Max_Norm_constraint_streams = arg_dict[\"kernels_Max_Norm_constraint_streams\"] \n",
    "    strides_streams = arg_dict[\"strides_streams\"] \n",
    "    paddings_streams = arg_dict[\"paddings_streams\"] \n",
    "    activations_streams = arg_dict[\"activations_streams\"]\n",
    "    dropouts_streams = arg_dict[\"dropouts_streams\"]\n",
    "    conv_kernel_regularizer_streams = arg_dict[\"conv_kernel_regularizer_streams\"]\n",
    "    \n",
    "    # dense\n",
    "    dense_layers = arg_dict[\"dense_layers\"] \n",
    "    dense_kernel_Max_Norm_constraints = arg_dict[\"dense_kernel_Max_Norm_constraints\"]\n",
    "    dense_kernel_inits = arg_dict[\"dense_kernel_inits\"] \n",
    "    dense_dropouts = arg_dict[\"dense_dropouts\"] \n",
    "    dense_activations = arg_dict[\"dense_activations\"] \n",
    "    dense_kernel_regularizer = arg_dict[\"dense_kernel_regularizer\"]\n",
    "    \n",
    "    # loss_func = get_loss_func(arg_dict[\"loss_func_name\"], arg_dict[\"loss_func_args\"]) improve in future\n",
    "    loss_func = get_loss_func(arg_dict)\n",
    "    optimizer = get_optimizer(arg_dict[\"optimizer_name\"] , lr=arg_dict[\"optimizer_lr\"], decay=arg_dict[\"optimizer_decay\"])\n",
    "    \n",
    "    # Define the tensors for the two input images\n",
    "    left_inputs = Input(input_shape, name=\"left_inputs\")\n",
    "    right_inputs = Input(input_shape, name=\"right_inputs\")\n",
    "    \n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "    inputs_dict = {}\n",
    "    for head_idx in range(len(filters_streams)):\n",
    "        inputs_dict[head_idx] = Input(input_shape, name=f\"inputs_{head_idx}\")\n",
    "\n",
    "    \n",
    "    flat_output_dict = {}\n",
    "    for head in inputs_dict:\n",
    "        x= inputs_dict[head]\n",
    "\n",
    "        x = Dropout(input_dropout_streams[head], seed=SEED, name=f\"stream_{head}_input_drop\")(x)\n",
    "        for idx in range(len(filters_streams[head])):\n",
    "            x = Conv1D(filters=filters_streams[head][idx], kernel_size=kernels_streams[head][idx], kernel_initializer=kernels_init_streams[head][idx], \n",
    "                       kernel_constraint=MaxNorm(kernels_Max_Norm_constraint_streams[head][idx]), strides=strides_streams[head][idx], activation=None, \n",
    "                       padding=paddings_streams[head][idx], kernel_regularizer=conv_kernel_regularizer_streams[head][idx], name=f\"stream_{head}_conv_{idx+1}\")(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation(activations_streams[head][idx])(x)\n",
    "            x = Dropout(dropouts_streams[head][idx], seed=SEED, name=f\"stream_{head}_drop_{idx+1}\")(x)\n",
    "\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        \n",
    "        flat_output_dict[head] = x\n",
    "    \n",
    "    # merge\n",
    "    merged = keras_concat([flat_output_dict[head_idx] for head_idx in range(len(flat_output_dict))])\n",
    "\n",
    "    x = merged\n",
    "    \n",
    "    for dense_idx in range(len(dense_layers)):\n",
    "        x = Dropout(dense_dropouts[dense_idx], name=f\"dense_{dense_idx+1}_dropout\")(x)\n",
    "        x = Dense(dense_layers[dense_idx], kernel_constraint=MaxNorm(dense_kernel_Max_Norm_constraints[dense_idx]), kernel_initializer=dense_kernel_inits[dense_idx], \n",
    "                  kernel_regularizer=dense_kernel_regularizer[dense_idx], name=f\"dense_{dense_idx+1}\")(x) # excluded activation=dense_activations[dense_idx]\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(dense_activations[dense_idx], name=f\"dense_activation_{dense_idx+1}\" )(x)\n",
    "    \n",
    "    # Basemodel instance\n",
    "    basemodel = Model(inputs=[inputs_dict[idx] for idx in range(len(inputs_dict))], outputs=x, name=\"basemodel\")\n",
    "\n",
    "    # using same instance of \"basemodel\" to share weights between left/right networks\n",
    "    encoded_l = basemodel([left_inputs for i in range(len(inputs_dict))])\n",
    "    encoded_r = basemodel([right_inputs for i in range(len(inputs_dict))])\n",
    "\n",
    "    # Add a customized layer to compute the distance between the encodings\n",
    "    distance_layer = Lambda(k_euclidean_dist, name=\"distance\")([encoded_l, encoded_r])\n",
    "\n",
    "    # Combine into one net\n",
    "    siamese_net = Model(inputs=[left_inputs, right_inputs], outputs=distance_layer)\n",
    "    if do_plot: \n",
    "        plot_model(siamese_net, show_shapes=True, to_file='multichannel.png')\n",
    "        plot_model(basemodel, show_shapes=True, to_file='basemodel.png')\n",
    "        \n",
    "    siamese_net.compile(loss=loss_func, optimizer=optimizer)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net#|, basemodel\n",
    "\n",
    "# def get_model(name, window_size, feature_cols, filters):\n",
    "#     print(f\"Using Model variant {name}...\")\n",
    "#     if name == \"1d\":\n",
    "#         model, basemodel = build_model_1d((window_size, len(feature_cols)), filters)\n",
    "#     elif name == \"2d\":\n",
    "#         model, basemodel = build_model_2d((window_size, len(feature_cols), 1), filters)\n",
    "#     elif name == \"fcn\":\n",
    "#         model, basemodel = build_model_fcn((window_size, len(feature_cols)), filters)\n",
    "#     else:\n",
    "#         raise BaseException(\"Error: Not a valid model name: {1d, 2d, fcn}\")\n",
    "\n",
    "#     return model, basemodel\n",
    "# temp_model, temp_basemodel = get_model(P.model_variant, P.window_size, P.feature_cols, P.filters)\n",
    "\n",
    "# temp_basemodel.summary()\n",
    "# temp_model.summary()\n",
    "\n",
    "def get_create_model_func(name, window_size, feature_cols):\n",
    "    print(f\"Using Model variant {name}...\")\n",
    "    if name == \"1d\":\n",
    "        return lambda filters, kernels, strides, dropouts, activations, dense, loss_func, optimizer: build_model_1d((window_size, len(feature_cols)),\n",
    "                                                                                                               filters, kernels, strides, \n",
    "                                                                                                                    dropouts, activations, dense, \n",
    "                                                                                                                    loss_func, optimizer)\n",
    "    elif name == \"2d\":\n",
    "        return lambda filters, kernels, strides, dropouts, activations, dense, loss_func, optimizer: build_model_2d((window_size, len(feature_cols)),\n",
    "                                                                                                               filters, kernels, strides, \n",
    "                                                                                                                    dropouts, activations, dense, \n",
    "                                                                                                                    loss_func, optimizer)\n",
    "    elif name == \"fcn\":\n",
    "        return lambda filters, kernels, strides, dropouts, activations, dense, loss_func, optimizer: create_model_fcn((window_size, len(feature_cols)),\n",
    "                                                                                                               filters, kernels, strides, \n",
    "                                                                                                                      dropouts, activations, dense, \n",
    "                                                                                                                      loss_func, optimizer)\n",
    "    elif name == \"multi_head_fcn\":\n",
    "         return lambda arg_dict: create_multi_headed_model_fcn((window_size, len(feature_cols)), arg_dict)\n",
    "        \n",
    "    else:\n",
    "        raise BaseException(\"Error: Not a valid model name: {1d, 2d, fcn}\")\n",
    "        \n",
    "        \n",
    "def generate_index_pairs(diagonal_size):\n",
    "    '''\n",
    "    return an array of indices for negative/positive pairs. It is assumed that every \n",
    "    index of pair corresponds to different sessions.\n",
    "    '''\n",
    "    indices = list(range(diagonal_size))\n",
    "    mesh = np.array(np.meshgrid(indices, indices))\n",
    "    negative_index_pairs = mesh.T.reshape(-1, 2)\n",
    "    \n",
    "    return negative_index_pairs\n",
    "\n",
    "\n",
    "def generate_pairs(X_exp2_dic, X_exp1_dic, user_id_list, fitted_raw_scaler_dict=None, num_pair_limit = 50000):\n",
    "    '''\n",
    "    return positive and negative pairs\n",
    "    '''\n",
    "    num_users = len(user_id_list)\n",
    "    # print(num_users)\n",
    "#     X_neg[np.random.choice(X_neg.shape[0], size=X_pos.shape[0], replace=False), :]\n",
    "    negative_pairs_dic = {}\n",
    "    positive_pairs_dic = {}\n",
    "    \n",
    "    total_samples_per_user = len(X_exp2_dic[list(X_exp2_dic.keys())[0]])\n",
    "    # print(total_samples_per_user)\n",
    "    positive_pairs_indices = generate_index_pairs(total_samples_per_user)\n",
    "    negative_pairs_indices = generate_index_pairs(total_samples_per_user)\n",
    "    \n",
    "    # negative_pairs_indices = negative_pairs_indices[np.random.choice(negative_pairs_indices.shape[0], size=positive_pairs_indices.shape[0], replace=False)]\n",
    "    # negative_pairs_indices = np.sort(negative_pairs_indices)\n",
    "    # print(negative_pairs_indices)\n",
    "    positive_left = positive_pairs_indices[:, 0]\n",
    "    positive_right = positive_pairs_indices[:, 1]\n",
    "    \n",
    "    negative_left = negative_pairs_indices[:, 0]\n",
    "    negative_right = negative_pairs_indices[:, 1]\n",
    "    # fitted_scaler_SNN_exp2_train_dic\n",
    "    # print(positive_pairs_indices.shape)\n",
    "    # print(negative_pairs_indices.shape)\n",
    "    \n",
    "    total_num_pairs = (positive_pairs_indices.shape[0]+negative_pairs_indices.shape[0]) * num_users \n",
    "    if num_pair_limit < total_num_pairs: \n",
    "        limit_ratio = num_pair_limit/total_num_pairs\n",
    "    else:\n",
    "        limit_ratio=1\n",
    "            \n",
    "    \n",
    "    # neg_fraction =  ((total_samples_per_user-1) / (total_samples_per_user * (num_users-1)))  # works accurate\n",
    "    neg_fraction =  1 / (num_users-1)  # works accurate\n",
    "    # neg_fraction =  (total_samples_per_user-1) / (total_samples_per_user * (num_users)) # works accurate\n",
    "    for owner_key in tqdm(user_id_list):\n",
    "        # debugging was: X_exp2_dic is not an array, but needs to be an array if we want integer indexing. This can be changed in utility_functions; however, need to modify other code. \n",
    "        # print(type(positive_left))\n",
    "        # print(len(X_exp2_dic[owner_key]))\n",
    "        # print(np.array(X_exp2_dic[owner_key])[positive_left])\n",
    "        np.random.seed(SEED + owner_key)\n",
    "        # transform_user_windows(X_exp_unknown_df_dict[unknown_user], fitted_raw_scaler_dict[owner_key])\n",
    "        u1_array = np.array(X_exp2_dic[owner_key])\n",
    "        u2_array = np.array(transform_user_windows(X_exp1_dic[owner_key], fitted_raw_scaler_dict[owner_key]))\n",
    "        \n",
    "        # print(total_samples_per_user)\n",
    "        # print(positive_right.shape)\n",
    "        # print(negative_right.shape)\n",
    "        # quick fix for user 47\n",
    "        if owner_key == 19:\n",
    "            \n",
    "            u1_array = np.concatenate([u1_array, u1_array[np.random.choice(u1_array.shape[0], size=total_samples_per_user-u1_array.shape[0], replace=False)]], axis=0)#55-47\n",
    "            u2_array = np.concatenate([u2_array, u2_array[np.random.choice(u2_array.shape[0], size=total_samples_per_user-u2_array.shape[0], replace=False)]], axis=0)\n",
    "                \n",
    "        # print(u1_array.shape)\n",
    "        # print(u2_array.shape)\n",
    "        # try:\n",
    "        # print(f\"owner_key: {owner_key}, u1_array.shape: {u1_array.shape}, u2_array.shape: {u2_array.shape}\")\n",
    "        positive_pairs_dic[owner_key] = np.array(list(zip(u1_array[positive_left], u2_array[positive_right])))\n",
    "        # except:\n",
    "        #     print(owner_key)\n",
    "        # print(f\"positive_pairs per user: {positive_pairs_dic[owner_key].shape[0]}\")\n",
    "        new_pos_size = int(np.round(limit_ratio * positive_pairs_dic[owner_key].shape[0]))\n",
    "        positive_pairs_dic[owner_key] = positive_pairs_dic[owner_key][np.random.choice(\n",
    "                    positive_pairs_dic[owner_key].shape[0], size=new_pos_size, replace=False), :]\n",
    "        \n",
    "\n",
    "        for u2_key in user_id_list:\n",
    "            # 47 55\n",
    "            np.random.seed(SEED + u2_key)\n",
    "            # should i eliminate less informative pairs like (w_n, w_m) vs (w_m, w_n)?\n",
    "            if u2_key != owner_key:\n",
    "                u2_array = np.array(transform_user_windows(X_exp1_dic[u2_key], fitted_raw_scaler_dict[owner_key]))\n",
    "                \n",
    "                # quick fix for user 47\n",
    "                if u2_key == 19:\n",
    "                    u2_array = np.concatenate([u2_array, u2_array[np.random.choice(u2_array.shape[0], size=u1_array.shape[0]-u2_array.shape[0], replace=False)]], axis=0)#55-47,\n",
    "                \n",
    "                \n",
    "                # print(u1_array.shape)\n",
    "                # print(u2_array.shape)\n",
    "                # print(f\"u2_key {u2_key}, u1_array.shape: {u1_array.shape}, u2_array.shape: {u2_array.shape}\")\n",
    "                negative_pairs_dic[(owner_key, u2_key)] = np.array(list(zip(u1_array[negative_left], u2_array[negative_right])))\n",
    "                # print(f\"neg_pairs per user: {negative_pairs_dic[(owner_key, u2_key)].shape[0]}\")\n",
    "                \n",
    "                new_neg_size = int(np.round(negative_pairs_dic[(owner_key, u2_key)].shape[0] * neg_fraction * limit_ratio))\n",
    "                negative_pairs_dic[(owner_key, u2_key)] = negative_pairs_dic[(owner_key, u2_key)][np.random.choice(\n",
    "                    negative_pairs_dic[(owner_key, u2_key)].shape[0], size=new_neg_size, replace=False), :]\n",
    "    # print(new_pos_size)\n",
    "    # print(new_neg_size)\n",
    "    return {\"positive_pairs_dic\": positive_pairs_dic, \"negative_pairs_dic\": negative_pairs_dic}\n",
    "\n",
    "\n",
    "def get_pos_array(X_dic):\n",
    "    \n",
    "    return np.concatenate([X_dic[key] for key in X_dic])\n",
    "\n",
    "def get_neg_array(X_dic):\n",
    "    \n",
    "    return np.concatenate([X_dic[key] for key in X_dic])\n",
    "\n",
    "def prep_X_y_pair(X_exp2_dic, X_exp1_dic, user_id_list, fitted_raw_scaler_dict=None, num_pair_limit = 50000):\n",
    "    \n",
    "    X_dic = generate_pairs(X_exp2_dic, X_exp1_dic, user_id_list, fitted_raw_scaler_dict=fitted_raw_scaler_dict, num_pair_limit=num_pair_limit)\n",
    "    # print(X_dic['negative_pairs_dic'].keys())\n",
    "    pos_X, neg_X = get_pos_array(X_dic['positive_pairs_dic']), get_neg_array(X_dic['negative_pairs_dic'])\n",
    "    \n",
    "    pos_y = np.repeat(1., pos_X.shape[0]).reshape((pos_X.shape[0], 1))\n",
    "    neg_y = np.repeat(0., neg_X.shape[0]).reshape((neg_X.shape[0], 1))\n",
    "    \n",
    "    pos_X_left = pos_X[:, 0, :, :]\n",
    "    pos_X_right = pos_X[:, 1, :, :]\n",
    "    \n",
    "    neg_X_left = neg_X[:, 0, :, :]\n",
    "    neg_X_right = neg_X[:, 1, :, :]\n",
    "    \n",
    "    X_left = np.concatenate([pos_X_left, neg_X_left])\n",
    "    X_right = np.concatenate([pos_X_right, neg_X_right])\n",
    "    \n",
    "    X = [X_left.astype(\"float32\"), X_right.astype(\"float32\")]\n",
    "    y = np.concatenate([pos_y, neg_y]).astype(\"float32\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# coppied and modified to get optimizer: https://github.com/keras-team/keras/blob/v2.11.0/keras/callbacks.py#L2905-L3042\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "class MyReduceLROnPlateau(Callback):\n",
    "    \"\"\"Reduce learning rate when a metric has stopped improving.\n",
    "    Models often benefit from reducing the learning rate by a factor\n",
    "    of 2-10 once learning stagnates. This callback monitors a\n",
    "    quantity and if no improvement is seen for a 'patience' number\n",
    "    of epochs, the learning rate is reduced.\n",
    "    Example:\n",
    "    ```python\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                  patience=5, min_lr=0.001)\n",
    "    model.fit(X_train, Y_train, callbacks=[reduce_lr])\n",
    "    ```\n",
    "    Args:\n",
    "        monitor: quantity to be monitored.\n",
    "        factor: factor by which the learning rate will be reduced.\n",
    "          `new_lr = lr * factor`.\n",
    "        patience: number of epochs with no improvement after which learning rate\n",
    "          will be reduced.\n",
    "        verbose: int. 0: quiet, 1: update messages.\n",
    "        mode: one of `{'auto', 'min', 'max'}`. In `'min'` mode,\n",
    "          the learning rate will be reduced when the\n",
    "          quantity monitored has stopped decreasing; in `'max'` mode it will be\n",
    "          reduced when the quantity monitored has stopped increasing; in\n",
    "          `'auto'` mode, the direction is automatically inferred from the name\n",
    "          of the monitored quantity.\n",
    "        min_delta: threshold for measuring the new optimum, to only focus on\n",
    "          significant changes.\n",
    "        cooldown: number of epochs to wait before resuming normal operation\n",
    "          after lr has been reduced.\n",
    "        min_lr: lower bound on the learning rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.1,\n",
    "        patience=10,\n",
    "        verbose=0,\n",
    "        mode=\"auto\",\n",
    "        min_delta=1e-4,\n",
    "        cooldown=0,\n",
    "        min_lr=0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.monitor = monitor\n",
    "        if factor >= 1.0:\n",
    "            raise ValueError(\n",
    "                \"ReduceLROnPlateau does not support \"\n",
    "                f\"a factor >= 1.0. Got {factor}\"\n",
    "            )\n",
    "        if \"epsilon\" in kwargs:\n",
    "            min_delta = kwargs.pop(\"epsilon\")\n",
    "            logging.warning(\n",
    "                \"`epsilon` argument is deprecated and \"\n",
    "                \"will be removed, use `min_delta` instead.\"\n",
    "            )\n",
    "        self.factor = factor\n",
    "        self.min_lr = min_lr\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.cooldown = cooldown\n",
    "        self.cooldown_counter = 0  # Cooldown counter.\n",
    "        self.wait = 0\n",
    "        self.best = 0\n",
    "        self.mode = mode\n",
    "        self.monitor_op = None\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Resets wait counter and cooldown counter.\"\"\"\n",
    "        if self.mode not in [\"auto\", \"min\", \"max\"]:\n",
    "            logging.warning(\n",
    "                \"Learning rate reduction mode %s is unknown, \"\n",
    "                \"fallback to auto mode.\",\n",
    "                self.mode,\n",
    "            )\n",
    "            self.mode = \"auto\"\n",
    "        if self.mode == \"min\" or (\n",
    "            self.mode == \"auto\" and \"acc\" not in self.monitor\n",
    "        ):\n",
    "            self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)\n",
    "            self.best = np.Inf\n",
    "        else:\n",
    "            self.monitor_op = lambda a, b: np.greater(a, b + self.min_delta)\n",
    "            self.best = -np.Inf\n",
    "        self.cooldown_counter = 0\n",
    "        self.wait = 0\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self._reset()\n",
    "\n",
    "    def on_epoch_end(self, epoch, model, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs[\"lr\"] = K.get_value(model.optimizer.lr)\n",
    "        # print(logs)\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            logging.warning(\n",
    "                \"Learning rate reduction is conditioned on metric `%s` \"\n",
    "                \"which is not available. Available metrics are: %s\",\n",
    "                self.monitor,\n",
    "                \",\".join(list(logs.keys())),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            if self.in_cooldown():\n",
    "                self.cooldown_counter -= 1\n",
    "                self.wait = 0\n",
    "\n",
    "            if self.monitor_op(current, self.best):\n",
    "                self.best = current\n",
    "                self.wait = 0\n",
    "            elif not self.in_cooldown():\n",
    "                self.wait += 1\n",
    "                if self.wait >= self.patience:\n",
    "                    old_lr = K.get_value(model.optimizer.lr)\n",
    "                    if old_lr > np.float32(self.min_lr):\n",
    "                        new_lr = old_lr * self.factor\n",
    "                        new_lr = max(new_lr, self.min_lr)\n",
    "                        K.set_value(model.optimizer.lr, new_lr)\n",
    "                        if self.verbose > 0:\n",
    "                            print(\n",
    "                                f\"\\nEpoch {epoch +1}: \"\n",
    "                                \"ReduceLROnPlateau reducing \"\n",
    "                                f\"learning rate to {new_lr}.\"\n",
    "                            )\n",
    "                        self.cooldown_counter = self.cooldown\n",
    "                        self.wait = 0\n",
    "\n",
    "    def in_cooldown(self):\n",
    "        return self.cooldown_counter > 0\n",
    "\n",
    "    \n",
    "    \n",
    "class MetricsCallback(Callback):\n",
    "    \"\"\"\n",
    "    Custom Keras Callback function.\n",
    "    \n",
    "    Used to predict and plot distances for positive and negative pairs\n",
    "    after each n-th epoch, along with some 'classification' metrics. \n",
    "    'Classification' here means to ability to distinguish between positive \n",
    "    and negative pairs using a threshold for the distance.\n",
    "    \n",
    "    Arguments:\n",
    "        payload {tuple}           -- Datasets used for evaluation: (X_valid, y_valid, X_train, y_train)\n",
    "        epoch_evaluate_freq {int} -- Frequency for evaluation. After every n-th epoch, \n",
    "                                     the results are evaluated and printed\n",
    "        save_plots {boolean}      -- Do you want to save plots as PDF? Path is configured via global\n",
    "                                     parameter REPORT_PATH.\n",
    "    \"\"\"\n",
    "    def __init__(self, payload, epoch_evaluate_freq=1, loss_record_dict={}, metric_record_dict={}, save_plots=False, \n",
    "                 plot_pca=False, print_interm_epochs=True, early_stoping=False, ReduceLROnPlateau_args=None, optimal_lr_epoch_dict=None, lr_epoch_log_dict={}):\n",
    "        # super(MetricsCallback, self).__init__()\n",
    "        \n",
    "        self.lr_epoch_log_dict=lr_epoch_log_dict\n",
    "        \n",
    "        self.optimal_lr_epoch_dict=optimal_lr_epoch_dict\n",
    "            \n",
    "        \n",
    "        self.ReduceLROnPlateau_args=ReduceLROnPlateau_args\n",
    "        self.valid_metrics={\"roc_val\", \"eer_val\", \"thres\", \"acc_val\", \"f1_val\"}\n",
    "        if self.ReduceLROnPlateau_args != None:\n",
    "            ReduceLROnPlateau_mode={\"val_loss\": \"min\", \"roc_val\": \"max\", \"eer_val\": \"min\", \"thres\": \"min\", \"acc_val\": \"max\", \"f1_val\": \"max\"}\n",
    "            self.reduce_lr = MyReduceLROnPlateau(monitor=ReduceLROnPlateau_args['mointored_metric'], factor=ReduceLROnPlateau_args[\"factor\"], \n",
    "                                               patience=ReduceLROnPlateau_args[\"patience\"], verbose=ReduceLROnPlateau_args[\"verbose\"], \n",
    "                                               min_lr=ReduceLROnPlateau_args[\"min_lr\"], mode=ReduceLROnPlateau_mode[ReduceLROnPlateau_args['mointored_metric']])\n",
    "\n",
    "        self.X_valid, self.y_valid, self.X_train, self.y_train = payload\n",
    "        self.save_plots = save_plots\n",
    "        self.epoch_evaluate_freq = epoch_evaluate_freq\n",
    "        self.loss_record_dict = loss_record_dict\n",
    "        self.metric_record_dict = metric_record_dict\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "        self.plot_pca = plot_pca\n",
    "        self.print_interm_epochs = print_interm_epochs\n",
    "        self.early_stoping = early_stoping\n",
    "        \n",
    "        self.metric_record_dict['roc_val'] = {\"Train\": [], \"Valid\": []}\n",
    "        self.metric_record_dict['eer_val'] = {\"Train\": [], \"Valid\": []}\n",
    "        self.metric_record_dict['thres'] = {\"Train\": [], \"Valid\": []}\n",
    "        self.metric_record_dict['acc_val'] = {\"Train\": [], \"Valid\": []}\n",
    "        self.metric_record_dict['f1_val'] = {\"Train\": [], \"Valid\": []}\n",
    "        \n",
    "        # Do we have train and valid set?\n",
    "        self.sets = []\n",
    "        if self.X_train:\n",
    "            self.sets.append([self.X_train, self.y_train, \"Train\"])\n",
    "        if self.X_valid:\n",
    "            self.sets.append([self.X_valid, self.y_valid, \"Valid\"])\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "\n",
    "        print(32 * \"=\" + f\"[ Initial State ]\" + 32 * \"=\", end=\"\")\n",
    "        for X, y, desc in self.sets:\n",
    "            self.evaluate(X, y, logs, desc, -1)\n",
    "            \n",
    "        \n",
    "\n",
    "        if self.plot_pca:\n",
    "            deep_feature_model = Model(\n",
    "                inputs=self.model.layers[0].get_input_at(0),  # get_layer(\"left_inputs\").input,\n",
    "                outputs=self.model.get_layer(\"basemodel\").get_output_at(0),\n",
    "                )\n",
    "            deep_feature_model.summary()\n",
    "            deep_features_test = None\n",
    "            for X, y, subj in samples_test:  \n",
    "                \n",
    "                # if 2d, maybe need to import P\n",
    "                # if P.model_variant == \"2d\":\n",
    "                #     X = X.reshape((*X.shape, 1))\n",
    "                pred = deep_feature_model.predict(X)\n",
    "                df_features = pd.DataFrame(pred)\n",
    "                df_features[\"subject\"] = subj\n",
    "                deep_features_test = pd.concat([deep_features_test, df_features])\n",
    "            \n",
    "            plot_pca(deep_features_test)\n",
    "            \n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        print(32 * \"=\" + f\"[ Final State ]\" + 32 * \"=\", end=\"\")\n",
    "        for X, y, desc in self.sets:\n",
    "            self.evaluate(X, y, logs, desc, -1)\n",
    "            \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        \n",
    "        optimizer = self.model.optimizer\n",
    "        if self.optimal_lr_epoch_dict != None:\n",
    "\n",
    "            K.set_value(optimizer.lr, self.optimal_lr_epoch_dict[epoch])\n",
    "            print(f\"Learning rate changed to {self.optimal_lr_epoch_dict[epoch]} for epoch {epoch}\")\n",
    "            \n",
    "        else:\n",
    "            self.lr_epoch_log_dict[epoch]=K.get_value(optimizer.lr)\n",
    "            print(f\"Logged learning rate of {self.lr_epoch_log_dict[epoch]} for epoch {epoch}\")\n",
    "            \n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if self.print_interm_epochs:\n",
    "            print(32 * \"=\" + f\"[   Epoch {epoch}   ]\" + 32 * \"=\", end=\"\")\n",
    "            if epoch % self.epoch_evaluate_freq == 0:  # Evaluate only every n-th epoch\n",
    "                for X, y, desc in self.sets:\n",
    "                    self.evaluate(X, y, logs, desc, epoch)\n",
    "            else:\n",
    "                print(f\"\\n{ ', '.join([k + ': ' + f'{v:.3f}' for k,v in logs.items()]) }\")\n",
    "            \n",
    "\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "    \n",
    "        \n",
    "        if 'val_loss' in logs:\n",
    "            if self.early_stoping and len(self.loss_record_dict['val_loss'])>1 and self.loss_record_dict['val_loss'][-1] < logs['val_loss']:\n",
    "                self.model.stop_training = True\n",
    "                \n",
    "            else:\n",
    "                self.loss_record_dict['val_loss'].append(logs['val_loss'])\n",
    "\n",
    "                self.loss_record_dict['loss'].append(logs['loss'])\n",
    "\n",
    "                for X, y, desc in self.sets:\n",
    "                    self.evaluate(X, y, logs, desc, epoch, print_metric=False, save_metric_record_dict=True)\n",
    "            \n",
    "            # print(logs)\n",
    "            if self.ReduceLROnPlateau_args != None:\n",
    "                if self.ReduceLROnPlateau_args['mointored_metric'] == \"val_loss\":\n",
    "                    self.reduce_lr.on_epoch_end(epoch, self.model, logs)\n",
    "\n",
    "                elif self.ReduceLROnPlateau_args['mointored_metric'] in self.valid_metrics:\n",
    "                    mointored_metric_dict={self.ReduceLROnPlateau_args['mointored_metric']: self.metric_record_dict[self.ReduceLROnPlateau_args['mointored_metric']][\"Valid\"][-1]}\n",
    "                    self.reduce_lr.on_epoch_end(epoch, self.model, mointored_metric_dict)\n",
    "\n",
    "                else:\n",
    "                    raise Exception(\"invalid mointored metric\")\n",
    "            \n",
    "                    \n",
    "        else:\n",
    "            self.loss_record_dict['loss'].append(logs['loss'])\n",
    "\n",
    "            for X, y, desc in self.sets:\n",
    "                self.evaluate(X, y, logs, desc, epoch, print_metric=False, save_metric_record_dict=True)\n",
    "                \n",
    "        \n",
    "\n",
    "            \n",
    "    def evaluate(self, X, y, logs, desc, epoch, print_metric=True, save_metric_record_dict=False):\n",
    "        # Predict\n",
    "        y_score = self.model.predict(X)\n",
    "        y_score_neg = y_score * -1  # lower distance means closer to positive class\n",
    "\n",
    "        # Calc Metrics\n",
    "        roc_val = metrics.roc_auc_score(y, y_score_neg)\n",
    "        eer_val, thres = utils_eer(y, y_score_neg, True)\n",
    "        y_pred = np.where(y_score_neg > thres, 1, 0)\n",
    "        acc_val = metrics.accuracy_score(y, y_pred)\n",
    "        f1_val = metrics.f1_score(y, y_pred)\n",
    "        \n",
    "        if print_metric:\n",
    "            print(\n",
    "                f\"\\n{desc.upper()}: roc_auc: {roc_val:.4f}, \"\n",
    "                + f\"eer: {eer_val:.4f}, thres: {thres*-1:.4f} => \"\n",
    "                + f\"acc: {acc_val:.4f}, f1: {f1_val:.4f}\\n\"\n",
    "                + f\"{ ', '.join([k + ': ' + f'{v:.3f}' for k,v in logs.items()]) }\"\n",
    "            )\n",
    "\n",
    "        if save_metric_record_dict:\n",
    "            self.metric_record_dict['roc_val'][desc].append(roc_val)\n",
    "            self.metric_record_dict['eer_val'][desc].append(eer_val)\n",
    "            self.metric_record_dict['thres'][desc].append(float(thres))\n",
    "            self.metric_record_dict['acc_val'][desc].append(acc_val)\n",
    "            self.metric_record_dict['f1_val'][desc].append(f1_val)\n",
    "        \n",
    "        # Plot distances\n",
    "        mask = np.where(y == 1, True, False)\n",
    "        dist_positive = y_score[mask]\n",
    "        dist_negative = y_score[~mask]\n",
    "        \n",
    "        \n",
    "        if self.plot_pca:\n",
    "            # Extract one of the child networks\n",
    "            deep_feature_model = Model(\n",
    "                inputs=self.model.layers[0].get_input_at(0),  # get_layer(\"left_inputs\").input,\n",
    "                outputs=self.model.get_layer(\"basemodel\").get_output_at(0),\n",
    "            )\n",
    "            deep_feature_model.summary()\n",
    "            deep_features_test = None\n",
    "            for X, y, subj in samples_test:  \n",
    "                # if 2d, maybe need to import P\n",
    "                # if P.model_variant == \"2d\":\n",
    "                #     X = X.reshape((*X.shape, 1))\n",
    "                pred = deep_feature_model.predict(X)\n",
    "                df_features = pd.DataFrame(pred)\n",
    "                df_features[\"subject\"] = subj\n",
    "                deep_features_test = pd.concat([deep_features_test, df_features])\n",
    "\n",
    "            plot_pca(deep_features_test)\n",
    "        \n",
    "        #plt = utils_plot_distance_hist(\n",
    "        #    dist_positive, dist_negative, thres * -1, desc=desc, fig_size=(12, 2), margin=P.margin\n",
    "        #)\n",
    "\n",
    "#         if self.save_plots:\n",
    "#             utils_save_plot(\n",
    "#                 plt,\n",
    "#                 REPORT_PATH\n",
    "#                 / f\"buech2019-siamese-{P.name.lower()}-epoch-{epoch+1}-{desc.lower()}.pdf\",\n",
    "#             )\n",
    "\n",
    "        # plt.show()\n",
    "    \n",
    "def tune_stream(data_dict, arg_dict, stream_key, pool_dict, pool_key, create_model_func, tuning_metric, tuning_metric_val_dict, verbose=0):\n",
    "    '''\n",
    "    return the best value found after one round of hyper paramteer tuning of a stream\n",
    "    \n",
    "    arg_dict is the dict of arguments needed for training\n",
    "    stream key is the dict key that specifies the stream from the arg_dict we want to tune\n",
    "    pool dict is the dict containing possible values for all hyperparameters of arg_dict\n",
    "    pool_key is a key for pool dict that specifies the stream key value type so we can choose appropriate \n",
    "    values to tune\n",
    "    create_model_func is the function that that takes an arg_dic tand turnes it into an architectuer.\n",
    "    '''\n",
    "    best_params_lst=[]\n",
    "    for stream_idx in range(len(arg_dict[stream_key])):\n",
    "        for idx in range(len(arg_dict[stream_key][stream_idx])):\n",
    "            result=find_param_ranking(data_dict, arg_dict=arg_dict, dict_key=stream_key, \n",
    "                                      pool_dict=pool_dict, pool_key=pool_key, \n",
    "                                      create_model_func=create_model_func, \n",
    "                                      tuning_metric=tuning_metric, idx=idx, \n",
    "                                      stream_idx=stream_idx, verbose=verbose)\n",
    "            arg_dict = result[\"improved_arg_dict\"]\n",
    "            best_params_lst.append(result[\"best_params\"])\n",
    "    \n",
    "    tuning_metric_val_dict[stream_key]=best_params_lst\n",
    "    return arg_dict, tuning_metric_val_dict\n",
    "\n",
    "def tune_param(data_dict, arg_dict, param_key, pool_dict, pool_key, create_model_func, tuning_metric, tuning_metric_val_dict, verbose=0):\n",
    "    \"\"\"\n",
    "    return the best value found after one round of hyper parameter tuning of a param\n",
    "    \n",
    "    arg_dict is the dict of arguments needed for training\n",
    "    param_key is the dict key that specifies the param from the arg_dict we want to tune\n",
    "    pool dict is the dict containing possible values for all hyperparameters of arg_dict\n",
    "    pool_key is a key for pool dict that specifies the param_key value type so we can choose appropriate \n",
    "    values to tune\n",
    "    create_model_func is the function that that takes an arg_dic tand turnes it into an architectuer.\n",
    "    \"\"\"\n",
    "    best_params_lst=[]\n",
    "    for idx in range(len(arg_dict[param_key])):\n",
    "        result=find_param_ranking(data_dict, arg_dict=arg_dict, dict_key=param_key, \n",
    "                                  pool_dict=pool_dict, pool_key=pool_key, \n",
    "                                  create_model_func=create_model_func, \n",
    "                                  tuning_metric=tuning_metric, idx=idx, \n",
    "                                  stream_idx=None, verbose=verbose)\n",
    "        \n",
    "        arg_dict = result[\"improved_arg_dict\"]\n",
    "        best_params_lst.append(result[\"best_params\"])\n",
    "        \n",
    "    tuning_metric_val_dict[param_key]=best_params_lst\n",
    "    return arg_dict, tuning_metric_val_dict\n",
    "\n",
    "def tune_dense_activations(data_dict, arg_dict, pool_dict, pool_key, create_model_func, tuning_metric, tuning_metric_val_dict, verbose=0):\n",
    "    \"\"\"\n",
    "    return the best value found after one round of hyper parameter tuning of a param\n",
    "    \n",
    "    arg_dict is the dict of arguments needed for training\n",
    "    pool dict is the dict containing possible values for all hyperparameters of arg_dict\n",
    "    pool_key is a key for pool dict that specifies the dense_activations value type so we can choose appropriate \n",
    "    values to tune\n",
    "    create_model_func is the function that that takes an arg_dic tand turnes it into an architectuer.\n",
    "    \"\"\"\n",
    "    best_params_lst=[]\n",
    "    for idx in range(len(arg_dict[\"dense_activations\"]) - 1): # do not tune the final sigmoid\n",
    "        result=find_param_ranking(data_dict, arg_dict=arg_dict, dict_key=\"dense_activations\", \n",
    "                                  pool_dict=pool_dict, pool_key=pool_key, \n",
    "                                  create_model_func=create_model_func, \n",
    "                                  tuning_metric=tuning_metric, idx=idx, \n",
    "                                  stream_idx=None, verbose=verbose)\n",
    "        \n",
    "        arg_dict = result[\"improved_arg_dict\"]\n",
    "        best_params_lst.append(result[\"best_params\"])\n",
    "        \n",
    "    tuning_metric_val_dict['dense_activations']=best_params_lst\n",
    "    return arg_dict, tuning_metric_val_dict\n",
    "\n",
    "def tune_training_params(data_dict, arg_dict, param_key, pool_dict, pool_key, create_model_func, tuning_metric, tuning_metric_val_dict, verbose=0):\n",
    "    \"\"\"\n",
    "    return the best value found after one round of hyper parameter tuning of a training param\n",
    "    training params are as follows: \n",
    "    batch_size, loss function, loss function args, optimizer function, optimizer function args\n",
    "    \n",
    "    arg_dict is the dict of arguments needed for training\n",
    "    param_key is the dict key that specifies the param from the arg_dict we want to tune\n",
    "    pool dict is the dict containing possible values for all hyperparameters of arg_dict\n",
    "    pool_key is a key for pool dict that specifies the param_key value type so we can choose appropriate \n",
    "    values to tune\n",
    "    create_model_func is the function that that takes an arg_dic tand turnes it into an architectuer.\n",
    "    \"\"\"\n",
    "    best_params_lst=[]\n",
    "    result=find_param_ranking(data_dict, arg_dict=arg_dict, dict_key=param_key, \n",
    "                              pool_dict=pool_dict, pool_key=pool_key, \n",
    "                              create_model_func=create_model_func, \n",
    "                              tuning_metric=tuning_metric, idx=None, \n",
    "                              stream_idx=None, verbose=verbose)\n",
    "\n",
    "    arg_dict = result[\"improved_arg_dict\"]\n",
    "    best_params_lst.append(result[\"best_params\"])\n",
    "    \n",
    "    tuning_metric_val_dict[param_key]=best_params_lst\n",
    "    return arg_dict, tuning_metric_val_dict\n",
    "\n",
    "def find_param_ranking(data_dict, arg_dict, dict_key, pool_dict, pool_key, create_model_func, tuning_metric, idx=None, stream_idx=None, verbose=0, early_stoping=False):\n",
    "\n",
    "    X_valid, y_valid, X_train, y_train = data_dict[\"X_valid\"], data_dict[\"y_valid\"], data_dict[\"X_train\"], data_dict[\"y_train\"]\n",
    "    param_metric_performance_dict = {}\n",
    "    \n",
    "    pool=pool_dict[pool_key]\n",
    "    for pool_param in pool:\n",
    "        # keep the initial state fixed\n",
    "        tf.random.set_seed(SEED)\n",
    "        \n",
    "        new_arg_dict = arg_dict.copy()\n",
    "        if stream_idx != None:\n",
    "            new_arg_dict[dict_key][stream_idx][idx] = pool_param\n",
    "        elif idx != None:\n",
    "            new_arg_dict[dict_key][idx] = pool_param\n",
    "        else:\n",
    "            new_arg_dict[dict_key] = pool_param\n",
    "            \n",
    "        # P.margin = new_arg_dict[\"contrastive_loss_margin\"] obsolete\n",
    "        print(new_arg_dict)\n",
    "        loss_record_dict = {'loss': [], 'val_loss': []}\n",
    "        metric_record_dict = {}\n",
    "        model = create_model_func(new_arg_dict)\n",
    "        \n",
    "        ReduceLROnPlateau_err_mode={}\n",
    "        ReduceLROnPlateau_args={'mointored_metric': tuning_metric, \"factor\": 0.2, \"patience\": 2, \"verbose\": 1, \"min_lr\": 1e-6}\n",
    "\n",
    "        # Train\n",
    "        history = model.fit(\n",
    "            x=X_train,\n",
    "            y=y_train,\n",
    "            batch_size=new_arg_dict[\"batch_size\"],\n",
    "            epochs=30,\n",
    "            verbose=verbose,\n",
    "            validation_data=(X_valid, y_valid),\n",
    "            callbacks=[MetricsCallback((X_valid, y_valid, X_train, y_train), epoch_evaluate_freq=5, \n",
    "                                       loss_record_dict=loss_record_dict, metric_record_dict=metric_record_dict, \n",
    "                                       save_plots=True, print_interm_epochs=False, early_stoping=early_stoping,\n",
    "                                      ReduceLROnPlateau_args=ReduceLROnPlateau_args)],\n",
    "        )\n",
    "        print(loss_record_dict)\n",
    "        print(\"Training History:\")\n",
    "        # loss_fig = utils_plot_training_loss(loss_record_dict)\n",
    "        if early_stoping:\n",
    "            param_metric_performance_dict[pool_param]={\n",
    "                'val_loss': loss_record_dict['val_loss'][-1], \n",
    "                'roc_val': metric_record_dict['roc_val']['Valid'][-1], \n",
    "                'eer_val': metric_record_dict['eer_val']['Valid'][-1], \n",
    "                'thres': metric_record_dict['thres']['Valid'][-1], \n",
    "                'acc_val': metric_record_dict['acc_val']['Valid'][-1], \n",
    "                'f1_val': metric_record_dict['f1_val']['Valid'][-1],\n",
    "                }\n",
    "            \n",
    "        else: \n",
    "            param_metric_performance_dict[pool_param]={\n",
    "                'val_loss': min(loss_record_dict['val_loss']), \n",
    "                'roc_val': max(metric_record_dict['roc_val']['Valid']), \n",
    "                'eer_val': min(metric_record_dict['eer_val']['Valid']), \n",
    "                'thres': min(metric_record_dict['thres']['Valid']), \n",
    "                'acc_val': max(metric_record_dict['acc_val']['Valid']), \n",
    "                'f1_val': max(metric_record_dict['f1_val']['Valid']),\n",
    "                }\n",
    "    \n",
    "    print(param_metric_performance_dict)\n",
    "    best_params = get_best_parameters(param_metric_performance_dict)\n",
    "    print((\"{}\\n\"*len(best_params)).format(*best_params.items()))\n",
    "    improved_arg_dict = arg_dict.copy()\n",
    "    \n",
    "    if stream_idx != None:\n",
    "        improved_arg_dict[dict_key][stream_idx][idx] = best_params['best_'+tuning_metric][\"param\"]\n",
    "    elif idx != None:\n",
    "        improved_arg_dict[dict_key][idx] = best_params['best_'+tuning_metric][\"param\"]\n",
    "    else:\n",
    "        improved_arg_dict[dict_key] = best_params['best_'+tuning_metric][\"param\"]\n",
    "    \n",
    "    return {\"improved_arg_dict\": improved_arg_dict, \"best_params\": best_params} #, param_ranking # not sure if returning this is useful for now\n",
    "\n",
    "# def log_metrics_tuning(arg_dict, best_params_dict):\n",
    "#     \"\"\"\n",
    "#     log best params through tuning in arg_dict\n",
    "#     \"\"\"\n",
    "#     best_params = {\n",
    "#     'best_val_loss': {\"param\": best_val_loss[0], \"value\": best_val_loss[1]['val_loss']}, \n",
    "#     'best_roc_val': {\"param\": best_roc_val[0], \"value\": best_roc_val[1]['roc_val']},\n",
    "#     'best_eer_val': {\"param\": best_eer_val[0], \"value\": best_eer_val[1]['eer_val']},\n",
    "#     'best_thres': {\"param\": best_thres[0], \"value\": best_thres[1]['thres']},\n",
    "#     'best_acc_val': {\"param\": best_acc_val[0], \"value\": best_acc_val[1]['acc_val']},\n",
    "#     'best_f1_val': {\"param\": best_f1_val[0], \"value\": best_f1_val[1]['f1_val']},\n",
    "#     }\n",
    "def get_best_parameters(param_metric_performance_dict):\n",
    "    '''\n",
    "    calculate and return best valiation parameters given the param_metric_dict.\n",
    "    param_metric_dict is the dicitonary containing the parameters with their correspongin performance\n",
    "    '''\n",
    "    lst = list(param_metric_performance_dict.items())\n",
    "    # sort all in descending order, first element is max and last element is min\n",
    "    # best_val_loss = sorted(lst, key = lambda x: x[1]['val_loss'], reverse=True)[-1] # min\n",
    "    # best_roc_val = sorted(lst, key = lambda x: x[1]['roc_val'], reverse=True)[0] # max\n",
    "    # best_eer_val = sorted(lst, key = lambda x: x[1]['eer_val'], reverse=True)[-1] # min\n",
    "    # best_thres = sorted(lst, key = lambda x: x[1]['thres'], reverse=True)[-1] # min\n",
    "    # best_acc_val = sorted(lst, key = lambda x: x[1]['acc_val'], reverse=True)[0] # max\n",
    "    # best_f1_val = sorted(lst, key = lambda x: x[1]['f1_val'], reverse=True)[0] # max\n",
    "    \n",
    "    best_val_loss = min(lst, key = lambda x: x[1]['val_loss']) # min\n",
    "    best_roc_val = max(lst, key = lambda x: x[1]['roc_val']) # max\n",
    "    best_eer_val = min(lst, key = lambda x: x[1]['eer_val']) # min\n",
    "    best_thres = min(lst, key = lambda x: x[1]['thres']) # min\n",
    "    best_acc_val = max(lst, key = lambda x: x[1]['acc_val']) # max\n",
    "    best_f1_val = max(lst, key = lambda x: x[1]['f1_val']) # max\n",
    "    \n",
    "    best_params = {\n",
    "        'best_val_loss': {\"param\": best_val_loss[0], \"value\": best_val_loss[1]['val_loss']}, \n",
    "        'best_roc_val': {\"param\": best_roc_val[0], \"value\": best_roc_val[1]['roc_val']},\n",
    "        'best_eer_val': {\"param\": best_eer_val[0], \"value\": best_eer_val[1]['eer_val']},\n",
    "        'best_thres': {\"param\": best_thres[0], \"value\": best_thres[1]['thres']},\n",
    "        'best_acc_val': {\"param\": best_acc_val[0], \"value\": best_acc_val[1]['acc_val']},\n",
    "        'best_f1_val': {\"param\": best_f1_val[0], \"value\": best_f1_val[1]['f1_val']},\n",
    "            }\n",
    "    \n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_module = 0\n",
    "if test_module:\n",
    "    # param_metric_performance_dict={}\n",
    "    param_metric_performance_dict={3: {'val_loss': 1, \n",
    "                                          'roc_val': 2, \n",
    "                                          'eer_val': 3, \n",
    "                                          'thres': 4, \n",
    "                                          'acc_val': 5, \n",
    "                                          'f1_val': 6,\n",
    "                                         },\n",
    "                                  4: {'val_loss': 6, \n",
    "                                          'roc_val': 5, \n",
    "                                          'eer_val': 4, \n",
    "                                          'thres': 3, \n",
    "                                          'acc_val': 2, \n",
    "                                          'f1_val': 1,\n",
    "                                         }\n",
    "                                  }\n",
    "\n",
    "    best_params = get_best_parameters(param_metric_performance_dict)\n",
    "    assert best_params['best_val_loss']['value'] == param_metric_performance_dict[3]['val_loss']\n",
    "    assert best_params['best_roc_val']['value'] == param_metric_performance_dict[4]['roc_val']\n",
    "    assert best_params['best_eer_val']['value'] == param_metric_performance_dict[3]['eer_val']\n",
    "    assert best_params['best_thres']['value'] == param_metric_performance_dict[4]['thres']\n",
    "    assert best_params['best_acc_val']['value'] == param_metric_performance_dict[3]['acc_val']\n",
    "    assert best_params['best_f1_val']['value'] == param_metric_performance_dict[3]['f1_val']\n",
    "\n",
    "    assert best_params['best_val_loss']['param'] == 3\n",
    "    assert best_params['best_roc_val']['param'] == 4\n",
    "    assert best_params['best_eer_val']['param'] == 3\n",
    "    assert best_params['best_thres']['param'] == 4\n",
    "    assert best_params['best_acc_val']['param'] == 3\n",
    "    assert best_params['best_f1_val']['param'] == 3\n",
    "    print(\"no err\")\n",
    "    # # rank_f = lambda \n",
    "    # # use lambda functions to sort and store\n",
    "    # lst = list(param_metric_performance_dict.items())\n",
    "    # print(max(lst, key = lambda x: x[1]['thres']))\n",
    "    # param_metric_performance_dict.items()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_pca_dataframe(Raw_X_exp_test_dic, session_id=None):\n",
    "    \n",
    "    columns=['X', 'subject', 'session', 'win_idx']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for subject_id in Raw_X_exp_test_dic:\n",
    "        win_idx = 0\n",
    "        for subject_window in Raw_X_exp_test_dic[subject_id]:\n",
    "            new_row = pd.DataFrame([[subject_window.to_numpy(), subject_id, session_id, win_idx]], columns=columns)\n",
    "            df = pd.concat([df, new_row])\n",
    "            win_idx +=1\n",
    "        \n",
    "            \n",
    "    return df\n",
    "   \n",
    "def prep_X_y_single(df):\n",
    "    X = np.stack(list(df[\"X\"].values))\n",
    "    y = df[\"label\"].values\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def transform_to_sample_by_subject(df):\n",
    "    sample_by_subject = []\n",
    "    df[\"label\"] = 1\n",
    "    for subj in df[\"subject\"].unique():\n",
    "        df_subj = df[df[\"subject\"] == subj]\n",
    "        X_sub, y_sub = prep_X_y_single(df_subj)\n",
    "        sample_by_subject.append((X_sub, y_sub, subj))\n",
    "    return sample_by_subject\n",
    "\n",
    "def plot_pca(df):\n",
    "    # PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    deep_transformed = pca.fit_transform(df.drop(columns=[\"subject\"]).values)\n",
    "\n",
    "    # Create df with data needed for chart only\n",
    "    df_viz = df.copy()\n",
    "    df_viz[\"PCA0\"] = deep_transformed[:, 0]\n",
    "    df_viz[\"PCA1\"] = deep_transformed[:, 1]\n",
    "    df_viz.drop(\n",
    "        columns=[c for c in df_viz.columns if c not in [\"PCA0\", \"PCA1\", \"subject\"]]\n",
    "    )\n",
    "\n",
    "    # Generate color index for every subject\n",
    "    df_viz[\"Subject\"] = pd.Categorical(df_viz[\"subject\"])\n",
    "    df_viz[\"colors\"] = df_viz[\"Subject\"].cat.codes\n",
    "\n",
    "    if len(df_viz[\"Subject\"].unique()) <= 10:\n",
    "        pal = sns.color_palette(\"tab10\")\n",
    "    else:\n",
    "        pal = sns.color_palette(\"tab20\")\n",
    "        \n",
    "    # Actual plot\n",
    "    fig = plt.figure(figsize=(10 / 1.5, 10 / 2), dpi=180)\n",
    "    sns.scatterplot(\n",
    "        x=\"PCA0\",\n",
    "        y=\"PCA1\",\n",
    "        data=df_viz,\n",
    "        hue=\"Subject\",\n",
    "        legend=\"full\",\n",
    "        # palette=pal,\n",
    "        s=5,\n",
    "        linewidth=0,\n",
    "        alpha=0.6,\n",
    "    )\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0, fontsize=5)\n",
    "    fig.tight_layout()\n",
    "    return plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MakeDeepXExpDic(X_exp1_train_dic, X_exp2_train_dic, fitted_scaler_ocsvm_exp2_train_dic, X_exp1_test_dic, X_exp2_test_dic, fitted_scaler_ocsvm_exp2_test_dic, deep_feature_model):\n",
    "    X_exp_train_dic = MakeDeepXExpDicOwner(X_exp2_train_dic, deep_feature_model, scaler_clip=True, scaler_type=\"MinMaxScaler\")\n",
    "    X_exp_train_dic = MakeDeepXExpDicUnknown(X_exp1_train_dic, deep_feature_model, X_exp_train_dic, fitted_raw_scaler_dict=fitted_scaler_ocsvm_exp2_train_dic)\n",
    "    X_exp_test_dic = MakeDeepXExpDicOwner(X_exp2_test_dic, deep_feature_model, scaler_clip=True, scaler_type=\"MinMaxScaler\")\n",
    "    X_exp_test_dic = MakeDeepXExpDicUnknown(X_exp1_test_dic, deep_feature_model, X_exp_test_dic, fitted_raw_scaler_dict=fitted_scaler_ocsvm_exp2_test_dic)\n",
    "    \n",
    "    return X_exp_train_dic, X_exp_test_dic\n",
    "\n",
    "def MakeWACAXExpDic(X_exp1_train_dic, X_exp2_train_dic, fitted_scaler_ocsvm_exp2_train_dic, X_exp1_test_dic, X_exp2_test_dic, fitted_scaler_ocsvm_exp2_test_dic):\n",
    "    X_exp_train_dic = MakeWACAXExpDicOwner(X_exp2_train_dic, scaler_clip=True, scaler_type=\"MinMaxScaler\")\n",
    "    X_exp_train_dic = MakeWACAXExpDicUnknown(X_exp1_train_dic, X_exp_train_dic, fitted_raw_scaler_dict=fitted_scaler_ocsvm_exp2_train_dic)\n",
    "    X_exp_test_dic = MakeWACAXExpDicOwner(X_exp2_test_dic, scaler_clip=True, scaler_type=\"MinMaxScaler\")\n",
    "    X_exp_test_dic = MakeWACAXExpDicUnknown(X_exp1_test_dic, X_exp_test_dic, fitted_raw_scaler_dict=fitted_scaler_ocsvm_exp2_test_dic)\n",
    "    \n",
    "    return X_exp_train_dic, X_exp_test_dic\n",
    "\n",
    "def combine_X_exp_dicts(deep_X_exp_dic, WACA_X_exp_dic):\n",
    "    \n",
    "    union_dic = {}\n",
    "    for user in deep_X_exp_dic:\n",
    "        union_dic[user] = {\"profile_windows\": np.concatenate([WACA_X_exp_dic[user][\"profile_windows\"], deep_X_exp_dic[user][\"profile_windows\"]], axis=1),\n",
    "                           'unknown_users_dict':{}}\n",
    "        \n",
    "        for unknown_user in deep_X_exp_dic[user]['unknown_users_dict']:\n",
    "            union_dic[user]['unknown_users_dict'][unknown_user] = np.concatenate([WACA_X_exp_dic[user][\"unknown_users_dict\"][unknown_user], \n",
    "                                                                               deep_X_exp_dic[user][\"unknown_users_dict\"][unknown_user]], axis=1)\n",
    "            \n",
    "    return union_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(Raw_X_exp2_train_dic.keys())\n",
    "# user_id_lst = list(Raw_X_exp2_train_dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# validation_pairs_lst = []\n",
    "# for user1_idx in range(len(user_id_lst)-1):\n",
    "#     validation_pairs_lst.append((user_id_lst[user1_idx], user_id_lst[user1_idx+1]))\n",
    "#     print(validation_pairs_lst)\n",
    "    \n",
    "# for user1, user2 in validation_pairs_lst:\n",
    "#     print(user1, user2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate_cv_split(valid_sets_lst[0], user_id_set, Raw_X_exp2_train_dic, Raw_X_exp1_train_dic, fitted_scaler_SNN_exp2_train_dic, num_pair_limit_train_2000, num_pair_limit_valid_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # valid_sets_lst = []\n",
    "# # for user1_idx in range(len(user_id_lst)-2):\n",
    "# #     valid_sets_lst.append((user_id_lst[user1_idx], user_id_lst[user1_idx+1], user_id_lst[user1_idx+2]))\n",
    "    \n",
    "# arr = [2, 3, 5, 7, 11, 13]\n",
    "# def make_valid_sets_lst(user_id_lst, k):\n",
    "#     valid_sets_lst=[]\n",
    "#     for i in range(0, len(user_id_lst)-k+1, k):\n",
    "#         valid_sets_lst.append(set(user_id_lst[i:i+k]))\n",
    "    \n",
    "#     return valid_sets_lst\n",
    "\n",
    "# make_valid_sets_lst(user_id_lst, 4)\n",
    "# valid_sets_lst = make_valid_sets_lst(user_id_lst, 5)\n",
    "# # key = list(valid_sets_lst[1])\n",
    "# # key.sort()\n",
    "# valid_sets_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "# user_id_array = list(range(10))\n",
    "# # Get the unique user_ids\n",
    "# unique_users = np.unique(user_id_array)\n",
    "\n",
    "\n",
    "# # Iterate through all possible pairs of users\n",
    "# # for user1, user2 in itertools.combinations(unique_users, 2):\n",
    "# #     # Select samples for the current pair of users\n",
    "# #     user1_samples = (user_id_array == user1)\n",
    "# #     user2_samples = (user_id_array == user2)\n",
    "    \n",
    "# #     print(user1, user2)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# import itertools\n",
    "\n",
    "# def generate_cv_split(valid_users_set, user_id_set, Raw_X_exp2_train_dic, Raw_X_exp1_train_dic, fitted_scaler_SNN_exp2_train_dic, num_pair_limit_train_2000, num_pair_limit_valid_2000):\n",
    "#     cv_dict = {}\n",
    "    \n",
    "#     cnn_valid_set = valid_users_set\n",
    "#     cnn_train_set = user_id_set - cnn_valid_set\n",
    "    \n",
    "#     cnn_train_exp2 = {key: Raw_X_exp2_train_dic[key] for key in cnn_train_set}\n",
    "#     cnn_train_exp1 = {key: Raw_X_exp1_train_dic[key] for key in cnn_train_set}\n",
    "#     cnn_valid_exp2 = {key: Raw_X_exp2_train_dic[key] for key in cnn_valid_set}\n",
    "#     cnn_valid_exp1 = {key: Raw_X_exp1_train_dic[key] for key in cnn_valid_set}\n",
    "    \n",
    "#     X_train, y_train = prep_X_y_pair(cnn_train_exp2, cnn_train_exp1, list(cnn_train_exp2.keys()), fitted_scaler_SNN_exp2_train_dic, num_pair_limit=num_pair_limit_train_2000)\n",
    "#     X_valid, y_valid = prep_X_y_pair(cnn_valid_exp2, cnn_valid_exp1, list(cnn_valid_exp2.keys()), fitted_scaler_SNN_exp2_train_dic, num_pair_limit=num_pair_limit_valid_2000)\n",
    "    \n",
    "#     key = list(valid_users_set)\n",
    "#     key.sort()\n",
    "#     key = tuple(key)\n",
    "#     cv_dict[key] = {}\n",
    "#     cv_dict[key][\"train\"] = X_train, y_train\n",
    "#     cv_dict[key][\"valid\"] = X_valid, y_valid\n",
    "    \n",
    "#     return cv_dict\n",
    "\n",
    "\n",
    "# num_pair_limit_train_2000 = 9182\n",
    "# num_pair_limit_valid_2000 = 5310\n",
    "# user_id_lst = list(Raw_X_exp2_train_dic.keys())\n",
    "# user_id_set = set(Raw_X_exp2_train_dic.keys())\n",
    "\n",
    "# valid_sets_lst = make_valid_sets_lst(user_id_lst, 5)\n",
    "\n",
    "# print(valid_sets_lst)\n",
    "# # user_id_array = {1,2,3}\n",
    "# results = Parallel(n_jobs=-1, verbose=100)(delayed(generate_cv_split)(valid_users_set, user_id_set, Raw_X_exp2_train_dic, Raw_X_exp1_train_dic, fitted_scaler_SNN_exp2_train_dic, \n",
    "#                                                    num_pair_limit_train_2000, num_pair_limit_valid_2000) for valid_users_set in valid_sets_lst)\n",
    "\n",
    "\n",
    "# results = {k: v for d in results for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# results[(5, 6, 7, 8, 9, 10)]['train'][0][0].shape\n",
    "# results.keys()\n",
    "# len(cnn_valid_exp1[list(cnn_valid_exp1.keys())[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def generate_negative_index_pairs(diagonal_size):\n",
    "#     '''\n",
    "#     return an array of indices for negative pairs for user u1 and u2, where u1 != u2\n",
    "#     u1 is the genuine user and u2 is the imposter.\n",
    "#     '''\n",
    "#     indices = list(range(diagonal_size))\n",
    "#     mesh = np.array(np.meshgrid(indices, indices))\n",
    "#     negative_index_pairs = mesh.T.reshape(-1, 2)\n",
    "    \n",
    "#     return negative_index_pairs\n",
    "\n",
    "# def generate_positive_index_pairs(diagonal_size):\n",
    "#     '''\n",
    "#     return an array of tuple indices for the positive pairs.\n",
    "#     '''\n",
    "#     indices = list(range(diagonal_size))\n",
    "#     mesh = np.array(np.meshgrid(indices, indices))\n",
    "#     positive_index_pairs = mesh.T.reshape(-1, 2)\n",
    "#     positive_index_pairs = positive_index_pairs.reshape(diagonal_size, diagonal_size, 2)\n",
    "\n",
    "#     new_array_size = diagonal_size * diagonal_size - diagonal_size\n",
    "\n",
    "#     positive_index_pairs = deleteDiagonal(positive_index_pairs).reshape(new_array_size, 2)\n",
    "#     return positive_index_pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lim = 2000\n",
    "# for total_samples_per_user in [27]:\n",
    "#     for num_users in [7, 10, 12, 15, 17, 20]:\n",
    "        \n",
    "        \n",
    "#         print(generate_negative_index_pairs(total_samples_per_user).shape)\n",
    "#         num_pos_per_owner = generate_positive_index_pairs(total_samples_per_user).shape[0]\n",
    "#         total_num_pairs = num_pos_per_owner * num_users * 2\n",
    "#         if lim < total_num_pairs: \n",
    "#             limit_ratio = lim/total_num_pairs\n",
    "#         else:\n",
    "#             limit_ratio=1\n",
    "#         num_pos_per_owner = np.round(limit_ratio * num_pos_per_owner)\n",
    "        \n",
    "#         ratio = ((total_samples_per_user-1) / (total_samples_per_user * (num_users-1)))*limit_ratio\n",
    "#         num_neg_per_owner = int(np.round(generate_negative_index_pairs(total_samples_per_user).shape[0]*ratio*(num_users-1)))\n",
    "#         print(num_pos_per_owner)\n",
    "#         print(num_neg_per_owner)\n",
    "#         print(num_pos_per_owner==num_neg_per_owner)\n",
    "#         print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "# def generate_cv_split(user1, user2, user_id_array, Raw_X_exp2_train_dic, Raw_X_exp1_train_dic, fitted_scaler_SNN_exp2_train_dic, num_pair_limit_train_2000, num_pair_limit_valid_2000):\n",
    "#     cv_dict = {}\n",
    "    \n",
    "#     cnn_valid_set = {user1, user2}\n",
    "#     cnn_train_set = user_id_array - cnn_valid_set\n",
    "    \n",
    "#     cnn_train_exp2 = {key: Raw_X_exp2_train_dic[key] for key in cnn_train_set}\n",
    "#     cnn_train_exp1 = {key: Raw_X_exp1_train_dic[key] for key in cnn_train_set}\n",
    "#     cnn_valid_exp2 = {key: Raw_X_exp2_train_dic[key] for key in cnn_valid_set}\n",
    "#     cnn_valid_exp1 = {key: Raw_X_exp1_train_dic[key] for key in cnn_valid_set}\n",
    "    \n",
    "#     X_train, y_train = prep_X_y_pair(cnn_train_exp2, cnn_train_exp1, list(cnn_train_exp2.keys()), fitted_scaler_SNN_exp2_train_dic, num_pair_limit=num_pair_limit_train_2000)\n",
    "#     X_valid, y_valid = prep_X_y_pair(cnn_valid_exp2, cnn_valid_exp1, list(cnn_valid_exp2.keys()), fitted_scaler_SNN_exp2_train_dic, num_pair_limit=num_pair_limit_valid_2000)\n",
    "    \n",
    "#     cv_dict[(user1, user2)] = {}\n",
    "#     cv_dict[(user1, user2)][\"train\"] = X_train, y_train\n",
    "#     cv_dict[(user1, user2)][\"valid\"] = X_valid, y_valid\n",
    "    \n",
    "#     return cv_dict\n",
    "\n",
    "\n",
    "# num_pair_limit_train_2000 = 9182\n",
    "# num_pair_limit_valid_2000 = 5310\n",
    "# user_id_array = set(Raw_X_exp2_train_dic.keys())\n",
    "# # user_id_array = {1,2,3,4,5}\n",
    "# results = Parallel(n_jobs=-1, verbose=100)(delayed(generate_cv_split)(user1, user2, user_id_array, Raw_X_exp2_train_dic, Raw_X_exp1_train_dic, fitted_scaler_SNN_exp2_train_dic, \n",
    "#                                                    num_pair_limit_train_2000, num_pair_limit_valid_2000) for user1, user2 in itertools.combinations(user_id_array, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "# user_id_array = set(Raw_X_exp2_train_dic.keys())\n",
    "\n",
    "# num_pair_limit_train_2000 = 9182\n",
    "# num_pair_limit_valid_2000 = 5310\n",
    "\n",
    "# cv_dict = {}\n",
    "# # Iterate through all possible pairs of users\n",
    "# for user1, user2 in tqdm(itertools.combinations(user_id_array, 2)):\n",
    "#     cnn_valid_set = {user1, user2}\n",
    "#     cnn_train_set = user_id_array - cnn_valid_set\n",
    "    \n",
    "#     cnn_train_exp2 = {key: Raw_X_exp2_train_dic[key] for key in cnn_train_set}\n",
    "#     cnn_train_exp1 = {key: Raw_X_exp1_train_dic[key] for key in cnn_train_set}\n",
    "#     cnn_valid_exp2 = {key: Raw_X_exp2_train_dic[key] for key in cnn_valid_set}\n",
    "#     cnn_valid_exp1 = {key: Raw_X_exp1_train_dic[key] for key in cnn_valid_set}\n",
    "    \n",
    "#     X_train, y_train = prep_X_y_pair(cnn_train_exp2, cnn_train_exp1, list(cnn_train_exp2.keys()), fitted_scaler_SNN_exp2_train_dic, num_pair_limit=num_pair_limit_train_2000)\n",
    "#     X_valid, y_valid = prep_X_y_pair(cnn_valid_exp2, cnn_valid_exp1, list(cnn_valid_exp2.keys()), fitted_scaler_SNN_exp2_train_dic, num_pair_limit=num_pair_limit_valid_2000)\n",
    "    \n",
    "#     cv_dict[(user1, user2)] = {}\n",
    "#     cv_dict[(user1, user2)][\"train\"] = X_train, y_train\n",
    "#     cv_dict[(user1, user2)][\"valid\"] = X_valid, y_valid\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def deleteDiagonal(array):\n",
    "#     depth = array.shape[-1]\n",
    "#     m = array.shape[1]\n",
    "#     strided = np.lib.stride_tricks.as_strided\n",
    "#     s0,s1,s2 = array.strides\n",
    "#     return strided(array.ravel()[depth:], shape=(m-1, m, depth), strides=(s0+s1,s1, s2)).reshape(m, m-1, depth)\n",
    "# print(mesh.shape)\n",
    "# deleteDiagonal(mesh).reshape(12,2)\n",
    "\n",
    "# def build_model_fcn(input_shape, filters):\n",
    "#     # Define the tensors for the two input images\n",
    "#     left_inputs = Input(input_shape, name=\"left_inputs\")\n",
    "#     right_inputs = Input(input_shape, name=\"right_inputs\")\n",
    "\n",
    "#     # Convolutional Neural Network\n",
    "#     inputs = Input(input_shape, name=\"input\")\n",
    "#     x = Conv1D(filters=filters[0], kernel_size=8, strides=1, activation=None, padding=\"same\", name=\"conv1\")(inputs)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Activation(\"relu\")(x)\n",
    "#     x = Dropout(0.1, name=\"drop1\")(x)\n",
    "#     x = Conv1D(filters=filters[1], kernel_size=5, strides=1, activation=None, padding=\"same\", name=\"conv2\")(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Activation(\"relu\")(x)\n",
    "#     x = Dropout(0.1, name=\"drop2\")(x)\n",
    "#     x = Conv1D(filters=filters[2], kernel_size=3, strides=1, activation=None, padding=\"same\", name=\"conv3\")(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Activation(\"relu\")(x)\n",
    "#     x = GlobalAveragePooling1D()(x)\n",
    "#     # x = Dense(32, activation=\"sigmoid\", name=\"dense\")(x) # <--- !!!!!!!!!!!!\n",
    "# #     x = Dense(64, activation=\"sigmoid\", name=\"dense\")(x) # <--- !!!!!!!!!!!!\n",
    "#     x = Dense(78, activation=\"sigmoid\", name=\"dense\")(x)\n",
    "\n",
    "#     # Basemodel instance\n",
    "#     basemodel = Model(inputs, x, name=\"basemodel\")\n",
    "\n",
    "#     # using same instance of \"basemodel\" to share weights between left/right networks\n",
    "#     encoded_l = basemodel(left_inputs)\n",
    "#     encoded_r = basemodel(right_inputs)\n",
    "\n",
    "#     # Add a customized layer to compute the distance between the encodings\n",
    "#     distance_layer = Lambda(k_euclidean_dist, name=\"distance\")([encoded_l, encoded_r])\n",
    "\n",
    "#     # Combine into one net\n",
    "#     siamese_net = Model(inputs=[left_inputs, right_inputs], outputs=distance_layer)\n",
    "\n",
    "#     # return the model\n",
    "#     return siamese_net, basemodel\n",
    "\n",
    "# def create_model_fcn(input_shape, filters, kernels, kernel_initializers, strides, dropouts, activations, dense_kernels, dense_kernel_initializers, loss_func, optimizer):\n",
    "#     '''\n",
    "#     it is assumed that len(dropouts)=len(filters)-1\n",
    "#     '''\n",
    "#     # Define the tensors for the two input images\n",
    "#     left_inputs = Input(input_shape, name=\"left_inputs\")\n",
    "#     right_inputs = Input(input_shape, name=\"right_inputs\")\n",
    "\n",
    "#     # Convolutional Neural Network\n",
    "#     inputs = Input(input_shape, name=\"input\")\n",
    "    \n",
    "    \n",
    "# #     ------\n",
    "#     x=inputs\n",
    "#     for idx in range(len(dropouts)):\n",
    "#         x = Conv1D(filters=filters[idx], kernel_size=kernels[idx], kernel_initializer=kernel_initializers[idx], kernel_constraint=MaxNorm(3), strides=strides[idx], \n",
    "#                    activation=None, padding=\"same\", name=f\"conv{idx+1}\")(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Activation(activations[idx])(x)\n",
    "#         x = Dropout(dropouts[idx], name=f\"drop{idx+1}\")(x)\n",
    "    \n",
    "#     x = Conv1D(filters=filters[idx+1], kernel_size=kernels[idx+1], kernel_initializer=kernel_initializers[idx+1], kernel_constraint=MaxNorm(3), strides=strides[idx+1], \n",
    "#                activation=None, padding=\"same\", name=f\"conv{idx+2}\")(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Activation(activations[idx+1])(x)\n",
    "#     x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "\n",
    "#     x = Dense(dense, kernel_constraint=MaxNorm(3), activation=\"relu\", name=\"dense1\")(x)\n",
    "#     x = Dense(dense, kernel_constraint=MaxNorm(3), activation=\"sigmoid\", name=\"dense\")(x)\n",
    "\n",
    "#     # Basemodel instance\n",
    "#     basemodel = Model(inputs, x, name=\"basemodel\")\n",
    "\n",
    "#     # using same instance of \"basemodel\" to share weights between left/right networks\n",
    "#     encoded_l = basemodel(left_inputs)\n",
    "#     encoded_r = basemodel(right_inputs)\n",
    "\n",
    "#     # Add a customized layer to compute the distance between the encodings\n",
    "#     distance_layer = Lambda(k_euclidean_dist, name=\"distance\")([encoded_l, encoded_r])\n",
    "\n",
    "#     # Combine into one net\n",
    "#     siamese_net = Model(inputs=[left_inputs, right_inputs], outputs=distance_layer)\n",
    "    \n",
    "#     siamese_net.compile(loss=loss_func, optimizer=optimizer)\n",
    "    \n",
    "#     # return the model\n",
    "#     return siamese_net#|, basemodel\n",
    "\n",
    "# # # cite: Siamese Model with 2D Filters, as derived from Centeno et al. (2018)\n",
    "# # # the thesis I found\n",
    "# # def build_model_2d(input_shape, filters):\n",
    "# #     \"\"\"\n",
    "# #         Siamese CNN architecture with 3D input and 2D filters\n",
    "# #     \"\"\"\n",
    "# #     # Define the tensors for the two input images\n",
    "# #     left_inputs = Input(input_shape, name=\"left_inputs\")\n",
    "# #     right_inputs = Input(input_shape, name=\"right_inputs\")\n",
    "\n",
    "# #     # Convolutional Neural Network\n",
    "# #     inputs = Input(input_shape, name=\"input\")\n",
    "# #     x = Conv2D(filters[0], (7, 7), padding=\"same\", activation=\"tanh\", name=\"conv1\")(inputs)\n",
    "# #     x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp1\")(x)\n",
    "# #     x = Conv2D(filters[1], (5, 5), padding=\"same\", activation=\"tanh\", name=\"conv2\")(x)\n",
    "# #     x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp2\")(x)\n",
    "# #     x = Conv2D(filters[2], (3, 3), padding=\"same\", activation=\"tanh\", name=\"conv3\")(x)\n",
    "# #     x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp3\")(x)\n",
    "# #     x = Conv2D(filters[3], (3, 3), padding=\"same\", activation=\"tanh\", name=\"conv4\")(x)\n",
    "# #     x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp4\")(x)\n",
    "# #     x = Flatten(name=\"flat\")(x)\n",
    "    \n",
    "# #     # Basemodel instance\n",
    "# #     basemodel = Model(inputs, x, name=\"basemodel\")\n",
    "\n",
    "# #     # using same instance of \"basemodel\" to share weights between left/right networks\n",
    "# #     encoded_l = basemodel(left_inputs)\n",
    "# #     encoded_r = basemodel(right_inputs)\n",
    "\n",
    "# #     # Add a customized layer to compute the distance between the encodings\n",
    "# #     distance_layer = Lambda(k_euclidean_dist, name=\"distance\")([encoded_l, encoded_r])\n",
    "\n",
    "# #     # Combine into one net\n",
    "# #     siamese_net = Model(inputs=[left_inputs, right_inputs], outputs=distance_layer)\n",
    "\n",
    "# #     # return the model\n",
    "# #     return siamese_net, basemodel\n",
    "\n",
    "# # cite: Siamese Model with 2D Filters, as derived from Centeno et al. (2018)\n",
    "# # the thesis I found\n",
    "# def create_model_2d(input_shape, filters):\n",
    "#     \"\"\"\n",
    "#         Siamese CNN architecture with 3D input and 2D filters\n",
    "#     \"\"\"\n",
    "#     # Define the tensors for the two input images\n",
    "#     left_inputs = Input(input_shape, name=\"left_inputs\")\n",
    "#     right_inputs = Input(input_shape, name=\"right_inputs\")\n",
    "\n",
    "#     # Convolutional Neural Network\n",
    "#     inputs = Input(input_shape, name=\"input\")\n",
    "#     x = Conv2D(filters[0], (7, 7), padding=\"same\", activation=\"tanh\", name=\"conv1\")(inputs)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp1\")(x)\n",
    "#     x = Conv2D(filters[1], (5, 5), padding=\"same\", activation=\"tanh\", name=\"conv2\")(x)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp2\")(x)\n",
    "#     x = Conv2D(filters[2], (3, 3), padding=\"same\", activation=\"tanh\", name=\"conv3\")(x)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp3\")(x)\n",
    "#     x = Conv2D(filters[3], (3, 3), padding=\"same\", activation=\"tanh\", name=\"conv4\")(x)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp4\")(x)\n",
    "#     x = Flatten(name=\"flat\")(x)\n",
    "    \n",
    "#     # Basemodel instance\n",
    "#     basemodel = Model(inputs, x, name=\"basemodel\")\n",
    "\n",
    "#     # using same instance of \"basemodel\" to share weights between left/right networks\n",
    "#     encoded_l = basemodel(left_inputs)\n",
    "#     encoded_r = basemodel(right_inputs)\n",
    "\n",
    "#     # Add a customized layer to compute the distance between the encodings\n",
    "#     distance_layer = Lambda(k_euclidean_dist, name=\"distance\")([encoded_l, encoded_r])\n",
    "\n",
    "#     # Combine into one net\n",
    "#     siamese_net = Model(inputs=[left_inputs, right_inputs], outputs=distance_layer)\n",
    "\n",
    "#     # return the model\n",
    "#     return siamese_net, basemodel\n",
    "\n",
    "# # def build_model_1d(input_shape, filters):\n",
    "# #     \"\"\"\n",
    "# #         Model architecture\n",
    "# #     \"\"\"\n",
    "# #     # Define the tensors for the two input images\n",
    "# #     left_inputs = Input(input_shape, name=\"left_inputs\")\n",
    "# #     right_inputs = Input(input_shape, name=\"right_inputs\")\n",
    "\n",
    "# #     # Convolutional Neural Network\n",
    "# #     inputs = Input(input_shape, name=\"input\")\n",
    "# #     x = Conv1D(filters[0], 7, activation=\"elu\", padding=\"same\", name=\"conv1\")(inputs)\n",
    "# #     x = MaxPooling1D(pool_size=2, name=\"mp1\")(x)\n",
    "# #     x = Conv1D(filters[1], 5, activation=\"elu\", padding=\"same\", name=\"conv2\")(x)\n",
    "# #     x = MaxPooling1D(pool_size=2, name=\"mp2\")(x)\n",
    "# #     x = Conv1D(filters[2], 3, activation=\"elu\", padding=\"same\", name=\"conv3\")(x)\n",
    "# #     x = MaxPooling1D(pool_size=2, name=\"mp3\")(x)\n",
    "# #     x = Conv1D(filters[3], 3, activation=\"elu\", padding=\"same\", name=\"conv4\")(x)\n",
    "# #     x = MaxPooling1D(pool_size=2, name=\"mp5\")(x)\n",
    "# #     x = Flatten(name=\"flat\")(x)\n",
    "\n",
    "# #     # Generate the encodings (feature vectors) for the two images\n",
    "# #     basemodel = Model(inputs, x, name=\"basemodel\")\n",
    "\n",
    "# #     # using same instance of \"basemodel\" to share weights between left/right networks\n",
    "# #     encoded_l = basemodel(left_inputs)\n",
    "# #     encoded_r = basemodel(right_inputs)\n",
    "\n",
    "# #     # Add a customized layer to compute the absolute difference between the encodings\n",
    "# #     distance_layer = Lambda(k_euclidean_dist, name=\"distance\")([encoded_l, encoded_r])\n",
    "\n",
    "# #     siamese_net = Model(inputs=[left_inputs, right_inputs], outputs=distance_layer)\n",
    "\n",
    "# #     # return the model\n",
    "# #     return siamese_net, basemodel\n",
    "\n",
    "\n",
    "# def create_model_1d(input_shape, filters):\n",
    "#     \"\"\"\n",
    "#         Model architecture\n",
    "#     \"\"\"\n",
    "#     # Define the tensors for the two input images\n",
    "#     left_inputs = Input(input_shape, name=\"left_inputs\")\n",
    "#     right_inputs = Input(input_shape, name=\"right_inputs\")\n",
    "\n",
    "#     # Convolutional Neural Network\n",
    "#     inputs = Input(input_shape, name=\"input\")\n",
    "#     x = Conv1D(filters[0], 7, activation=\"elu\", padding=\"same\", name=\"conv1\")(inputs)\n",
    "#     x = MaxPooling1D(pool_size=2, name=\"mp1\")(x)\n",
    "#     x = Conv1D(filters[1], 5, activation=\"elu\", padding=\"same\", name=\"conv2\")(x)\n",
    "#     x = MaxPooling1D(pool_size=2, name=\"mp2\")(x)\n",
    "#     x = Conv1D(filters[2], 3, activation=\"elu\", padding=\"same\", name=\"conv3\")(x)\n",
    "#     x = MaxPooling1D(pool_size=2, name=\"mp3\")(x)\n",
    "#     x = Conv1D(filters[3], 3, activation=\"elu\", padding=\"same\", name=\"conv4\")(x)\n",
    "#     x = MaxPooling1D(pool_size=2, name=\"mp5\")(x)\n",
    "#     x = Flatten(name=\"flat\")(x)\n",
    "\n",
    "#     # Generate the encodings (feature vectors) for the two images\n",
    "#     basemodel = Model(inputs, x, name=\"basemodel\")\n",
    "\n",
    "#     # using same instance of \"basemodel\" to share weights between left/right networks\n",
    "#     encoded_l = basemodel(left_inputs)\n",
    "#     encoded_r = basemodel(right_inputs)\n",
    "\n",
    "#     # Add a customized layer to compute the absolute difference between the encodings\n",
    "#     distance_layer = Lambda(k_euclidean_dist, name=\"distance\")([encoded_l, encoded_r])\n",
    "\n",
    "#     siamese_net = Model(inputs=[left_inputs, right_inputs], outputs=distance_layer)\n",
    "\n",
    "#     # return the model\n",
    "#     return siamese_net, basemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_windows_2(window_size, step_width, user_set):\n",
    "    P.window_size = window_size\n",
    "    P.ocsvm_step_width = step_width\n",
    "    # preparing data\n",
    "    # set = r\n",
    "    dfList_exp1_user_set, dfList_exp2_user_set = [dfList_exp1[i] for i in user_set], [dfList_exp2[i] for i in user_set]\n",
    "    print(f\"len(dfList_exp1_user_set): {len(dfList_exp1_user_set)}\")\n",
    "    print(f\"len(dfList_exp2_user_set): {len(dfList_exp2_user_set)}\")\n",
    "    XExpDict = MakeRawXExpDic(dfList_exp1_user_set, dfList_exp2_user_set, window_size = P.window_size, step=P.ocsvm_step_width, numSamplePoints=P.num_sample_points_per_exp, \n",
    "                                          scale_exp1=False, scale_exp2=True, scaler=P.scaler)\n",
    "\n",
    "    fitted_scaler_ocsvm_exp2_dic = XExpDict[\"fitted_scaler_exp2_dic\"]\n",
    "    X_exp1_dic, X_exp2_dic = XExpDict[\"Raw_X_exp1_dic\"], XExpDict[\"Raw_X_exp2_dic\"]\n",
    "\n",
    "    \n",
    "    #----\n",
    "    return X_exp1_dic, X_exp2_dic, fitted_scaler_ocsvm_exp2_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_2(X_exp1_dic, X_exp2_dic, fitted_scaler_ocsvm_exp2_dic):\n",
    "    X_exp_dic = MakeWACAXExpDicOwner(X_exp2_dic, scaler_clip=True, scaler_type=\"MinMaxScaler\")\n",
    "    X_exp_dic = MakeWACAXExpDicUnknown(X_exp1_dic, X_exp_dic, fitted_raw_scaler_dict=fitted_scaler_ocsvm_exp2_dic)\n",
    "    \n",
    "    return X_exp_dic\n",
    "\n",
    "def extract_deep_features_2(X_exp1_dic, X_exp2_dic, fitted_scaler_ocsvm_exp2_dic, deep_feature_model):\n",
    "    X_exp_dic = MakeDeepXExpDicOwner(X_exp2_dic, deep_feature_model, scaler_clip=True, scaler_type=\"MinMaxScaler\")\n",
    "    X_exp_dic = MakeDeepXExpDicUnknown(X_exp1_dic, deep_feature_model, X_exp_dic, fitted_raw_scaler_dict=fitted_scaler_ocsvm_exp2_dic)\n",
    "    \n",
    "    return X_exp_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_EER_distro_df(X_exp_dic, n_neighbors=3):\n",
    "    columns = ['owner', \"adv_user_id\", \"EER\"]\n",
    "    EER_distro_df = pd.DataFrame(columns = columns)\n",
    "    for owner in X_exp_dic.keys():\n",
    "        nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(X_exp_dic[owner][\"profile_windows\"])\n",
    "        curr_EER_distro_lst = [] \n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        distFRR, FRR_indices = nbrs.kneighbors(X_exp_dic[owner][\"unknown_users_dict\"][owner], n_neighbors=n_neighbors)\n",
    "        distFRR = distFRR.mean(axis=1).reshape(distFRR.shape[0], 1)\n",
    "        for adv_user_id in X_exp_dic.keys():\n",
    "    #         if adv_user_id != 16 and adv_user_id != gen_user_id and gen_user_id != 16:\n",
    "            if adv_user_id != owner and owner != 16:\n",
    "                distFAR, FAR_indices = nbrs.kneighbors(X_exp_dic[owner][\"unknown_users_dict\"][adv_user_id], n_neighbors=n_neighbors)\n",
    "                # print(distFAR)                      \n",
    "                distFAR = distFAR.mean(axis=1).reshape(distFAR.shape[0], 1)\n",
    "                # print(distFAR)\n",
    "                df[adv_user_id] = distFAR.mean(axis=1)\n",
    "\n",
    "\n",
    "                all_dist = np.concatenate([distFAR, distFRR])\n",
    "    #             print(all_dist.shape)\n",
    "                scaler = MinMaxScaler().fit(all_dist)\n",
    "                scaled_distFRR, scaled_distFAR = scaler.transform(distFRR), scaler.transform(distFAR)\n",
    "\n",
    "\n",
    "\n",
    "                y = [1]*distFRR.shape[0] + [-1]*distFAR.shape[0]\n",
    "                y_pred = 1-np.concatenate([scaled_distFRR, scaled_distFAR]).ravel() \n",
    "\n",
    "                temp_eer, tres = utils_eer(y, y_pred, return_threshold=True)\n",
    "                curr_EER_distro_lst.append(temp_eer)\n",
    "                print(f\"EER: {temp_eer:.9f}, owner_id: {owner}, adv_user_id: {adv_user_id}, Threshold: {tres:.9f}\")\n",
    "\n",
    "                new_row = pd.DataFrame([[owner, adv_user_id, temp_eer]], columns=columns)\n",
    "                EER_distro_df = pd.concat([EER_distro_df, new_row])\n",
    "    #     if gen_user_id != 16:\n",
    "    #         print(len(curr_EER_distro_lst))\n",
    "    # #         EER_distro_df[\"Owner\"] = gen_user_id\n",
    "    #         EER_distro_df[gen_user_id] = np.array(curr_EER_distro_lst)\n",
    "\n",
    "#         if owner != 16:\n",
    "#             df.plot(figsize=(30, 6), title=f\"owner_id: {owner}\") # uncomment when more storage\n",
    "    \n",
    "    return EER_distro_df\n",
    "\n",
    "\n",
    "def calculate_EER_different_window_sizes_optimize_num_neighbors(overlap=.25, n_neighbors_params=[1], train_set=None):\n",
    "    EER_distro_df_dict = {}\n",
    "    window_size_lst = [125, 250, 500, 750, 1000, 1250, 1500, 1750, 2000]# \n",
    "    scaler_clip = True\n",
    "    columns = ['window_size', \"step_width\", \"scaler_clip\", \"Mean_EER\", \"best_n_neighbors\"]\n",
    "    Mean_EER_df = pd.DataFrame(columns = columns)\n",
    "    for window_size in tqdm(window_size_lst):\n",
    "        print(window_size)\n",
    "        # blockPrint()\n",
    "        step_width = int(window_size * (1-overlap))\n",
    "        X_exp1_train_dic, X_exp2_train_dic, fitted_scaler_ocsvm_exp2_train_dic = get_raw_windows_2(window_size, step_width, user_set=train_set)\n",
    "        X_exp_train_dic = extract_features_2(X_exp1_train_dic, X_exp2_train_dic, fitted_scaler_ocsvm_exp2_train_dic)\n",
    "\n",
    "\n",
    "        mean_eer_dict = {}\n",
    "        for n_neighbors in n_neighbors_params:\n",
    "            EER_distro_df = make_EER_distro_df(X_exp_train_dic, n_neighbors=n_neighbors)\n",
    "            EER_distro_df_dict[f\"window_size: {window_size}, step_width: {step_width}, scaler_clip: {scaler_clip}\"] = EER_distro_df\n",
    "\n",
    "            y_col = \"EER\"\n",
    "            mean_col = EER_distro_df[y_col].mean()\n",
    "            print(EER_distro_df)\n",
    "            mean_eer_dict[n_neighbors] = mean_col\n",
    "            \n",
    "        print(mean_eer_dict)\n",
    "        best_n_neighbors = min(mean_eer_dict, key=mean_eer_dict.get)\n",
    "        Mean_EER = mean_eer_dict[best_n_neighbors]\n",
    "\n",
    "        new_row = pd.DataFrame([[window_size, step_width, scaler_clip, Mean_EER, best_n_neighbors]], columns=columns)\n",
    "        Mean_EER_df = pd.concat([Mean_EER_df, new_row])\n",
    "        # enablePrint()\n",
    "    return Mean_EER_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_EER_different_window_sizes_test(overlap=.25, best_param_dict={}, test_set=None):\n",
    "    EER_distro_df_dict = {}\n",
    "    window_size_lst = [125, 250, 500, 750, 1000, 1250, 1500, 1750, 2000]# \n",
    "    scaler_clip = True\n",
    "    columns = ['window_size', \"step_width\", \"scaler_clip\", \"Mean_EER\"]\n",
    "    Mean_EER_df = pd.DataFrame(columns = columns)\n",
    "    for window_size in tqdm(window_size_lst):\n",
    "        print(window_size)\n",
    "        # blockPrint()\n",
    "        step_width = int(window_size * (1-overlap))\n",
    "        X_exp1_test_dic, X_exp2_test_dic, fitted_scaler_ocsvm_exp2_test_dic = get_raw_windows_2(window_size, step_width, user_set=test_set)\n",
    "        X_exp_test_dic = extract_features_2(X_exp1_test_dic, X_exp2_test_dic, fitted_scaler_ocsvm_exp2_test_dic)\n",
    "\n",
    "        \n",
    "        best_n_neighbors = int(best_param_dict[best_param_dict.window_size==window_size].best_n_neighbors)\n",
    "        EER_distro_df = make_EER_distro_df(X_exp_test_dic, n_neighbors=best_n_neighbors)\n",
    "        # EER_distro_df_dict[f\"window_size: {window_size}, step_width: {step_width}, scaler_clip: {scaler_clip}\"] = EER_distro_df\n",
    "        EER_distro_df_dict[window_size] = EER_distro_df\n",
    "\n",
    "        y_col = \"EER\"\n",
    "        mean_col = EER_distro_df[y_col].mean()\n",
    "        Mean_EER = mean_col\n",
    "\n",
    "        new_row = pd.DataFrame([[window_size, step_width, scaler_clip, Mean_EER]], columns=columns)\n",
    "        Mean_EER_df = pd.concat([Mean_EER_df, new_row])\n",
    "        # enablePrint()\n",
    "    return Mean_EER_df, EER_distro_df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_butter(df, filter_order=10, cut_off_freq=10, sampling_freq=100, plot=False):\n",
    "    N = np.array(df).shape[0]\n",
    "    # sample spacing\n",
    "    T = 1.0 / sampling_freq\n",
    "\n",
    "    sos = signal.butter(filter_order, cut_off_freq, 'lowpass', fs=sampling_freq, output='sos')\n",
    "    sig = np.array(df)\n",
    "    y = signal.sosfilt(sos, sig) #filtered time domain signal\n",
    "\n",
    "\n",
    "    yf = fft(y)\n",
    "    xf = fftfreq(N, T)[:N//2]\n",
    "    \n",
    "    if plot:\n",
    "        plt.plot(xf, 2.0/N * np.abs(yf[0:N//2]))\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    \n",
    "    return y\n",
    "\n",
    "def get_ffted_dfList(dfList_exp, filter_order=10, cut_off_freq=10, sampling_freq=100):\n",
    "    \n",
    "    ffted_dfList_exp = []\n",
    "    for df in dfList_exp:\n",
    "        ffted_df = df.copy()\n",
    "        ffted_df['EMA_x_a'] = apply_butter(ffted_df['EMA_x_a'], filter_order=filter_order, cut_off_freq=cut_off_freq, sampling_freq=sampling_freq)\n",
    "        ffted_df['EMA_y_a'] = apply_butter(ffted_df['EMA_y_a'], filter_order=filter_order, cut_off_freq=cut_off_freq, sampling_freq=sampling_freq)\n",
    "        ffted_df['EMA_z_a'] = apply_butter(ffted_df['EMA_z_a'], filter_order=filter_order, cut_off_freq=cut_off_freq, sampling_freq=sampling_freq)\n",
    "        \n",
    "        ffted_df['EMA_x_g'] = apply_butter(ffted_df['EMA_x_g'], filter_order=filter_order, cut_off_freq=cut_off_freq, sampling_freq=sampling_freq)\n",
    "        ffted_df['EMA_y_g'] = apply_butter(ffted_df['EMA_y_g'], filter_order=filter_order, cut_off_freq=cut_off_freq, sampling_freq=sampling_freq)\n",
    "        ffted_df['EMA_z_g'] = apply_butter(ffted_df['EMA_z_g'], filter_order=filter_order, cut_off_freq=cut_off_freq, sampling_freq=sampling_freq)\n",
    "        \n",
    "        ffted_dfList_exp.append(ffted_df)\n",
    "        \n",
    "    return ffted_dfList_exp\n",
    "\n",
    "def get_EMAed_dfList(dfList_exp, span=None):\n",
    "    \n",
    "    EMAed_dfList_exp = []\n",
    "    for df in dfList_exp:\n",
    "        EMAed_df = df.copy()\n",
    "        EMAed_df['EMA_x_a'] = EMAed_df['EMA_x_a'].ewm(span=span,adjust=False).mean()\n",
    "        EMAed_df['EMA_y_a'] = EMAed_df['EMA_y_a'].ewm(span=span,adjust=False).mean()\n",
    "        EMAed_df['EMA_z_a'] = EMAed_df['EMA_z_a'].ewm(span=span,adjust=False).mean()\n",
    "        \n",
    "        EMAed_df['EMA_x_g'] = EMAed_df['EMA_x_g'].ewm(span=span,adjust=False).mean()\n",
    "        EMAed_df['EMA_y_g'] = EMAed_df['EMA_y_g'].ewm(span=span,adjust=False).mean()\n",
    "        EMAed_df['EMA_z_g'] = EMAed_df['EMA_z_g'].ewm(span=span,adjust=False).mean()\n",
    "        \n",
    "        EMAed_dfList_exp.append(EMAed_df)\n",
    "        \n",
    "    return EMAed_dfList_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "d7a8c48d-ce60-4891-a613-8cac6bcad885",
    "_uuid": "74529ec9-2ddf-43e8-8ef3-d2bba209f31b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def matchAccelGyroData(accel, gyro):\n",
    "    # Match the numbers by merge_asof to the higher length vector\n",
    "    accel_count = accel.count().time_stamp\n",
    "    gyro_count = gyro.count().time_stamp\n",
    "    names =['sensor_id', 'time_stamp', 'x', 'y', 'z']\n",
    "    if accel_count > gyro_count:\n",
    "        df = pd.merge_asof(accel, gyro, on=\"time_stamp\", direction='nearest')\n",
    "        df = df.sort_values(by=['time_stamp'])\n",
    "        df = df.dropna()\n",
    "        accel = df[[\"sensor_id_x\", \"time_stamp\", \"x_x\", \"y_x\", \"z_x\"]]\n",
    "        gyro = df[[\"sensor_id_y\", \"time_stamp\", \"x_y\", \"y_y\", \"z_y\"]]\n",
    "    else:\n",
    "        df = pd.merge_asof(gyro, accel, on=\"time_stamp\", direction='nearest')\n",
    "        df = df.sort_values(by=['time_stamp'])\n",
    "        df = df.dropna()\n",
    "        gyro = df[[\"sensor_id_x\", \"time_stamp\", \"x_x\", \"y_x\", \"z_x\"]]\n",
    "        accel = df[[\"sensor_id_y\", \"time_stamp\", \"x_y\", \"y_y\", \"z_y\"]]\n",
    "\n",
    "    accel.columns = names\n",
    "    gyro.columns = names\n",
    "    \n",
    "    return {'accel': accel, 'gyro': gyro}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "d8b4d9b0-695e-4551-be65-38a6f1817241",
    "_uuid": "fb41768e-82f1-4fb4-ba68-e1dd6cfcfba3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getDataStats1(i, print_accel_gyro_array_size=1, print_na_df_array_size=0, begin_idx=500, end_idx=-500):\n",
    "    \n",
    "    #load the data\n",
    "    names =['sensor_id', 'time_stamp', 'x', 'y', 'z']\n",
    "#     if i!=8:\n",
    "#         data = pd.read_csv('../input/wearable-assisted-ca/user{}_1.csv'.format(i), error_bad_lines = False, header=None, usecols = range(len(names)))\n",
    "#     else:\n",
    "    \n",
    "    data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
    "    data.columns = names\n",
    "    data = data[(data.sensor_id == '10') | (data.sensor_id =='4')]\n",
    "    data.head(10)\n",
    "\n",
    "\n",
    "    types_dict = {'sensor_id': 'int32', 'time_stamp': 'float64', 'x': 'float64', 'y': 'float64', 'z': 'float64'}\n",
    "    for col, col_type in types_dict.items():\n",
    "        data[col] = data[col].astype(col_type)\n",
    "\n",
    "    # find how many NAN values in the data\n",
    "    data.isna().sum()\n",
    "\n",
    "    # since only 7 NAN is a very small amount, drop them\n",
    "    df = data.dropna()\n",
    "\n",
    "    # get rid of begin and end noise\n",
    "    #sort df and filter\n",
    "    df = df.sort_values(by=['time_stamp'])\n",
    "    df = df[begin_idx:end_idx]\n",
    "    \n",
    "    # cleaning extreme outliers\n",
    "    df = df[(df.x < 10.1) & ( -10.1 < df.x) & (df.y < 10.1) & ( -10.1 < df.y) & (df.z < 10.1) & ( -10.1 < df.z) ]\n",
    "\n",
    "    # Extract Accelerometer values and sort\n",
    "    accel = df[df.sensor_id == 10]\n",
    "    accel = accel.sort_values(by=['time_stamp'])\n",
    "\n",
    "    # Extract gyro values and sort\n",
    "    gyro = df[df.sensor_id == 4]\n",
    "    gyro = gyro.sort_values(by=['time_stamp'])\n",
    "    \n",
    "    if print_accel_gyro_array_size:\n",
    "        print(\"{}) accel_count: {}, gyro_count: {}\".format(i, accel.count().time_stamp, gyro.count().time_stamp))\n",
    "    \n",
    "    result = matchAccelGyroData(accel, gyro)\n",
    "    accel, gyro = result['accel'], result['gyro']\n",
    "    \n",
    "#     accel['EMA_x_a'] = accel['x'].ewm(span=40,adjust=False).mean()\n",
    "#     accel['EMA_y_a'] = accel['y'].ewm(span=40,adjust=False).mean()\n",
    "#     accel['EMA_z_a'] = accel['z'].ewm(span=40,adjust=False).mean()\n",
    "\n",
    "#     gyro['EMA_x_g'] = gyro['x'].ewm(span=40,adjust=False).mean()\n",
    "#     gyro['EMA_y_g'] = gyro['y'].ewm(span=40,adjust=False).mean()\n",
    "#     gyro['EMA_z_g'] = gyro['z'].ewm(span=40,adjust=False).mean()\n",
    "    \n",
    "    accel['EMA_x_a'] = accel['x']\n",
    "    accel['EMA_y_a'] = accel['y']\n",
    "    accel['EMA_z_a'] = accel['z']\n",
    "\n",
    "    gyro['EMA_x_g'] = gyro['x']\n",
    "    gyro['EMA_y_g'] = gyro['y']\n",
    "    gyro['EMA_z_g'] = gyro['z']\n",
    "    \n",
    "    left = accel[[\"time_stamp\", \"EMA_x_a\", \"EMA_y_a\", \"EMA_z_a\"]]\n",
    "    right = gyro[[\"time_stamp\", \"EMA_x_g\", \"EMA_y_g\", \"EMA_z_g\"]].set_index('time_stamp')\n",
    "    df = left.join(right, on='time_stamp')\n",
    "\n",
    "    if print_na_df_array_size:\n",
    "        print(\"{}) na_count: {}, df count: {}\".format(i, df.isna().sum().sum(), df.count().time_stamp))\n",
    "    \n",
    "    return {\"accel\":accel.count().time_stamp, \"gyro\": gyro.count().time_stamp, \"df\": df, \"userIdx\": i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "81b39c51-3e35-4210-a3ae-618543e164df",
    "_uuid": "287b6e21-fe46-4032-8e82-2ed1837e9f44",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getDataStats2(i, print_accel_gyro_array_size=1, print_na_df_array_size=0, begin_idx=500, end_idx=-500):\n",
    "    \n",
    "    #load the data\n",
    "    names =['sensor_id', 'time_stamp', 'x', 'y', 'z']\n",
    "#     if i!=8:\n",
    "#         data = pd.read_csv('../input/wearable-assisted-ca/user{}_1.csv'.format(i), error_bad_lines = False, header=None, usecols = range(len(names)))\n",
    "#     else:\n",
    "    \n",
    "    data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
    "    data.columns = names\n",
    "    data = data[(data.sensor_id == '10') | (data.sensor_id =='4')]\n",
    "    data.head(10)\n",
    "\n",
    "\n",
    "    types_dict = {'sensor_id': 'int32', 'time_stamp': 'float64', 'x': 'float64', 'y': 'float64', 'z': 'float64'}\n",
    "    for col, col_type in types_dict.items():\n",
    "        data[col] = data[col].astype(col_type)\n",
    "\n",
    "    # find how many NAN values in the data\n",
    "    data.isna().sum()\n",
    "\n",
    "    # since only 7 NAN is a very small amount, drop them\n",
    "    df = data.dropna()\n",
    "\n",
    "    # get rid of begin and end noise\n",
    "    #sort df and filter\n",
    "    df = df.sort_values(by=['time_stamp'])\n",
    "    df = df[begin_idx:end_idx]\n",
    "    \n",
    "    # cleaning extreme outliers\n",
    "    df = df[(df.x < 10.1) & ( -10.1 < df.x) & (df.y < 10.1) & ( -10.1 < df.y) & (df.z < 10.1) & ( -10.1 < df.z) ]\n",
    "\n",
    "    # Extract Accelerometer values and sort\n",
    "    accel = df[df.sensor_id == 10]\n",
    "    accel = accel.sort_values(by=['time_stamp'])\n",
    "\n",
    "    # Extract gyro values and sort\n",
    "    gyro = df[df.sensor_id == 4]\n",
    "    gyro = gyro.sort_values(by=['time_stamp'])\n",
    "    \n",
    "    if print_accel_gyro_array_size:\n",
    "        print(\"{}) accel_count: {}, gyro_count: {}\".format(i, accel.count().time_stamp, gyro.count().time_stamp))\n",
    "    \n",
    "    result = matchAccelGyroData(accel, gyro)\n",
    "    accel, gyro = result['accel'], result['gyro']\n",
    "    \n",
    "#     accel['EMA_x_a'] = accel['x'].ewm(span=40,adjust=False).mean()\n",
    "#     accel['EMA_y_a'] = accel['y'].ewm(span=40,adjust=False).mean()\n",
    "#     accel['EMA_z_a'] = accel['z'].ewm(span=40,adjust=False).mean()\n",
    "\n",
    "#     gyro['EMA_x_g'] = gyro['x'].ewm(span=40,adjust=False).mean()\n",
    "#     gyro['EMA_y_g'] = gyro['y'].ewm(span=40,adjust=False).mean()\n",
    "#     gyro['EMA_z_g'] = gyro['z'].ewm(span=40,adjust=False).mean()\n",
    "\n",
    "    accel['EMA_x_a'] = accel['x']\n",
    "    accel['EMA_y_a'] = accel['y']\n",
    "    accel['EMA_z_a'] = accel['z']\n",
    "\n",
    "    gyro['EMA_x_g'] = gyro['x']\n",
    "    gyro['EMA_y_g'] = gyro['y']\n",
    "    gyro['EMA_z_g'] = gyro['z']\n",
    "    \n",
    "    left = accel[[\"time_stamp\", \"EMA_x_a\", \"EMA_y_a\", \"EMA_z_a\"]]\n",
    "    right = gyro[[\"time_stamp\", \"EMA_x_g\", \"EMA_y_g\", \"EMA_z_g\"]].set_index('time_stamp')\n",
    "    df = left.join(right, on='time_stamp')\n",
    "\n",
    "    if print_na_df_array_size:\n",
    "        print(\"{}) na_count: {}, df count: {}\".format(i, df.isna().sum().sum(), df.count().time_stamp))\n",
    "    \n",
    "    return {\"accel\":accel.count().time_stamp, \"gyro\": gyro.count().time_stamp, \"df\": df, \"userIdx\": i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "852f6592-f958-4c9b-b8f4-73946030cfef",
    "_uuid": "52758db0-8d25-455e-91ec-b1bd76be8f7b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample_rate = 10 #Hz\n",
    "# #3352843.3\n",
    "# x = np.array([318.45,302.78,316.47,334.14,333.41,326.15,320.07,318.68,314.12,308.64,\n",
    "#               300.15,304.33,318.42,322.72,329.56,339.18,338.03,343.27,351.44,353.23,\n",
    "#               352.35,352.88,353.43,352.14,351.28,352.82,353.36,353.35,353.19,353.82])\n",
    "\n",
    "# mn=np.mean(x)\n",
    "# print(f' mean = {mn:.3f} unit')\n",
    "# print(f' sum x[i]**2  : {np.sum(x**2) :.1f} unit^2 ')\n",
    "\n",
    "\n",
    "# print(f' n *sum X[k]**2   : {spectral_energy(x) :.1f} unit^2 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4054a4f7-9d44-4c4d-865d-b30fc17e48db",
    "_uuid": "e31d6f97-ed7c-4146-bec0-ba773f823c37",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-07T19:01:25.942693Z",
     "iopub.status.busy": "2022-05-07T19:01:25.942065Z",
     "iopub.status.idle": "2022-05-07T19:01:25.952827Z",
     "shell.execute_reply": "2022-05-07T19:01:25.951688Z",
     "shell.execute_reply.started": "2022-05-07T19:01:25.942645Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def spectral_energy(x):\n",
    "    '''\n",
    "    spectral_energy according to Parseval's theorem\n",
    "    '''\n",
    "    # should i keep using rfft or just fft\n",
    "    return (1/len(x)) * np.sum(np.abs(np.fft.rfft(x))**2)\n",
    "\n",
    "def window_entropy(x, n_bins=13):\n",
    "    \n",
    "    heights, bins = np.histogram(x, bins=n_bins)\n",
    "    heights = heights/sum(heights)\n",
    "    window_entropy=entropy(heights, base=2, axis=0)\n",
    "    \n",
    "    return window_entropy\n",
    "\n",
    "def average_absolute_difference_peaks(peaks, window_size):\n",
    "    \n",
    "    if len(peaks) == 0:\n",
    "        return window_size\n",
    "    elif len(peaks) == 1:\n",
    "        return window_size\n",
    "    \n",
    "    n = len(peaks) * (len(peaks)-1)\n",
    "    \n",
    "    return np.abs(peaks[:, None] - peaks[None, :]).ravel().sum()/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "77192541-b078-4a38-9a98-29b1868f61b2",
    "_uuid": "d0eaad53-bd24-45f3-8261-1e1f4a244ce5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:30.925626Z",
     "iopub.status.busy": "2022-03-29T07:28:30.925005Z",
     "iopub.status.idle": "2022-03-29T07:28:30.971124Z",
     "shell.execute_reply": "2022-03-29T07:28:30.97013Z",
     "shell.execute_reply.started": "2022-03-29T07:28:30.92558Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def signal_to_encoding(signal_df, freq=100):\n",
    "    dic = {}\n",
    "\n",
    "#     print(\"mean calculation started\")\n",
    "    dic['mean_x_a'] = np.mean(signal_df['EMA_x_a'])\n",
    "    dic['mean_y_a'] = np.mean(signal_df['EMA_y_a'])\n",
    "    dic['mean_z_a'] = np.mean(signal_df['EMA_z_a'])\n",
    "    dic['mean_x_g'] = np.mean(signal_df['EMA_x_g'])\n",
    "    dic['mean_y_g'] = np.mean(signal_df['EMA_y_g'])\n",
    "    dic['mean_z_g'] = np.mean(signal_df['EMA_z_g'])\n",
    "#     print(\"mean calculation ended\")\n",
    "    \n",
    "#     print(\"median calculation started\")\n",
    "    dic['median_x_a'] = np.median(signal_df['EMA_x_a'])\n",
    "    dic['median_y_a'] = np.median(signal_df['EMA_y_a'])\n",
    "    dic['median_z_a'] = np.median(signal_df['EMA_z_a'])\n",
    "    dic['median_x_g'] = np.median(signal_df['EMA_x_g'])\n",
    "    dic['median_y_g'] = np.median(signal_df['EMA_y_g'])\n",
    "    dic['median_z_g'] = np.median(signal_df['EMA_z_g'])\n",
    "#     print(\"median calculation ended\")\n",
    "    \n",
    "#     print(\"var calculation started\")\n",
    "    dic['var_x_a'] = np.var(signal_df['EMA_x_a'])\n",
    "    dic['var_y_a'] = np.var(signal_df['EMA_y_a'])\n",
    "    dic['var_z_a'] = np.var(signal_df['EMA_z_a'])\n",
    "    dic['var_x_g'] = np.var(signal_df['EMA_x_g'])\n",
    "    dic['var_y_g'] = np.var(signal_df['EMA_y_g'])\n",
    "    dic['var_z_g'] = np.var(signal_df['EMA_z_g'])\n",
    "#     print(\"var calculation ended\")\n",
    "\n",
    "#     print(\"avg absolute difference of peaks calculation started\")\n",
    "    peaks_x_a, _ = find_peaks(signal_df['EMA_x_a'], height=None)\n",
    "    peaks_y_a, _ = find_peaks(signal_df['EMA_y_a'], height=None)\n",
    "    peaks_z_a, _ = find_peaks(signal_df['EMA_z_a'], height=None)\n",
    "    peaks_x_g, _ = find_peaks(signal_df['EMA_x_g'], height=None)\n",
    "    peaks_y_g, _ = find_peaks(signal_df['EMA_y_g'], height=None)\n",
    "    peaks_z_g, _ = find_peaks(signal_df['EMA_z_g'], height=None)\n",
    "    \n",
    "    window_size=len(signal_df['EMA_x_a'])\n",
    "    dic['aadp_x_a'] = average_absolute_difference_peaks(peaks=peaks_x_a, window_size=window_size)\n",
    "    dic['aadp_y_a'] = average_absolute_difference_peaks(peaks=peaks_y_a, window_size=window_size)\n",
    "    dic['aadp_z_a'] = average_absolute_difference_peaks(peaks=peaks_z_a, window_size=window_size)\n",
    "    dic['aadp_x_g'] = average_absolute_difference_peaks(peaks=peaks_x_g, window_size=window_size)\n",
    "    dic['aadp_y_g'] = average_absolute_difference_peaks(peaks=peaks_y_g, window_size=window_size)\n",
    "    dic['aadp_z_g'] = average_absolute_difference_peaks(peaks=peaks_z_g, window_size=window_size)\n",
    "    \n",
    "#     print(type(peak_widths(peaks_x_a, signal_df['EMA_x_a'], rel_height=0.5)[0]))\n",
    "    # dic['aadp_x_a'] = np.mean(peak_widths(signal_df['EMA_x_a'], peaks_x_a, rel_height=0.5)[0])\n",
    "    # dic['aadp_y_a'] = np.mean(peak_widths(signal_df['EMA_y_a'], peaks_y_a, rel_height=0.5)[0])\n",
    "    # dic['aadp_z_a'] = np.mean(peak_widths(signal_df['EMA_z_a'], peaks_z_a, rel_height=0.5)[0])\n",
    "    # dic['aadp_x_g'] = np.mean(peak_widths(signal_df['EMA_x_g'], peaks_x_g, rel_height=0.5)[0])\n",
    "    # dic['aadp_y_g'] = np.mean(peak_widths(signal_df['EMA_y_g'], peaks_y_g, rel_height=0.5)[0])\n",
    "    # dic['aadp_z_g'] = np.mean(peak_widths(signal_df['EMA_z_g'], peaks_z_g, rel_height=0.5)[0])\n",
    "#     print(\"avg absolute difference of peaks calculation ended\")\n",
    "    \n",
    "#     print(\"range calculation started\")\n",
    "    dic['ptp_x_a'] = np.ptp(signal_df['EMA_x_a'])\n",
    "    dic['ptp_y_a'] = np.ptp(signal_df['EMA_y_a'])\n",
    "    dic['ptp_z_a'] = np.ptp(signal_df['EMA_z_a'])\n",
    "    dic['ptp_x_g'] = np.ptp(signal_df['EMA_x_g'])\n",
    "    dic['ptp_y_g'] = np.ptp(signal_df['EMA_y_g'])\n",
    "    dic['ptp_z_g'] = np.ptp(signal_df['EMA_z_g'])\n",
    "#     print(\"range calculation ended\")\n",
    "    \n",
    "#     print(\"mode calculation started\")\n",
    "    dic['mode_x_a'] = mode(signal_df['EMA_x_a'])[0][0]\n",
    "    dic['mode_y_a'] = mode(signal_df['EMA_y_a'])[0][0]\n",
    "    dic['mode_z_a'] = mode(signal_df['EMA_z_a'])[0][0]\n",
    "    dic['mode_x_g'] = mode(signal_df['EMA_x_g'])[0][0]\n",
    "    dic['mode_y_g'] = mode(signal_df['EMA_y_g'])[0][0]\n",
    "    dic['mode_z_g'] = mode(signal_df['EMA_z_g'])[0][0]\n",
    "#     print(\"mode calculation ended\")\n",
    "    \n",
    "#     print(\"cov calculation started\")\n",
    "# seem to require 2 axes according to waca pattent\n",
    "    # dic['cov_x_a'] = np.cov(signal_df['EMA_x_a']) * 1\n",
    "    # dic['cov_y_a'] = np.cov(signal_df['EMA_y_a']) * 1\n",
    "    # dic['cov_z_a'] = np.cov(signal_df['EMA_z_a']) * 1\n",
    "    # dic['cov_x_g'] = np.cov(signal_df['EMA_x_g']) * 1\n",
    "    # dic['cov_y_g'] = np.cov(signal_df['EMA_y_g']) * 1\n",
    "    # dic['cov_z_g'] = np.cov(signal_df['EMA_z_g']) * 1\n",
    "    dic['cov_xy_a'] = np.cov(signal_df['EMA_x_a'], signal_df['EMA_y_a'])[0][1]\n",
    "    dic['cov_yz_a'] = np.cov(signal_df['EMA_y_a'], signal_df['EMA_z_a'])[0][1]\n",
    "    dic['cov_xz_a'] = np.cov(signal_df['EMA_x_a'], signal_df['EMA_z_a'])[0][1]\n",
    "    dic['cov_xy_g'] = np.cov(signal_df['EMA_x_g'], signal_df['EMA_y_g'])[0][1]\n",
    "    dic['cov_yz_g'] = np.cov(signal_df['EMA_y_g'], signal_df['EMA_z_g'])[0][1]\n",
    "    dic['cov_xz_g'] = np.cov(signal_df['EMA_x_g'], signal_df['EMA_z_g'])[0][1]\n",
    "    \n",
    "#     print(\"cov calculation ended\")\n",
    "    \n",
    "#     print(\"mean absolute deviation calculation started\")\n",
    "    dic['mad_x_a'] = median_abs_deviation(signal_df['EMA_x_a'])\n",
    "    dic['mad_y_a'] = median_abs_deviation(signal_df['EMA_y_a'])\n",
    "    dic['mad_z_a'] = median_abs_deviation(signal_df['EMA_z_a'])\n",
    "    dic['mad_x_g'] = median_abs_deviation(signal_df['EMA_x_g'])\n",
    "    dic['mad_y_g'] = median_abs_deviation(signal_df['EMA_y_g'])\n",
    "    dic['mad_z_g'] = median_abs_deviation(signal_df['EMA_z_g'])\n",
    "#     print(\"mean absolute deviation calculation ended\")\n",
    "    \n",
    "#     print(\"inter-quartile range calculation started\")\n",
    "    dic['iqr_x_a'] = iqr(signal_df['EMA_x_a'])\n",
    "    dic['iqr_y_a'] = iqr(signal_df['EMA_y_a'])\n",
    "    dic['iqr_z_a'] = iqr(signal_df['EMA_z_a'])\n",
    "    dic['iqr_x_g'] = iqr(signal_df['EMA_x_g'])\n",
    "    dic['iqr_y_g'] = iqr(signal_df['EMA_y_g'])\n",
    "    dic['iqr_z_g'] = iqr(signal_df['EMA_z_g'])\n",
    "#     print(\"inter-quartile range calculation ended\")\n",
    "    \n",
    "#     print(\"correlation calculation started\")\n",
    "    dic['correlate_xy_a'] = np.corrcoef(signal_df['EMA_x_a'], signal_df['EMA_y_a'])[0][1]\n",
    "    dic['correlate_yz_a'] = np.corrcoef(signal_df['EMA_y_a'], signal_df['EMA_z_a'])[0][1]\n",
    "    dic['correlate_xz_a'] = np.corrcoef(signal_df['EMA_x_a'], signal_df['EMA_z_a'])[0][1]\n",
    "    dic['correlate_xy_g'] = np.corrcoef(signal_df['EMA_x_g'], signal_df['EMA_y_g'])[0][1]\n",
    "    dic['correlate_yz_g'] = np.corrcoef(signal_df['EMA_y_g'], signal_df['EMA_z_g'])[0][1]\n",
    "    dic['correlate_xz_g'] = np.corrcoef(signal_df['EMA_x_g'], signal_df['EMA_z_g'])[0][1]\n",
    "#     print(\"correlation calculation ended\")\n",
    "    \n",
    "#     print(\"skew calculation started\")\n",
    "    dic['skew_x_a'] = skew(signal_df['EMA_x_a'])\n",
    "    dic['skew_y_a'] = skew(signal_df['EMA_y_a'])\n",
    "    dic['skew_z_a'] = skew(signal_df['EMA_z_a'])\n",
    "    dic['skew_x_g'] = skew(signal_df['EMA_x_g'])\n",
    "    dic['skew_y_g'] = skew(signal_df['EMA_y_g'])\n",
    "    dic['skew_z_g'] = skew(signal_df['EMA_z_g'])\n",
    "#     print(\"skew calculation ended\")\n",
    "    \n",
    "#     print(\"kurtosis calculation started\")\n",
    "    dic['kurtosis_x_a'] = kurtosis(signal_df['EMA_x_a'])\n",
    "    dic['kurtosis_y_a'] = kurtosis(signal_df['EMA_y_a'])\n",
    "    dic['kurtosis_z_a'] = kurtosis(signal_df['EMA_z_a'])\n",
    "    dic['kurtosis_x_g'] = kurtosis(signal_df['EMA_x_g'])\n",
    "    dic['kurtosis_y_g'] = kurtosis(signal_df['EMA_y_g'])\n",
    "    dic['kurtosis_z_g'] = kurtosis(signal_df['EMA_z_g'])\n",
    "#     print(\"kurtosis calculation ended\")\n",
    "    \n",
    "    \n",
    "#     print(\"spectral energy calculation started\")\n",
    "    dic['spectral_energy_x_a'] = spectral_energy(signal_df['EMA_x_a'])\n",
    "    dic['spectral_energy_y_a'] = spectral_energy(signal_df['EMA_y_a'])\n",
    "    dic['spectral_energy_z_a'] = spectral_energy(signal_df['EMA_z_a'])\n",
    "    dic['spectral_energy_x_g'] = spectral_energy(signal_df['EMA_x_g'])\n",
    "    dic['spectral_energy_y_g'] = spectral_energy(signal_df['EMA_y_g'])\n",
    "    dic['spectral_energy_z_g'] = spectral_energy(signal_df['EMA_z_g'])\n",
    "#     print(\"spectral energy calculation ended\")\n",
    "\n",
    "    # print(\"entropy calculation started\")\n",
    "\n",
    "    dic['entropy_x_a'] = window_entropy(signal_df['EMA_x_a'])\n",
    "    dic['entropy_y_a'] = window_entropy(signal_df['EMA_y_a'])\n",
    "    dic['entropy_z_a'] = window_entropy(signal_df['EMA_z_a'])\n",
    "    dic['entropy_x_g'] = window_entropy(signal_df['EMA_x_g'])\n",
    "    dic['entropy_y_g'] = window_entropy(signal_df['EMA_y_g'])\n",
    "    dic['entropy_z_g'] = window_entropy(signal_df['EMA_z_g'])\n",
    "    \n",
    "    # print(\"entropy calculation started\")\n",
    "\n",
    "\n",
    "    # print(\"spectral entropy calculation started\")\n",
    "    # method = 'fft'\n",
    "    # normalize = False\n",
    "    # sf=freq\n",
    "#     print(signal_df['EMA_x_a'])\n",
    "#     print(signal_df['EMA_x_a'].shape)\n",
    "    # axis = -1\n",
    "    # dic['spectral_entropy_x_a'] = ant.spectral_entropy(signal_df['EMA_x_a'], sf=sf, method=method, normalize=normalize, axis=axis)\n",
    "    # dic['spectral_entropy_y_a'] = ant.spectral_entropy(signal_df['EMA_y_a'], sf=sf, method=method, normalize=normalize, axis=axis)\n",
    "    # dic['spectral_entropy_z_a'] = ant.spectral_entropy(signal_df['EMA_z_a'], sf=sf, method=method, normalize=normalize, axis=axis)\n",
    "    # dic['spectral_entropy_x_g'] = ant.spectral_entropy(signal_df['EMA_x_g'], sf=sf, method=method, normalize=normalize, axis=axis)\n",
    "    # dic['spectral_entropy_y_g'] = ant.spectral_entropy(signal_df['EMA_y_g'], sf=sf, method=method, normalize=normalize, axis=axis)\n",
    "    # dic['spectral_entropy_z_g'] = ant.spectral_entropy(signal_df['EMA_z_g'], sf=sf, method=method, normalize=normalize, axis=axis)\n",
    "    \n",
    "    # print(dic['spectral_entropy_x_a'],\n",
    "    #           dic['spectral_entropy_y_a'],\n",
    "    #           dic['spectral_entropy_z_a'],\n",
    "    #           dic['spectral_entropy_x_g'],\n",
    "    #           dic['spectral_entropy_y_g'],\n",
    "    #           dic['spectral_entropy_z_g'])\n",
    "    # print(\"spectral entropy calculation ended\")\n",
    "\n",
    "\n",
    "#     print(\"entropy calculation started\")\n",
    "    \n",
    "#     cols = signal_df[[\"EMA_x_a\", \"EMA_y_a\", \"EMA_z_a\", \"EMA_x_g\", \"EMA_y_g\", \"EMA_z_g\"]]\n",
    "#     cols = normalize(cols, norm='l2', axis = 0)\n",
    "#     print(cols.sum(axis = 0))\n",
    "#     cols = StandardScaler().fit_transform(cols)\n",
    "#     p = cols/cols.sum(axis=0)\n",
    "#     print(p.sum(axis=0))\n",
    "#     print(p.shape)\n",
    "#     print(cols.sum(axis=0))\n",
    "#     entropy = entr(p).sum(axis=0)\n",
    "#     print(entropy.shape)\n",
    "#     dic['entropy_x_a'] = entropy[0]\n",
    "#     dic['entropy_y_a'] = entropy[1]\n",
    "#     dic['entropy_z_a'] = entropy[2]\n",
    "#     dic['entropy_x_g'] = entropy[3]\n",
    "#     dic['entropy_y_g'] = entropy[4]\n",
    "#     dic['entropy_z_g'] = entropy[5]\n",
    "#     print(\"entropy calculation ended\")\n",
    "    \n",
    "    vector = [dic['mean_x_a'], \n",
    "              dic['mean_y_a'],\n",
    "              dic['mean_z_a'],\n",
    "              dic['mean_x_g'],\n",
    "              dic['mean_y_g'],\n",
    "              dic['mean_z_g'],\n",
    "              \n",
    "              dic['median_x_a'],\n",
    "              dic['median_y_a'],\n",
    "              dic['median_z_a'],\n",
    "              dic['median_x_g'],\n",
    "              dic['median_y_g'],\n",
    "              dic['median_z_g'],\n",
    "              \n",
    "              dic['var_x_a'],\n",
    "              dic['var_y_a'],\n",
    "              dic['var_z_a'],\n",
    "              dic['var_x_g'],\n",
    "              dic['var_y_g'],\n",
    "              dic['var_z_g'],\n",
    "              \n",
    "              dic['aadp_x_a'],\n",
    "              dic['aadp_y_a'],\n",
    "              dic['aadp_z_a'],\n",
    "              dic['aadp_x_g'],\n",
    "              dic['aadp_y_g'],\n",
    "              dic['aadp_z_g'],\n",
    "              \n",
    "              dic['ptp_x_a'],\n",
    "              dic['ptp_y_a'],\n",
    "              dic['ptp_z_a'],\n",
    "              dic['ptp_x_g'],\n",
    "              dic['ptp_y_g'],\n",
    "              dic['ptp_z_g'],\n",
    "              \n",
    "              dic['mode_x_a'],\n",
    "              dic['mode_y_a'],\n",
    "              dic['mode_z_a'],\n",
    "              dic['mode_x_g'],\n",
    "              dic['mode_y_g'],\n",
    "              dic['mode_z_g'],\n",
    "              \n",
    "              dic['cov_xy_a'],\n",
    "              dic['cov_yz_a'],\n",
    "              dic['cov_xz_a'],\n",
    "              dic['cov_xy_g'],\n",
    "              dic['cov_yz_g'],\n",
    "              dic['cov_xz_g'],\n",
    "              \n",
    "              dic['mad_x_a'],\n",
    "              dic['mad_y_a'],\n",
    "              dic['mad_z_a'],\n",
    "              dic['mad_x_g'],\n",
    "              dic['mad_y_g'],\n",
    "              dic['mad_z_g'],\n",
    "              \n",
    "              dic['iqr_x_a'],\n",
    "              dic['iqr_y_a'],\n",
    "              dic['iqr_z_a'],\n",
    "              dic['iqr_x_g'],\n",
    "              dic['iqr_y_g'],\n",
    "              dic['iqr_z_g'],\n",
    "              \n",
    "              dic['correlate_xy_a'],\n",
    "              dic['correlate_yz_a'],\n",
    "              dic['correlate_xz_a'],\n",
    "              dic['correlate_xy_g'],\n",
    "              dic['correlate_yz_g'],\n",
    "              dic['correlate_xz_g'],\n",
    "              \n",
    "              dic['skew_x_a'],\n",
    "              dic['skew_y_a'],\n",
    "              dic['skew_z_a'],\n",
    "              dic['skew_x_g'],\n",
    "              dic['skew_y_g'],\n",
    "              dic['skew_z_g'],\n",
    "              \n",
    "              dic['kurtosis_x_a'],\n",
    "              dic['kurtosis_y_a'],\n",
    "              dic['kurtosis_z_a'],\n",
    "              dic['kurtosis_x_g'],\n",
    "              dic['kurtosis_y_g'],\n",
    "              dic['kurtosis_z_g'],\n",
    "              \n",
    "              dic['spectral_energy_x_a'],\n",
    "              dic['spectral_energy_y_a'],\n",
    "              dic['spectral_energy_z_a'],\n",
    "              dic['spectral_energy_x_g'],\n",
    "              dic['spectral_energy_y_g'],\n",
    "              dic['spectral_energy_z_g'],\n",
    "              \n",
    "              dic['entropy_x_a'],\n",
    "              dic['entropy_y_a'],\n",
    "              dic['entropy_z_a'],\n",
    "              dic['entropy_x_g'],\n",
    "              dic['entropy_y_g'],\n",
    "              dic['entropy_z_g']\n",
    "              \n",
    "#               dic['spectral_entropy_x_a'],\n",
    "#               dic['spectral_entropy_y_a'],\n",
    "#               dic['spectral_entropy_z_a'],\n",
    "#               dic['spectral_entropy_x_g'],\n",
    "#               dic['spectral_entropy_y_g'],\n",
    "#               dic['spectral_entropy_z_g']\n",
    "             ]\n",
    "    \n",
    "    \n",
    "    return dic, np.array(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "405969a0-3049-4c9a-a3b1-fd5f15905170",
    "_uuid": "86a331fc-b630-4411-ab56-0375c262e607",
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:31.353612Z",
     "iopub.status.busy": "2022-03-29T07:28:31.353275Z",
     "iopub.status.idle": "2022-03-29T07:28:31.39463Z",
     "shell.execute_reply": "2022-03-29T07:28:31.393714Z",
     "shell.execute_reply.started": "2022-03-29T07:28:31.35358Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rolling_window(a, window, stride):\n",
    "    shape = a.shape[:-1] + (int((a.shape[-1] - window)/stride + 1), window)\n",
    "    strides = (stride*a.strides[-1],) + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "\n",
    "\n",
    "\n",
    "def getIndices2(sampleSize=1000, step=1000, numSamplePoints=24000):\n",
    "    indices = np.arange(0, numSamplePoints, 1)\n",
    "    indices = rolling_window(indices, sampleSize, step)\n",
    "    \n",
    "    return indices\n",
    "\n",
    "\n",
    "def getEncodingArray(df, windows):\n",
    "    a = []\n",
    "    for i in range(len(windows)):\n",
    "        # replaced loc with iloc per documentation\n",
    "#         a.append(signal_to_encoding(df.loc[windows[i], :])[1])\n",
    "        a.append(signal_to_encoding(df.iloc[windows[i], :])[1])\n",
    "        \n",
    "    return np.array(a)\n",
    "\n",
    "\n",
    "def deleteDiagonal(array):\n",
    "    depth = array.shape[-1]\n",
    "    m = array.shape[1]\n",
    "    strided = np.lib.stride_tricks.as_strided\n",
    "    s0,s1,s2 = array.strides\n",
    "    return strided(array.ravel()[depth:], shape=(m-1, m, depth), strides=(s0+s1,s1, s2)).reshape(m, m-1, depth)\n",
    "\n",
    "\n",
    "def MinMaxTransformation(windows_features_array):\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(windows_features_array)\n",
    "#     scaled_array = scaler.transform(windows_features_array)\n",
    "    \n",
    "    return scaler\n",
    "    \n",
    "    \n",
    "def getDistFRR1(dfList, window_size = 1000, step = 1000, numSamplePoints= 18000):\n",
    "    \n",
    "    windows = getIndices2(sampleSize=window_size, step=step, numSamplePoints= numSamplePoints)\n",
    "\n",
    "    norm_dist = []\n",
    "    norm_distro_dict = {}\n",
    "    counter = 1\n",
    "    for m in range(len(dfList)):\n",
    "        \n",
    "        \n",
    "        encoding_array = getEncodingArray(dfList[m], windows)\n",
    "#         print(dfList[m].columns)\n",
    "#         print(encoding_array.shape)\n",
    "\n",
    "#         print(np.sum(encoding_array, axis = 1))\n",
    "\n",
    "        # Doesn't make sense to normalize this here\n",
    "#         scaler = MinMaxTransformation(encoding_array)\n",
    "#         encoding_array = scaler.transform(encoding_array)\n",
    "#         print(encoding_array.shape)\n",
    "        # Should this be put between braces before indexing??? No\n",
    "        encoding_array = encoding_array / np.linalg.norm(encoding_array, axis = 1)[:, None]\n",
    "#         print(np.linalg.norm(encoding_array, axis=1)[:, None].shape)\n",
    "#         print(encoding_array.shape)\n",
    "#         print(np.linalg.norm(encoding_array, axis=1))\n",
    "        # Is dist_array distance calculation done correctly??? It seems it does not square\n",
    "        # It appears so dist = numpy.linalg.norm(a-b)\n",
    "        # https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy\n",
    "        dist_array = (encoding_array[None, :] - encoding_array[:, None])\n",
    "\n",
    "        dist_array = deleteDiagonal(dist_array)\n",
    "\n",
    "        dist_array = np.linalg.norm(dist_array, axis = 2)\n",
    "        \n",
    "        norm_dist.append(dist_array)\n",
    "        \n",
    "        # for err dist\n",
    "        norm_distro_dict[m] = np.array(dist_array).ravel()\n",
    "        \n",
    "        counter += 1\n",
    "    \n",
    "    return {\"dist_array\": np.array(norm_dist).ravel(), \"dist_dict\": norm_distro_dict}\n",
    "\n",
    "\n",
    "def getDistFRRFinal(dfList_exp1, dfList_exp2, window_size = 1000, step = 1000, numSamplePoints= 18000):\n",
    "    '''\n",
    "    dfLists are of the same size.\n",
    "    '''\n",
    "    if len(dfList_exp1) != len(dfList_exp2): \n",
    "        raise Exception(\"dfLists are not of the same size.\")\n",
    "        \n",
    "    windows = getIndices2(sampleSize=window_size, step=step, numSamplePoints= numSamplePoints)\n",
    "\n",
    "    norm_dist = []\n",
    "    norm_distro_dict = {}\n",
    "    voting_dist_dict = {}\n",
    "    counter = 1\n",
    "    for m in range(len(dfList_exp1)):\n",
    "        \n",
    "        \n",
    "        encoding_array_exp1 = getEncodingArray(dfList_exp1[m], windows)\n",
    "        encoding_array_exp2 = getEncodingArray(dfList_exp2[m], windows)\n",
    "        \n",
    "        # Doesn't make sense to normalize this here\n",
    "        scaler = MinMaxTransformation(encoding_array_exp1)\n",
    "        encoding_array_exp1 = scaler.transform(encoding_array_exp1)\n",
    "#         encoding_array_exp2 = scaler.transform(encoding_array_exp2)\n",
    "        \n",
    "#         # approach 2: Not intuitive as you dont have access to all of user2s stream of 4 min, only windowsizes at a time\n",
    "#         scaler = MinMaxTransformation(encoding_array_exp2)\n",
    "\n",
    "        encoding_array_exp2 = scaler.transform(encoding_array_exp2)\n",
    "        \n",
    "#         print(dfList[m].columns)\n",
    "#         print(encoding_array.shape)\n",
    "\n",
    "#         print(np.sum(encoding_array, axis = 1))\n",
    "#         print((encoding_array_exp1 / np.linalg.norm(encoding_array_exp1, axis = 1)[:, None]).shape)\n",
    "#         print(encoding_array_exp1.shape)\n",
    "#         print(encoding_array.shape)\n",
    "        # Should this be put between braces before indexing??? No\n",
    "        encoding_array_exp1 = encoding_array_exp1 / np.linalg.norm(encoding_array_exp1, axis = 1)[:, None]\n",
    "        encoding_array_exp2 = encoding_array_exp2 / np.linalg.norm(encoding_array_exp2, axis = 1)[:, None]\n",
    "#         print(np.linalg.norm(encoding_array, axis=1)[:, None].shape)\n",
    "#         print(encoding_array.shape)\n",
    "#         print(np.linalg.norm(encoding_array, axis=1))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # Is dist_array distance calculation done correctly??? It seems it does not square\n",
    "        # It appears so dist = numpy.linalg.norm(a-b)\n",
    "        # https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy\n",
    "        dist_array = (encoding_array_exp1[None, :] - encoding_array_exp2[:, None])\n",
    "#         print(dist_array.shape)\n",
    "        \n",
    "        dist_array = np.linalg.norm(dist_array, axis = 2)\n",
    "        \n",
    "        norm_dist.append(dist_array)\n",
    "        \n",
    "#         print(dist_array.shape)\n",
    "        # for err dist\n",
    "        norm_distro_dict[m] = np.array(dist_array).ravel()\n",
    "        \n",
    "        # for voting dist\n",
    "        voting_dist_dict[m] = dist_array[None, :]\n",
    "        \n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "#         print(voting_dist_dict[0].shape)\n",
    "    \n",
    "    return {\"dist_array\": np.array(norm_dist).ravel(), \"dist_dict\": norm_distro_dict, \"voting_dist_dict\": voting_dist_dict}\n",
    "\n",
    "\n",
    "def getDistFARFinal(dfList, window_size = 1000, step = 1000, numSamplePoints= 18000):\n",
    "    \n",
    "    windows = getIndices2(sampleSize=window_size, step=step, numSamplePoints= numSamplePoints)\n",
    "\n",
    "    norm_dist = []\n",
    "    norm_distro_dict = {}\n",
    "    voting_dist_dict = {}\n",
    "    counter = 1\n",
    "    \n",
    "    encoding_array_dic = {}\n",
    "    for i in range(len(dfList)):\n",
    "        encoding_array_dic[i] = getEncodingArray(dfList[i], windows)\n",
    "        # Should this be put between braces before indexing???\n",
    "#         # Should this be done here? Not here this only makes the result vector small and should be at the end\n",
    "#         encoding_array_dic[i] = encoding_array_dic[i] / np.linalg.norm(encoding_array_dic[i], axis = 1)[:, None]\n",
    "        \n",
    "    for m in range(len(dfList)):\n",
    "        \n",
    "        cum_distro_array = []\n",
    "        encoding_array_m = encoding_array_dic[m]\n",
    "        \n",
    "        # Should I scale the new vector with the transform of the user profile?\n",
    "        scaler = MinMaxTransformation(encoding_array_m)\n",
    "        encoding_array_m = scaler.transform(encoding_array_m)\n",
    "        \n",
    "        encoding_array_m = encoding_array_m / np.linalg.norm(encoding_array_m, axis = 1)[:, None]\n",
    "        \n",
    "        for k in range(len(dfList)):\n",
    "            \n",
    "            if m != k:\n",
    "                \n",
    "                encoding_array_k = encoding_array_dic[k]\n",
    "                \n",
    "#                 # approach 2\n",
    "#                 scaler = MinMaxTransformation(encoding_array_k)\n",
    "                \n",
    "                # Scale array_k with array_m transform\n",
    "                encoding_array_k = scaler.transform(encoding_array_k)\n",
    "                \n",
    "                encoding_array_k = encoding_array_k / np.linalg.norm(encoding_array_k, axis = 1)[:, None]\n",
    "                \n",
    "                # Is dist_array distance calculation done correctly??? It seems it does not square. No, resolved\n",
    "                # It appears so dist = numpy.linalg.norm(a-b)\n",
    "                # https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy\n",
    "                dist_array = (encoding_array_m[None, :] - encoding_array_k[:, None])\n",
    "                \n",
    "                # print and check dimensions\n",
    "                \n",
    "                dist_array = np.linalg.norm(dist_array, axis = 2)\n",
    "\n",
    "                norm_dist.append(dist_array)\n",
    "                \n",
    "                # for err dist\n",
    "                cum_distro_array.append(dist_array)\n",
    "                \n",
    "\n",
    "        \n",
    "        norm_distro_dict[m] = np.array(cum_distro_array).ravel()\n",
    "        \n",
    "        # for voting dist\n",
    "        voting_dist_dict[m] = np.array(cum_distro_array)\n",
    "\n",
    "        counter += 1\n",
    "        \n",
    "#         print(voting_dist_dict[0].shape)\n",
    "    \n",
    "    return {\"dist_array\": np.array(norm_dist).ravel(), \"dist_dict\": norm_distro_dict, \"voting_dist_dict\": voting_dist_dict}\n",
    "\n",
    "\n",
    "def decision_confidence(dist_array, dist_threshold):\n",
    "    '''\n",
    "    input: dist_array: (N, # unknown_user windows, # genuine_user windows)\n",
    "    output: (N, de/auth decision percentage)\n",
    "    '''\n",
    "    vals = np.where(dist_array < dist_threshold, 1, 0)\n",
    "    windows_decision_confidence = np.mean(vals, axis = 2)\n",
    "\n",
    "    return windows_decision_confidence\n",
    "\n",
    "\n",
    "def decision_module(dist_array, dist_threshold, acceptance_threshold):\n",
    "    '''\n",
    "    input: dist_array: (N, # unknown_user windows, # genuine_user windows)\n",
    "    output: (N, de/auth boolean decision)\n",
    "    '''\n",
    "    \n",
    "    windows_decision_confidence = decision_confidence(dist_array, dist_threshold = dist_threshold)\n",
    "    windows_final_decision = np.where(windows_decision_confidence >= acceptance_threshold, 1, 0)\n",
    "    \n",
    "    return windows_final_decision\n",
    "    \n",
    "def FRR_vote_based(dist_array, dist_threshold = None, acceptance_threshold = None):\n",
    "    '''\n",
    "    input: dist_array: (N, # unknown_user windows, # genuine_user windows)\n",
    "    output: vote based FRR\n",
    "    '''\n",
    "    \n",
    "    #good note but not applicable here np.where((a==0)|(a==1), a^1, a)\n",
    "\n",
    "    windows_final_decision = decision_module(dist_array, dist_threshold, acceptance_threshold)\n",
    "    vals = windows_final_decision^1\n",
    "    \n",
    "    return np.mean(vals)\n",
    "\n",
    "\n",
    "def FAR_vote_based(dist_array, dist_threshold = None, acceptance_threshold = None):\n",
    "    '''\n",
    "    input: dist_array: (M*N, # unknown_user windows, # genuine_user windows)\n",
    "    output: vote based FAR\n",
    "    '''\n",
    "    windows_final_decision = decision_module(dist_array, dist_threshold, acceptance_threshold)\n",
    "    vals = windows_final_decision\n",
    "    \n",
    "    return np.mean(vals)\n",
    "\n",
    "\n",
    "def FRR(dist, threshold):\n",
    "    \n",
    "    vals = np.where(dist < threshold, 0, 1)\n",
    "    \n",
    "    return np.mean(vals)\n",
    "\n",
    "\n",
    "def FAR(dist, threshold):\n",
    "    \n",
    "    vals = np.where(dist < threshold, 1, 0)\n",
    "    \n",
    "    return np.mean(vals)\n",
    "\n",
    "\n",
    "def DistroFRR(dist_dict, threshold):\n",
    "    \n",
    "    distro = []\n",
    "    for i in range(len(dist_dict)):\n",
    "        vals = np.where(dist_dict[i] < threshold, 0, 1)\n",
    "        distro.append(sum(vals))\n",
    "        \n",
    "    return distro\n",
    "\n",
    "\n",
    "def DistroFAR(dist_dict, threshold):\n",
    "    \n",
    "    distro = []\n",
    "    for i in range(len(dist_dict)):\n",
    "        vals = np.where(dist_dict[i] < threshold, 1, 0)\n",
    "        distro.append(sum(vals))\n",
    "        \n",
    "    return distro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "22910d65-b4d1-4e4a-84d3-a25c50e209d4",
    "_uuid": "84284a3d-28ba-4306-b7a6-5f75fc60d6ef",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:31.771334Z",
     "iopub.status.busy": "2022-03-29T07:28:31.771015Z",
     "iopub.status.idle": "2022-03-29T07:28:31.776454Z",
     "shell.execute_reply": "2022-03-29T07:28:31.775517Z",
     "shell.execute_reply.started": "2022-03-29T07:28:31.771306Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# np.where(np.array([.5, 1]) < .6, 1, 0)\n",
    "# a = np.array(range(5, 11))\n",
    "# b = np.array(range(2, 6))\n",
    "\n",
    "# res = a[None, :] - b[:, None]\n",
    "# # print(res)\n",
    "# a = res % 2\n",
    "# print(a)\n",
    "# a^1\n",
    "# b = np.array([a, a-9])\n",
    "# print(b)\n",
    "# np.concatenate(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f65d8fe7-ec15-4c54-bf10-60be43248f67",
    "_uuid": "44582f38-8dfe-4b42-bb60-0b9fb69c7d03",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:32.146322Z",
     "iopub.status.busy": "2022-03-29T07:28:32.14601Z",
     "iopub.status.idle": "2022-03-29T07:28:32.149655Z",
     "shell.execute_reply": "2022-03-29T07:28:32.149003Z",
     "shell.execute_reply.started": "2022-03-29T07:28:32.146295Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# d = {}\n",
    "# d[0] = np.array([[[1,2]]])\n",
    "# d[1] = np.array([[[2,3]]])\n",
    "# a = np.array(list(d.values()))\n",
    "# print(a.shape)\n",
    "# a = np.concatenate(a)\n",
    "# print(a.shape)\n",
    "# a = np.concatenate(a)\n",
    "# print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0fb342ee-f860-4038-8e5d-e5f15e05a0b7",
    "_uuid": "789c9f3b-abe9-4d26-b9ce-1e2ef446819c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:32.539084Z",
     "iopub.status.busy": "2022-03-29T07:28:32.538726Z",
     "iopub.status.idle": "2022-03-29T07:28:32.547034Z",
     "shell.execute_reply": "2022-03-29T07:28:32.546336Z",
     "shell.execute_reply.started": "2022-03-29T07:28:32.53905Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def getEER(distFRR, distFAR, thresholdList=None):\n",
    "    \n",
    "    if thresholdList is None:\n",
    "        thresholdList = np.arange(0, 3, 0.001)\n",
    "    \n",
    "    farList = []\n",
    "    frrList = []\n",
    "    \n",
    "    eer = []\n",
    "    for t in thresholdList:\n",
    "        far = FAR(distFAR, threshold = t)\n",
    "        frr = FRR(distFRR, threshold = t)\n",
    "        farList.append(far)\n",
    "        frrList.append(frr)\n",
    "        eer.append(abs(far-frr))\n",
    "        \n",
    "    eer = np.array(eer)\n",
    "    eer[eer==0] = 99999\n",
    "    print(farList[np.argmin(eer)])\n",
    "    print(frrList[np.argmin(eer)])\n",
    "\n",
    "#     print(\"EER: {}\".format((frrList[np.argmin(eer)] + farList[np.argmin(eer)])/2))\n",
    "    return {\"EER\": (frrList[np.argmin(eer)] + farList[np.argmin(eer)])/2, \"farList\": farList, \"frrList\": frrList, \"EER_threshold\": thresholdList[np.argmin(eer)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f960835d-39be-43fa-8133-1ad0afd703b3",
    "_uuid": "0fe92d18-0abf-49c8-bee3-45757483d7f0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:32.976193Z",
     "iopub.status.busy": "2022-03-29T07:28:32.975578Z",
     "iopub.status.idle": "2022-03-29T07:28:32.984663Z",
     "shell.execute_reply": "2022-03-29T07:28:32.983803Z",
     "shell.execute_reply.started": "2022-03-29T07:28:32.976154Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def getEERVoteBased(dist_array_FRR, dist_array_FAR, thresholdList=None):\n",
    "    \n",
    "    if thresholdList is None:\n",
    "        thresholdList = np.arange(0, 3, 0.001)\n",
    "    \n",
    "    farList = []\n",
    "    frrList = []\n",
    "    \n",
    "    eer = []\n",
    "    acceptance_threshold = .6\n",
    "    \n",
    "    for t in thresholdList:\n",
    "        far = FAR_vote_based(dist_array_FAR, dist_threshold = t, acceptance_threshold = acceptance_threshold)\n",
    "        frr = FRR_vote_based(dist_array_FRR, dist_threshold = t, acceptance_threshold = acceptance_threshold)\n",
    "        farList.append(far)\n",
    "        frrList.append(frr)\n",
    "        eer.append(abs(far-frr))\n",
    "        \n",
    "    eer = np.array(eer)\n",
    "    eer[eer==0] = 99999\n",
    "    print(farList[np.argmin(eer)])\n",
    "    print(frrList[np.argmin(eer)])\n",
    "\n",
    "#     print(\"EER: {}\".format((frrList[np.argmin(eer)] + farList[np.argmin(eer)])/2))\n",
    "    return {\"EER\": (frrList[np.argmin(eer)] + farList[np.argmin(eer)])/2, \"farList\": farList, \"frrList\": frrList, \"EER_threshold\": thresholdList[np.argmin(eer)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c3d38626-be37-4339-9588-cbb6f973631f",
    "_uuid": "61a4c410-4d16-4105-b115-77a477c1a094",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:33.341998Z",
     "iopub.status.busy": "2022-03-29T07:28:33.341667Z",
     "iopub.status.idle": "2022-03-29T07:28:33.354483Z",
     "shell.execute_reply": "2022-03-29T07:28:33.353407Z",
     "shell.execute_reply.started": "2022-03-29T07:28:33.341969Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def getEERWindowsDict(dfList_exp1, start_window_size=250, end_window_size=3000, increment_step=250, numSamplePoints=22001, isEqualSampleSize = False, fixedSampleStep=3000, thresholdList=None, dfList_exp2=None):\n",
    "    \n",
    "    window_EER_dict = {}\n",
    "    window_EER_threshold_dict = {}\n",
    "    window_farList_dict = {}\n",
    "    window_frrList_dict = {}\n",
    "    window_farDistro_dict = {}\n",
    "    window_frrDistro_dict = {}\n",
    "    \n",
    "    lst = np.arange(start_window_size, end_window_size + 1, increment_step)\n",
    "    \n",
    "    for w in lst:\n",
    "        if isEqualSampleSize:\n",
    "            sampleStep = fixedSampleStep\n",
    "        else:\n",
    "            sampleStep = w\n",
    "        \n",
    "        if dfList_exp2 is None:\n",
    "            print(\"dfList_exp2 is None\")\n",
    "            distFRRDATA = getDistFRR1(dfList_exp1, window_size = w, step = sampleStep, numSamplePoints= numSamplePoints)\n",
    "        else:\n",
    "            print(\"dfList_exp2 is Not None\")\n",
    "            distFRRDATA = getDistFRRFinal(dfList_exp1, dfList_exp2, window_size = w, step = sampleStep, numSamplePoints= numSamplePoints)\n",
    "            \n",
    "        distFARDATA = getDistFARFinal(dfList_exp1, window_size = w, step = sampleStep, numSamplePoints= numSamplePoints)\n",
    "        \n",
    "#         print('--- start of voting based')\n",
    "#         print(distFRRDATA[\"voting_dist_dict\"][0].shape)\n",
    "#         print(distFARDATA[\"voting_dist_dict\"][0].shape)\n",
    "#         print(np.concatenate(list(distFRRDATA[\"voting_dist_dict\"].values())).shape)\n",
    "#         print(np.concatenate(list(distFARDATA[\"voting_dist_dict\"].values())).shape)\n",
    "        \n",
    "        \n",
    "#         voting_dist_FRR = np.concatenate(list(distFRRDATA[\"voting_dist_dict\"].values()))\n",
    "#         voting_dist_FAR = np.concatenate(list(distFARDATA[\"voting_dist_dict\"].values()))\n",
    "#         voting_EER_data = getEERVoteBased(voting_dist_FRR, voting_dist_FAR, thresholdList=thresholdList)\n",
    "        \n",
    "#         print(\"numParticipants: {}, windowSize: {}, isEqualSampleSize: {}, EER: {}\".format(len(dfList_exp1), w, isEqualSampleSize, voting_EER_data[\"EER\"]))\n",
    "        \n",
    "#         print(\"--- end of voting based\")\n",
    "        \n",
    "        distFRR = distFRRDATA[\"dist_array\"]\n",
    "        distFAR = distFARDATA[\"dist_array\"]\n",
    "        EER_data = getEER(distFRR, distFAR, thresholdList=thresholdList)\n",
    "        \n",
    "        window_EER_dict[w] = EER_data[\"EER\"]\n",
    "        window_EER_threshold_dict[w] = EER_data[\"EER_threshold\"]\n",
    "        window_farList_dict[w] = EER_data[\"farList\"]\n",
    "        window_frrList_dict[w] = EER_data[\"frrList\"]\n",
    "        window_farDistro_dict[w] = distFARDATA[\"dist_dict\"]\n",
    "        window_frrDistro_dict[w] = distFRRDATA[\"dist_dict\"]\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"numParticipants: {}, windowSize: {}, isEqualSampleSize: {}, EER: {}\".format(len(dfList_exp1), w, isEqualSampleSize, window_EER_dict[w]))\n",
    "        \n",
    "    return { \"window_EER_dict\": window_EER_dict, \"window_EER_threshold_dict\": window_EER_threshold_dict , \"window_farList_dict\": window_farList_dict, \"window_frrList_dict\": window_frrList_dict, \"window_farDistro_dict\": window_farDistro_dict, \"window_frrDistro_dict\": window_frrDistro_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c8a14ba8-0697-40f9-86cc-89421fb6a520",
    "_uuid": "a53170a9-9345-41a5-a95d-29ef35396180",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:33.696707Z",
     "iopub.status.busy": "2022-03-29T07:28:33.696138Z",
     "iopub.status.idle": "2022-03-29T07:28:33.701551Z",
     "shell.execute_reply": "2022-03-29T07:28:33.700645Z",
     "shell.execute_reply.started": "2022-03-29T07:28:33.696663Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def getErrFixedThreshold(distFRR, distFAR, threshold):\n",
    "        \n",
    "    far = FAR(distFAR, threshold = threshold)\n",
    "    frr = FRR(distFRR, threshold = threshold)\n",
    "    \n",
    "\n",
    "    return {\"FAR\": far, \"FRR\": frr, \"threshold\": threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1d0c061-c833-4051-88cd-b03cf6665d3a",
    "_uuid": "e709a3ce-9cac-4c5b-908a-5a9f0eaaf4ac",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:34.106091Z",
     "iopub.status.busy": "2022-03-29T07:28:34.105346Z",
     "iopub.status.idle": "2022-03-29T07:28:34.115418Z",
     "shell.execute_reply": "2022-03-29T07:28:34.114408Z",
     "shell.execute_reply.started": "2022-03-29T07:28:34.106052Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# To be done\n",
    "def getErrFixedThresholdWindowsDict(dfList, start_window_size=250, end_window_size=3000, increment_step=250, numSamplePoints=22001, isEqualSampleSize = False, fixedSampleStep=3000):\n",
    "    \n",
    "    window_EER_dict = {}\n",
    "    window_EER_threshold_dict = {}\n",
    "    window_farDistro_dict = {}\n",
    "    window_frrDistro_dict = {}\n",
    "    \n",
    "    lst = np.arange(start_window_size, end_window_size + 1, increment_step)\n",
    "    \n",
    "    for w in lst:\n",
    "        if isEqualSampleSize:\n",
    "            sampleStep = fixedSampleStep\n",
    "        else:\n",
    "            sampleStep = w\n",
    "            \n",
    "        distFRRDATA = getDistFRRFinal(dfList, window_size = w, step = sampleStep, numSamplePoints= numSamplePoints)\n",
    "        distFARDATA = getDistFARFinal(dfList, window_size = w, step = sampleStep, numSamplePoints= numSamplePoints)\n",
    "        \n",
    "        distFRR = distFRRDATA[\"dist_array\"]\n",
    "        distFAR = distFARDATA[\"dist_array\"]\n",
    "        EER_data = getEER(distFRR, distFAR)\n",
    "\n",
    "        \n",
    "        window_EER_dict[w] = EER_data[\"EER\"]\n",
    "        window_EER_threshold_dict[w] = EER_data[\"EER_threshold\"]\n",
    "        window_farDistro_dict[w] = distFARDATA[\"dist_dict\"]\n",
    "        window_frrDistro_dict[w] = distFRRDATA[\"dist_dict\"]\n",
    "        \n",
    "        print(\"numParticipants: {}, windowSize: {}, isEqualSampleSize: {}, EER: {}\".format(len(dfList), w, isEqualSampleSize, window_EER_dict[w]))\n",
    "        \n",
    "    return { \"window_EER_dict\": window_EER_dict, \"window_EER_threshold_dict\": window_EER_threshold_dict , \"window_farList_dict\": window_farList_dict, \"window_frrList_dict\": window_frrList_dict, \"window_farDistro_dict\": window_farDistro_dict, \"window_frrDistro_dict\": window_frrDistro_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "ae9b2983-8cec-4f02-922a-d7b8c0e36822",
    "_uuid": "6afdfc26-d91b-4d3a-8604-528d56426826",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cite the thesis paper i found\n",
    "def utils_ppp(P):\n",
    "    \"\"\"Pretty print parameters of an experiment.\"\"\"\n",
    "    df = pd.DataFrame([asdict(P)])\n",
    "    df = df.T\n",
    "    df.columns = [\"Value\"]\n",
    "    \n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "ef3ded46-6c27-44c6-8792-7c30cf5c0af3",
    "_uuid": "449eb557-cbf1-416c-b0ec-ddcc4f104119",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# source: https://github.com/dynobo/ContinAuth/blob/master/notebooks/utils.ipynb\n",
    "def utils_eer(y_true, y_pred, return_threshold=False):\n",
    "    \"\"\"Calculate the Equal Error Rate.\n",
    "\n",
    "    Based on https://stackoverflow.com/a/49555212, https://yangcha.github.io/EER-ROC/\n",
    "    and https://scikit-learn.org/stable/modules/model_evaluation.html#implementing-your-own-scoring-object\n",
    "\n",
    "    Arguments:\n",
    "        y_true {np.array}  -- Actual labels\n",
    "        y_pred {np.array}  -- Predicted labels or probability\n",
    "        \n",
    "    Returns:\n",
    "        float              -- Equal Error Rate        \n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred, pos_label=1)\n",
    "    eer = brentq(lambda x: 1.0 - x - interp1d(fpr, tpr)(x), 0.0, 1.0)\n",
    "    thresh = interp1d(fpr, thresholds)(eer)  # Calculated threshold, not needed for score\n",
    "    if return_threshold:\n",
    "        return eer, thresh\n",
    "    else:\n",
    "        return eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "8e188ef2-60ad-44e7-8c98-77d859e152ec",
    "_uuid": "c755e89d-f293-4da4-a418-85186fcfbe17",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER: 0.333, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.400, Threshold: 0.200 <-- Worse case\n"
     ]
    }
   ],
   "source": [
    "TEST_MODE = 1\n",
    "if TEST_MODE:\n",
    "    temp_eer, tres = utils_eer(\n",
    "        [-1, -1, -1, 1, 1, 1], [0, 0.9, 0.1, 0.74, 0.8, .6], return_threshold=True\n",
    "    )\n",
    "    print(f\"EER: {temp_eer:.3f}, Threshold: {tres:.3f} <-- Arbitrary case\")\n",
    "\n",
    "    temp_eer, tres = utils_eer(\n",
    "        [-1, -1, -1, 1, 1], [0.1, 0.2, 0.3, 1, 0.9], return_threshold=True\n",
    "    )\n",
    "    print(f\"EER: {temp_eer:.3f}, Threshold: {tres:.3f} <-- Best case\")\n",
    "\n",
    "    temp_eer, tres = utils_eer(\n",
    "        [1, 1, 1, -1, -1], [0.1, 0.2, 0.3, 1, 0.9], return_threshold=True\n",
    "    )\n",
    "    print(f\"EER: {temp_eer:.3f}, Threshold: {tres:.3f} <-- Worse case\")\n",
    "    \n",
    "#     new case does it make sense? I don't think so\n",
    "    temp_eer, tres = utils_eer(\n",
    "        [1, 1, 1, -1, -1], [-1, 1, -1, -1, -1], return_threshold=True\n",
    "    )\n",
    "    print(f\"EER: {temp_eer:.3f}, Threshold: {tres:.3f} <-- Worse case\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array([0.74, 0.8, .6]) > .6).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array([0, 0.9, 0.1])<.6).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "304dbab1-3cf2-4166-a521-178f5a3f55fe",
    "_uuid": "1f3464be-7fe5-4d37-bf84-e2f63e99b345",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:36.875497Z",
     "iopub.status.busy": "2022-03-29T07:28:36.874802Z",
     "iopub.status.idle": "2022-03-29T07:28:36.879647Z",
     "shell.execute_reply": "2022-03-29T07:28:36.878748Z",
     "shell.execute_reply.started": "2022-03-29T07:28:36.875453Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "utils_eer_scorer = make_scorer(utils_eer, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0092c407-5812-43bd-a3a9-f78bfadff3b7",
    "_uuid": "35621051-b220-428d-b505-85ef106f8ce6"
   },
   "source": [
    "# Split Dataset for Valid/Test  \n",
    "In two splits: one used during hyperparameter optimization, and one used during testing.\n",
    "\n",
    "The split is done along the subjects: All sessions of a single subject will either be in the validation split or in the testing split, never in both.\n",
    "\n",
    "They did a 30 60 split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "92d8caee-6b8e-45c7-b272-010046daec00",
    "_uuid": "0c7969f6-43b9-4d95-9253-76d5d040d30a"
   },
   "source": [
    "# Reshaping Raw Features.\n",
    "We have our own function of windows for this. Do this for both training and testing.\n",
    "\n",
    "# Extracting time and frequency based features.\n",
    "Again, we have a function for this. Do this for both training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "11bbc563-4576-4588-a73c-44936653c13b",
    "_uuid": "b9984fbd-a13f-4d5d-ad57-8b0c41aafe1d"
   },
   "source": [
    "# Hyperparameter Optimization \n",
    "\n",
    "I do not find any reaqsonable explaination how to use a cross-validation as we are talking about anomaly detection.\n",
    "\n",
    "I am using the experiment 1 data as train, and experiment 2 data as validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d09c240f-d000-4840-880c-260436115209",
    "_uuid": "aca85e4e-ed2f-4c54-bf0f-2f938ac45a07"
   },
   "source": [
    "# Using SVM in a real-world Scenario with multiple genuine users and intruders\n",
    "Source: https://datascience.stackexchange.com/questions/23623/what-is-the-best-way-to-classify-data-not-belonging-to-set-of-classes\n",
    "\n",
    "Stage 1: \n",
    "    Use one-class SVM to assign those images that do not belong to the set of predefined classes as the 9-th class.\n",
    "\n",
    "Stage 2:\n",
    "    For those images that passes through your filter, let the multi-class SVM assign them to one of the 8 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79b10ce2-42b2-4156-a1df-633bf52c05e6",
    "_uuid": "bf0780fa-46ed-4d13-90ee-3640fe7cc4aa"
   },
   "source": [
    "Loading data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "72d60ae5-be59-4add-868d-4ba29d842b9f",
    "_uuid": "126e56a7-4e62-4f9d-adf7-89308283fccd",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:39.619456Z",
     "iopub.status.busy": "2022-03-29T07:28:39.618793Z",
     "iopub.status.idle": "2022-03-29T07:28:39.626966Z",
     "shell.execute_reply": "2022-03-29T07:28:39.626146Z",
     "shell.execute_reply.started": "2022-03-29T07:28:39.619418Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def load_data_frames(user_ids, begin_idx, end_idx, min_len):\n",
    "    '''\n",
    "    input: \n",
    "        user_ids: list of approved user_ids after exploratory data analysis\n",
    "        begin_idx: the index before which data is discarded for user i\n",
    "        end_idx: the index after which data is discarded for user i\n",
    "        min_len: the minimum length that a dataframe has to be after cutting of both endings\n",
    "        \n",
    "    output:\n",
    "        {dfList_exp1, dfList_exp2}: return dfList for exp1 and exp2 of the selected user_ids\n",
    "    '''\n",
    "    print(\"Loading exp1 data:\")\n",
    "    dfList_exp1 = []\n",
    "    for i in user_ids:\n",
    "        dic = getDataStats1(i, begin_idx=begin_idx, end_idx=end_idx)\n",
    "\n",
    "        if(dic['accel']<min_len):\n",
    "            raise Exception(\"The Stream is shorter than {}\".format(min_len))\n",
    "\n",
    "        dfList_exp1 = dfList_exp1 + [dic['df'].reset_index(drop=True)]\n",
    "\n",
    "\n",
    "    print(\"Loading exp2 data:\")\n",
    "    dfList_exp2 = []\n",
    "    for i in user_ids:\n",
    "        dic = getDataStats2(i, begin_idx=begin_idx, end_idx=end_idx)\n",
    "\n",
    "        if(dic['accel']<min_len):\n",
    "            raise Exception(\"The Stream is shorter than {}\".format(min_len))\n",
    "\n",
    "        dfList_exp2 = dfList_exp2 + [dic['df'].reset_index(drop=True)]\n",
    "    #     dfList = dfList + [dic['df']]\n",
    "    \n",
    "    return {\"dfList_exp1\": dfList_exp1, \"dfList_exp2\": dfList_exp2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "adb215c0-9999-4e8f-a64f-1ce0bef9da1a",
    "_uuid": "696cb33f-f151-4447-8e46-7018e15cc5be",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:40.156606Z",
     "iopub.status.busy": "2022-03-29T07:28:40.15626Z",
     "iopub.status.idle": "2022-03-29T07:28:40.171091Z",
     "shell.execute_reply": "2022-03-29T07:28:40.170208Z",
     "shell.execute_reply.started": "2022-03-29T07:28:40.156575Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def MakeXExpDic(dfList_exp1, dfList_exp2, window_size = 1000, step = 1000, numSamplePoints= 18000):\n",
    "    '''\n",
    "    return \n",
    "    X_exp1_dic\n",
    "    X_exp2_dic\n",
    "    dfLists are of the same size.\n",
    "    '''\n",
    "    if len(dfList_exp1) != len(dfList_exp2): \n",
    "        raise Exception(\"dfLists are not of the same size.\")\n",
    "    \n",
    "    windows = getIndices2(sampleSize=window_size, step=step, numSamplePoints= numSamplePoints)\n",
    "\n",
    "    X_exp1_dic = {}\n",
    "    X_exp2_dic = {}\n",
    "    for i in range(len(dfList_exp1)):\n",
    "        \n",
    "        \n",
    "        encoding_array_exp1 = getEncodingArray(dfList_exp1[i], windows)\n",
    "        encoding_array_exp2 = getEncodingArray(dfList_exp2[i], windows)\n",
    "        \n",
    "        X_exp1_dic[i] = encoding_array_exp1\n",
    "        X_exp2_dic[i] = encoding_array_exp2\n",
    "        \n",
    "    return {\"X_exp1_dic\": X_exp1_dic, \"X_exp2_dic\": X_exp2_dic}\n",
    "\n",
    "\n",
    "def OneClassSVMSets(k, X_exp1_dic, X_exp2_dic, cv=5):\n",
    "    '''\n",
    "    return the required sets for an OCSVM trained on the user with key. \n",
    "    X_train: X data from X_exp1_dic[k]\n",
    "    X_test_regular: X data from X_exp2_dic[k]\n",
    "    X_test_anomalous: X data from X_exp2_dic[!k]\n",
    "    '''\n",
    "    \n",
    "    if k not in  X_exp1_dic:\n",
    "        raise Exception(\"invalid key for dic\")\n",
    "        \n",
    "    \n",
    "    X_pos = X_exp1_dic[k]\n",
    "#     X_neg = np.concatenate([X_exp1_dic[key] for key in X_exp1_dic.keys() if key != k], axis=0)\n",
    "    X_test_regular = X_exp2_dic[k]\n",
    "    X_test_anomalous = np.concatenate([X_exp2_dic[key] for key in X_exp2_dic.keys() if key != k], axis=0)\n",
    "    \n",
    "    \n",
    "#     n, m = len(Xpos), len(Xneg)\n",
    "    np.random.shuffle(X_neg)\n",
    "    print((X_neg.shape[0], X_pos.shape[0]))\n",
    "    X_neg = X_neg[np.random.choice(X_neg.shape[0], size=X_pos.shape[0], replace=False), :]\n",
    "    print(X_pos.shape, X_neg.shape)\n",
    "    # Creating (train, test) tuples of indices for k-folds cross-validation\n",
    "    # We split the positive class (normal data) as we only want the positive examples in the training set.\n",
    "    \n",
    "    train_splits = KFold(n_splits=cv, shuffle=True).split(X_pos)\n",
    "    anomalous_splits = KFold(n_splits=cv, shuffle=True).split(X_neg)\n",
    "\n",
    "#     print(len(train_splits), len(anomalous_splits))\n",
    "    # Negative examples (abnormal data) are added to the test set (see https://stackoverflow.com/a/58459322/3673842)\n",
    "    y_train = np.concatenate([np.repeat(1.0, len(X_pos)), np.repeat(-1.0, len(X_neg))])\n",
    "    X_train = np.concatenate([X_pos, X_neg], axis=0)\n",
    "    \n",
    "    # https://github.com/steppi/adeft/blob/anomaly_detection/adeft/modeling/find_anomalies.py#L170\n",
    "    cv_splits = ((train, np.concatenate((test, anom_test + X_pos.shape[0]), axis = 0))\n",
    "                  for (train, test), (_, anom_test)\n",
    "                  in zip(train_splits, anomalous_splits))\n",
    "    \n",
    "    return {\"X_train\": X_train, \"y_train\": y_train, \"X_test_regular\": X_test_regular, \"X_test_anomalous\": X_test_anomalous, \"cv_splits\": cv_splits}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For CNN file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRawDataChunks(df, windows, scale=False, scaler=None, user_idx=None, exp_num=None, verbose=0):\n",
    "    a = []\n",
    "    df = df.drop(columns=[\"time_stamp\"]).copy()\n",
    "    # df_array = df.to_numpy()\n",
    "    \n",
    "    if scale:\n",
    "        if verbose: print(f\"user_idx: {user_idx}, exp_num: {exp_num}, scale: {scale}, scaler: {scaler}\")\n",
    "        # print(df_array.shape)\n",
    "        scaler = get_new_scaler_dict[scaler]\n",
    "        # scaler = scaler().fit(df_array)\n",
    "        scaler = scaler().fit(df)\n",
    "        # df_array = scaler.transform(df_array)\n",
    "        df_array = scaler.transform(df)\n",
    "        # scaled_df = pd.DataFrame(data=df_array, columns = df.columns, dtype=df_array.dtype)\n",
    "        scaled_df = pd.DataFrame(data=df_array, columns = df.columns, dtype=df_array.dtype)\n",
    "        df = scaled_df\n",
    "        \n",
    "        \n",
    "    for i in range(len(windows)):\n",
    "        # replaced loc with iloc per documentation\n",
    "        # a.append(df_array[windows[i], :]) #CNN\n",
    "        a.append(df.iloc[windows[i], :]) #waca\n",
    "    \n",
    "#     print(len(a))\n",
    "#     print(len(a))\n",
    "#     return np.array([a])\n",
    "    # return np.array(a), scaler #CNN\n",
    "    if not scale:\n",
    "        scaler = None\n",
    "    \n",
    "    return a, scaler #waca\n",
    "\n",
    "def MakeRawXExpDic(dfList_exp1, dfList_exp2, window_size = 1000, step = 1000, numSamplePoints= 18000, scale_exp1=False, scale_exp2=True, scaler=\"MinMaxScaler\"):\n",
    "    '''\n",
    "    return \n",
    "    X_exp1_dic\n",
    "    X_exp2_dic\n",
    "    dfLists are of the same size.\n",
    "    '''\n",
    "\n",
    "    if len(dfList_exp1) != len(dfList_exp2): \n",
    "        raise Exception(\"dfLists are not of the same size.\")\n",
    "    \n",
    "    windows = getIndices2(sampleSize=window_size, step=step, numSamplePoints= numSamplePoints)\n",
    "\n",
    "    X_exp1_dic = {}\n",
    "    X_exp2_dic = {}\n",
    "    fitted_scaler_exp1_dic={}\n",
    "    fitted_scaler_exp2_dic={}\n",
    "    for i in range(len(dfList_exp1)):\n",
    "        \n",
    "        \n",
    "        encoding_array_exp1, fitted_scaler_exp1 = getRawDataChunks(dfList_exp1[i], windows, scale=scale_exp1, scaler=scaler, user_idx=i, exp_num=1)\n",
    "        encoding_array_exp2, fitted_scaler_exp2 = getRawDataChunks(dfList_exp2[i], windows, scale=scale_exp2, scaler=scaler, user_idx=i, exp_num=2)\n",
    "        \n",
    "        X_exp1_dic[i] = encoding_array_exp1\n",
    "        X_exp2_dic[i] = encoding_array_exp2\n",
    "        \n",
    "        fitted_scaler_exp1_dic[i]=fitted_scaler_exp1\n",
    "        fitted_scaler_exp2_dic[i]=fitted_scaler_exp2\n",
    "        \n",
    "    return {\"Raw_X_exp1_dic\": X_exp1_dic, \"Raw_X_exp2_dic\": X_exp2_dic, \"fitted_scaler_exp1_dic\": fitted_scaler_exp1_dic, \"fitted_scaler_exp2_dic\": fitted_scaler_exp2_dic}\n",
    "\n",
    "\n",
    "\n",
    "# def MakeDeepXExpDic(dfList_exp, deep_feature_model, fitted_scaler_dic=None):\n",
    "#     '''\n",
    "#     ???\n",
    "#     return \n",
    "#     X_exp_dic\n",
    "#     dfLists are of the same size.\n",
    "#     '''\n",
    "\n",
    "#     X_exp_dic = {}\n",
    "#     for k in dfList_exp.keys():\n",
    "#         if fitted_scaler_dic:\n",
    "#             print(f\"scaling exp1 samples of user: {k}\")\n",
    "#             X_exp_dic[k] = deep_feature_model.predict(transform_user_windows(dfList_exp[k], fitted_scaler_dic[k]))\n",
    "#         else:\n",
    "#             print(f\"not scaling exp2 samples of user: {k}\")\n",
    "#             X_exp_dic[k] = deep_feature_model.predict(dfList_exp[k])\n",
    "        \n",
    "        \n",
    "#     return X_exp_dic\n",
    "\n",
    "\n",
    "\n",
    "def MakeXExpDicOwner(X_exp_reg_df_dict, scaler_clip=True, scaler_type=\"MinMaxScaler\", feature_extractor=None, feature_extractor_transformer=None, verbose=0):\n",
    "    '''k\n",
    "    ???\n",
    "    return \n",
    "    X_exp_dic\n",
    "    dfLists are of the same size.\n",
    "    '''\n",
    "\n",
    "    X_exp_dic = {}\n",
    "    for owner in X_exp_reg_df_dict.keys():\n",
    "\n",
    "        if verbose: print(f\"raw exp2 samples of owner: {owner} are scaled already so we {feature_extractor.__name__}\")\n",
    "        \n",
    "        X_exp_dic[owner] = {\"profile_windows\": feature_extractor(X_exp_reg_df_dict[owner]), \"unknown_users_dict\": {}, \"fitted_feature_scaler\": None}\n",
    "        \n",
    "#         #------- DEBUG--------\n",
    "#         if owner == 2: \n",
    "#             X_exp_dic['debug']= X_exp_dic[owner], X_exp_reg_df_dict[owner]\n",
    "#             return X_exp_dic\n",
    "        \n",
    "#         #------- DEBUG--------\n",
    "        \n",
    "        # do minmax scaling here\n",
    "        if verbose: print(f\"fit_transform {feature_extractor.__name__} features using scaler of type: {scaler_type}, for profile_windows of owner: {owner}\")\n",
    "        if verbose: print(f\"owner: {owner}, exp_num: {2}, scaler: {scaler_type}, scaler_clip: {scaler_clip}\")\n",
    "        \n",
    "        scaler = get_new_scaler_dict[scaler_type]\n",
    "        scaler = scaler(clip=scaler_clip).fit(X_exp_dic[owner][\"profile_windows\"])\n",
    "        X_exp_dic[owner][\"fitted_feature_scaler\"] = scaler\n",
    "\n",
    "        X_exp_dic[owner][\"profile_windows\"] = feature_extractor_transformer(X_exp_dic[owner][\"profile_windows\"], X_exp_dic[owner][\"fitted_feature_scaler\"])\n",
    "        \n",
    "        \n",
    "    return X_exp_dic\n",
    "    \n",
    "\n",
    "# def extract_features_and_scale_for_parallel_call(X_exp_unknown_df_dict, X_exp_dic, fitted_raw_scaler_dict, feature_extractor, feature_extractor_transformer, unknown_user, owner):\n",
    "#     X_exp_dic[owner][\"unknown_users_dict\"][unknown_user] = feature_extractor(transform_user_windows(X_exp_unknown_df_dict[unknown_user], fitted_raw_scaler_dict[owner]))\n",
    "#     fitted_feature_scaler = X_exp_dic[owner][\"fitted_feature_scaler\"]\n",
    "#     X_exp_dic[owner][\"unknown_users_dict\"][unknown_user] = feature_extractor_transformer(X_exp_dic[owner][\"unknown_users_dict\"][unknown_user], fitted_feature_scaler)\n",
    "#     # return X_exp_dic[owner][\"unknown_users_dict\"][unknown_user]\n",
    "\n",
    "def extract_features_and_scale_for_parallel_call_owner(X_exp_unknown_df_dict, X_exp_dic, fitted_raw_scaler_dict, feature_extractor, feature_extractor_transformer, owner, verbose=0):\n",
    "    if fitted_raw_scaler_dict:\n",
    "        \n",
    "        if verbose: print(f\"scaling exp1 unknown windows for when user: {owner}, is owner\")\n",
    "        \n",
    "        for unknown_user in X_exp_unknown_df_dict.keys():\n",
    "            X_exp_dic[owner][\"unknown_users_dict\"][unknown_user] = feature_extractor(transform_user_windows(X_exp_unknown_df_dict[unknown_user], fitted_raw_scaler_dict[owner]))\n",
    "            fitted_feature_scaler = X_exp_dic[owner][\"fitted_feature_scaler\"]\n",
    "            X_exp_dic[owner][\"unknown_users_dict\"][unknown_user] = feature_extractor_transformer(X_exp_dic[owner][\"unknown_users_dict\"][unknown_user], fitted_feature_scaler)\n",
    "            \n",
    "        if verbose: print(f\"done scaling exp1 unknown windows for when user: {owner}, is owner\")\n",
    "        \n",
    "    return {\"owner\": owner, \"unknown_users_dict\": X_exp_dic[owner][\"unknown_users_dict\"]}\n",
    "    \n",
    "def MakeXExpDicUnknown(X_exp_unknown_df_dict, X_exp_dic, fitted_raw_scaler_dict, feature_extractor=None, feature_extractor_transformer=None):\n",
    "    '''k\n",
    "    ???\n",
    "    return \n",
    "    X_exp_dic\n",
    "    dfLists are of the same size.\n",
    "    '''\n",
    "\n",
    "    for owner in X_exp_unknown_df_dict.keys():\n",
    "        if fitted_raw_scaler_dict:\n",
    "            print(f\"scaling exp1 unknown windows for when user: {owner}, is owner\")\n",
    "            for unknown_user in X_exp_unknown_df_dict.keys():\n",
    "                X_exp_dic[owner][\"unknown_users_dict\"][unknown_user] = feature_extractor(transform_user_windows(X_exp_unknown_df_dict[unknown_user], fitted_raw_scaler_dict[owner]))\n",
    "                fitted_feature_scaler = X_exp_dic[owner][\"fitted_feature_scaler\"]\n",
    "                X_exp_dic[owner][\"unknown_users_dict\"][unknown_user] = feature_extractor_transformer(X_exp_dic[owner][\"unknown_users_dict\"][unknown_user], fitted_feature_scaler)\n",
    "            \n",
    "            \n",
    "            \n",
    "#     results = Parallel(n_jobs=-1, verbose=100)(delayed(extract_features_and_scale_for_parallel_call_owner)(X_exp_unknown_df_dict, X_exp_dic, fitted_raw_scaler_dict, \n",
    "#                                                                                                                      feature_extractor, feature_extractor_transformer,\n",
    "#                                                                                                                      owner) for owner in X_exp_unknown_df_dict.keys())\n",
    "    \n",
    "#     for item in results:\n",
    "#         owner = item[\"owner\"]\n",
    "#         unknown_users_dict = item[\"unknown_users_dict\"]\n",
    "#         X_exp_dic[owner][\"unknown_users_dict\"] = unknown_users_dict\n",
    "    \n",
    "    return X_exp_dic\n",
    "\n",
    "    # df_reports = Parallel(n_jobs=-1)(delayed(evaluate_owner_IF_train_valid)(owner_idx, X_exp_train_dic, SEED, run, param_dist, CORES=1) for owner_idx in range(len(train_set)))\n",
    "    # # df_reports = Parallel(n_jobs=-1)(delayed(evaluate_owner_IF_train_valid)(owner_idx, X_exp_train_dic, SEED, run, param_dist, CORES=1) for owner_idx in range(len([0])))\n",
    "    # df_results = pd.concat([df_results] + df_reports, sort=False)\n",
    "\n",
    "def MakeDeepXExpDicOwner(X_exp_reg_df_dict, deep_feature_model, scaler_clip=True, scaler_type=\"MinMaxScaler\"):\n",
    "    '''k\n",
    "    ???\n",
    "    return \n",
    "    X_exp_dic\n",
    "    dfLists are of the same size.\n",
    "    '''\n",
    "    \n",
    "    extract_deep_features = lambda X_exp : ExtractDeepFeatures(X_exp, deep_feature_model)\n",
    "    return MakeXExpDicOwner(X_exp_reg_df_dict, scaler_clip=scaler_clip, scaler_type=\"MinMaxScaler\",\n",
    "                                feature_extractor=extract_deep_features, feature_extractor_transformer=transform_user_WACA_windows)\n",
    "    \n",
    "\n",
    "    \n",
    "def MakeDeepXExpDicUnknown(X_exp_unknown_df_dict, deep_feature_model, X_exp_dic, fitted_raw_scaler_dict):\n",
    "    '''k\n",
    "    ???\n",
    "    return \n",
    "    X_exp_dic\n",
    "    dfLists are of the same size.\n",
    "    '''  \n",
    "    \n",
    "    extract_deep_features = lambda X_exp : ExtractDeepFeatures(X_exp, deep_feature_model)\n",
    "    return MakeXExpDicUnknown(X_exp_unknown_df_dict, X_exp_dic, fitted_raw_scaler_dict, \n",
    "                                  feature_extractor=extract_deep_features, feature_extractor_transformer=transform_user_WACA_windows)\n",
    "\n",
    "\n",
    "\n",
    "def MakeWACAXExpDicOwner(X_exp_reg_df_dict, scaler_clip=True, scaler_type=\"MinMaxScaler\"):\n",
    "    '''k\n",
    "    ???\n",
    "    return \n",
    "    X_exp_dic\n",
    "    dfLists are of the same size.\n",
    "    '''\n",
    "        \n",
    "    return MakeXExpDicOwner(X_exp_reg_df_dict, scaler_clip=scaler_clip, scaler_type=\"MinMaxScaler\",\n",
    "                                feature_extractor=ExtractWACAFeatures, feature_extractor_transformer=transform_user_WACA_windows)\n",
    "    \n",
    "\n",
    "    \n",
    "def MakeWACAXExpDicUnknown(X_exp_unknown_df_dict, X_exp_dic, fitted_raw_scaler_dict):\n",
    "    '''k\n",
    "    ???\n",
    "    return \n",
    "    X_exp_dic\n",
    "    dfLists are of the same size.\n",
    "    '''  \n",
    "        \n",
    "    return MakeXExpDicUnknown(X_exp_unknown_df_dict, X_exp_dic, fitted_raw_scaler_dict, \n",
    "                                  feature_extractor=ExtractWACAFeatures, feature_extractor_transformer=transform_user_WACA_windows)\n",
    "\n",
    "\n",
    "def ExtractDeepFeatures(X_exp, deep_feature_model):\n",
    "        \n",
    "    # if X_exp and deep_feature_model:\n",
    "    return deep_feature_model.predict(np.array(X_exp))\n",
    "    \n",
    "#     elif deep_feature_model:\n",
    "#         return lambda X_exp : deep_feature_model.predict(np.array(X_exp))\n",
    "    \n",
    "#     else:\n",
    "#         raise Exception(f'arguments: (X_exp, deep_feature_model) are ({X_exp}, {deep_feature_model})')\n",
    "\n",
    "\n",
    "\n",
    "def ExtractWACAFeatures(X_exp):\n",
    "    a = []\n",
    "    \n",
    "    for window in X_exp:\n",
    "        a.append(signal_to_encoding(window)[1])\n",
    "        \n",
    "    return np.array(a)\n",
    "\n",
    "\n",
    "# def MakeScaledXExpDic(df_exp_dict, fitted_scaler_dic):\n",
    "#     '''\n",
    "#     ???\n",
    "#     return \n",
    "#     X_exp_dic\n",
    "#     dfLists are of the same size.\n",
    "#     '''\n",
    "\n",
    "#     X_exp_dic = {}\n",
    "#     for k in df_exp_dict.keys():\n",
    "#         print(f\"scaling exp1 samples of user: {k}\")\n",
    "#         X_exp_dic[k] = transform_user_windows(df_exp_dict[k], fitted_scaler_dic[k])\n",
    "        \n",
    "        \n",
    "#     return X_exp_dic\n",
    "\n",
    "\n",
    "def scale_feature_windows(df_exp_dict, fitted_scaler_dic=None, scaler_type=None, scaler_clip=False):\n",
    "    '''\n",
    "    ???\n",
    "    return \n",
    "    X_exp_dic\n",
    "    dfLists are of the same size.\n",
    "    '''\n",
    "    if fitted_scaler_dic == None:\n",
    "        fitted_scaler_dic={}\n",
    "        \n",
    "    X_exp_dic = {}\n",
    "    for k in df_exp_dict.keys():\n",
    "        if k in fitted_scaler_dic:\n",
    "            print(f\"transform exp1 samples of user: {k}\")\n",
    "        else:\n",
    "            print(f\"fit_transform exp2 samples of user: {k}\")\n",
    "            print(f\"user_idx: {k}, exp_num: {2}, scaler: {scaler_type}, scaler_clip: {scaler_clip}\")\n",
    "            scaler = get_new_scaler_dict[scaler_type]\n",
    "            scaler = scaler(clip=scaler_clip).fit(df_exp_dict[k])\n",
    "            fitted_scaler_dic[k] = scaler\n",
    "\n",
    "#         print(df_exp_dict[k].shape)\n",
    "        X_exp_dic[k] = transform_user_WACA_windows(df_exp_dict[k], fitted_scaler_dic[k])\n",
    "        \n",
    "        \n",
    "    return {\"X_exp_dic\": X_exp_dic, \"fitted_scaler_dic\": fitted_scaler_dic}\n",
    "\n",
    "\n",
    "def transform_user_WACA_windows(X_exp, fitted_scaler):\n",
    "    \n",
    "    \n",
    "    transformed_X_exp = []\n",
    "    \n",
    "#     print(X_exp[0].shape)\n",
    "    for window in X_exp:\n",
    "        if len(window.shape) == 1:\n",
    "            window = window.reshape(1, -1)\n",
    "        scaled_array = fitted_scaler.transform(window)\n",
    "        transformed_X_exp.append(scaled_array.reshape(-1))\n",
    "        \n",
    "    return np.array(transformed_X_exp)\n",
    "\n",
    "\n",
    "def transform_user_windows(X_exp, fitted_scaler):\n",
    "    \n",
    "    \n",
    "    transformed_X_exp = []\n",
    "    \n",
    "#     print(X_exp[0].shape)\n",
    "    for window in X_exp:\n",
    "        scaled_array = fitted_scaler.transform(window)\n",
    "        scaled_window_df = pd.DataFrame(data=scaled_array, columns = window.columns, dtype=scaled_array.dtype)\n",
    "        transformed_X_exp.append(scaled_window_df)\n",
    "        \n",
    "    return transformed_X_exp\n",
    "\n",
    "\n",
    "def extract_deep_feature_extactor(model):\n",
    "    \"\"\"\n",
    "    extracts and returns the a subnetwork of a two branch siamese network.\n",
    "    input: siamese model containing sub networks\n",
    "    \"\"\"\n",
    "    \n",
    "    return Model(\n",
    "                inputs=model.layers[0].get_input_at(0),\n",
    "                outputs=model.get_layer(\"basemodel\").get_output_at(0))\n",
    "\n",
    "\n",
    "def custom_save_model(arg_dict, win_size, loss_record_dict, metric_record_dict, deep_feature_model=None, test_res_fig_dic = None, other_dict=None, custom_prefix_dir_name=\"\"):\n",
    "    \"\"\"\n",
    "    save the deep learning feature extractor model, along with dictionary of arguments as a json,\n",
    "    best epoch found, a dictionary containing the accurcy and EER figures, and the window size, and\n",
    "    a dicgionary of validation and training loss values over time that can be later plotted.\n",
    "    inputs: \n",
    "    deep_feature_model: tf model\n",
    "    arg_dict: serializable dictionary\n",
    "    test_res_fig_dic: dict containing keys {\"acc\", \"eer\"}\n",
    "    win_size: int\n",
    "    loss_record_dict\n",
    "    \"\"\"\n",
    "    \n",
    "    dir_name = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    if deep_feature_model:\n",
    "        save_dir = f\"siamese_cnn_results/{win_size}/\"+ custom_prefix_dir_name + \"best_model_\" + dir_name\n",
    "        os.mkdir(save_dir)\n",
    "        deep_feature_model.save(save_dir)\n",
    "    else:\n",
    "        save_dir = f\"siamese_cnn_results/{win_size}/\"+ custom_prefix_dir_name + dir_name\n",
    "        os.mkdir(save_dir)\n",
    "    \n",
    "    \n",
    "    if test_res_fig_dic:\n",
    "        test_res_fig_dic[\"acc\"].savefig(f'{save_dir}/accuracy.svg', bbox_inches='tight')\n",
    "        test_res_fig_dic[\"eer\"].savefig(f'{save_dir}/eer.svg', bbox_inches='tight')\n",
    "    \n",
    "    with open(f\"{save_dir}/arg_dict.json\", 'w') as file:\n",
    "        arg_dict_json = json.dumps(arg_dict)\n",
    "        file.write(arg_dict_json)\n",
    "        \n",
    "    with open(f\"{save_dir}/loss_record.json\", 'w') as file:\n",
    "        loss_record_json = json.dumps(loss_record_dict)\n",
    "        file.write(loss_record_json)\n",
    "        \n",
    "    with open(f\"{save_dir}/metric_record.json\", 'w') as file:\n",
    "        metric_record_json = json.dumps(metric_record_dict)\n",
    "        file.write(metric_record_json)\n",
    "    \n",
    "    if other_dict != None:\n",
    "        with open(f\"{save_dir}/other_dict.json\", 'w') as file:\n",
    "            other_dict_json = json.dumps(other_dict)\n",
    "            file.write(other_dict_json)\n",
    "\n",
    "        \n",
    "    fig_dict = utils_plot_validation_metric(metric_record_dict)\n",
    "    for metric in fig_dict:\n",
    "        fig = fig_dict[metric]\n",
    "        fig.savefig(f'{save_dir}/{metric}_epoch.svg', bbox_inches='tight')\n",
    "    \n",
    "    print(f\"saved model at {save_dir}\")\n",
    "    \n",
    "    \n",
    "def utils_plot_validation_metric(metric_record_dict):\n",
    "    \"\"\"Plot Train/Valid metric during Epochs.\"\"\"\n",
    "    \n",
    "    fig_dict = {}\n",
    "    for metric in metric_record_dict:\n",
    "        fig = plt.figure(figsize=(5.473, 2.7), dpi=180)\n",
    "        plt.plot(metric_record_dict[metric]['Train'], label=\"train\", color=\"tab:blue\")\n",
    "        plt.plot(metric_record_dict[metric]['Valid'], label=\"valid\", color=MAGENTA)\n",
    "        plt.ylabel(metric)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        fig.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        fig.tight_layout()\n",
    "        \n",
    "        fig_dict[metric] = fig\n",
    "    \n",
    "    return fig_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6db5413f-6f83-4172-a988-bbd6b2c80ed8",
    "_uuid": "b3e92b0e-b8f0-4521-bb00-99e6b15496d7",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:42.417444Z",
     "iopub.status.busy": "2022-03-29T07:28:42.416737Z",
     "iopub.status.idle": "2022-03-29T07:28:42.42771Z",
     "shell.execute_reply": "2022-03-29T07:28:42.427032Z",
     "shell.execute_reply.started": "2022-03-29T07:28:42.417386Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.choice(range(5), 5, replace = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils_plot_distance_hist() For CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T00:56:38.183267Z",
     "iopub.status.busy": "2022-03-31T00:56:38.182871Z",
     "iopub.status.idle": "2022-03-31T00:56:38.201973Z",
     "shell.execute_reply": "2022-03-31T00:56:38.200775Z",
     "shell.execute_reply.started": "2022-03-31T00:56:38.183231Z"
    }
   },
   "outputs": [],
   "source": [
    "def utils_plot_distance_hist(dist_positive, dist_negative, thres, desc, fig_size=(12, 4), margin=None):\n",
    "    \"\"\"Plot histogramm of Euclidean Distances for Positive & Negative Pairs.\"\"\"\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Plot Distributions\n",
    "    plt.figure(figsize=fig_size, dpi=180)\n",
    "    bins = np.linspace(\n",
    "        min(dist_positive.min(), dist_negative.min()),\n",
    "        max(dist_positive.max(), dist_negative.max()),\n",
    "        num=21,\n",
    "    )\n",
    "    g1 = sns.distplot(\n",
    "        dist_positive,\n",
    "        label=\"positive pairs\",\n",
    "        bins=bins,\n",
    "        axlabel=False,\n",
    "        hist_kws=dict(edgecolor=\"k\", lw=0.5),\n",
    "        kde_kws=dict(linewidth=0.8),\n",
    "        color=\"tab:blue\",\n",
    "    )\n",
    "    g2 = sns.distplot(\n",
    "        dist_negative,\n",
    "        label=\"negative pairs\",\n",
    "        bins=bins,\n",
    "        hist_kws=dict(edgecolor=\"k\", lw=0.5),\n",
    "        kde_kws=dict(linewidth=0.8),\n",
    "        color=\"tab:gray\",\n",
    "    )\n",
    "\n",
    "    # Plot vertical lines\n",
    "    if thres > 0:\n",
    "        max_y = max(g1.get_ylim()[1], g2.get_ylim()[1])\n",
    "        plt.axvline(x=thres, color=MAGENTA, linestyle=\"--\", lw=0.8, alpha=0.7)\n",
    "        plt.text(\n",
    "            x=thres + 0.001,\n",
    "            y=max_y * 0.65,\n",
    "            s=f\"EER Threshold\\n({thres:.2f})\",\n",
    "            color=MAGENTA,\n",
    "            weight=\"bold\",\n",
    "            fontsize=5,\n",
    "            alpha=1\n",
    "        )\n",
    "        if margin:\n",
    "            plt.axvline(x=margin, color=MAGENTA, linestyle=\"--\", lw=0.8, alpha=0.7)\n",
    "            plt.text(\n",
    "                x=margin + 0.001,\n",
    "                y=max_y * 0.15,\n",
    "                s=f\"Margin\\n({margin})\",\n",
    "                color=MAGENTA,\n",
    "                weight=\"bold\",\n",
    "                fontsize=5,\n",
    "                alpha=1\n",
    "            )\n",
    "\n",
    "    # Legend\n",
    "    plt.legend(\n",
    "        loc=\"upper right\",\n",
    "        title=f\"{desc} Distances\",\n",
    "        title_fontsize=5,\n",
    "        fontsize=6,\n",
    "    )\n",
    "\n",
    "    warnings.filterwarnings(\"default\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-03-31T00:56:39.056016Z",
     "iopub.status.busy": "2022-03-31T00:56:39.055672Z",
     "iopub.status.idle": "2022-03-31T00:56:39.557205Z",
     "shell.execute_reply": "2022-03-31T00:56:39.556323Z",
     "shell.execute_reply.started": "2022-03-31T00:56:39.055986Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    dist_pos = np.array([0.0, 0.1, 0.1, 0.2, 0.2, 0.3, 0.3, 0.3, 0.3, 0.3, 0.5, 0.8])\n",
    "    dist_neg = np.array([0.4, 0.5, 0.6, 0.6, 0.6, 0.7, 0.7, 0.7, 0.7, 0.8, 1, 1])\n",
    "    utils_plot_distance_hist(\n",
    "        dist_pos, dist_neg, thres=0.4, desc=\"Pair\", fig_size=(12, 4), margin=0.8\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "478e1e7e-68bd-4f32-94a8-e2aa98381323",
    "_uuid": "59ce837f-6626-42f9-bd8c-ac2fdf93c5c2"
   },
   "source": [
    "# utils_create_cv_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1cfaf4af-a653-485b-be5b-c28d2797798f",
    "_uuid": "38ee9619-b1c4-40d6-9980-1809d1775875",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:43.591762Z",
     "iopub.status.busy": "2022-03-29T07:28:43.591444Z",
     "iopub.status.idle": "2022-03-29T07:28:43.608086Z",
     "shell.execute_reply": "2022-03-29T07:28:43.605627Z",
     "shell.execute_reply.started": "2022-03-29T07:28:43.591735Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def utils_create_cv_splits(owner_key, train_dic, valid_test_dic, seed=0):\n",
    "    '''\n",
    "    return the required sets for an OCSVM trained on the user with key. \n",
    "    X_train: X data from train_dic[k], comes from exp2\n",
    "    X_test_regular: X data from valid_test_dic[k], comes from exp1\n",
    "    X_test_anomalous: X data from valid_test_dic[!k], comes from exp1\n",
    "    \n",
    "    Create cross-validation mask with train-valid pairs.\n",
    "    \n",
    "    See e.g. https://stackoverflow.com/a/37591377\n",
    "    \n",
    "    Arguments:\n",
    "        cv_mask {np.ndarray} --\n",
    "        \n",
    "    Return:\n",
    "        {list} -- List of tuple: (<train indices>, <valid indices>)\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    if owner_key not in  train_dic:\n",
    "        raise Exception(\"invalid key for dic\")\n",
    "    \n",
    "        \n",
    "    X_pos = train_dic[owner_key].copy()\n",
    "    X_test_regular = valid_test_dic[owner_key].copy()\n",
    "    X_test_anomalous = np.concatenate([valid_test_dic[key] for key in valid_test_dic.keys() if key != owner_key], axis=0).copy()\n",
    "    \n",
    "    train_idx_owner = np.arange(X_pos.shape[0])\n",
    "    valid_idx_owner = np.arange(X_test_regular.shape[0]) + train_idx_owner.shape[0]\n",
    "    \n",
    "    print(f\"owner: {owner_key} train_idx range: {train_idx_owner[0]}, {train_idx_owner[-1]}\")\n",
    "    print(f\"owner: {owner_key} valid_idx range: {valid_idx_owner[0]}, {valid_idx_owner[-1]}\")\n",
    "    np.random.seed(seed + owner_key)\n",
    "    np.random.shuffle(train_idx_owner)\n",
    "    np.random.shuffle(valid_idx_owner)\n",
    "\n",
    "    \n",
    "    cv_splits = []\n",
    "    base_idx = train_idx_owner.shape[0] + valid_idx_owner.shape[0]\n",
    "    for key in valid_test_dic.keys():\n",
    "        \n",
    "        if key != owner_key:\n",
    "            # Impostor validation indices\n",
    "            valid_idx_impostor = np.arange(valid_test_dic[key].shape[0]) + base_idx\n",
    "            print(f\"imposter: {key} valid_idx range: {valid_idx_impostor[0]}, {valid_idx_impostor[-1]}\")\n",
    "\n",
    "            # Balance classes\n",
    "            min_samples = min(valid_idx_owner.shape[0], valid_idx_impostor.shape[0])\n",
    "            np.random.seed(seed + key)\n",
    "            valid_idx_owner_samp = np.random.choice(\n",
    "                valid_idx_owner, size=min_samples, replace=False\n",
    "            )\n",
    "            np.random.seed(seed + key)\n",
    "            valid_idx_impostor_samp = np.random.choice(\n",
    "                valid_idx_impostor, size=min_samples, replace=False\n",
    "            )\n",
    "\n",
    "            # Concat owner & impostor validation indices\n",
    "            valid_idx_both = np.hstack([valid_idx_owner_samp, valid_idx_impostor_samp])\n",
    "\n",
    "            # Add train/valid pair to cv\n",
    "            cv_splits.append((list(train_idx_owner), list(valid_idx_both)))\n",
    "            \n",
    "            base_idx += valid_idx_impostor.shape[0]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    y_train = np.concatenate([np.repeat(1.0, X_pos.shape[0]), np.repeat(1.0, X_test_regular.shape[0]), np.repeat(-1.0, X_test_anomalous.shape[0])])\n",
    "    X_train = np.concatenate([X_pos, X_test_regular, X_test_anomalous], axis=0)\n",
    "    \n",
    "    \n",
    "    return {\"X_train\": X_train, \"y_train\": y_train, \"X_test_regular\": X_test_regular, \"X_test_anomalous\": X_test_anomalous, \"cv_splits\": cv_splits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6abc81ef-9648-4c12-95c7-d8af54bd1923",
    "_uuid": "03caeebf-831e-40bd-8724-6eeeb713e63b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:44.42086Z",
     "iopub.status.busy": "2022-03-29T07:28:44.420184Z",
     "iopub.status.idle": "2022-03-29T07:28:44.425311Z",
     "shell.execute_reply": "2022-03-29T07:28:44.423991Z",
     "shell.execute_reply.started": "2022-03-29T07:28:44.42081Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# if TEST_MODE:\n",
    "#     # Mask Explained:\n",
    "#     # -2 => Training data (owner)\n",
    "#     # -1 => Validation data (owner)\n",
    "#     # 0+ => Validation impostors\n",
    "#     #              Indices:    0   1   2   3   4   5  6  7  8  9  10 11 12 13 14 15\n",
    "#     dummy_cv_mask = np.array([-2, -2, -1, -1, -1, -1, 0, 0, 0, 1, 1, 1, 2, 2, 2, -2])\n",
    "\n",
    "#     # Generate tuples of training data and validation data, one tuple for each impostor (0, 1, 2).\n",
    "#     # Training data (1st list in tuple) contains only indices of owner training data (-2)\n",
    "#     # Validation data (2nd list in tuple) contains  indices of validation data from owner (-1) and\n",
    "#     # from a single impostor (0+), each 50 %\n",
    "#     splits = utils_create_cv_splits(dummy_cv_mask, seed=123)\n",
    "#     [print(s) for s in splits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9f0938b3-dc22-4b9b-9418-3be1a44595e3",
    "_uuid": "04c3c832-4264-44b4-b076-195e0d551630"
   },
   "source": [
    "# utils_cv_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T11:25:38.019826Z",
     "iopub.status.busy": "2022-03-29T11:25:38.019469Z",
     "iopub.status.idle": "2022-03-29T11:25:38.024614Z",
     "shell.execute_reply": "2022-03-29T11:25:38.023103Z",
     "shell.execute_reply.started": "2022-03-29T11:25:38.019796Z"
    }
   },
   "outputs": [],
   "source": [
    "TEST_MODE=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T11:26:11.290958Z",
     "iopub.status.busy": "2022-03-29T11:26:11.290567Z",
     "iopub.status.idle": "2022-03-29T11:26:11.304482Z",
     "shell.execute_reply": "2022-03-29T11:26:11.303355Z",
     "shell.execute_reply.started": "2022-03-29T11:26:11.290918Z"
    }
   },
   "outputs": [],
   "source": [
    "def utils_cv_report(random_search, owner, impostors):\n",
    "    \"\"\"Transform the random_search.cv_results_ into nice formatted dataframe.\"\"\"\n",
    "    # Create report\n",
    "    df_report = pd.DataFrame(random_search.cv_results_)\n",
    "\n",
    "    # Add owner information\n",
    "    df_report[\"owner\"] = owner\n",
    "\n",
    "    # Drop uninteressting columns\n",
    "    drop_columns = [col for col in df_report.columns if \"_train_\" in col]\n",
    "    drop_columns = drop_columns + [col for col in df_report.columns if col.startswith(\"split\") and (col.endswith(\"recall\") or col.endswith(\"precision\") or col.endswith(\"f1\") or col.endswith(\"roc_auc\"))]\n",
    "    drop_columns = drop_columns + [\"params\"]\n",
    "    df_report = df_report.drop(columns=drop_columns)\n",
    "\n",
    "    # Flip sign of eer (revert flip by sklearn scorer)\n",
    "    eer_columns = [col for col in df_report.columns if col.endswith(\"_eer\")]\n",
    "    df_report[eer_columns] = df_report[eer_columns].abs()\n",
    "    \n",
    "    # Rename split result columns with impostor-ids used in split\n",
    "    rename_cols = {}\n",
    "    for idx, impostor in enumerate(impostors):\n",
    "        print(f\"idx: {idx}, impostor: {impostor}\")\n",
    "        to_rename_cols = [col for col in df_report.columns if col.startswith(f\"split{idx}\")]\n",
    "        for col in to_rename_cols:\n",
    "            rename_cols[col] = str(impostor)+col[len(f\"split{idx}\"):]\n",
    "    df_report = df_report.rename(columns=rename_cols)      \n",
    "\n",
    "    return df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T11:02:53.336815Z",
     "iopub.status.busy": "2022-03-29T11:02:53.336501Z",
     "iopub.status.idle": "2022-03-29T11:02:53.425955Z",
     "shell.execute_reply": "2022-03-29T11:02:53.424527Z",
     "shell.execute_reply.started": "2022-03-29T11:02:53.33679Z"
    }
   },
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    print(\"Performing Dummy RandomSearch...\")\n",
    "    from sklearn import svm, datasets\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "    iris = datasets.load_iris()\n",
    "    parameters = {\"kernel\": (\"linear\", \"rbf\"), \"C\": [1, 2, 3, 4, 5, 6, 7, 10]}\n",
    "    svc = svm.SVC(gamma=\"scale\")\n",
    "    clf = RandomizedSearchCV(svc, parameters, cv=3, iid=False)\n",
    "    clf.fit(iris.data, iris.target)\n",
    "    print(\"Create report:\")\n",
    "    df_temp = utils_cv_report(clf, \"owner x\", [\"impo_1\", \"impo_2\", \"impo_3\"])\n",
    "    display(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_plot_randomsearch_results(df_results, n_top=1):\n",
    "    # Prepare data for plotting\n",
    "    df_plot = df_results[df_results[\"rank_test_eer\"] <= n_top].rename(\n",
    "        columns={\n",
    "            \"param_model__nu\": r\"$\\nu$\",\n",
    "            \"param_model__gamma\": r\"$\\gamma$\",\n",
    "            \"mean_test_accuracy\": \"Mean Test Acc.\",\n",
    "            \"mean_test_eer\": \"Mean Test EER\",\n",
    "        }\n",
    "    )\n",
    "    df_plot[\"Mean Test EER\"] = df_plot[\"Mean Test EER\"] * -1  # Because fewer is more\n",
    "\n",
    "    median_nu = df_plot[r\"$\\nu$\"].median()\n",
    "    median_gamma = df_plot[r\"$\\gamma$\"].median()\n",
    "\n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(5.473 / 1.3, 2), dpi=180)\n",
    "    g = sns.scatterplot(\n",
    "        x=r\"$\\nu$\",\n",
    "        y=r\"$\\gamma$\",\n",
    "        data=df_plot,\n",
    "        size=\"Mean Test EER\",\n",
    "        sizes=(7, 60),\n",
    "        hue=\"Mean Test EER\",\n",
    "        alpha=1,\n",
    "        #        palette=\"Blues\",\n",
    "        linewidth=0,\n",
    "    )\n",
    "\n",
    "    # Format Legend labels\n",
    "    leg = g.get_legend()\n",
    "    new_handles = [h for h in leg.legendHandles]\n",
    "    new_labels = []\n",
    "    for i, handle in enumerate(leg.legendHandles):\n",
    "        label = handle.get_label()\n",
    "        print(f'{i}, {label}')\n",
    "        if ord(label[0]) == 8722:\n",
    "            label = '-' + label[1:]\n",
    "            \n",
    "        if i != 0:\n",
    "            \n",
    "            try:\n",
    "                new_labels.append(f\"{abs(float(label)):.3f}\")\n",
    "\n",
    "            except ValueError:\n",
    "                new_labels.append(\"\")\n",
    "\n",
    "    # Plot mean values\n",
    "    plt.plot(\n",
    "        [-0.01, 0.31],\n",
    "        [median_gamma, median_gamma],\n",
    "        linestyle=\"dashed\",\n",
    "        linewidth=0.8,\n",
    "        alpha=0.7,\n",
    "        color=\"black\",\n",
    "    )\n",
    "    plt.text(\n",
    "        0.23,\n",
    "        median_gamma * 1.7 ** 2,\n",
    "        r\"median($\\gamma$)\",\n",
    "        fontsize=6,\n",
    "        color=\"black\",\n",
    "        alpha=0.9,\n",
    "    )\n",
    "    plt.text(\n",
    "        0.23,\n",
    "        median_gamma * 1.2 ** 2,\n",
    "        f\"{median_gamma:.3f}\",\n",
    "        fontsize=5,\n",
    "        color=\"black\",\n",
    "        alpha=0.9,\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        [median_nu, median_nu],\n",
    "        [0.0001, 1000],\n",
    "        linestyle=\"dashed\",\n",
    "        linewidth=0.8,\n",
    "        alpha=0.7,\n",
    "        color=\"black\",\n",
    "    )\n",
    "    plt.text(\n",
    "        median_nu + 0.005, 400, r\"median($\\nu$)\", fontsize=6, color=\"black\", alpha=0.9\n",
    "    )\n",
    "    plt.text(\n",
    "        median_nu + 0.005, 200, f\"{median_nu:.3f}\", fontsize=5, color=\"black\", alpha=0.9\n",
    "    )\n",
    "\n",
    "    # Adjust axes & legend\n",
    "    plt.yscale(\"log\")\n",
    "    plt.ylim(0.0001, 1000)\n",
    "    plt.xlim(0, 0.305)\n",
    "#     print(new_handles)\n",
    "    print(new_labels)\n",
    "    plt.legend(\n",
    "        new_handles,\n",
    "        new_labels,\n",
    "        bbox_to_anchor=(1.02, 1),\n",
    "        loc=2,\n",
    "        borderaxespad=0.0,\n",
    "        title=\"Mean EER per Owner\\n(Validation Results)\",\n",
    "        title_fontsize=5,\n",
    "    )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return median_nu, median_gamma, fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_plot_2d_hyperparam_selection(h1_name, h1_val, h2_name, h2_val, df_plot):\n",
    "    \n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(5.473 / 1.3, 2), dpi=100)\n",
    "    g = sns.scatterplot(\n",
    "        x=h1_name,\n",
    "        y=h2_name,\n",
    "        data=df_plot,\n",
    "        size=\"Mean Test EER\",\n",
    "        sizes=(7, 60),\n",
    "        hue=\"Mean Test EER\",\n",
    "        alpha=1,\n",
    "        #        palette=\"Blues\",\n",
    "        linewidth=0,\n",
    "    )\n",
    "\n",
    "    # Format Legend labels\n",
    "    leg = g.get_legend()\n",
    "    new_handles = [h for h in leg.legendHandles]\n",
    "    new_labels = []\n",
    "    for i, handle in enumerate(leg.legendHandles):\n",
    "        label = handle.get_label()\n",
    "        print(f'{i}, {label}')\n",
    "        if ord(label[0]) == 8722:\n",
    "            label = '-' + label[1:]\n",
    "            \n",
    "        if i != 0:\n",
    "            \n",
    "            try:\n",
    "                new_labels.append(f\"{abs(float(label)):.3f}\")\n",
    "\n",
    "            except ValueError:\n",
    "                new_labels.append(\"\")\n",
    "\n",
    "    # Plot mean values\n",
    "    plt.plot(\n",
    "        [-0.01, 0.31],\n",
    "        [h2_val, h2_val],\n",
    "        linestyle=\"dashed\",\n",
    "        linewidth=0.8,\n",
    "        alpha=0.7,\n",
    "        color=\"black\",\n",
    "    )\n",
    "    plt.text(\n",
    "        0.23,\n",
    "        h2_val * 1.7 ** 2,\n",
    "        f\"median({h2_name})\",\n",
    "        fontsize=6,\n",
    "        color=\"black\",\n",
    "        alpha=0.9,\n",
    "    )\n",
    "    plt.text(\n",
    "        0.23,\n",
    "        h2_val * 1.2 ** 2,\n",
    "        f\"{h2_val:.3f}\",\n",
    "        fontsize=5,\n",
    "        color=\"black\",\n",
    "        alpha=0.9,\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        [h1_val, h1_val],\n",
    "        [0.0001, 1000],\n",
    "        linestyle=\"dashed\",\n",
    "        linewidth=0.8,\n",
    "        alpha=0.7,\n",
    "        color=\"black\",\n",
    "    )\n",
    "    plt.text(\n",
    "        h1_val + 0.005, 400, f\"median({h1_name})\", fontsize=6, color=\"black\", alpha=0.9\n",
    "    )\n",
    "    plt.text(\n",
    "        h1_val + 0.005, 200, f\"{h1_val:.3f}\", fontsize=5, color=\"black\", alpha=0.9\n",
    "    )\n",
    "\n",
    "    # Adjust axes & legend\n",
    "    plt.yscale(\"log\")\n",
    "    plt.ylim(0.0001, 1000)\n",
    "    plt.xlim(0, 0.305)\n",
    "#     print(new_handles)\n",
    "    print(new_labels)\n",
    "    plt.legend(\n",
    "        new_handles,\n",
    "        new_labels,\n",
    "        bbox_to_anchor=(1.02, 1),\n",
    "        loc=2,\n",
    "        borderaxespad=0.0,\n",
    "        title=\"Mean EER per Owner\\n(Validation Results)\",\n",
    "        title_fontsize=5,\n",
    "    )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/11159436/multiple-figures-in-a-single-window\n",
    "def arrange_figures(figures, nrows = 1, ncols=1):\n",
    "    \"\"\"Plot a dictionary of figures.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    figures : <title, figure> dictionary\n",
    "    ncols : number of columns of subplots wanted in the display\n",
    "    nrows : number of rows of subplots wanted in the figure\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows)\n",
    "    for ind,title in enumerate(figures):\n",
    "        axeslist.ravel()[ind].imshow(figures[title], cmap=plt.gray())\n",
    "        axeslist.ravel()[ind].set_title(title)\n",
    "        axeslist.ravel()[ind].set_axis_off()\n",
    "        \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_plot_randomsearch_results_IF(df_results, n_top=1):\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    df_plot = df_results[df_results[\"rank_test_eer\"] <= n_top].rename(\n",
    "        columns={\n",
    "                \"param_model__n_estimators\": \"n_estimators\",\n",
    "                \"param_model__max_samples\": \"max_samples\",\n",
    "                \"param_model__contamination\": \"contamination\",\n",
    "                \"param_model__max_features\": \"max_features\",\n",
    "                \"mean_test_accuracy\": \"Mean Test Acc.\",\n",
    "                \"mean_test_eer\": \"Mean Test EER\",\n",
    "        }\n",
    "    )\n",
    "    df_plot[\"Mean Test EER\"] = df_plot[\"Mean Test EER\"] * -1  # Because fewer is more\n",
    "\n",
    "    print(df_plot.keys())\n",
    "    median_n_estimators = df_plot[\"n_estimators\"].median()\n",
    "    median_max_samples = df_plot[\"max_samples\"].median()\n",
    "    median_contamination = df_plot[\"contamination\"].median()\n",
    "    median_max_features = df_plot[\"max_features\"].median()\n",
    "\n",
    "    figures = []\n",
    "    figures.append(utils_plot_2d_hyperparam_selection(\"n_estimators\", median_n_estimators, \"max_samples\", median_max_samples, df_plot))\n",
    "    figures.append(utils_plot_2d_hyperparam_selection(\"n_estimators\", median_n_estimators, \"contamination\", median_contamination, df_plot))\n",
    "    figures.append(utils_plot_2d_hyperparam_selection(\"n_estimators\", median_n_estimators, \"max_features\", median_max_features, df_plot))\n",
    "    figures.append(utils_plot_2d_hyperparam_selection(\"max_samples\", median_max_samples, \"contamination\", median_contamination, df_plot))\n",
    "    figures.append(utils_plot_2d_hyperparam_selection(\"max_samples\", median_max_samples, \"max_features\", median_max_features, df_plot))\n",
    "    figures.append(utils_plot_2d_hyperparam_selection(\"contamination\", median_contamination, \"max_features\", median_max_features, df_plot))\n",
    "    \n",
    "    # fig = arrange_figures(figures, nrows=3, ncols=2)\n",
    "    \n",
    "\n",
    "    return median_n_estimators, median_max_samples, median_contamination, median_max_features, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b7fd521d-f17e-4894-84c3-4d3ab3655c44",
    "_uuid": "081f655b-dd63-4dc9-8b08-9cb0f7982129",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def utils_plot_acc_eer_dist(df_plot, y_col):\n",
    "    n_subject = len(df_plot[\"Owner\"].unique()) - 1\n",
    "    mean_col = df_plot[y_col].mean()\n",
    "\n",
    "    fig = plt.figure(figsize=(5.473, 2), dpi=180)\n",
    "    ax = sns.boxplot(x=\"Owner\", y=y_col, data=df_plot, **utils_boxplot_style)\n",
    "    ax.set_ylim((0, 1))\n",
    "\n",
    "    plt.plot(\n",
    "        [-0.6, n_subject + 0.6],\n",
    "        [mean_col, mean_col],\n",
    "        linestyle=\"dashed\",\n",
    "        linewidth=1,\n",
    "        color=MAGENTA,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    plt.text(n_subject + 0.6, mean_col, f\"mean\", fontsize=6, color=MAGENTA)\n",
    "    plt.text(\n",
    "        n_subject + 0.6, mean_col - 0.04, f\"{mean_col:.3f}\", fontsize=4.5, color=MAGENTA\n",
    "    )\n",
    "    plt.xticks(rotation=45)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    print(f\"Overall mean: {mean_col:.4f}\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "# X_vals_owner_idx['X_train']\n",
    "def utils_plot_acc_eer_dist_thesis(EER_distro_df, y_col, discription, save_file_name=None, boxplot_color=\"springgreen\", sys_err=True):\n",
    "\n",
    "    sns.set(err_distro_rc)\n",
    "    n_subject = len(EER_distro_df['Owner'].unique())+1\n",
    "    mean_col = EER_distro_df[y_col].mean()\n",
    "    fig = plt.figure()\n",
    "    ax = sns.boxplot(x=\"Owner\", y=y_col, data=EER_distro_df, color=boxplot_color)#, **utils_boxplot_style)\n",
    "    \n",
    "    # Select which box you want to change    \n",
    "    mybox = ax.patches[4]\n",
    "\n",
    "    \n",
    "    ax.set_ylim((0, 1))\n",
    "    # sns.swarmplot(x=\"Owner\", y=y_col, data=EER_distro_df, color=\".25\")\n",
    "    \n",
    "\n",
    "    plt.plot(\n",
    "        [-0.5, n_subject -1.5],\n",
    "        [mean_col, mean_col],\n",
    "        linestyle=\"dashed\",\n",
    "        linewidth=2,\n",
    "        color=MAGENTA,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    plt.text(n_subject/2, mean_col + 0.01, f\"mean\", fontsize=err_distro_rc[\"ytick.labelsize\"], color=MAGENTA)\n",
    "    plt.text(\n",
    "        n_subject/2, mean_col - 0.04, f\"{mean_col:.3f}\", fontsize=err_distro_rc[\"ytick.labelsize\"], color=MAGENTA\n",
    "    )\n",
    "    plt.xticks(rotation=45)\n",
    "    fig.tight_layout()\n",
    "    plt.title(discription)\n",
    "    \n",
    "    ax.set_xlabel(\"Owner id\")\n",
    "    ax.set_ylabel(f\"Mean {y_col}\")\n",
    "    \n",
    "    plt.savefig(f'{save_file_name}', bbox_inches='tight')\n",
    "    print(f\"Overall mean: {mean_col:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "230b6da5-a942-4634-a5cc-6e3a07d81646",
    "_uuid": "df80a643-53d2-4d31-9dc2-437d83a306eb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T11:02:45.271051Z",
     "iopub.status.busy": "2022-03-29T11:02:45.270467Z",
     "iopub.status.idle": "2022-03-29T11:02:45.276097Z",
     "shell.execute_reply": "2022-03-29T11:02:45.275043Z",
     "shell.execute_reply.started": "2022-03-29T11:02:45.271009Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# if TEST_MODE:\n",
    "#     print(\"Performing Dummy RandomSearch...\")\n",
    "#     from sklearn import svm, datasets\n",
    "#     from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#     iris = datasets.load_iris()\n",
    "#     parameters = {\"kernel\": (\"linear\", \"rbf\"), \"C\": [1, 2, 3, 4, 5, 6, 7, 10]}\n",
    "#     svc = svm.SVC(gamma=\"scale\")\n",
    "#     clf = RandomizedSearchCV(svc, parameters, cv=3, iid=False)\n",
    "#     clf.fit(iris.data, iris.target)\n",
    "#     print(\"Create report:\")\n",
    "#     df_temp = utils_cv_report(clf, \"owner x\", [\"impo_1\", \"impo_2\", \"impo_3\"])\n",
    "#     display(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "61fb5ecc-ec29-4f86-8590-0db7735de365",
    "_uuid": "bd017a22-bbf8-4c0b-890b-07cc71a417ae",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T11:02:45.905429Z",
     "iopub.status.busy": "2022-03-29T11:02:45.904866Z",
     "iopub.status.idle": "2022-03-29T11:02:45.926361Z",
     "shell.execute_reply": "2022-03-29T11:02:45.925209Z",
     "shell.execute_reply.started": "2022-03-29T11:02:45.905388Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class pca_feature_selector:\n",
    "    def __init__(self, n_components):\n",
    "        self._pca_dict = {}\n",
    "        self.n_components = n_components\n",
    "        \n",
    "    def add_user_pca(self, owner_idx, user_pca):\n",
    "        if owner_idx in self._pca_dict:\n",
    "            raise Exception(f\"owner_idx: {owner_idx} alraedy exists!\")\n",
    "        \n",
    "        self._pca_dict[owner_idx] = user_pca\n",
    "        \n",
    "    def user_feature_ranking(self, owner_idx):\n",
    "        '''\n",
    "        these two are the same \n",
    "        np.matmul(pca.explained_variance_ratio_[np.newaxis], abs_components) == np.dot(pca.explained_variance_ratio_, abs_components)[np.newaxis]\n",
    "        '''\n",
    "        pca = self._pca_dict[owner_idx]\n",
    "        abs_components = np.abs(pca.components_)\n",
    "        feature_importance = np.dot(pca.explained_variance_ratio_, abs_components)[np.newaxis]\n",
    "        top_feature_indices = np.argsort(-1*feature_importance)\n",
    "        \n",
    "        return {\"top_feature_indices\": top_feature_indices, \"feature_importance\": feature_importance}\n",
    "    \n",
    "    def get_comparison_matrix(self):\n",
    "        \n",
    "        feature_importance_matrix = []\n",
    "        top_feature_matrix = []\n",
    "        for owner_idx in self._pca_dict:\n",
    "            user_feature_dict = self.user_feature_ranking(owner_idx) \n",
    "            feature_importance_matrix += [user_feature_dict[\"feature_importance\"]]\n",
    "            top_feature_matrix += [user_feature_dict[\"top_feature_indices\"]]\n",
    "            \n",
    "        self._feature_importance_matrix = np.concatenate(feature_importance_matrix, axis=0)\n",
    "        self._top_feature_matrix = np.concatenate(top_feature_matrix, axis=0)\n",
    "\n",
    "        return {\"feature_importance_matrix\": self._feature_importance_matrix, \"top_feature_matrix\" :self._top_feature_matrix}\n",
    "    \n",
    "    def find_top_n_features(self):\n",
    "        \n",
    "        best_feature_lst = []\n",
    "        for i in range(self.n_components):\n",
    "            best_feature_lst.append(self.find_next_best_feature(best_feature_lst))\n",
    "            \n",
    "        return best_feature_lst\n",
    "        \n",
    "    def find_next_best_feature(self, curr_feature_lst):\n",
    "        \n",
    "        curr_pc_idx = len(curr_feature_lst)\n",
    "        feature_column_count = np.bincount(self._top_feature_matrix[:, curr_pc_idx])\n",
    "        print(f\"top_f_m: {self._top_feature_matrix[:, curr_pc_idx]}\")\n",
    "        print(curr_feature_lst)\n",
    "        \n",
    "        i = 0\n",
    "        #probably need to use a tree type or heap structure\n",
    "        while i < len(feature_column_count):\n",
    "            top_feature_idx = np.argmax(feature_column_count[i:]) + i\n",
    "            if top_feature_idx not in curr_feature_lst:\n",
    "                return top_feature_idx\n",
    "            print('-------')\n",
    "            print(feature_column_count)\n",
    "            print(f\"i: {i}, top_feature_idx: {top_feature_idx}\")\n",
    "\n",
    "            i = top_feature_idx + 1\n",
    "            \n",
    "        raise Exception('could not find best feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_plot_training_loss(history):\n",
    "    \"\"\"Plot Train/Valid Loss during Epochs.\"\"\"\n",
    "    fig = plt.figure(figsize=(5.473, 2.7), dpi=180)\n",
    "    plt.plot(history[\"loss\"], label=\"train\", color=\"tab:blue\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"valid\", color=MAGENTA)\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    fig.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    fig.tight_layout()\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    HistoryDummy = type(\"History\", (object,), {})\n",
    "    history = HistoryDummy()\n",
    "    history.history = {}\n",
    "    history.history[\"loss\"] = [0.6, 0.4, 0.3, 0.2, 0.21, 0.15]\n",
    "    history.history[\"val_loss\"] = [0.9, 0.7, 0.5, 0.4, 0.35, 0.3]\n",
    "    utils_plot_training_loss(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9a6a5c02-ace2-47be-bad3-10b646266720",
    "_uuid": "b03ec3d1-6ac2-4da7-9d90-eadad41f35b7",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:46.405019Z",
     "iopub.status.busy": "2022-03-29T07:28:46.404687Z",
     "iopub.status.idle": "2022-03-29T07:28:46.408929Z",
     "shell.execute_reply": "2022-03-29T07:28:46.407833Z",
     "shell.execute_reply.started": "2022-03-29T07:28:46.40499Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# randomized_data_idx = list(range(len(r)))\n",
    "# random.shuffle(randomized_data_idx)\n",
    "# split_idx = 2 * (len(randomized_data_idx)//3) + 1\n",
    "# train_set = randomized_data_idx[: split_idx]\n",
    "# test_set = randomized_data_idx[split_idx: ]\n",
    "# print(f\"train_set: {train_set}\\ntest_set: {test_set}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c8931e16-a4e2-42c8-b626-7dfd6e8f2ed4",
    "_uuid": "d73dc5fe-0599-458f-ba69-ee6405152e40",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:46.900568Z",
     "iopub.status.busy": "2022-03-29T07:28:46.900021Z",
     "iopub.status.idle": "2022-03-29T07:28:46.904243Z",
     "shell.execute_reply": "2022-03-29T07:28:46.903422Z",
     "shell.execute_reply.started": "2022-03-29T07:28:46.900532Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # preparing train data\n",
    "# # train_set = r\n",
    "# dfList_exp1_train, dfList_exp2_train = [dfList_exp1[i] for i in train_set], [dfList_exp2[i] for i in train_set]\n",
    "# print(f\"len(dfList_exp1_train): {len(dfList_exp1_train)}\")\n",
    "# print(f\"len(dfList_exp2_train): {len(dfList_exp2_train)}\")\n",
    "# XExpTrainDict = MakeXExpDic(dfList_exp1_train, dfList_exp2_train, window_size = 250, step = 251, numSamplePoints= 18000)\n",
    "# X_exp1_train_dic, X_exp2_train_dic = XExpTrainDict[\"X_exp1_dic\"], XExpTrainDict[\"X_exp2_dic\"]\n",
    "\n",
    "# # preparing test data\n",
    "# dfList_exp1_test, dfList_exp2_test = [dfList_exp1[i] for i in test_set], [dfList_exp2[i] for i in test_set]\n",
    "# print(f\"len(dfList_exp1_test): {len(dfList_exp1_test)}\")\n",
    "# print(f\"len(dfList_exp2_test): {len(dfList_exp2_test)}\")\n",
    "# XExpTestDict = MakeXExpDic(dfList_exp1_test, dfList_exp2_test, window_size = 250, step = 251, numSamplePoints= 18000)\n",
    "# X_exp1_test_dic, X_exp2_test_dic = XExpTestDict[\"X_exp1_dic\"], XExpTestDict[\"X_exp2_dic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "92b22de7-7efb-4cce-aa17-a941d9bc262d",
    "_uuid": "7900b26e-3f79-4af8-b0dd-3c66f8193ad6"
   },
   "source": [
    "**use the following to write tests for distro functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "34212531-d2e8-4ddf-85fb-9735252c8b1b",
    "_uuid": "18ad7845-afb9-46a8-b194-28b36a1acf3e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:47.441543Z",
     "iopub.status.busy": "2022-03-29T07:28:47.440969Z",
     "iopub.status.idle": "2022-03-29T07:28:47.452093Z",
     "shell.execute_reply": "2022-03-29T07:28:47.451079Z",
     "shell.execute_reply.started": "2022-03-29T07:28:47.441496Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# source: https://zhiyzuo.github.io/Plot-Lorenz/\n",
    "#  0 representing perfect equality, and 1 absolute inequality.\n",
    "def gini(arr):\n",
    "    ## first sort\n",
    "    sorted_arr = arr.copy()\n",
    "    sorted_arr.sort()\n",
    "    n = arr.size\n",
    "    coef_ = 2. / n\n",
    "    const_ = (n + 1.) / n\n",
    "    weighted_sum = sum([(i+1)*yi for i, yi in enumerate(sorted_arr)])\n",
    "    return coef_*weighted_sum/(sorted_arr.sum()) - const_\n",
    "\n",
    "def lorenz_curve(X):\n",
    "    ## first sort\n",
    "    X = X.copy()\n",
    "    X.sort()\n",
    "    X_lorenz = X.cumsum() / X.sum()\n",
    "    X_lorenz = np.insert(X_lorenz, 0, 0) \n",
    "    X_lorenz[0], X_lorenz[-1]\n",
    "    fig, ax = plt.subplots(figsize=[6,6])\n",
    "    ## scatter plot of Lorenz curve\n",
    "    ax.scatter(np.arange(X_lorenz.size)/(X_lorenz.size-1), X_lorenz, \n",
    "               marker='x', color='darkgreen', s=100)\n",
    "    ## line plot of equality\n",
    "    ax.plot([0,1], [0,1], color='k')\n",
    "    ax.set_xlabel('% of Population')\n",
    "    ax.set_ylabel('% of Errors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9fcf45a3-f8fc-4df5-8463-ae1ec5a02e70",
    "_uuid": "d9c1d4f4-21a7-40f7-af56-bd0f48ab254e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:47.654627Z",
     "iopub.status.busy": "2022-03-29T07:28:47.65404Z",
     "iopub.status.idle": "2022-03-29T07:28:47.659605Z",
     "shell.execute_reply": "2022-03-29T07:28:47.65826Z",
     "shell.execute_reply.started": "2022-03-29T07:28:47.654572Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# window = 1000\n",
    "# window_farDistro_array = DistroFAR(dic[\"window_farDistro_dict\"][window], threshold = dic[\"window_EER_threshold_dict\"][window])\n",
    "# window_frrDistro_array = DistroFRR(dic[\"window_frrDistro_dict\"][window], threshold = dic[\"window_EER_threshold_dict\"][window])\n",
    "# X = np.array(window_frrDistro_array)\n",
    "# # print(X)\n",
    "# print(gini(X))\n",
    "# lorenz_curve(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8c1a264f-679a-4cf6-ae79-0750956ba0d7",
    "_uuid": "3598674f-4484-4ffc-a276-045f6fdd71a2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:47.996757Z",
     "iopub.status.busy": "2022-03-29T07:28:47.996401Z",
     "iopub.status.idle": "2022-03-29T07:28:48.000378Z",
     "shell.execute_reply": "2022-03-29T07:28:47.999235Z",
     "shell.execute_reply.started": "2022-03-29T07:28:47.996725Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# X = np.array(window_farDistro_array)\n",
    "# # print(X)\n",
    "# print(gini(X))\n",
    "# lorenz_curve(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "84cef82a-53b1-46ef-b9ea-53b56e9dd7ae",
    "_uuid": "decd2faf-d977-4db9-a53e-e303a433e7f5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:48.679392Z",
     "iopub.status.busy": "2022-03-29T07:28:48.679043Z",
     "iopub.status.idle": "2022-03-29T07:28:48.683453Z",
     "shell.execute_reply": "2022-03-29T07:28:48.682467Z",
     "shell.execute_reply.started": "2022-03-29T07:28:48.679363Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# keys_list = r\n",
    "# values_list = window_frrDistro_array\n",
    "# #Get pairs of elements\n",
    "\n",
    "\n",
    "# zip_iterator = zip(keys_list, values_list)\n",
    "\n",
    "# Distro_dict = dict(zip_iterator)\n",
    "\n",
    "# Distro_dict = {k: v for k, v in sorted(Distro_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "# fig, ax =plt.subplots(1,1, figsize=(8,8))\n",
    "\n",
    "# ax.set_title('FRR Distrobution')\n",
    "# print(Distro_dict)\n",
    "# data = {\"User\": list(Distro_dict.keys()), \"False Rejects\": list(Distro_dict.values())}\n",
    "# g = sns.barplot(x=data[\"User\"], y=data[\"False Rejects\"], order=data[\"User\"],ax = ax)\n",
    "# #y=EER_dict.values()\n",
    "\n",
    "# # y_ticks = np.arange(0, .25 + 0.001, .05)\n",
    "\n",
    "# # g.set_yticks(y_ticks)\n",
    "# ax.axhline(np.mean(data[\"False Rejects\"]), ls='--')\n",
    "# ax.set_xlabel('User')\n",
    "# ax.set_ylabel('False Rejects')\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "aabe956d-b0cc-44b0-a65c-47058842dbd1",
    "_uuid": "b629854f-9aca-444f-9b87-2309ba0a5d44",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:49.07909Z",
     "iopub.status.busy": "2022-03-29T07:28:49.078475Z",
     "iopub.status.idle": "2022-03-29T07:28:49.082701Z",
     "shell.execute_reply": "2022-03-29T07:28:49.08185Z",
     "shell.execute_reply.started": "2022-03-29T07:28:49.079048Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# X = np.array(window_farDistro_array)\n",
    "# # print(X)\n",
    "# print(gini(X))\n",
    "# lorenz_curve(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5438d555-f070-4019-8536-c063e35b7e94",
    "_uuid": "5a18caf2-bb56-45b4-a085-da83f4b35404",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:49.533056Z",
     "iopub.status.busy": "2022-03-29T07:28:49.532561Z",
     "iopub.status.idle": "2022-03-29T07:28:49.536687Z",
     "shell.execute_reply": "2022-03-29T07:28:49.535806Z",
     "shell.execute_reply.started": "2022-03-29T07:28:49.53301Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# auc(frrList, farList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e00ca0fd-f3b5-40b4-8e6b-4813887fe906",
    "_uuid": "2ef37b96-5eae-466c-9d73-cf9ee3abb975",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:49.888236Z",
     "iopub.status.busy": "2022-03-29T07:28:49.887827Z",
     "iopub.status.idle": "2022-03-29T07:28:49.894702Z",
     "shell.execute_reply": "2022-03-29T07:28:49.893711Z",
     "shell.execute_reply.started": "2022-03-29T07:28:49.888198Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def getAUROCDist(window_frrList_dict, window_farList_dict, start_window_size=250, end_window_size=3000, increment_step=250):\n",
    "    \n",
    "    window_AUROC_dict = {}\n",
    "    lst = np.arange(start_window_size, end_window_size + 1, increment_step)\n",
    "    \n",
    "    for w in lst:\n",
    "        \n",
    "        frrList = dic[\"window_frrList_dict\"][w]\n",
    "        farList = dic[\"window_farList_dict\"][w]\n",
    "        window_AUROC_dict[w] = auc(frrList, farList)\n",
    "        \n",
    "    return window_AUROC_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "09da174d-f14b-4a97-be01-ea606e593339",
    "_uuid": "b2e8ce39-9be1-4e28-8439-2ed3a704d924",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:50.359726Z",
     "iopub.status.busy": "2022-03-29T07:28:50.359407Z",
     "iopub.status.idle": "2022-03-29T07:28:50.364Z",
     "shell.execute_reply": "2022-03-29T07:28:50.362938Z",
     "shell.execute_reply.started": "2022-03-29T07:28:50.359698Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# AUROC_dict = getAUROCDist(dic[\"window_frrList_dict\"], dic[\"window_farList_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "84bd2da7-ede2-4a35-8495-5bc96140ca01",
    "_uuid": "1bbd0ddf-1782-49f8-94d6-d72dafd3bd1c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:51.624382Z",
     "iopub.status.busy": "2022-03-29T07:28:51.624045Z",
     "iopub.status.idle": "2022-03-29T07:28:51.62855Z",
     "shell.execute_reply": "2022-03-29T07:28:51.627496Z",
     "shell.execute_reply.started": "2022-03-29T07:28:51.624353Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# data = pd.read_csv('../input/wearable-assisted-ca/user10_1.docx', error_bad_lines = False, header=None, dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f3569ab8-ad0e-4ddc-8e5f-231c149e7995",
    "_uuid": "50be2017-9348-4fad-9f17-901e13021645",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:52.058309Z",
     "iopub.status.busy": "2022-03-29T07:28:52.057563Z",
     "iopub.status.idle": "2022-03-29T07:28:52.065136Z",
     "shell.execute_reply": "2022-03-29T07:28:52.064354Z",
     "shell.execute_reply.started": "2022-03-29T07:28:52.058249Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# def extractTextFromDocx(path):\n",
    "#     try:\n",
    "#         doc = docx.Document(path)  # Creating word reader object.\n",
    "#         data = \"\"\n",
    "#         fullText = []\n",
    "#         for para in doc.paragraphs:\n",
    "#             fullText.append(para.text)\n",
    "#             data = '\\n'.join(fullText)\n",
    "\n",
    "#     except IOError:\n",
    "#         print('There was an error opening the file!')\n",
    "#         return\n",
    "#     return data\n",
    "\n",
    "# # %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-03-18T18:34:19.771958Z\",\"iopub.execute_input\":\"2022-03-18T18:34:19.772266Z\",\"iopub.status.idle\":\"2022-03-18T18:34:19.793851Z\",\"shell.execute_reply.started\":\"2022-03-18T18:34:19.772229Z\",\"shell.execute_reply\":\"2022-03-18T18:34:19.792569Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n",
    "# def numberOfWords(text):\n",
    "#     return len(text.strip().split())\n",
    "\n",
    "# def numberOfChars(text):\n",
    "#     return len(text)\n",
    "\n",
    "# def wordsPerMinute(text, mins):\n",
    "#     return numberOfWords(text)/mins\n",
    "\n",
    "# def charsPerMinute(text, mins):\n",
    "#     return numberOfChars(text)/mins\n",
    "\n",
    "# def classifyTypists(typistsSpeeds):\n",
    "#     '''\n",
    "#         WPM\n",
    "#     Beginner\t0 - 24\n",
    "#     Intermediate\t25 - 30\n",
    "#     Average\t31 - 41\n",
    "#     Pro\t42 - 54\n",
    "#     Typemaster\t55 - 79\n",
    "#     Megaracer\t80+\n",
    "#     '''\n",
    "#     exp2_typingspeeds = [29.96428571, 37.42857143, 44.89285714, 52.35714286, 59.82142857, 67.28571429]\n",
    "#     speedDict = {\"Beginner\": 24, \"Intermediate\": 30, \"Average\": 41, \"Pro\": 54, \"Typemaster\": 79, \"Megaracer\": 1000}\n",
    "    \n",
    "#     keys = list(speedDict.keys())\n",
    "#     for i in range(len(speedDict.keys())):\n",
    "#         speedDict[keys[i]] = exp2_typingspeeds[i]\n",
    "    \n",
    "#     speedStats = {\"Beginner\": 0, \"Intermediate\": 0, \"Average\": 0, \"Pro\": 0, \"Typemaster\": 0, \"Megaracer\": 0}\n",
    "#     typistsIDStats = {\"Beginner\": [], \"Intermediate\": [], \"Average\": [], \"Pro\": [], \"Typemaster\": [], \"Megaracer\": []}\n",
    "    \n",
    "#     for typist, speed in typistsSpeeds.items():\n",
    "#         if speed <= speedDict[\"Beginner\"]:\n",
    "#             speedStats[\"Beginner\"] += 1\n",
    "#             typistsIDStats[\"Beginner\"].append(typist)\n",
    "            \n",
    "#         elif speed <= speedDict[\"Intermediate\"]:\n",
    "#             speedStats[\"Intermediate\"] += 1\n",
    "#             typistsIDStats[\"Intermediate\"].append(typist)\n",
    "            \n",
    "#         elif speed <= speedDict[\"Average\"]:\n",
    "#             speedStats[\"Average\"] += 1\n",
    "#             typistsIDStats[\"Average\"].append(typist)\n",
    "            \n",
    "#         elif speed <= speedDict[\"Pro\"]:\n",
    "#             speedStats[\"Pro\"] += 1\n",
    "#             typistsIDStats[\"Pro\"].append(typist)\n",
    "            \n",
    "#         elif speed <= speedDict[\"Typemaster\"]:\n",
    "#             speedStats[\"Typemaster\"] += 1\n",
    "#             typistsIDStats[\"Typemaster\"].append(typist)\n",
    "            \n",
    "#         else:\n",
    "#             speedStats[\"Megaracer\"] += 1\n",
    "#             typistsIDStats[\"Megaracer\"].append(typist)\n",
    "            \n",
    "#     return {\"speedStats\": speedStats, \"typistsIDStats\": typistsIDStats}\n",
    "\n",
    "# # %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-03-18T18:34:19.795159Z\",\"iopub.execute_input\":\"2022-03-18T18:34:19.795733Z\",\"iopub.status.idle\":\"2022-03-18T18:34:19.814022Z\",\"shell.execute_reply.started\":\"2022-03-18T18:34:19.795693Z\",\"shell.execute_reply\":\"2022-03-18T18:34:19.812934Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n",
    "# r = [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 28, 29, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49]\n",
    "# #r = [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 28, 29, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
    "\n",
    "\n",
    "# def users_typing_speed(user_ids_lst):\n",
    "#     '''\n",
    "#     input: \n",
    "#         user_ids_lst: list of selected user ids\n",
    "#     Return {user_id: typing_speed in words per minute}\n",
    "#     '''\n",
    "#     typistsSpeeds = {}\n",
    "#     for i in user_ids_lst:\n",
    "#         user_text_data = extractTextFromDocx('../input/wearable-assisted-ca/user{0}_{1}.docx'.format(i, 2))\n",
    "\n",
    "#         typistsSpeeds[i] = wordsPerMinute(user_text_data, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "19063d95-fc9e-4094-8ab7-5999e611c04e",
    "_uuid": "d119855a-a09a-4c7e-b31b-b26a65900d93",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-29T07:28:52.36913Z",
     "iopub.status.busy": "2022-03-29T07:28:52.368551Z",
     "iopub.status.idle": "2022-03-29T07:28:52.373777Z",
     "shell.execute_reply": "2022-03-29T07:28:52.372688Z",
     "shell.execute_reply.started": "2022-03-29T07:28:52.36908Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# dic = classifyTypists(typistsSpeeds)\n",
    "# dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1215f780-6abe-4c99-9c42-de10cf6fbd57",
    "_uuid": "50d96fcf-e53b-4424-9b98-f58d5133e2d9"
   },
   "source": [
    "# Divide the users using histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a style I use a lot for boxplots:\n",
    "utils_boxplot_style = dict(\n",
    "    color=\"tab:blue\",\n",
    "    linewidth=0.5,\n",
    "    saturation=1,\n",
    "    width=0.7,\n",
    "    flierprops=dict(\n",
    "        marker=\"o\", markersize=2, markerfacecolor=\"none\", markeredgewidth=0.5\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Define a style I use a lot for lineplots:\n",
    "utils_lineplot_style = dict(\n",
    "    color=\"tab:blue\", linewidth=0.5, marker=\"o\", markersize=3, markeredgewidth=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T04:26:44.057446Z",
     "iopub.status.busy": "2022-03-31T04:26:44.057Z",
     "iopub.status.idle": "2022-03-31T04:26:44.063077Z",
     "shell.execute_reply": "2022-03-31T04:26:44.062158Z",
     "shell.execute_reply.started": "2022-03-31T04:26:44.057407Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"utility functions imported\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
