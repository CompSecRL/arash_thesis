{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746913e1-9192-43eb-b5cf-1179e440edc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cite: Siamese Model with 2D Filters, as derived from Centeno et al. (2018)\n",
    "# # the thesis I found\n",
    "# def build_model_2d(input_shape, filters):\n",
    "#     \"\"\"\n",
    "#         Siamese CNN architecture with 3D input and 2D filters\n",
    "#     \"\"\"\n",
    "#     # Define the tensors for the two input images\n",
    "#     left_inputs = Input(input_shape, name=\"left_inputs\")\n",
    "#     right_inputs = Input(input_shape, name=\"right_inputs\")\n",
    "\n",
    "#     # Convolutional Neural Network\n",
    "#     inputs = Input(input_shape, name=\"input\")\n",
    "#     x = Conv2D(filters[0], (7, 7), padding=\"same\", activation=\"tanh\", name=\"conv1\")(inputs)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp1\")(x)\n",
    "#     x = Conv2D(filters[1], (5, 5), padding=\"same\", activation=\"tanh\", name=\"conv2\")(x)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp2\")(x)\n",
    "#     x = Conv2D(filters[2], (3, 3), padding=\"same\", activation=\"tanh\", name=\"conv3\")(x)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp3\")(x)\n",
    "#     x = Conv2D(filters[3], (3, 3), padding=\"same\", activation=\"tanh\", name=\"conv4\")(x)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp4\")(x)\n",
    "#     x = Flatten(name=\"flat\")(x)\n",
    "    \n",
    "#     # Basemodel instance\n",
    "#     basemodel = Model(inputs, x, name=\"basemodel\")\n",
    "\n",
    "#     # using same instance of \"basemodel\" to share weights between left/right networks\n",
    "#     encoded_l = basemodel(left_inputs)\n",
    "#     encoded_r = basemodel(right_inputs)\n",
    "\n",
    "#     # Add a customized layer to compute the distance between the encodings\n",
    "#     distance_layer = Lambda(k_euclidean_dist, name=\"distance\")([encoded_l, encoded_r])\n",
    "\n",
    "#     # Combine into one net\n",
    "#     siamese_net = Model(inputs=[left_inputs, right_inputs], outputs=distance_layer)\n",
    "\n",
    "#     # return the model\n",
    "#     return siamese_net, basemodel\n",
    "\n",
    "\n",
    "# def get_model(name, window_size, feature_cols, filters):\n",
    "#     print(f\"Using Model variant {name}...\")\n",
    "#     if name == \"1d\":\n",
    "#         model, basemodel = build_model_1d((window_size, len(feature_cols)), filters)\n",
    "#     elif name == \"2d\":\n",
    "#         model, basemodel = build_model_2d((window_size, len(feature_cols), 1), filters)\n",
    "#     elif name == \"fcn\":\n",
    "#         model, basemodel = build_model_fcn((window_size, len(feature_cols)), filters)\n",
    "#     else:\n",
    "#         raise BaseException(\"Error: Not a valid model name: {1d, 2d, fcn}\")\n",
    "\n",
    "#     return model, basemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058a2ea7-d635-4a24-8d29-4591b3080815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_index_pairs_multi_input(index_arr_1, index_arr_2):\n",
    "#     '''\n",
    "#     return an array of indices for negative/positive pairs. It is assumed that every \n",
    "#     index of pair corresponds to different sessions.\n",
    "#     '''\n",
    "#     mesh = np.array(np.meshgrid(index_arr_1, index_arr_2))\n",
    "#     negative_index_pairs = mesh.T.reshape(-1, 2)\n",
    "    \n",
    "#     return negative_index_pairs\n",
    "\n",
    "# def generate_pairs_multi_input(X_exp2_dic, X_exp1_dic, user_id_list, fitted_raw_scaler_dict=None, num_pair_limit = 50000):\n",
    "#     '''\n",
    "#     return positive and negative pairs\n",
    "#     '''\n",
    "#     num_users = len(user_id_list)\n",
    "#     # print(num_users)\n",
    "# #     X_neg[np.random.choice(X_neg.shape[0], size=X_pos.shape[0], replace=False), :]\n",
    "#     negative_pairs_dic = {}\n",
    "#     positive_pairs_dic = {}\n",
    "    \n",
    "#     total_samples_per_user = len(X_exp2_dic[list(X_exp2_dic.keys())[0]])\n",
    "#     # print(total_samples_per_user)\n",
    "#     multi_input_anchor_index_array = getIndices(sampleSize=3, step=5, numSamplePoints=total_samples_per_user)\n",
    "    \n",
    "#     print(multi_input_anchor_index_array)\n",
    "#     anchor_index_arr=range(multi_input_anchor_index_array.shape[0])\n",
    "#     new_user_index_arr=range(total_samples_per_user)\n",
    "    \n",
    "#     positive_pairs_indices = generate_index_pairs_multi_input(anchor_index_arr, new_user_index_arr)\n",
    "#     negative_pairs_indices = generate_index_pairs_multi_input(anchor_index_arr, new_user_index_arr)\n",
    "    \n",
    "#     # negative_pairs_indices = negative_pairs_indices[np.random.choice(negative_pairs_indices.shape[0], size=positive_pairs_indices.shape[0], replace=False)]\n",
    "#     # negative_pairs_indices = np.sort(negative_pairs_indices)\n",
    "#     # print(negative_pairs_indices)\n",
    "#     positive_left = positive_pairs_indices[:, 0]\n",
    "#     positive_right = positive_pairs_indices[:, 1]\n",
    "    \n",
    "#     negative_left = negative_pairs_indices[:, 0]\n",
    "#     negative_right = negative_pairs_indices[:, 1]\n",
    "#     # fitted_scaler_SNN_exp2_train_dic\n",
    "#     # print(positive_pairs_indices.shape)\n",
    "#     # print(negative_pairs_indices.shape)\n",
    "    \n",
    "#     total_num_pairs = (positive_pairs_indices.shape[0]+negative_pairs_indices.shape[0]) * num_users \n",
    "#     if num_pair_limit < total_num_pairs: \n",
    "#         limit_ratio = num_pair_limit/total_num_pairs\n",
    "#     else:\n",
    "#         limit_ratio=1\n",
    "            \n",
    "    \n",
    "#     # neg_fraction =  ((total_samples_per_user-1) / (total_samples_per_user * (num_users-1)))  # works accurate\n",
    "#     neg_fraction =  1 / (num_users-1)  # works accurate\n",
    "#     # neg_fraction =  (total_samples_per_user-1) / (total_samples_per_user * (num_users)) # works accurate\n",
    "#     for owner_key in tqdm(user_id_list):\n",
    "#         # debugging was: X_exp2_dic is not an array, but needs to be an array if we want integer indexing. This can be changed in utility_functions; however, need to modify other code. \n",
    "#         # print(type(positive_left))\n",
    "#         # print(len(X_exp2_dic[owner_key]))\n",
    "#         # print(np.array(X_exp2_dic[owner_key])[positive_left])\n",
    "#         np.random.seed(SEED + owner_key)\n",
    "#         # transform_user_windows(X_exp_unknown_df_dict[unknown_user], fitted_raw_scaler_dict[owner_key])\n",
    "#         u1_array = np.array(X_exp2_dic[owner_key])\n",
    "#         u2_array = np.array(transform_user_windows(X_exp1_dic[owner_key], fitted_raw_scaler_dict[owner_key]))\n",
    "        \n",
    "#         # print(multi_input_anchor_index_array.shape)\n",
    "#         # print(u1_array.shape)\n",
    "#         # print(positive_pairs_indices)\n",
    "#         # print(total_samples_per_user)\n",
    "#         # print(positive_right.shape)\n",
    "#         # print(negative_right.shape)\n",
    "#         # quick fix for user 47\n",
    "#         if owner_key == 29:\n",
    "            \n",
    "#             u1_array = np.concatenate([u1_array, u1_array[np.random.choice(u1_array.shape[0], size=total_samples_per_user-u1_array.shape[0], replace=False)]], axis=0)#55-47\n",
    "#             u2_array = np.concatenate([u2_array, u2_array[np.random.choice(u2_array.shape[0], size=total_samples_per_user-u2_array.shape[0], replace=False)]], axis=0)\n",
    "          \n",
    "#         u1_array = u1_array[multi_input_anchor_index_array, :]\n",
    "#         # print(u1_array.shape)\n",
    "#         # print(u1_array.shape)\n",
    "#         # print(u2_array.shape)\n",
    "#         # try:\n",
    "#         # print(f\"owner_key: {owner_key}, u1_array.shape: {u1_array.shape}, u2_array.shape: {u2_array.shape}\")\n",
    "#         positive_pairs_dic[owner_key] = np.array(list(zip(u1_array[positive_left], u2_array[positive_right])))\n",
    "#         # except:\n",
    "#         #     print(owner_key)\n",
    "#         # print(f\"positive_pairs per user: {positive_pairs_dic[owner_key].shape[0]}\")\n",
    "#         new_pos_size = int(np.round(limit_ratio * positive_pairs_dic[owner_key].shape[0]))\n",
    "#         positive_pairs_dic[owner_key] = positive_pairs_dic[owner_key][np.random.choice(\n",
    "#                     positive_pairs_dic[owner_key].shape[0], size=new_pos_size, replace=False), :]\n",
    "        \n",
    "\n",
    "#         for u2_key in user_id_list:\n",
    "#             # 47 55\n",
    "#             np.random.seed(SEED + u2_key)\n",
    "#             # should i eliminate less informative pairs like (w_n, w_m) vs (w_m, w_n)?\n",
    "#             if u2_key != owner_key:\n",
    "#                 u2_array = np.array(transform_user_windows(X_exp1_dic[u2_key], fitted_raw_scaler_dict[owner_key]))\n",
    "                \n",
    "#                 # quick fix for user 47\n",
    "#                 if u2_key == 29:\n",
    "#                     u2_array = np.concatenate([u2_array, u2_array[np.random.choice(u2_array.shape[0], size=total_samples_per_user-u2_array.shape[0], replace=False)]], axis=0)#55-47,\n",
    "                \n",
    "                \n",
    "#                 # print(u1_array.shape)\n",
    "#                 # print(u2_array.shape)\n",
    "#                 # print(f\"u2_key {u2_key}, u1_array.shape: {u1_array.shape}, u2_array.shape: {u2_array.shape}\")\n",
    "#                 negative_pairs_dic[(owner_key, u2_key)] = np.array(list(zip(u1_array[negative_left], u2_array[negative_right])))\n",
    "#                 # print(f\"neg_pairs per user: {negative_pairs_dic[(owner_key, u2_key)].shape[0]}\")\n",
    "                \n",
    "#                 new_neg_size = int(np.round(negative_pairs_dic[(owner_key, u2_key)].shape[0] * neg_fraction * limit_ratio))\n",
    "#                 negative_pairs_dic[(owner_key, u2_key)] = negative_pairs_dic[(owner_key, u2_key)][np.random.choice(\n",
    "#                     negative_pairs_dic[(owner_key, u2_key)].shape[0], size=new_neg_size, replace=False), :]\n",
    "#     # print(new_pos_size)\n",
    "#     # print(new_neg_size)\n",
    "#     return {\"positive_pairs_dic\": positive_pairs_dic, \"negative_pairs_dic\": negative_pairs_dic}\n",
    "\n",
    "# def prep_X_y_pair_multi_input(X_exp2_dic, X_exp1_dic, user_id_list, fitted_raw_scaler_dict=None, num_pair_limit = 50000):\n",
    "    \n",
    "#     X_dic = generate_pairs_multi_input(X_exp2_dic, X_exp1_dic, user_id_list, fitted_raw_scaler_dict=fitted_raw_scaler_dict, num_pair_limit=num_pair_limit)\n",
    "#     # print(X_dic['negative_pairs_dic'].keys())\n",
    "#     pos_X, neg_X = get_pos_array(X_dic['positive_pairs_dic']), get_neg_array(X_dic['negative_pairs_dic'])\n",
    "    \n",
    "#     pos_y = np.repeat(1., pos_X.shape[0]).reshape((pos_X.shape[0], 1))\n",
    "#     neg_y = np.repeat(0., neg_X.shape[0]).reshape((neg_X.shape[0], 1))\n",
    "    \n",
    "#     print(np.stack(pos_X[:, 0]).shape)\n",
    "#     print(pos_X[:, 1][0].shape)\n",
    "#     pos_X_left = np.stack(pos_X[:, 0])\n",
    "#     pos_X_right = np.stack(pos_X[:, 1])\n",
    "#     print(pos_X_left.shape)\n",
    "#     print(pos_X_right.shape)\n",
    "    \n",
    "#     neg_X_left = np.stack(neg_X[:, 0])\n",
    "#     neg_X_right = np.stack(neg_X[:, 1])\n",
    "    \n",
    "#     X_left = np.concatenate([pos_X_left, neg_X_left]).astype(\"float32\")\n",
    "#     X_right = np.concatenate([pos_X_right, neg_X_right]).astype(\"float32\")\n",
    "#     y = np.concatenate([pos_y, neg_y]).astype(\"float32\")\n",
    "\n",
    "#     np.random.seed(SEED)\n",
    "#     X_left, X_right, y = sklearn_shuffle(X_left, X_right, y, random_state=SEED)\n",
    "    \n",
    "#     X = [X_left, X_right]\n",
    "    \n",
    "#     return X, y\n",
    "\n",
    "\n",
    "# def create_multi_input_model_fcn(anchor_shape, input_shape, arg_dict, do_plot=True):\n",
    "#     '''\n",
    "#     input_dropout_streams = arg_dict[\"input_dropout_streams\"]\n",
    "#     filters_streams = arg_dict[\"filters_streams\"]\n",
    "#     kernels_streams = arg_dict[\"kernels_streams\"]\n",
    "#     kernels_init_streams = arg_dict[\"kernels_init_streams\"] \n",
    "#     kernels_constraint_streams = arg_dict[\"kernels_constraint_streams\"] \n",
    "#     strides_streams = arg_dict[\"strides_streams\"] \n",
    "#     paddings_streams = arg_dict[\"paddings_streams\"] \n",
    "#     activations_streams = arg_dict[\"activations_streams\"]\n",
    "#     dropouts_streams = arg_dict[\"dropouts_streams\"] \n",
    "    \n",
    "#     dense_layers = arg_dict[\"dense_layers\"] \n",
    "#     dense_kernel_constraints = arg_dict[\"dense_kernel_constraints\"]\n",
    "#     dense_kernel_inits = arg_dict[\"dense_kernel_inits\"] \n",
    "#     dense_dropouts = arg_dict[\"dense_dropouts\"] \n",
    "#     dense_activations = arg_dict[\"dense_activations\"] \n",
    "    \n",
    "#     loss_func = arg_dict[\"loss_func\"] \n",
    "#     optimizer = arg_dict[\"optimizer\"] \n",
    "    \n",
    "#     # Define the tensors for the two input images\n",
    "#     left_inputs = Input(input_shape, name=\"left_inputs\")\n",
    "#     right_inputs = Input(input_shape, name=\"right_inputs\")\n",
    "    \n",
    "#     '''\n",
    "#     # conv\n",
    "#     input_dropout_streams = arg_dict[\"input_dropout_streams\"]\n",
    "#     filters_streams = arg_dict[\"filters_streams\"]\n",
    "#     kernels_streams = arg_dict[\"kernels_streams\"]\n",
    "#     kernels_init_streams = arg_dict[\"kernels_init_streams\"] \n",
    "#     kernels_Max_Norm_constraint_streams = arg_dict[\"kernels_Max_Norm_constraint_streams\"] \n",
    "#     strides_streams = arg_dict[\"strides_streams\"] \n",
    "#     paddings_streams = arg_dict[\"paddings_streams\"] \n",
    "#     activations_streams = arg_dict[\"activations_streams\"]\n",
    "#     dropouts_streams = arg_dict[\"dropouts_streams\"]\n",
    "#     conv_kernel_regularizer_streams = arg_dict[\"conv_kernel_regularizer_streams\"]\n",
    "    \n",
    "#     # dense\n",
    "#     dense_layers = arg_dict[\"dense_layers\"] \n",
    "#     dense_kernel_Max_Norm_constraints = arg_dict[\"dense_kernel_Max_Norm_constraints\"]\n",
    "#     dense_kernel_inits = arg_dict[\"dense_kernel_inits\"] \n",
    "#     dense_dropouts = arg_dict[\"dense_dropouts\"] \n",
    "#     dense_activations = arg_dict[\"dense_activations\"] \n",
    "#     dense_kernel_regularizer = arg_dict[\"dense_kernel_regularizer\"]\n",
    "    \n",
    "#     # loss_func = get_loss_func(arg_dict[\"loss_func_name\"], arg_dict[\"loss_func_args\"]) improve in future\n",
    "#     loss_func = get_loss_func(arg_dict)\n",
    "#     optimizer = get_optimizer(arg_dict[\"optimizer_name\"] , lr=arg_dict[\"optimizer_lr\"], decay=arg_dict[\"optimizer_decay\"])\n",
    "    \n",
    "#     # Define the tensors for the two input images\n",
    "#     left_inputs = Input(anchor_shape, name=\"left_inputs\")\n",
    "#     right_inputs = Input(input_shape, name=\"right_inputs\")\n",
    "    \n",
    "#     # print(tf.squeeze(tf.slice(left_inputs, [0, 1, 0, 0], [-1, 1, -1, -1]), axis=1))\n",
    "#     # Convolutional Neural Network\n",
    "#     inputs_dict = {}\n",
    "#     for head_idx in range(len(filters_streams)):\n",
    "#         inputs_dict[head_idx] = Input(input_shape, name=f\"inputs_{head_idx}\")\n",
    "\n",
    "    \n",
    "#     flat_output_dict = {}\n",
    "#     for head in inputs_dict:\n",
    "#         x= inputs_dict[head]\n",
    "\n",
    "#         x = Dropout(input_dropout_streams[head], seed=SEED, name=f\"stream_{head}_input_drop\")(x)\n",
    "#         for idx in range(len(filters_streams[head])):\n",
    "#             x = Conv1D(filters=filters_streams[head][idx], kernel_size=kernels_streams[head][idx], kernel_initializer=kernels_init_streams[head][idx], \n",
    "#                        kernel_constraint=MaxNorm(kernels_Max_Norm_constraint_streams[head][idx]), strides=strides_streams[head][idx], activation=None, \n",
    "#                        padding=paddings_streams[head][idx], kernel_regularizer=conv_kernel_regularizer_streams[head][idx], name=f\"stream_{head}_conv_{idx+1}\")(x)\n",
    "#             x = BatchNormalization()(x)\n",
    "#             x = Activation(activations_streams[head][idx])(x)\n",
    "#             x = Dropout(dropouts_streams[head][idx], seed=SEED, name=f\"stream_{head}_drop_{idx+1}\")(x)\n",
    "\n",
    "#         x = GlobalAveragePooling1D()(x)\n",
    "        \n",
    "#         flat_output_dict[head] = x\n",
    "    \n",
    "#     # merge\n",
    "#     merged = keras_concat([flat_output_dict[head_idx] for head_idx in range(len(flat_output_dict))])\n",
    "\n",
    "#     x = merged\n",
    "    \n",
    "#     for dense_idx in range(len(dense_layers)):\n",
    "#         x = Dropout(dense_dropouts[dense_idx], name=f\"dense_{dense_idx+1}_dropout\")(x)\n",
    "#         x = Dense(dense_layers[dense_idx], kernel_constraint=MaxNorm(dense_kernel_Max_Norm_constraints[dense_idx]), kernel_initializer=dense_kernel_inits[dense_idx], \n",
    "#                   kernel_regularizer=dense_kernel_regularizer[dense_idx], name=f\"dense_{dense_idx+1}\")(x) # excluded activation=dense_activations[dense_idx]\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Activation(dense_activations[dense_idx], name=f\"dense_activation_{dense_idx+1}\" )(x)\n",
    "    \n",
    "#     # Basemodel instance\n",
    "#     basemodel = Model(inputs=[inputs_dict[idx] for idx in range(len(inputs_dict))], outputs=x, name=\"basemodel\")\n",
    "\n",
    "#     # using same instance of \"basemodel\" to share weights between left/right networks\n",
    "#     print(left_inputs.shape)\n",
    "#     # basemodel_left_inputs_lst=[tf.squeeze(tf.slice(left_inputs, [0, i, 0, 0], [-1, 1, -1, -1]), axis=1) for i in range(len(inputs_dict))]\n",
    "#     # basemodel_left_inputs_lst = Lambda(lambda x: tf.squeeze(tf.split(x, num_or_size_splits=len(inputs_dict), axis=1)))(left_inputs)\n",
    "#     basemodel_left_inputs_lst=[left_inputs[:, i, :, :] for i in range(len(inputs_dict))]\n",
    "\n",
    "#     basemodel_right_inputs_lst=[right_inputs for i in range(len(inputs_dict))]\n",
    "    \n",
    "#     print(basemodel_left_inputs_lst)\n",
    "#     print(basemodel_right_inputs_lst)\n",
    "\n",
    "#     encoded_l = basemodel(basemodel_left_inputs_lst)\n",
    "#     encoded_r = basemodel(basemodel_right_inputs_lst)\n",
    "\n",
    "#     # Add a customized layer to compute the distance between the encodings\n",
    "#     distance_layer = Lambda(k_euclidean_dist, name=\"distance\")([encoded_l, encoded_r])\n",
    "\n",
    "#     # Combine into one net\n",
    "#     siamese_net = Model(inputs=[left_inputs, right_inputs], outputs=distance_layer)\n",
    "#     if do_plot: \n",
    "#         plot_model(siamese_net, show_shapes=True, to_file='multichannel.png')\n",
    "#         plot_model(basemodel, show_shapes=True, to_file='basemodel.png')\n",
    "        \n",
    "#     siamese_net.compile(loss=loss_func, optimizer=optimizer)\n",
    "    \n",
    "#     # return the model\n",
    "#     return siamese_net\n",
    "\n",
    "# def get_create_model_func_multi_input(name, window_size, feature_cols, num_input):\n",
    "#     print(f\"Using Model variant {name}...\")\n",
    "#     if name == \"multi_input\":\n",
    "#          return lambda arg_dict: create_multi_input_model_fcn((num_input, window_size, len(feature_cols)), (window_size, len(feature_cols)), arg_dict)\n",
    "        \n",
    "#     else:\n",
    "#         raise BaseException(\"Error: Not a valid model name: {1d, 2d, fcn}\")\n",
    "        \n",
    "        \n",
    "# class MetricsCallback_multi_input(Callback):\n",
    "#     \"\"\"\n",
    "#     Custom Keras Callback function.\n",
    "    \n",
    "#     Used to predict and plot distances for positive and negative pairs\n",
    "#     after each n-th epoch, along with some 'classification' metrics. \n",
    "#     'Classification' here means to ability to distinguish between positive \n",
    "#     and negative pairs using a threshold for the distance.\n",
    "    \n",
    "#     Arguments:\n",
    "#         payload {tuple}           -- Datasets used for evaluation: (X_valid, y_valid, X_train, y_train)\n",
    "#         epoch_evaluate_freq {int} -- Frequency for evaluation. After every n-th epoch, \n",
    "#                                      the results are evaluated and printed\n",
    "#         save_plots {boolean}      -- Do you want to save plots as PDF? Path is configured via global\n",
    "#                                      parameter REPORT_PATH.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, payload, epoch_evaluate_freq=1, loss_record_dict={}, metric_record_dict={}, save_plots=False, \n",
    "#                  plot_pca=False, print_interm_epochs=True, early_stoping=False, ReduceLROnPlateau_args=None, optimal_lr_epoch_dict=None, lr_epoch_log_dict={}):\n",
    "#         # super(MetricsCallback, self).__init__()\n",
    "        \n",
    "#         self.lr_epoch_log_dict=lr_epoch_log_dict\n",
    "        \n",
    "#         self.optimal_lr_epoch_dict=optimal_lr_epoch_dict\n",
    "            \n",
    "        \n",
    "#         self.ReduceLROnPlateau_args=ReduceLROnPlateau_args\n",
    "#         self.valid_metrics={\"roc_val\", \"eer_val\", \"thres\", \"acc_val\", \"f1_val\"}\n",
    "#         if self.ReduceLROnPlateau_args != None:\n",
    "#             ReduceLROnPlateau_mode={\"val_loss\": \"min\", \"roc_val\": \"max\", \"eer_val\": \"min\", \"thres\": \"min\", \"acc_val\": \"max\", \"f1_val\": \"max\"}\n",
    "#             self.reduce_lr = MyReduceLROnPlateau(monitor=ReduceLROnPlateau_args['mointored_metric'], factor=ReduceLROnPlateau_args[\"factor\"], \n",
    "#                                                patience=ReduceLROnPlateau_args[\"patience\"], verbose=ReduceLROnPlateau_args[\"verbose\"], \n",
    "#                                                min_lr=ReduceLROnPlateau_args[\"min_lr\"], mode=ReduceLROnPlateau_mode[ReduceLROnPlateau_args['mointored_metric']])\n",
    "\n",
    "#         self.X_valid, self.y_valid, self.X_train, self.y_train = payload\n",
    "#         self.save_plots = save_plots\n",
    "#         self.epoch_evaluate_freq = epoch_evaluate_freq\n",
    "#         self.loss_record_dict = loss_record_dict\n",
    "#         self.metric_record_dict = metric_record_dict\n",
    "#         self.epoch = []\n",
    "#         self.history = {}\n",
    "#         self.plot_pca = plot_pca\n",
    "#         self.print_interm_epochs = print_interm_epochs\n",
    "#         self.early_stoping = early_stoping\n",
    "        \n",
    "#         self.metric_record_dict['roc_val'] = {\"Train\": [], \"Valid\": []}\n",
    "#         self.metric_record_dict['eer_val'] = {\"Train\": [], \"Valid\": []}\n",
    "#         self.metric_record_dict['thres'] = {\"Train\": [], \"Valid\": []}\n",
    "#         self.metric_record_dict['acc_val'] = {\"Train\": [], \"Valid\": []}\n",
    "#         self.metric_record_dict['f1_val'] = {\"Train\": [], \"Valid\": []}\n",
    "        \n",
    "#         # Do we have train and valid set?\n",
    "#         self.sets = []\n",
    "#         if self.X_train:\n",
    "#             self.sets.append([self.X_train, self.y_train, \"Train\"])\n",
    "#         if self.X_valid:\n",
    "#             self.sets.append([self.X_valid, self.y_valid, \"Valid\"])\n",
    "\n",
    "#     def on_train_begin(self, logs={}):\n",
    "\n",
    "#         print(32 * \"=\" + f\"[ Initial State ]\" + 32 * \"=\", end=\"\")\n",
    "#         for X, y, desc in self.sets:\n",
    "#             self.evaluate(X, y, logs, desc, -1)\n",
    "            \n",
    "        \n",
    "\n",
    "#         if self.plot_pca:\n",
    "#             deep_feature_model = Model(\n",
    "#                 inputs=self.model.layers[0].get_input_at(0),  # get_layer(\"left_inputs\").input,\n",
    "#                 outputs=self.model.get_layer(\"basemodel\").get_output_at(0),\n",
    "#                 )\n",
    "#             deep_feature_model.summary()\n",
    "#             deep_features_test = None\n",
    "#             for X, y, subj in samples_test:  \n",
    "                \n",
    "#                 # if 2d, maybe need to import P\n",
    "#                 # if P.model_variant == \"2d\":\n",
    "#                 #     X = X.reshape((*X.shape, 1))\n",
    "#                 pred = deep_feature_model.predict(X)\n",
    "#                 df_features = pd.DataFrame(pred)\n",
    "#                 df_features[\"subject\"] = subj\n",
    "#                 deep_features_test = pd.concat([deep_features_test, df_features])\n",
    "            \n",
    "#             plot_pca(deep_features_test)\n",
    "            \n",
    "\n",
    "#     def on_train_end(self, logs={}):\n",
    "#         print(32 * \"=\" + f\"[ Final State ]\" + 32 * \"=\", end=\"\")\n",
    "#         for X, y, desc in self.sets:\n",
    "#             self.evaluate(X, y, logs, desc, -1)\n",
    "            \n",
    "#     def on_epoch_begin(self, epoch, logs={}, verbose=0):\n",
    "        \n",
    "#         optimizer = self.model.optimizer\n",
    "#         if self.optimal_lr_epoch_dict != None:\n",
    "\n",
    "#             K.set_value(optimizer.lr, self.optimal_lr_epoch_dict[epoch])\n",
    "#             if verbose:\n",
    "#                 print(f\"Learning rate changed to {self.optimal_lr_epoch_dict[epoch]} for epoch {epoch}\")\n",
    "            \n",
    "#         else:\n",
    "#             self.lr_epoch_log_dict[epoch]=K.get_value(optimizer.lr)\n",
    "#             if verbose:\n",
    "#                 print(f\"Logged learning rate of {self.lr_epoch_log_dict[epoch]} for epoch {epoch}\") \n",
    "            \n",
    "            \n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         if self.print_interm_epochs:\n",
    "#             print(32 * \"=\" + f\"[   Epoch {epoch}   ]\" + 32 * \"=\", end=\"\")\n",
    "#             if epoch % self.epoch_evaluate_freq == 0:  # Evaluate only every n-th epoch\n",
    "#                 for X, y, desc in self.sets:\n",
    "#                     self.evaluate(X, y, logs, desc, epoch)\n",
    "#             else:\n",
    "#                 print(f\"\\n{ ', '.join([k + ': ' + f'{v:.3f}' for k,v in logs.items()]) }\")\n",
    "            \n",
    "\n",
    "#         logs = logs or {}\n",
    "#         self.epoch.append(epoch)\n",
    "#         for k, v in logs.items():\n",
    "#             self.history.setdefault(k, []).append(v)\n",
    "    \n",
    "        \n",
    "#         if 'val_loss' in logs:\n",
    "#             if self.early_stoping and len(self.loss_record_dict['val_loss'])>1 and self.loss_record_dict['val_loss'][-1] < logs['val_loss']:\n",
    "#                 self.model.stop_training = True\n",
    "                \n",
    "#             else:\n",
    "#                 self.loss_record_dict['val_loss'].append(logs['val_loss'])\n",
    "\n",
    "#                 self.loss_record_dict['loss'].append(logs['loss'])\n",
    "\n",
    "#                 for X, y, desc in self.sets:\n",
    "#                     self.evaluate(X, y, logs, desc, epoch, print_metric=False, save_metric_record_dict=True)\n",
    "            \n",
    "#             # print(logs)\n",
    "#             if self.ReduceLROnPlateau_args != None:\n",
    "#                 if self.ReduceLROnPlateau_args['mointored_metric'] == \"val_loss\":\n",
    "#                     self.reduce_lr.on_epoch_end(epoch, self.model, logs)\n",
    "\n",
    "#                 elif self.ReduceLROnPlateau_args['mointored_metric'] in self.valid_metrics:\n",
    "#                     mointored_metric_dict={self.ReduceLROnPlateau_args['mointored_metric']: self.metric_record_dict[self.ReduceLROnPlateau_args['mointored_metric']][\"Valid\"][-1]}\n",
    "#                     self.reduce_lr.on_epoch_end(epoch, self.model, mointored_metric_dict)\n",
    "\n",
    "#                 else:\n",
    "#                     raise Exception(\"invalid mointored metric\")\n",
    "            \n",
    "                    \n",
    "#         else:\n",
    "#             self.loss_record_dict['loss'].append(logs['loss'])\n",
    "\n",
    "#             for X, y, desc in self.sets:\n",
    "#                 self.evaluate(X, y, logs, desc, epoch, print_metric=False, save_metric_record_dict=True)\n",
    "                \n",
    "        \n",
    "\n",
    "            \n",
    "#     def evaluate(self, X, y, logs, desc, epoch, print_metric=True, save_metric_record_dict=False):\n",
    "#         print(X[0].shape)\n",
    "#         print(X[1].shape)\n",
    "\n",
    "#         print(len(X))\n",
    "#         print(self.model.layers)\n",
    "#         print(self.model.summary())\n",
    "# #         anchor_model = Model(\n",
    "# #                 inputs=self.model.get_layer(\"basemodel\").get_input_at(0),  # get_layer(\"left_inputs\").input,\n",
    "# #                 outputs=self.model.get_layer(\"basemodel\").get_output_at(0),\n",
    "# #             )\n",
    "        \n",
    "#         # test_model = Model(\n",
    "#         #         inputs=self.model.get_layer(\"right_inputs\").get_input_at(0),  # get_layer(\"left_inputs\").input,\n",
    "#         #         outputs=self.model.get_layer(\"basemodel\").get_output_at(0),\n",
    "#         #     )\n",
    "#         # test_model.predict(X[1])\n",
    "#         # Predict\n",
    "#         y_score = self.model.predict(X)\n",
    "#         y_score_neg = y_score * -1  # lower distance means closer to positive class\n",
    "\n",
    "#         # Calc Metrics\n",
    "#         roc_val = metrics.roc_auc_score(y, y_score_neg)\n",
    "#         eer_val, thres = utils_eer(y, y_score_neg, True)\n",
    "#         y_pred = np.where(y_score_neg > thres, 1, 0)\n",
    "#         acc_val = metrics.accuracy_score(y, y_pred)\n",
    "#         f1_val = metrics.f1_score(y, y_pred)\n",
    "        \n",
    "#         if print_metric:\n",
    "#             print(\n",
    "#                 f\"\\n{desc.upper()}: roc_auc: {roc_val:.4f}, \"\n",
    "#                 + f\"eer: {eer_val:.4f}, thres: {thres*-1:.4f} => \"\n",
    "#                 + f\"acc: {acc_val:.4f}, f1: {f1_val:.4f}\\n\"\n",
    "#                 + f\"{ ', '.join([k + ': ' + f'{v:.3f}' for k,v in logs.items()]) }\"\n",
    "#             )\n",
    "\n",
    "#         if save_metric_record_dict:\n",
    "#             self.metric_record_dict['roc_val'][desc].append(roc_val)\n",
    "#             self.metric_record_dict['eer_val'][desc].append(eer_val)\n",
    "#             self.metric_record_dict['thres'][desc].append(float(thres))\n",
    "#             self.metric_record_dict['acc_val'][desc].append(acc_val)\n",
    "#             self.metric_record_dict['f1_val'][desc].append(f1_val)\n",
    "        \n",
    "#         # Plot distances\n",
    "#         mask = np.where(y == 1, True, False)\n",
    "#         dist_positive = y_score[mask]\n",
    "#         dist_negative = y_score[~mask]\n",
    "        \n",
    "        \n",
    "#         if self.plot_pca:\n",
    "#             # Extract one of the child networks\n",
    "#             deep_feature_model = Model(\n",
    "#                 inputs=self.model.layers[0].get_input_at(0),  # get_layer(\"left_inputs\").input,\n",
    "#                 outputs=self.model.get_layer(\"basemodel\").get_output_at(0),\n",
    "#             )\n",
    "#             deep_feature_model.summary()\n",
    "#             deep_features_test = None\n",
    "#             for X, y, subj in samples_test:  \n",
    "#                 # if 2d, maybe need to import P\n",
    "#                 # if P.model_variant == \"2d\":\n",
    "#                 #     X = X.reshape((*X.shape, 1))\n",
    "#                 pred = deep_feature_model.predict(X)\n",
    "#                 df_features = pd.DataFrame(pred)\n",
    "#                 df_features[\"subject\"] = subj\n",
    "#                 deep_features_test = pd.concat([deep_features_test, df_features])\n",
    "\n",
    "#             plot_pca(deep_features_test)\n",
    "        \n",
    "#         #plt = utils_plot_distance_hist(\n",
    "#         #    dist_positive, dist_negative, thres * -1, desc=desc, fig_size=(12, 2), margin=P.margin\n",
    "#         #)\n",
    "\n",
    "# #         if self.save_plots:\n",
    "# #             utils_save_plot(\n",
    "# #                 plt,\n",
    "# #                 REPORT_PATH\n",
    "# #                 / f\"buech2019-siamese-{P.name.lower()}-epoch-{epoch+1}-{desc.lower()}.pdf\",\n",
    "# #             )\n",
    "\n",
    "#         # plt.show()\n",
    "    \n",
    "    \n",
    "#         #|, basemodel\n",
    "# # generate_index_pairs(50)\n",
    "# # anchor_indices = getIndices(sampleSize=10, step=5, numSamplePoints=136)\n",
    "# # len(pos_idices) * 136 * 20\n",
    "# # anchor_indices\n",
    "# # 17000/125\n",
    "# # generate_index_pairs_multi_input(range(2), range(10))\n",
    "\n",
    "#         # # a.append(df_array[windows[i], :]) #CNN\n",
    "#         # a.append(df.iloc[windows[i], :]) #waca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89eb7fe-f163-4c56-9e04-de953d558945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model(name, window_size, feature_cols, filters):\n",
    "#     print(f\"Using Model variant {name}...\")\n",
    "#     if name == \"1d\":\n",
    "#         model, basemodel = build_model_1d((window_size, len(feature_cols)), filters)\n",
    "#     elif name == \"2d\":\n",
    "#         model, basemodel = build_model_2d((window_size, len(feature_cols), 1), filters)\n",
    "#     elif name == \"fcn\":\n",
    "#         model, basemodel = build_model_fcn((window_size, len(feature_cols)), filters)\n",
    "#     else:\n",
    "#         raise BaseException(\"Error: Not a valid model name: {1d, 2d, fcn}\")\n",
    "\n",
    "#     return model, basemodel\n",
    "# temp_model, temp_basemodel = get_model(P.model_variant, P.window_size, P.feature_cols, P.filters)\n",
    "\n",
    "# temp_basemodel.summary()\n",
    "# temp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbad6af-a658-4eae-8e91-6f52c84e2b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "# user_id_array = list(range(10))\n",
    "# # Get the unique user_ids\n",
    "# unique_users = np.unique(user_id_array)\n",
    "\n",
    "\n",
    "# # Iterate through all possible pairs of users\n",
    "# # for user1, user2 in itertools.combinations(unique_users, 2):\n",
    "# #     # Select samples for the current pair of users\n",
    "# #     user1_samples = (user_id_array == user1)\n",
    "# #     user2_samples = (user_id_array == user2)\n",
    "    \n",
    "# #     print(user1, user2)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# import itertools\n",
    "\n",
    "# def generate_cv_split(valid_users_set, user_id_set, Raw_X_exp2_train_dic, Raw_X_exp1_train_dic, fitted_scaler_SNN_exp2_train_dic, num_pair_limit_train_2000, num_pair_limit_valid_2000):\n",
    "#     cv_dict = {}\n",
    "    \n",
    "#     cnn_valid_set = valid_users_set\n",
    "#     cnn_train_set = user_id_set - cnn_valid_set\n",
    "    \n",
    "#     cnn_train_exp2 = {key: Raw_X_exp2_train_dic[key] for key in cnn_train_set}\n",
    "#     cnn_train_exp1 = {key: Raw_X_exp1_train_dic[key] for key in cnn_train_set}\n",
    "#     cnn_valid_exp2 = {key: Raw_X_exp2_train_dic[key] for key in cnn_valid_set}\n",
    "#     cnn_valid_exp1 = {key: Raw_X_exp1_train_dic[key] for key in cnn_valid_set}\n",
    "    \n",
    "#     X_train, y_train = prep_X_y_pair(cnn_train_exp2, cnn_train_exp1, list(cnn_train_exp2.keys()), fitted_scaler_SNN_exp2_train_dic, num_pair_limit=num_pair_limit_train_2000)\n",
    "#     X_valid, y_valid = prep_X_y_pair(cnn_valid_exp2, cnn_valid_exp1, list(cnn_valid_exp2.keys()), fitted_scaler_SNN_exp2_train_dic, num_pair_limit=num_pair_limit_valid_2000)\n",
    "    \n",
    "#     key = list(valid_users_set)\n",
    "#     key.sort()\n",
    "#     key = tuple(key)\n",
    "#     cv_dict[key] = {}\n",
    "#     cv_dict[key][\"train\"] = X_train, y_train\n",
    "#     cv_dict[key][\"valid\"] = X_valid, y_valid\n",
    "    \n",
    "#     return cv_dict\n",
    "\n",
    "\n",
    "# num_pair_limit_train_2000 = 9182\n",
    "# num_pair_limit_valid_2000 = 5310\n",
    "# user_id_lst = list(Raw_X_exp2_train_dic.keys())\n",
    "# user_id_set = set(Raw_X_exp2_train_dic.keys())\n",
    "\n",
    "# valid_sets_lst = make_valid_sets_lst(user_id_lst, 5)\n",
    "\n",
    "# print(valid_sets_lst)\n",
    "# # user_id_array = {1,2,3}\n",
    "# results = Parallel(n_jobs=-1, verbose=100)(delayed(generate_cv_split)(valid_users_set, user_id_set, Raw_X_exp2_train_dic, Raw_X_exp1_train_dic, fitted_scaler_SNN_exp2_train_dic, \n",
    "#                                                    num_pair_limit_train_2000, num_pair_limit_valid_2000) for valid_users_set in valid_sets_lst)\n",
    "\n",
    "\n",
    "# results = {k: v for d in results for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021627ec-44d9-4692-a11b-18f394352b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_pairs_lst = []\n",
    "# for user1_idx in range(len(user_id_lst)-1):\n",
    "#     validation_pairs_lst.append((user_id_lst[user1_idx], user_id_lst[user1_idx+1]))\n",
    "#     print(validation_pairs_lst)\n",
    "    \n",
    "# for user1, user2 in validation_pairs_lst:\n",
    "#     print(user1, user2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d7d3c-6042-4606-9b28-a0d837c0398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(Raw_X_exp2_train_dic.keys())\n",
    "# user_id_lst = list(Raw_X_exp2_train_dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d451ec-d09d-4110-8737-dfdc92a156da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # valid_sets_lst = []\n",
    "# # for user1_idx in range(len(user_id_lst)-2):\n",
    "# #     valid_sets_lst.append((user_id_lst[user1_idx], user_id_lst[user1_idx+1], user_id_lst[user1_idx+2]))\n",
    "    \n",
    "# arr = [2, 3, 5, 7, 11, 13]\n",
    "# def make_valid_sets_lst(user_id_lst, k):\n",
    "#     valid_sets_lst=[]\n",
    "#     for i in range(0, len(user_id_lst)-k+1, k):\n",
    "#         valid_sets_lst.append(set(user_id_lst[i:i+k]))\n",
    "    \n",
    "#     return valid_sets_lst\n",
    "\n",
    "# make_valid_sets_lst(user_id_lst, 4)\n",
    "# valid_sets_lst = make_valid_sets_lst(user_id_lst, 5)\n",
    "# # key = list(valid_sets_lst[1])\n",
    "# # key.sort()\n",
    "# valid_sets_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0840217-e680-4c13-bbfa-1600b22b5883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_cv_split(valid_sets_lst[0], user_id_set, Raw_X_exp2_train_dic, Raw_X_exp1_train_dic, fitted_scaler_SNN_exp2_train_dic, num_pair_limit_train_2000, num_pair_limit_valid_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522663f-6bbd-4554-80f4-1e8665296948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_negative_index_pairs(diagonal_size):\n",
    "#     '''\n",
    "#     return an array of indices for negative pairs for user u1 and u2, where u1 != u2\n",
    "#     u1 is the genuine user and u2 is the imposter.\n",
    "#     '''\n",
    "#     indices = list(range(diagonal_size))\n",
    "#     mesh = np.array(np.meshgrid(indices, indices))\n",
    "#     negative_index_pairs = mesh.T.reshape(-1, 2)\n",
    "    \n",
    "#     return negative_index_pairs\n",
    "\n",
    "# def generate_positive_index_pairs(diagonal_size):\n",
    "#     '''\n",
    "#     return an array of tuple indices for the positive pairs.\n",
    "#     '''\n",
    "#     indices = list(range(diagonal_size))\n",
    "#     mesh = np.array(np.meshgrid(indices, indices))\n",
    "#     positive_index_pairs = mesh.T.reshape(-1, 2)\n",
    "#     positive_index_pairs = positive_index_pairs.reshape(diagonal_size, diagonal_size, 2)\n",
    "\n",
    "#     new_array_size = diagonal_size * diagonal_size - diagonal_size\n",
    "\n",
    "#     positive_index_pairs = deleteDiagonal(positive_index_pairs).reshape(new_array_size, 2)\n",
    "#     return positive_index_pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9176decc-6260-4c39-81d6-ab3dad95c92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lim = 2000\n",
    "# for total_samples_per_user in [27]:\n",
    "#     for num_users in [7, 10, 12, 15, 17, 20]:\n",
    "        \n",
    "        \n",
    "#         print(generate_negative_index_pairs(total_samples_per_user).shape)\n",
    "#         num_pos_per_owner = generate_positive_index_pairs(total_samples_per_user).shape[0]\n",
    "#         total_num_pairs = num_pos_per_owner * num_users * 2\n",
    "#         if lim < total_num_pairs: \n",
    "#             limit_ratio = lim/total_num_pairs\n",
    "#         else:\n",
    "#             limit_ratio=1\n",
    "#         num_pos_per_owner = np.round(limit_ratio * num_pos_per_owner)\n",
    "        \n",
    "#         ratio = ((total_samples_per_user-1) / (total_samples_per_user * (num_users-1)))*limit_ratio\n",
    "#         num_neg_per_owner = int(np.round(generate_negative_index_pairs(total_samples_per_user).shape[0]*ratio*(num_users-1)))\n",
    "#         print(num_pos_per_owner)\n",
    "#         print(num_neg_per_owner)\n",
    "#         print(num_pos_per_owner==num_neg_per_owner)\n",
    "#         print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfdc2e1-b4d3-43cb-ae10-6b725f67ed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results[(5, 6, 7, 8, 9, 10)]['train'][0][0].shape\n",
    "# results.keys()\n",
    "# len(cnn_valid_exp1[list(cnn_valid_exp1.keys())[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9c045f-af98-494a-b06b-0a8b8bcf04f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "# def generate_cv_split(user1, user2, user_id_array, Raw_X_exp2_train_dic, Raw_X_exp1_train_dic, fitted_scaler_SNN_exp2_train_dic, num_pair_limit_train_2000, num_pair_limit_valid_2000):\n",
    "#     cv_dict = {}\n",
    "    \n",
    "#     cnn_valid_set = {user1, user2}\n",
    "#     cnn_train_set = user_id_array - cnn_valid_set\n",
    "    \n",
    "#     cnn_train_exp2 = {key: Raw_X_exp2_train_dic[key] for key in cnn_train_set}\n",
    "#     cnn_train_exp1 = {key: Raw_X_exp1_train_dic[key] for key in cnn_train_set}\n",
    "#     cnn_valid_exp2 = {key: Raw_X_exp2_train_dic[key] for key in cnn_valid_set}\n",
    "#     cnn_valid_exp1 = {key: Raw_X_exp1_train_dic[key] for key in cnn_valid_set}\n",
    "    \n",
    "#     X_train, y_train = prep_X_y_pair(cnn_train_exp2, cnn_train_exp1, list(cnn_train_exp2.keys()), fitted_scaler_SNN_exp2_train_dic, num_pair_limit=num_pair_limit_train_2000)\n",
    "#     X_valid, y_valid = prep_X_y_pair(cnn_valid_exp2, cnn_valid_exp1, list(cnn_valid_exp2.keys()), fitted_scaler_SNN_exp2_train_dic, num_pair_limit=num_pair_limit_valid_2000)\n",
    "    \n",
    "#     cv_dict[(user1, user2)] = {}\n",
    "#     cv_dict[(user1, user2)][\"train\"] = X_train, y_train\n",
    "#     cv_dict[(user1, user2)][\"valid\"] = X_valid, y_valid\n",
    "    \n",
    "#     return cv_dict\n",
    "\n",
    "\n",
    "# num_pair_limit_train_2000 = 9182\n",
    "# num_pair_limit_valid_2000 = 5310\n",
    "# user_id_array = set(Raw_X_exp2_train_dic.keys())\n",
    "# # user_id_array = {1,2,3,4,5}\n",
    "# results = Parallel(n_jobs=-1, verbose=100)(delayed(generate_cv_split)(user1, user2, user_id_array, Raw_X_exp2_train_dic, Raw_X_exp1_train_dic, fitted_scaler_SNN_exp2_train_dic, \n",
    "#                                                    num_pair_limit_train_2000, num_pair_limit_valid_2000) for user1, user2 in itertools.combinations(user_id_array, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15702bc3-357d-4abc-9519-c25abb7d996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "# user_id_array = set(Raw_X_exp2_train_dic.keys())\n",
    "\n",
    "# num_pair_limit_train_2000 = 9182\n",
    "# num_pair_limit_valid_2000 = 5310\n",
    "\n",
    "# cv_dict = {}\n",
    "# # Iterate through all possible pairs of users\n",
    "# for user1, user2 in tqdm(itertools.combinations(user_id_array, 2)):\n",
    "#     cnn_valid_set = {user1, user2}\n",
    "#     cnn_train_set = user_id_array - cnn_valid_set\n",
    "    \n",
    "#     cnn_train_exp2 = {key: Raw_X_exp2_train_dic[key] for key in cnn_train_set}\n",
    "#     cnn_train_exp1 = {key: Raw_X_exp1_train_dic[key] for key in cnn_train_set}\n",
    "#     cnn_valid_exp2 = {key: Raw_X_exp2_train_dic[key] for key in cnn_valid_set}\n",
    "#     cnn_valid_exp1 = {key: Raw_X_exp1_train_dic[key] for key in cnn_valid_set}\n",
    "    \n",
    "#     X_train, y_train = prep_X_y_pair(cnn_train_exp2, cnn_train_exp1, list(cnn_train_exp2.keys()), fitted_scaler_SNN_exp2_train_dic, num_pair_limit=num_pair_limit_train_2000)\n",
    "#     X_valid, y_valid = prep_X_y_pair(cnn_valid_exp2, cnn_valid_exp1, list(cnn_valid_exp2.keys()), fitted_scaler_SNN_exp2_train_dic, num_pair_limit=num_pair_limit_valid_2000)\n",
    "    \n",
    "#     cv_dict[(user1, user2)] = {}\n",
    "#     cv_dict[(user1, user2)][\"train\"] = X_train, y_train\n",
    "#     cv_dict[(user1, user2)][\"valid\"] = X_valid, y_valid\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db74496-7ec1-4d4c-820e-db7055081e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def deleteDiagonal(array):\n",
    "#     depth = array.shape[-1]\n",
    "#     m = array.shape[1]\n",
    "#     strided = np.lib.stride_tricks.as_strided\n",
    "#     s0,s1,s2 = array.strides\n",
    "#     return strided(array.ravel()[depth:], shape=(m-1, m, depth), strides=(s0+s1,s1, s2)).reshape(m, m-1, depth)\n",
    "# print(mesh.shape)\n",
    "# deleteDiagonal(mesh).reshape(12,2)\n",
    "\n",
    "# def build_model_fcn(input_shape, filters):\n",
    "#     # Define the tensors for the two input images\n",
    "#     left_inputs = Input(input_shape, name=\"left_inputs\")\n",
    "#     right_inputs = Input(input_shape, name=\"right_inputs\")\n",
    "\n",
    "#     # Convolutional Neural Network\n",
    "#     inputs = Input(input_shape, name=\"input\")\n",
    "#     x = Conv1D(filters=filters[0], kernel_size=8, strides=1, activation=None, padding=\"same\", name=\"conv1\")(inputs)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Activation(\"relu\")(x)\n",
    "#     x = Dropout(0.1, name=\"drop1\")(x)\n",
    "#     x = Conv1D(filters=filters[1], kernel_size=5, strides=1, activation=None, padding=\"same\", name=\"conv2\")(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Activation(\"relu\")(x)\n",
    "#     x = Dropout(0.1, name=\"drop2\")(x)\n",
    "#     x = Conv1D(filters=filters[2], kernel_size=3, strides=1, activation=None, padding=\"same\", name=\"conv3\")(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Activation(\"relu\")(x)\n",
    "#     x = GlobalAveragePooling1D()(x)\n",
    "#     # x = Dense(32, activation=\"sigmoid\", name=\"dense\")(x) # <--- !!!!!!!!!!!!\n",
    "# #     x = Dense(64, activation=\"sigmoid\", name=\"dense\")(x) # <--- !!!!!!!!!!!!\n",
    "#     x = Dense(78, activation=\"sigmoid\", name=\"dense\")(x)\n",
    "\n",
    "#     # Basemodel instance\n",
    "#     basemodel = Model(inputs, x, name=\"basemodel\")\n",
    "\n",
    "#     # using same instance of \"basemodel\" to share weights between left/right networks\n",
    "#     encoded_l = basemodel(left_inputs)\n",
    "#     encoded_r = basemodel(right_inputs)\n",
    "\n",
    "#     # Add a customized layer to compute the distance between the encodings\n",
    "#     distance_layer = Lambda(k_euclidean_dist, name=\"distance\")([encoded_l, encoded_r])\n",
    "\n",
    "#     # Combine into one net\n",
    "#     siamese_net = Model(inputs=[left_inputs, right_inputs], outputs=distance_layer)\n",
    "\n",
    "#     # return the model\n",
    "#     return siamese_net, basemodel\n",
    "\n",
    "# def create_model_fcn(input_shape, filters, kernels, kernel_initializers, strides, dropouts, activations, dense_kernels, dense_kernel_initializers, loss_func, optimizer):\n",
    "#     '''\n",
    "#     it is assumed that len(dropouts)=len(filters)-1\n",
    "#     '''\n",
    "#     # Define the tensors for the two input images\n",
    "#     left_inputs = Input(input_shape, name=\"left_inputs\")\n",
    "#     right_inputs = Input(input_shape, name=\"right_inputs\")\n",
    "\n",
    "#     # Convolutional Neural Network\n",
    "#     inputs = Input(input_shape, name=\"input\")\n",
    "    \n",
    "    \n",
    "# #     ------\n",
    "#     x=inputs\n",
    "#     for idx in range(len(dropouts)):\n",
    "#         x = Conv1D(filters=filters[idx], kernel_size=kernels[idx], kernel_initializer=kernel_initializers[idx], kernel_constraint=MaxNorm(3), strides=strides[idx], \n",
    "#                    activation=None, padding=\"same\", name=f\"conv{idx+1}\")(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Activation(activations[idx])(x)\n",
    "#         x = Dropout(dropouts[idx], name=f\"drop{idx+1}\")(x)\n",
    "    \n",
    "#     x = Conv1D(filters=filters[idx+1], kernel_size=kernels[idx+1], kernel_initializer=kernel_initializers[idx+1], kernel_constraint=MaxNorm(3), strides=strides[idx+1], \n",
    "#                activation=None, padding=\"same\", name=f\"conv{idx+2}\")(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Activation(activations[idx+1])(x)\n",
    "#     x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "\n",
    "#     x = Dense(dense, kernel_constraint=MaxNorm(3), activation=\"relu\", name=\"dense1\")(x)\n",
    "#     x = Dense(dense, kernel_constraint=MaxNorm(3), activation=\"sigmoid\", name=\"dense\")(x)\n",
    "\n",
    "#     # Basemodel instance\n",
    "#     basemodel = Model(inputs, x, name=\"basemodel\")\n",
    "\n",
    "#     # using same instance of \"basemodel\" to share weights between left/right networks\n",
    "#     encoded_l = basemodel(left_inputs)\n",
    "#     encoded_r = basemodel(right_inputs)\n",
    "\n",
    "#     # Add a customized layer to compute the distance between the encodings\n",
    "#     distance_layer = Lambda(k_euclidean_dist, name=\"distance\")([encoded_l, encoded_r])\n",
    "\n",
    "#     # Combine into one net\n",
    "#     siamese_net = Model(inputs=[left_inputs, right_inputs], outputs=distance_layer)\n",
    "    \n",
    "#     siamese_net.compile(loss=loss_func, optimizer=optimizer)\n",
    "    \n",
    "#     # return the model\n",
    "#     return siamese_net#|, basemodel\n",
    "\n",
    "# # # cite: Siamese Model with 2D Filters, as derived from Centeno et al. (2018)\n",
    "# # # the thesis I found\n",
    "# # def build_model_2d(input_shape, filters):\n",
    "# #     \"\"\"\n",
    "# #         Siamese CNN architecture with 3D input and 2D filters\n",
    "# #     \"\"\"\n",
    "# #     # Define the tensors for the two input images\n",
    "# #     left_inputs = Input(input_shape, name=\"left_inputs\")\n",
    "# #     right_inputs = Input(input_shape, name=\"right_inputs\")\n",
    "\n",
    "# #     # Convolutional Neural Network\n",
    "# #     inputs = Input(input_shape, name=\"input\")\n",
    "# #     x = Conv2D(filters[0], (7, 7), padding=\"same\", activation=\"tanh\", name=\"conv1\")(inputs)\n",
    "# #     x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp1\")(x)\n",
    "# #     x = Conv2D(filters[1], (5, 5), padding=\"same\", activation=\"tanh\", name=\"conv2\")(x)\n",
    "# #     x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp2\")(x)\n",
    "# #     x = Conv2D(filters[2], (3, 3), padding=\"same\", activation=\"tanh\", name=\"conv3\")(x)\n",
    "# #     x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp3\")(x)\n",
    "# #     x = Conv2D(filters[3], (3, 3), padding=\"same\", activation=\"tanh\", name=\"conv4\")(x)\n",
    "# #     x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp4\")(x)\n",
    "# #     x = Flatten(name=\"flat\")(x)\n",
    "    \n",
    "# #     # Basemodel instance\n",
    "# #     basemodel = Model(inputs, x, name=\"basemodel\")\n",
    "\n",
    "# #     # using same instance of \"basemodel\" to share weights between left/right networks\n",
    "# #     encoded_l = basemodel(left_inputs)\n",
    "# #     encoded_r = basemodel(right_inputs)\n",
    "\n",
    "# #     # Add a customized layer to compute the distance between the encodings\n",
    "# #     distance_layer = Lambda(k_euclidean_dist, name=\"distance\")([encoded_l, encoded_r])\n",
    "\n",
    "# #     # Combine into one net\n",
    "# #     siamese_net = Model(inputs=[left_inputs, right_inputs], outputs=distance_layer)\n",
    "\n",
    "# #     # return the model\n",
    "# #     return siamese_net, basemodel\n",
    "\n",
    "# # cite: Siamese Model with 2D Filters, as derived from Centeno et al. (2018)\n",
    "# # the thesis I found\n",
    "# def create_model_2d(input_shape, filters):\n",
    "#     \"\"\"\n",
    "#         Siamese CNN architecture with 3D input and 2D filters\n",
    "#     \"\"\"\n",
    "#     # Define the tensors for the two input images\n",
    "#     left_inputs = Input(input_shape, name=\"left_inputs\")\n",
    "#     right_inputs = Input(input_shape, name=\"right_inputs\")\n",
    "\n",
    "#     # Convolutional Neural Network\n",
    "#     inputs = Input(input_shape, name=\"input\")\n",
    "#     x = Conv2D(filters[0], (7, 7), padding=\"same\", activation=\"tanh\", name=\"conv1\")(inputs)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp1\")(x)\n",
    "#     x = Conv2D(filters[1], (5, 5), padding=\"same\", activation=\"tanh\", name=\"conv2\")(x)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp2\")(x)\n",
    "#     x = Conv2D(filters[2], (3, 3), padding=\"same\", activation=\"tanh\", name=\"conv3\")(x)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp3\")(x)\n",
    "#     x = Conv2D(filters[3], (3, 3), padding=\"same\", activation=\"tanh\", name=\"conv4\")(x)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp4\")(x)\n",
    "#     x = Flatten(name=\"flat\")(x)\n",
    "    \n",
    "#     # Basemodel instance\n",
    "#     basemodel = Model(inputs, x, name=\"basemodel\")\n",
    "\n",
    "#     # using same instance of \"basemodel\" to share weights between left/right networks\n",
    "#     encoded_l = basemodel(left_inputs)\n",
    "#     encoded_r = basemodel(right_inputs)\n",
    "\n",
    "#     # Add a customized layer to compute the distance between the encodings\n",
    "#     distance_layer = Lambda(k_euclidean_dist, name=\"distance\")([encoded_l, encoded_r])\n",
    "\n",
    "#     # Combine into one net\n",
    "#     siamese_net = Model(inputs=[left_inputs, right_inputs], outputs=distance_layer)\n",
    "\n",
    "#     # return the model\n",
    "#     return siamese_net, basemodel\n",
    "\n",
    "# # def build_model_1d(input_shape, filters):\n",
    "# #     \"\"\"\n",
    "# #         Model architecture\n",
    "# #     \"\"\"\n",
    "# #     # Define the tensors for the two input images\n",
    "# #     left_inputs = Input(input_shape, name=\"left_inputs\")\n",
    "# #     right_inputs = Input(input_shape, name=\"right_inputs\")\n",
    "\n",
    "# #     # Convolutional Neural Network\n",
    "# #     inputs = Input(input_shape, name=\"input\")\n",
    "# #     x = Conv1D(filters[0], 7, activation=\"elu\", padding=\"same\", name=\"conv1\")(inputs)\n",
    "# #     x = MaxPooling1D(pool_size=2, name=\"mp1\")(x)\n",
    "# #     x = Conv1D(filters[1], 5, activation=\"elu\", padding=\"same\", name=\"conv2\")(x)\n",
    "# #     x = MaxPooling1D(pool_size=2, name=\"mp2\")(x)\n",
    "# #     x = Conv1D(filters[2], 3, activation=\"elu\", padding=\"same\", name=\"conv3\")(x)\n",
    "# #     x = MaxPooling1D(pool_size=2, name=\"mp3\")(x)\n",
    "# #     x = Conv1D(filters[3], 3, activation=\"elu\", padding=\"same\", name=\"conv4\")(x)\n",
    "# #     x = MaxPooling1D(pool_size=2, name=\"mp5\")(x)\n",
    "# #     x = Flatten(name=\"flat\")(x)\n",
    "\n",
    "# #     # Generate the encodings (feature vectors) for the two images\n",
    "# #     basemodel = Model(inputs, x, name=\"basemodel\")\n",
    "\n",
    "# #     # using same instance of \"basemodel\" to share weights between left/right networks\n",
    "# #     encoded_l = basemodel(left_inputs)\n",
    "# #     encoded_r = basemodel(right_inputs)\n",
    "\n",
    "# #     # Add a customized layer to compute the absolute difference between the encodings\n",
    "# #     distance_layer = Lambda(k_euclidean_dist, name=\"distance\")([encoded_l, encoded_r])\n",
    "\n",
    "# #     siamese_net = Model(inputs=[left_inputs, right_inputs], outputs=distance_layer)\n",
    "\n",
    "# #     # return the model\n",
    "# #     return siamese_net, basemodel\n",
    "\n",
    "\n",
    "# def create_model_1d(input_shape, filters):\n",
    "#     \"\"\"\n",
    "#         Model architecture\n",
    "#     \"\"\"\n",
    "#     # Define the tensors for the two input images\n",
    "#     left_inputs = Input(input_shape, name=\"left_inputs\")\n",
    "#     right_inputs = Input(input_shape, name=\"right_inputs\")\n",
    "\n",
    "#     # Convolutional Neural Network\n",
    "#     inputs = Input(input_shape, name=\"input\")\n",
    "#     x = Conv1D(filters[0], 7, activation=\"elu\", padding=\"same\", name=\"conv1\")(inputs)\n",
    "#     x = MaxPooling1D(pool_size=2, name=\"mp1\")(x)\n",
    "#     x = Conv1D(filters[1], 5, activation=\"elu\", padding=\"same\", name=\"conv2\")(x)\n",
    "#     x = MaxPooling1D(pool_size=2, name=\"mp2\")(x)\n",
    "#     x = Conv1D(filters[2], 3, activation=\"elu\", padding=\"same\", name=\"conv3\")(x)\n",
    "#     x = MaxPooling1D(pool_size=2, name=\"mp3\")(x)\n",
    "#     x = Conv1D(filters[3], 3, activation=\"elu\", padding=\"same\", name=\"conv4\")(x)\n",
    "#     x = MaxPooling1D(pool_size=2, name=\"mp5\")(x)\n",
    "#     x = Flatten(name=\"flat\")(x)\n",
    "\n",
    "#     # Generate the encodings (feature vectors) for the two images\n",
    "#     basemodel = Model(inputs, x, name=\"basemodel\")\n",
    "\n",
    "#     # using same instance of \"basemodel\" to share weights between left/right networks\n",
    "#     encoded_l = basemodel(left_inputs)\n",
    "#     encoded_r = basemodel(right_inputs)\n",
    "\n",
    "#     # Add a customized layer to compute the absolute difference between the encodings\n",
    "#     distance_layer = Lambda(k_euclidean_dist, name=\"distance\")([encoded_l, encoded_r])\n",
    "\n",
    "#     siamese_net = Model(inputs=[left_inputs, right_inputs], outputs=distance_layer)\n",
    "\n",
    "#     # return the model\n",
    "#     return siamese_net, basemodel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a0582-d56c-491e-ba82-64ddc20855eb",
   "metadata": {},
   "source": [
    "Code from earlier version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b3ac82-db0e-406d-b2b2-593455eb1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteDiagonal(array):\n",
    "    depth = array.shape[-1]\n",
    "    m = array.shape[1]\n",
    "    strided = np.lib.stride_tricks.as_strided\n",
    "    s0,s1,s2 = array.strides\n",
    "    return strided(array.ravel()[depth:], shape=(m-1, m, depth), strides=(s0+s1,s1, s2)).reshape(m, m-1, depth)\n",
    "\n",
    "\n",
    "def MinMaxTransformation(windows_features_array):\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(windows_features_array)\n",
    "#     scaled_array = scaler.transform(windows_features_array)\n",
    "    \n",
    "    return scaler\n",
    "    \n",
    "    \n",
    "def getDistFRR1(dfList, window_size = 1000, step = 1000, numSamplePoints= 18000):\n",
    "    \n",
    "    windows = getIndices(sampleSize=window_size, step=step, numSamplePoints= numSamplePoints)\n",
    "\n",
    "    norm_dist = []\n",
    "    norm_distro_dict = {}\n",
    "    counter = 1\n",
    "    for m in range(len(dfList)):\n",
    "        \n",
    "        \n",
    "        encoding_array = getEncodingArray(dfList[m], windows)\n",
    "#         print(dfList[m].columns)\n",
    "#         print(encoding_array.shape)\n",
    "\n",
    "#         print(np.sum(encoding_array, axis = 1))\n",
    "\n",
    "        # Doesn't make sense to normalize this here\n",
    "#         scaler = MinMaxTransformation(encoding_array)\n",
    "#         encoding_array = scaler.transform(encoding_array)\n",
    "#         print(encoding_array.shape)\n",
    "        # Should this be put between braces before indexing??? No\n",
    "        encoding_array = encoding_array / np.linalg.norm(encoding_array, axis = 1)[:, None]\n",
    "#         print(np.linalg.norm(encoding_array, axis=1)[:, None].shape)\n",
    "#         print(encoding_array.shape)\n",
    "#         print(np.linalg.norm(encoding_array, axis=1))\n",
    "        # Is dist_array distance calculation done correctly??? It seems it does not square\n",
    "        # It appears so dist = numpy.linalg.norm(a-b)\n",
    "        # https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy\n",
    "        dist_array = (encoding_array[None, :] - encoding_array[:, None])\n",
    "\n",
    "        dist_array = deleteDiagonal(dist_array)\n",
    "\n",
    "        dist_array = np.linalg.norm(dist_array, axis = 2)\n",
    "        \n",
    "        norm_dist.append(dist_array)\n",
    "        \n",
    "        # for err dist\n",
    "        norm_distro_dict[m] = np.array(dist_array).ravel()\n",
    "        \n",
    "        counter += 1\n",
    "    \n",
    "    return {\"dist_array\": np.array(norm_dist).ravel(), \"dist_dict\": norm_distro_dict}\n",
    "\n",
    "\n",
    "def getDistFRRFinal(dfList_exp1, dfList_exp2, window_size = 1000, step = 1000, numSamplePoints= 18000):\n",
    "    '''\n",
    "    dfLists are of the same size.\n",
    "    '''\n",
    "    if len(dfList_exp1) != len(dfList_exp2): \n",
    "        raise Exception(\"dfLists are not of the same size.\")\n",
    "        \n",
    "    windows = getIndices(sampleSize=window_size, step=step, numSamplePoints= numSamplePoints)\n",
    "\n",
    "    norm_dist = []\n",
    "    norm_distro_dict = {}\n",
    "    voting_dist_dict = {}\n",
    "    counter = 1\n",
    "    for m in range(len(dfList_exp1)):\n",
    "        \n",
    "        \n",
    "        encoding_array_exp1 = getEncodingArray(dfList_exp1[m], windows)\n",
    "        encoding_array_exp2 = getEncodingArray(dfList_exp2[m], windows)\n",
    "        \n",
    "        # Doesn't make sense to normalize this here\n",
    "        scaler = MinMaxTransformation(encoding_array_exp1)\n",
    "        encoding_array_exp1 = scaler.transform(encoding_array_exp1)\n",
    "#         encoding_array_exp2 = scaler.transform(encoding_array_exp2)\n",
    "        \n",
    "#         # approach 2: Not intuitive as you dont have access to all of user2s stream of 4 min, only windowsizes at a time\n",
    "#         scaler = MinMaxTransformation(encoding_array_exp2)\n",
    "\n",
    "        encoding_array_exp2 = scaler.transform(encoding_array_exp2)\n",
    "        \n",
    "#         print(dfList[m].columns)\n",
    "#         print(encoding_array.shape)\n",
    "\n",
    "#         print(np.sum(encoding_array, axis = 1))\n",
    "#         print((encoding_array_exp1 / np.linalg.norm(encoding_array_exp1, axis = 1)[:, None]).shape)\n",
    "#         print(encoding_array_exp1.shape)\n",
    "#         print(encoding_array.shape)\n",
    "        # Should this be put between braces before indexing??? No\n",
    "        encoding_array_exp1 = encoding_array_exp1 / np.linalg.norm(encoding_array_exp1, axis = 1)[:, None]\n",
    "        encoding_array_exp2 = encoding_array_exp2 / np.linalg.norm(encoding_array_exp2, axis = 1)[:, None]\n",
    "#         print(np.linalg.norm(encoding_array, axis=1)[:, None].shape)\n",
    "#         print(encoding_array.shape)\n",
    "#         print(np.linalg.norm(encoding_array, axis=1))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # Is dist_array distance calculation done correctly??? It seems it does not square\n",
    "        # It appears so dist = numpy.linalg.norm(a-b)\n",
    "        # https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy\n",
    "        dist_array = (encoding_array_exp1[None, :] - encoding_array_exp2[:, None])\n",
    "#         print(dist_array.shape)\n",
    "        \n",
    "        dist_array = np.linalg.norm(dist_array, axis = 2)\n",
    "        \n",
    "        norm_dist.append(dist_array)\n",
    "        \n",
    "#         print(dist_array.shape)\n",
    "        # for err dist\n",
    "        norm_distro_dict[m] = np.array(dist_array).ravel()\n",
    "        \n",
    "        # for voting dist\n",
    "        voting_dist_dict[m] = dist_array[None, :]\n",
    "        \n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "#         print(voting_dist_dict[0].shape)\n",
    "    \n",
    "    return {\"dist_array\": np.array(norm_dist).ravel(), \"dist_dict\": norm_distro_dict, \"voting_dist_dict\": voting_dist_dict}\n",
    "\n",
    "\n",
    "def getDistFARFinal(dfList, window_size = 1000, step = 1000, numSamplePoints= 18000):\n",
    "    \n",
    "    windows = getIndices(sampleSize=window_size, step=step, numSamplePoints= numSamplePoints)\n",
    "\n",
    "    norm_dist = []\n",
    "    norm_distro_dict = {}\n",
    "    voting_dist_dict = {}\n",
    "    counter = 1\n",
    "    \n",
    "    encoding_array_dic = {}\n",
    "    for i in range(len(dfList)):\n",
    "        encoding_array_dic[i] = getEncodingArray(dfList[i], windows)\n",
    "        # Should this be put between braces before indexing???\n",
    "#         # Should this be done here? Not here this only makes the result vector small and should be at the end\n",
    "#         encoding_array_dic[i] = encoding_array_dic[i] / np.linalg.norm(encoding_array_dic[i], axis = 1)[:, None]\n",
    "        \n",
    "    for m in range(len(dfList)):\n",
    "        \n",
    "        cum_distro_array = []\n",
    "        encoding_array_m = encoding_array_dic[m]\n",
    "        \n",
    "        # Should I scale the new vector with the transform of the user profile?\n",
    "        scaler = MinMaxTransformation(encoding_array_m)\n",
    "        encoding_array_m = scaler.transform(encoding_array_m)\n",
    "        \n",
    "        encoding_array_m = encoding_array_m / np.linalg.norm(encoding_array_m, axis = 1)[:, None]\n",
    "        \n",
    "        for k in range(len(dfList)):\n",
    "            \n",
    "            if m != k:\n",
    "                \n",
    "                encoding_array_k = encoding_array_dic[k]\n",
    "                \n",
    "#                 # approach 2\n",
    "#                 scaler = MinMaxTransformation(encoding_array_k)\n",
    "                \n",
    "                # Scale array_k with array_m transform\n",
    "                encoding_array_k = scaler.transform(encoding_array_k)\n",
    "                \n",
    "                encoding_array_k = encoding_array_k / np.linalg.norm(encoding_array_k, axis = 1)[:, None]\n",
    "                \n",
    "                # Is dist_array distance calculation done correctly??? It seems it does not square. No, resolved\n",
    "                # It appears so dist = numpy.linalg.norm(a-b)\n",
    "                # https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy\n",
    "                dist_array = (encoding_array_m[None, :] - encoding_array_k[:, None])\n",
    "                \n",
    "                # print and check dimensions\n",
    "                \n",
    "                dist_array = np.linalg.norm(dist_array, axis = 2)\n",
    "\n",
    "                norm_dist.append(dist_array)\n",
    "                \n",
    "                # for err dist\n",
    "                cum_distro_array.append(dist_array)\n",
    "                \n",
    "\n",
    "        \n",
    "        norm_distro_dict[m] = np.array(cum_distro_array).ravel()\n",
    "        \n",
    "        # for voting dist\n",
    "        voting_dist_dict[m] = np.array(cum_distro_array)\n",
    "\n",
    "        counter += 1\n",
    "        \n",
    "#         print(voting_dist_dict[0].shape)\n",
    "    \n",
    "    return {\"dist_array\": np.array(norm_dist).ravel(), \"dist_dict\": norm_distro_dict, \"voting_dist_dict\": voting_dist_dict}\n",
    "\n",
    "\n",
    "def decision_confidence(dist_array, dist_threshold):\n",
    "    '''\n",
    "    input: dist_array: (N, # unknown_user windows, # genuine_user windows)\n",
    "    output: (N, de/auth decision percentage)\n",
    "    '''\n",
    "    vals = np.where(dist_array < dist_threshold, 1, 0)\n",
    "    windows_decision_confidence = np.mean(vals, axis = 2)\n",
    "\n",
    "    return windows_decision_confidence\n",
    "\n",
    "\n",
    "def decision_module(dist_array, dist_threshold, acceptance_threshold):\n",
    "    '''\n",
    "    input: dist_array: (N, # unknown_user windows, # genuine_user windows)\n",
    "    output: (N, de/auth boolean decision)\n",
    "    '''\n",
    "    \n",
    "    windows_decision_confidence = decision_confidence(dist_array, dist_threshold = dist_threshold)\n",
    "    windows_final_decision = np.where(windows_decision_confidence >= acceptance_threshold, 1, 0)\n",
    "    \n",
    "    return windows_final_decision\n",
    "    \n",
    "def FRR_vote_based(dist_array, dist_threshold = None, acceptance_threshold = None):\n",
    "    '''\n",
    "    input: dist_array: (N, # unknown_user windows, # genuine_user windows)\n",
    "    output: vote based FRR\n",
    "    '''\n",
    "    \n",
    "    #good note but not applicable here np.where((a==0)|(a==1), a^1, a)\n",
    "\n",
    "    windows_final_decision = decision_module(dist_array, dist_threshold, acceptance_threshold)\n",
    "    vals = windows_final_decision^1\n",
    "    \n",
    "    return np.mean(vals)\n",
    "\n",
    "\n",
    "def FAR_vote_based(dist_array, dist_threshold = None, acceptance_threshold = None):\n",
    "    '''\n",
    "    input: dist_array: (M*N, # unknown_user windows, # genuine_user windows)\n",
    "    output: vote based FAR\n",
    "    '''\n",
    "    windows_final_decision = decision_module(dist_array, dist_threshold, acceptance_threshold)\n",
    "    vals = windows_final_decision\n",
    "    \n",
    "    return np.mean(vals)\n",
    "\n",
    "\n",
    "def FRR(dist, threshold):\n",
    "    \n",
    "    vals = np.where(dist < threshold, 0, 1)\n",
    "    \n",
    "    return np.mean(vals)\n",
    "\n",
    "\n",
    "def FAR(dist, threshold):\n",
    "    \n",
    "    vals = np.where(dist < threshold, 1, 0)\n",
    "    \n",
    "    return np.mean(vals)\n",
    "\n",
    "\n",
    "def DistroFRR(dist_dict, threshold):\n",
    "    \n",
    "    distro = []\n",
    "    for i in range(len(dist_dict)):\n",
    "        vals = np.where(dist_dict[i] < threshold, 0, 1)\n",
    "        distro.append(sum(vals))\n",
    "        \n",
    "    return distro\n",
    "\n",
    "\n",
    "def DistroFAR(dist_dict, threshold):\n",
    "    \n",
    "    distro = []\n",
    "    for i in range(len(dist_dict)):\n",
    "        vals = np.where(dist_dict[i] < threshold, 1, 0)\n",
    "        distro.append(sum(vals))\n",
    "        \n",
    "    return distro\n",
    "\n",
    "\n",
    "def getEER(distFRR, distFAR, thresholdList=None):\n",
    "    \n",
    "    if thresholdList is None:\n",
    "        thresholdList = np.arange(0, 3, 0.001)\n",
    "    \n",
    "    farList = []\n",
    "    frrList = []\n",
    "    \n",
    "    eer = []\n",
    "    for t in thresholdList:\n",
    "        far = FAR(distFAR, threshold = t)\n",
    "        frr = FRR(distFRR, threshold = t)\n",
    "        farList.append(far)\n",
    "        frrList.append(frr)\n",
    "        eer.append(abs(far-frr))\n",
    "        \n",
    "    eer = np.array(eer)\n",
    "    eer[eer==0] = 99999\n",
    "    print(farList[np.argmin(eer)])\n",
    "    print(frrList[np.argmin(eer)])\n",
    "\n",
    "#     print(\"EER: {}\".format((frrList[np.argmin(eer)] + farList[np.argmin(eer)])/2))\n",
    "    return {\"EER\": (frrList[np.argmin(eer)] + farList[np.argmin(eer)])/2, \"farList\": farList, \"frrList\": frrList, \"EER_threshold\": thresholdList[np.argmin(eer)]}\n",
    "\n",
    "\n",
    "def getEERVoteBased(dist_array_FRR, dist_array_FAR, thresholdList=None):\n",
    "    \n",
    "    if thresholdList is None:\n",
    "        thresholdList = np.arange(0, 3, 0.001)\n",
    "    \n",
    "    farList = []\n",
    "    frrList = []\n",
    "    \n",
    "    eer = []\n",
    "    acceptance_threshold = .6\n",
    "    \n",
    "    for t in thresholdList:\n",
    "        far = FAR_vote_based(dist_array_FAR, dist_threshold = t, acceptance_threshold = acceptance_threshold)\n",
    "        frr = FRR_vote_based(dist_array_FRR, dist_threshold = t, acceptance_threshold = acceptance_threshold)\n",
    "        farList.append(far)\n",
    "        frrList.append(frr)\n",
    "        eer.append(abs(far-frr))\n",
    "        \n",
    "    eer = np.array(eer)\n",
    "    eer[eer==0] = 99999\n",
    "    print(farList[np.argmin(eer)])\n",
    "    print(frrList[np.argmin(eer)])\n",
    "\n",
    "#     print(\"EER: {}\".format((frrList[np.argmin(eer)] + farList[np.argmin(eer)])/2))\n",
    "    return {\"EER\": (frrList[np.argmin(eer)] + farList[np.argmin(eer)])/2, \"farList\": farList, \"frrList\": frrList, \"EER_threshold\": thresholdList[np.argmin(eer)]}\n",
    "\n",
    "\n",
    "def getEERWindowsDict(dfList_exp1, start_window_size=250, end_window_size=3000, increment_step=250, numSamplePoints=22001, isEqualSampleSize = False, fixedSampleStep=3000, thresholdList=None, dfList_exp2=None):\n",
    "    \n",
    "    window_EER_dict = {}\n",
    "    window_EER_threshold_dict = {}\n",
    "    window_farList_dict = {}\n",
    "    window_frrList_dict = {}\n",
    "    window_farDistro_dict = {}\n",
    "    window_frrDistro_dict = {}\n",
    "    \n",
    "    lst = np.arange(start_window_size, end_window_size + 1, increment_step)\n",
    "    \n",
    "    for w in lst:\n",
    "        if isEqualSampleSize:\n",
    "            sampleStep = fixedSampleStep\n",
    "        else:\n",
    "            sampleStep = w\n",
    "        \n",
    "        if dfList_exp2 is None:\n",
    "            print(\"dfList_exp2 is None\")\n",
    "            distFRRDATA = getDistFRR1(dfList_exp1, window_size = w, step = sampleStep, numSamplePoints= numSamplePoints)\n",
    "        else:\n",
    "            print(\"dfList_exp2 is Not None\")\n",
    "            distFRRDATA = getDistFRRFinal(dfList_exp1, dfList_exp2, window_size = w, step = sampleStep, numSamplePoints= numSamplePoints)\n",
    "            \n",
    "        distFARDATA = getDistFARFinal(dfList_exp1, window_size = w, step = sampleStep, numSamplePoints= numSamplePoints)\n",
    "        \n",
    "#         print('--- start of voting based')\n",
    "#         print(distFRRDATA[\"voting_dist_dict\"][0].shape)\n",
    "#         print(distFARDATA[\"voting_dist_dict\"][0].shape)\n",
    "#         print(np.concatenate(list(distFRRDATA[\"voting_dist_dict\"].values())).shape)\n",
    "#         print(np.concatenate(list(distFARDATA[\"voting_dist_dict\"].values())).shape)\n",
    "        \n",
    "        \n",
    "#         voting_dist_FRR = np.concatenate(list(distFRRDATA[\"voting_dist_dict\"].values()))\n",
    "#         voting_dist_FAR = np.concatenate(list(distFARDATA[\"voting_dist_dict\"].values()))\n",
    "#         voting_EER_data = getEERVoteBased(voting_dist_FRR, voting_dist_FAR, thresholdList=thresholdList)\n",
    "        \n",
    "#         print(\"numParticipants: {}, windowSize: {}, isEqualSampleSize: {}, EER: {}\".format(len(dfList_exp1), w, isEqualSampleSize, voting_EER_data[\"EER\"]))\n",
    "        \n",
    "#         print(\"--- end of voting based\")\n",
    "        \n",
    "        distFRR = distFRRDATA[\"dist_array\"]\n",
    "        distFAR = distFARDATA[\"dist_array\"]\n",
    "        EER_data = getEER(distFRR, distFAR, thresholdList=thresholdList)\n",
    "        \n",
    "        window_EER_dict[w] = EER_data[\"EER\"]\n",
    "        window_EER_threshold_dict[w] = EER_data[\"EER_threshold\"]\n",
    "        window_farList_dict[w] = EER_data[\"farList\"]\n",
    "        window_frrList_dict[w] = EER_data[\"frrList\"]\n",
    "        window_farDistro_dict[w] = distFARDATA[\"dist_dict\"]\n",
    "        window_frrDistro_dict[w] = distFRRDATA[\"dist_dict\"]\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"numParticipants: {}, windowSize: {}, isEqualSampleSize: {}, EER: {}\".format(len(dfList_exp1), w, isEqualSampleSize, window_EER_dict[w]))\n",
    "        \n",
    "    return { \"window_EER_dict\": window_EER_dict, \"window_EER_threshold_dict\": window_EER_threshold_dict , \"window_farList_dict\": window_farList_dict, \"window_frrList_dict\": window_frrList_dict, \"window_farDistro_dict\": window_farDistro_dict, \"window_frrDistro_dict\": window_frrDistro_dict}\n",
    "\n",
    "\n",
    "def getErrFixedThreshold(distFRR, distFAR, threshold):\n",
    "        \n",
    "    far = FAR(distFAR, threshold = threshold)\n",
    "    frr = FRR(distFRR, threshold = threshold)\n",
    "    \n",
    "\n",
    "    return {\"FAR\": far, \"FRR\": frr, \"threshold\": threshold}\n",
    "\n",
    "\n",
    "# To be done\n",
    "def getErrFixedThresholdWindowsDict(dfList, start_window_size=250, end_window_size=3000, increment_step=250, numSamplePoints=22001, isEqualSampleSize = False, fixedSampleStep=3000):\n",
    "    \n",
    "    window_EER_dict = {}\n",
    "    window_EER_threshold_dict = {}\n",
    "    window_farDistro_dict = {}\n",
    "    window_frrDistro_dict = {}\n",
    "    \n",
    "    lst = np.arange(start_window_size, end_window_size + 1, increment_step)\n",
    "    \n",
    "    for w in lst:\n",
    "        if isEqualSampleSize:\n",
    "            sampleStep = fixedSampleStep\n",
    "        else:\n",
    "            sampleStep = w\n",
    "            \n",
    "        distFRRDATA = getDistFRRFinal(dfList, window_size = w, step = sampleStep, numSamplePoints= numSamplePoints)\n",
    "        distFARDATA = getDistFARFinal(dfList, window_size = w, step = sampleStep, numSamplePoints= numSamplePoints)\n",
    "        \n",
    "        distFRR = distFRRDATA[\"dist_array\"]\n",
    "        distFAR = distFARDATA[\"dist_array\"]\n",
    "        EER_data = getEER(distFRR, distFAR)\n",
    "\n",
    "        \n",
    "        window_EER_dict[w] = EER_data[\"EER\"]\n",
    "        window_EER_threshold_dict[w] = EER_data[\"EER_threshold\"]\n",
    "        window_farDistro_dict[w] = distFARDATA[\"dist_dict\"]\n",
    "        window_frrDistro_dict[w] = distFRRDATA[\"dist_dict\"]\n",
    "        \n",
    "        print(\"numParticipants: {}, windowSize: {}, isEqualSampleSize: {}, EER: {}\".format(len(dfList), w, isEqualSampleSize, window_EER_dict[w]))\n",
    "        \n",
    "    return { \"window_EER_dict\": window_EER_dict, \"window_EER_threshold_dict\": window_EER_threshold_dict , \"window_farList_dict\": window_farList_dict,\n",
    "            \"window_frrList_dict\": window_frrList_dict, \"window_farDistro_dict\": window_farDistro_dict, \"window_frrDistro_dict\": window_frrDistro_dict}\n",
    "\n",
    "\n",
    "# def extractTextFromDocx(path):\n",
    "#     try:\n",
    "#         doc = docx.Document(path)  # Creating word reader object.\n",
    "#         data = \"\"\n",
    "#         fullText = []\n",
    "#         for para in doc.paragraphs:\n",
    "#             fullText.append(para.text)\n",
    "#             data = '\\n'.join(fullText)\n",
    "\n",
    "#     except IOError:\n",
    "#         print('There was an error opening the file!')\n",
    "#         return\n",
    "#     return data\n",
    "\n",
    "# # %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-03-18T18:34:19.771958Z\",\"iopub.execute_input\":\"2022-03-18T18:34:19.772266Z\",\"iopub.status.idle\":\"2022-03-18T18:34:19.793851Z\",\"shell.execute_reply.started\":\"2022-03-18T18:34:19.772229Z\",\"shell.execute_reply\":\"2022-03-18T18:34:19.792569Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n",
    "# def numberOfWords(text):\n",
    "#     return len(text.strip().split())\n",
    "\n",
    "# def numberOfChars(text):\n",
    "#     return len(text)\n",
    "\n",
    "# def wordsPerMinute(text, mins):\n",
    "#     return numberOfWords(text)/mins\n",
    "\n",
    "# def charsPerMinute(text, mins):\n",
    "#     return numberOfChars(text)/mins\n",
    "\n",
    "# def classifyTypists(typistsSpeeds):\n",
    "#     '''\n",
    "#         WPM\n",
    "#     Beginner\t0 - 24\n",
    "#     Intermediate\t25 - 30\n",
    "#     Average\t31 - 41\n",
    "#     Pro\t42 - 54\n",
    "#     Typemaster\t55 - 79\n",
    "#     Megaracer\t80+\n",
    "#     '''\n",
    "#     exp2_typingspeeds = [29.96428571, 37.42857143, 44.89285714, 52.35714286, 59.82142857, 67.28571429]\n",
    "#     speedDict = {\"Beginner\": 24, \"Intermediate\": 30, \"Average\": 41, \"Pro\": 54, \"Typemaster\": 79, \"Megaracer\": 1000}\n",
    "    \n",
    "#     keys = list(speedDict.keys())\n",
    "#     for i in range(len(speedDict.keys())):\n",
    "#         speedDict[keys[i]] = exp2_typingspeeds[i]\n",
    "    \n",
    "#     speedStats = {\"Beginner\": 0, \"Intermediate\": 0, \"Average\": 0, \"Pro\": 0, \"Typemaster\": 0, \"Megaracer\": 0}\n",
    "#     typistsIDStats = {\"Beginner\": [], \"Intermediate\": [], \"Average\": [], \"Pro\": [], \"Typemaster\": [], \"Megaracer\": []}\n",
    "    \n",
    "#     for typist, speed in typistsSpeeds.items():\n",
    "#         if speed <= speedDict[\"Beginner\"]:\n",
    "#             speedStats[\"Beginner\"] += 1\n",
    "#             typistsIDStats[\"Beginner\"].append(typist)\n",
    "            \n",
    "#         elif speed <= speedDict[\"Intermediate\"]:\n",
    "#             speedStats[\"Intermediate\"] += 1\n",
    "#             typistsIDStats[\"Intermediate\"].append(typist)\n",
    "            \n",
    "#         elif speed <= speedDict[\"Average\"]:\n",
    "#             speedStats[\"Average\"] += 1\n",
    "#             typistsIDStats[\"Average\"].append(typist)\n",
    "            \n",
    "#         elif speed <= speedDict[\"Pro\"]:\n",
    "#             speedStats[\"Pro\"] += 1\n",
    "#             typistsIDStats[\"Pro\"].append(typist)\n",
    "            \n",
    "#         elif speed <= speedDict[\"Typemaster\"]:\n",
    "#             speedStats[\"Typemaster\"] += 1\n",
    "#             typistsIDStats[\"Typemaster\"].append(typist)\n",
    "            \n",
    "#         else:\n",
    "#             speedStats[\"Megaracer\"] += 1\n",
    "#             typistsIDStats[\"Megaracer\"].append(typist)\n",
    "            \n",
    "#     return {\"speedStats\": speedStats, \"typistsIDStats\": typistsIDStats}\n",
    "\n",
    "# # %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-03-18T18:34:19.795159Z\",\"iopub.execute_input\":\"2022-03-18T18:34:19.795733Z\",\"iopub.status.idle\":\"2022-03-18T18:34:19.814022Z\",\"shell.execute_reply.started\":\"2022-03-18T18:34:19.795693Z\",\"shell.execute_reply\":\"2022-03-18T18:34:19.812934Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n",
    "# r = [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 28, 29, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49]\n",
    "# #r = [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 28, 29, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
    "\n",
    "\n",
    "# def users_typing_speed(user_ids_lst):\n",
    "#     '''\n",
    "#     input: \n",
    "#         user_ids_lst: list of selected user ids\n",
    "#     Return {user_id: typing_speed in words per minute}\n",
    "#     '''\n",
    "#     typistsSpeeds = {}\n",
    "#     for i in user_ids_lst:\n",
    "#         user_text_data = extractTextFromDocx('../input/wearable-assisted-ca/user{0}_{1}.docx'.format(i, 2))\n",
    "\n",
    "#         typistsSpeeds[i] = wordsPerMinute(user_text_data, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd479b4-09e9-42b9-9e1d-13f0da0ae314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimented with the follwoing LR scheduling functions, but ended up using from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "# commented out as it is not in use\n",
    "# from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "# from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "# def step_decay(epoch):\n",
    "#     # initial_lr = 0.00001 #used for original size\n",
    "#     # drop = 0.2\n",
    "#     # # if epoch < 3:\n",
    "#     # epochs_drop = 5# used for original size\n",
    "#     # else: \n",
    "#     #     epochs_drop = 5\n",
    "\n",
    "#     # initial_lr = 0.00003\n",
    "#     # drop=1\n",
    "#     # epochs_drop = 5\n",
    "\n",
    "#     # if epoch<10:\n",
    "#     #     lr = initial_lr - (initial_lr*0.1*epoch)\n",
    "#     # elif epoch<20:\n",
    "#     #     lr = initial_lr - (initial_lr*0.01*epoch)\n",
    "#     # elif epoch<30:\n",
    "#     #     lr = initial_lr - (initial_lr*0.001*epoch)\n",
    "#     # else:\n",
    "#     #     lr = initial_lr * drop ** (epoch // epochs_drop)\n",
    "    \n",
    "#     initial_lrate = 0.0001\n",
    "#     k = 0.1\n",
    "#     lrate = initial_lrate * exp(-k*t)\n",
    "#     print(lr)\n",
    "#     return lr\n",
    "\n",
    "\n",
    "# def lr_time_based_decay(epoch):\n",
    "#     initial_learning_rate=0.00005\n",
    "#     decay = initial_learning_rate/epoch\n",
    "#     return lr * 1 / (1 + decay * epoch)\n",
    "    \n",
    "# def lr_exp_decay(epoch, lr=0.001):\n",
    "    \n",
    "#     k = 0.5\n",
    "#     lr = lr * math.exp(-k*epoch)\n",
    "#     print(lr)\n",
    "#     return lr\n",
    "    \n",
    "# def lr_step_based_decay(epoch, lr=.001):\n",
    "#     drop_rate = 0.1\n",
    "#     epochs_drop = 3\n",
    "#     epochs_drop_rate=3\n",
    "\n",
    "#     # lr = lr * math.pow(drop_rate, math.floor(epoch/epochs_drop))\n",
    "#     lr = lr * drop_rate ** (int(np.emath.logn(epochs_drop, epoch+1)))\n",
    "#     print(lr)\n",
    "#     return lr\n",
    "\n",
    "# def lr_cyclic_step_based_decay(epoch, lr=0.001):\n",
    "    \n",
    "#     new_epoch = epoch%5\n",
    "#     # initial_learning_rate=0.001 #0.0001, epoch 20 gives 10%\n",
    "#     drop_rate = 0.5\n",
    "#     epochs_drop = 1\n",
    "#     lr = lr * math.pow(drop_rate, math.floor(new_epoch/epochs_drop))\n",
    "#     print(lr)\n",
    "#     return lr\n",
    "\n",
    "# def lr_exp_decay_step_based_decay(epoch, lr=0.001):\n",
    "#     initial_learning_rate=0.001\n",
    "    \n",
    "#     if epoch<3:\n",
    "#         k = 0.3\n",
    "#         lr = initial_learning_rate * math.exp(-k*epoch)\n",
    "    \n",
    "#     else:\n",
    "#         drop_rate = 0.5\n",
    "#         epochs_drop = 1\n",
    "#         lr = lr * math.pow(drop_rate, math.floor(epoch/epochs_drop))\n",
    "#     print(lr)\n",
    "#     return lr\n",
    "\n",
    "# def lr_poly_scheduler(epoch, lr=0.001):\n",
    "#     decay_rate = 10\n",
    "#     lr=lr * (1 - epoch/50) ** decay_rate\n",
    "#     print(lr)\n",
    "#     return lr\n",
    "\n",
    "# # define the learning rate scheduler callback\n",
    "\n",
    "# def lr_schedule(epoch):\n",
    "#     if epoch < 3:\n",
    "#         return 0.001\n",
    "#     else:\n",
    "#         return 0.001 - (epoch - 2) * 0.0001\n",
    "    \n",
    "    \n",
    "# # lr_scheduler = lr_exp_decay(lr_exp_decay)\n",
    "\n",
    "\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='eer_val', factor=0.01,\n",
    "#                               patience=0, min_lr=0.000000001, verbose=1)\n",
    "\n",
    "# # lr_schedule = tf.keras.experimental.CosineDecay(0.0001, 10)\n",
    "# # lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "# # lr_schedule(tf.range(30, dtype=tf.float32))\n",
    "\n",
    "# x = np.arange(50)\n",
    "# y = map(lr_exp_decay, x)\n",
    "# sns.lineplot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7a5786-5f85-4726-8bfb-8ba9dd51a2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeRawXExpDict_old(users_exp1_df_dict, users_exp2_df_dict, window_size = 1000, step = 1000, numSamplePoints= 18000, scale_exp1=False, scale_exp2=True, \n",
    "                   scaler=\"MinMaxScaler\", EMA_per_win_span=None, SMA_per_win_winsize=None):\n",
    "    '''\n",
    "    return \n",
    "    X_exp1_dic\n",
    "    X_exp2_dic\n",
    "    dfLists are of the same size.\n",
    "    '''\n",
    "\n",
    "    if len(users_exp1_df_dict) != len(users_exp2_df_dict): \n",
    "        raise Exception(\"dfLists are not of the same size.\")\n",
    "    \n",
    "    windows = getIndices(sampleSize=window_size, step=step, numSamplePoints= numSamplePoints)\n",
    "\n",
    "    X_exp1_dic = {}\n",
    "    X_exp2_dic = {}\n",
    "    fitted_scaler_exp1_dic={}\n",
    "    fitted_scaler_exp2_dic={}\n",
    "    \n",
    "    exp1_scaler = scaler\n",
    "    exp2_scaler = scaler\n",
    "    \n",
    "    if scale_exp1!=True:\n",
    "        exp1_scaler=None\n",
    "    if scale_exp2!=True:\n",
    "        exp2_scaler=None\n",
    "    for user_key in users_exp1_df_dict:\n",
    "        \n",
    "        \n",
    "        encoding_array_exp1, fitted_scaler_exp1 = getRawDataChunks(users_exp1_df_dict[user_key], windows, scaler=exp1_scaler, user_key=user_key, exp_num=1,\n",
    "                                                                  EMA_per_win_span=EMA_per_win_span, SMA_per_win_winsize=SMA_per_win_winsize)\n",
    "        encoding_array_exp2, fitted_scaler_exp2 = getRawDataChunks(users_exp2_df_dict[user_key], windows, scaler=exp2_scaler, user_key=user_key, exp_num=2,\n",
    "                                                                  EMA_per_win_span=EMA_per_win_span, SMA_per_win_winsize=SMA_per_win_winsize)\n",
    "        \n",
    "        X_exp1_dic[user_key] = encoding_array_exp1\n",
    "        X_exp2_dic[user_key] = encoding_array_exp2\n",
    "        \n",
    "        fitted_scaler_exp1_dic[user_key]=fitted_scaler_exp1\n",
    "        fitted_scaler_exp2_dic[user_key]=fitted_scaler_exp2\n",
    "        \n",
    "    return {\"Raw_X_exp1_dic\": X_exp1_dic, \"Raw_X_exp2_dic\": X_exp2_dic, \"fitted_scaler_exp1_dic\": fitted_scaler_exp1_dic, \"fitted_scaler_exp2_dic\": fitted_scaler_exp2_dic}\n",
    "\n",
    "\n",
    "def MakeXExpDic_old(dfList_exp1, dfList_exp2, window_size = 1000, step = 1000, numSamplePoints= 18000):\n",
    "    '''\n",
    "    return \n",
    "    X_exp1_dic\n",
    "    X_exp2_dic\n",
    "    dfLists are of the same size.\n",
    "    '''\n",
    "    if len(dfList_exp1) != len(dfList_exp2): \n",
    "        raise Exception(\"dfLists are not of the same size.\")\n",
    "    \n",
    "    windows = getIndices(sampleSize=window_size, step=step, numSamplePoints= numSamplePoints)\n",
    "\n",
    "    X_exp1_dic = {}\n",
    "    X_exp2_dic = {}\n",
    "    for i in range(len(dfList_exp1)):\n",
    "        \n",
    "        \n",
    "        encoding_array_exp1 = getEncodingArray(dfList_exp1[i], windows)\n",
    "        encoding_array_exp2 = getEncodingArray(dfList_exp2[i], windows)\n",
    "        \n",
    "        X_exp1_dic[i] = encoding_array_exp1\n",
    "        X_exp2_dic[i] = encoding_array_exp2\n",
    "        \n",
    "    return {\"X_exp1_dic\": X_exp1_dic, \"X_exp2_dic\": X_exp2_dic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93301085-bfca-43e5-b675-1c7db8102e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pca_feature_selector:\n",
    "    def __init__(self, n_components):\n",
    "        self._pca_dict = {}\n",
    "        self.n_components = n_components\n",
    "        \n",
    "    def add_user_pca(self, owner_idx, user_pca):\n",
    "        if owner_idx in self._pca_dict:\n",
    "            raise Exception(f\"owner_idx: {owner_idx} alraedy exists!\")\n",
    "        \n",
    "        self._pca_dict[owner_idx] = user_pca\n",
    "        \n",
    "    def user_feature_ranking(self, owner_idx):\n",
    "        '''\n",
    "        these two are the same \n",
    "        np.matmul(pca.explained_variance_ratio_[np.newaxis], abs_components) == np.dot(pca.explained_variance_ratio_, abs_components)[np.newaxis]\n",
    "        '''\n",
    "        pca = self._pca_dict[owner_idx]\n",
    "        abs_components = np.abs(pca.components_)\n",
    "        feature_importance = np.dot(pca.explained_variance_ratio_, abs_components)[np.newaxis]\n",
    "        top_feature_indices = np.argsort(-1*feature_importance)\n",
    "        \n",
    "        return {\"top_feature_indices\": top_feature_indices, \"feature_importance\": feature_importance}\n",
    "    \n",
    "    def get_comparison_matrix(self):\n",
    "        \n",
    "        feature_importance_matrix = []\n",
    "        top_feature_matrix = []\n",
    "        for owner_idx in self._pca_dict:\n",
    "            user_feature_dict = self.user_feature_ranking(owner_idx) \n",
    "            feature_importance_matrix += [user_feature_dict[\"feature_importance\"]]\n",
    "            top_feature_matrix += [user_feature_dict[\"top_feature_indices\"]]\n",
    "            \n",
    "        self._feature_importance_matrix = np.concatenate(feature_importance_matrix, axis=0)\n",
    "        self._top_feature_matrix = np.concatenate(top_feature_matrix, axis=0)\n",
    "\n",
    "        return {\"feature_importance_matrix\": self._feature_importance_matrix, \"top_feature_matrix\" :self._top_feature_matrix}\n",
    "    \n",
    "    def find_top_n_features(self):\n",
    "        \n",
    "        best_feature_lst = []\n",
    "        for i in range(self.n_components):\n",
    "            best_feature_lst.append(self.find_next_best_feature(best_feature_lst))\n",
    "            \n",
    "        return best_feature_lst\n",
    "        \n",
    "    def find_next_best_feature(self, curr_feature_lst):\n",
    "        \n",
    "        curr_pc_idx = len(curr_feature_lst)\n",
    "        feature_column_count = np.bincount(self._top_feature_matrix[:, curr_pc_idx])\n",
    "        print(f\"top_f_m: {self._top_feature_matrix[:, curr_pc_idx]}\")\n",
    "        print(curr_feature_lst)\n",
    "        \n",
    "        i = 0\n",
    "        #probably need to use a tree type or heap structure\n",
    "        while i < len(feature_column_count):\n",
    "            top_feature_idx = np.argmax(feature_column_count[i:]) + i\n",
    "            if top_feature_idx not in curr_feature_lst:\n",
    "                return top_feature_idx\n",
    "            print('-------')\n",
    "            print(feature_column_count)\n",
    "            print(f\"i: {i}, top_feature_idx: {top_feature_idx}\")\n",
    "\n",
    "            i = top_feature_idx + 1\n",
    "            \n",
    "        raise Exception('could not find best feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4d4be3-13de-4cfa-b601-dd5249ad5549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # exp1 idx 10 has corrupted data frist 50 sec\n",
    "# # # exp2 idx 12 has some artifacts first 12.5 sec\n",
    "# # # exp1 idx 17 has some artifacts first 75 sec\n",
    "# # # exp2 idx 23 has some artifacts last 6.5 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40105b41-4fb1-4e88-a7dd-5e7c6d75f618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8f106590-9a34-47a9-8ab5-64d4eb2ad764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def evaluate_owner_ocsvm_train_valid(owner_key, X_exp_train_dic, seed, run, param_dist, cores, ocsvm_kernel=\"rbf\"):\n",
    "#     run_seed = seed + run\n",
    "#     train_dic, valid_test_dic = { owner_key: X_exp_train_dic[owner_key][\"profile_windows\"]}, X_exp_train_dic[owner_key][\"unknown_users_dict\"] \n",
    "\n",
    "#     X_vals_owner_idx = utils_create_cv_splits(owner_key, train_dic, valid_test_dic, seed=run_seed) #check\n",
    "\n",
    "#     # X_test_regular = X_vals_owner_idx['X_test_regular'].copy()\n",
    "#     # X_test_anomalous = X_vals_owner_idx['X_test_anomalous'].copy()\n",
    "\n",
    "#     cv_splits = X_vals_owner_idx['cv_splits']\n",
    "\n",
    "\n",
    "#     clf = OneClassSVM(kernel=ocsvm_kernel)\n",
    "\n",
    "#     pipeline = Pipeline([\n",
    "# #                             ('scaler', StandardScaler()), \n",
    "#                         # ('scaler', get_new_scaler_dict[P.scaler]()), \n",
    "# #                             ('scaler', Normalizer()),\n",
    "# #                              ('pca', pca), \n",
    "# #                              ('selector', VarianceThreshold()), \n",
    "#                          ('model', clf)\n",
    "#                         ])\n",
    "\n",
    "#     search = RandomizedSearchCV(\n",
    "#         pipeline,\n",
    "#         param_distributions=param_dist,\n",
    "#         cv=cv_splits,\n",
    "# #             n_iter=120,\n",
    "#         n_iter=80,\n",
    "# #             n_iter=480,\n",
    "#         n_jobs=cores,\n",
    "#         refit=False,\n",
    "#         # scoring={\"eer\": utils_eer_scorer, \"accuracy\": \"accuracy\"},\n",
    "#         # scoring={\n",
    "#         #         \"eer\": utils_eer_scorer,\n",
    "#         #         \"accuracy\": \"accuracy\",\n",
    "#         #         \"precision\": \"precision\",\n",
    "#         #         \"recall\": \"recall\",\n",
    "#         #     },\n",
    "#         # scoring={\n",
    "#         #         \"eer\": utils_eer_scorer,\n",
    "#         #     },\n",
    "#         scoring={\n",
    "#                 \"cs\": cs,\n",
    "#             },\n",
    "#         verbose=100,\n",
    "#         return_train_score=False,\n",
    "#         # iid=False, #invalid arg in sklearn 1.0.2\n",
    "#         error_score=\"raise\",\n",
    "#         random_state=run_seed\n",
    "#     )\n",
    "# #         search = GridSearchCV(\n",
    "# #             pipeline,\n",
    "# #             param_grid=param_dist,\n",
    "# #             cv=cv_splits,\n",
    "# #             n_jobs=cores,\n",
    "# #             refit=False,\n",
    "# #             scoring={\"eer\": utils_eer_scorer, \"accuracy\": \"accuracy\"},\n",
    "# #             verbose=1,\n",
    "# #             return_train_score=False,\n",
    "# #             iid=False,\n",
    "# #             error_score=\"raise\",\n",
    "# #         )\n",
    "# #         print(X_train.shape)\n",
    "# #         print(X_test_anomalous.shape)\n",
    "\n",
    "\n",
    "#     search.fit(X_vals_owner_idx['X_train'], X_vals_owner_idx['y_train'])\n",
    "#     impostors = [user_key for user_key in X_exp_train_dic.keys() if user_key != owner_key]\n",
    "\n",
    "#     df_report = utils_cv_report(search, owner_key, impostors) # check\n",
    "#     df_report[\"run\"] = run\n",
    "#     return df_report #{\"df_report\": df_report, \"owner_key\": owner_key}\n",
    "\n",
    "# from sklearn.metrics import make_scorer, precision_score, recall_score\n",
    "\n",
    "# # Create a custom scoring function\n",
    "# def custom_scorer(estimator, X, y_true):\n",
    "#     # Predict the labels for the validation data\n",
    "#     y_pred = estimator.predict(X)\n",
    "\n",
    "#     # Calculate precision and recall\n",
    "#     precision = precision_score(y_true, y_pred, pos_label=-1, zero_division=0)\n",
    "#     recall = recall_score(y_true, y_pred, pos_label=-1)\n",
    "\n",
    "#     # Return the average of precision and recall as the final score\n",
    "#     return (precision + recall) / 2\n",
    "\n",
    "# cs = make_scorer(custom_scorer, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2498c-3dd2-4515-a800-775d45bb8bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was parallelized\n",
    "test_df_results = None  # Will be filled with randomsearch scores\n",
    "for run in tqdm(range(5)):\n",
    "\n",
    "\n",
    "    for owner_key in tqdm(\n",
    "        X_exp_test_dic.keys(),\n",
    "        desc=\"Owner\",\n",
    "    ):\n",
    "\n",
    "        df_score=evaluate_owner_ocsvm_train_test(owner_key, X_exp_test_dic, SEED, run, param_dist, cores=CORES, exp_config=P, verbose=0)\n",
    " \n",
    "        test_df_results = pd.concat([test_df_results, df_score], axis=0)\n",
    "        \n",
    "\n",
    "test_df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b29c84d-f6dd-465a-90e6-ec3c6708f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# col=\"mean\"\n",
    "# for i in range(len(df)):\n",
    "#     print((df[(\"\", f\"{col}\")].loc[i]-df[(\"\", f\"{col}\")].loc[0])/df[(\"\", f\"{col}\")].loc[0]*100)\n",
    "\n",
    "# col=2000\n",
    "# for i in range(len(df)):\n",
    "#     print((df[('Window Size', col)].loc[i]-df[('Window Size', col)].loc[0])/df[('Window Size', col)].loc[0]*100)\n",
    "\n",
    "# columns you want to apply the operation to\n",
    "selected_columns = window_size_cols+[(\"\", \"mean\"), (\"\", \"variance\")]\n",
    "\n",
    "# apply the operation to selected columns\n",
    "df_selected = df[selected_columns]\n",
    "# get the other columns\n",
    "df_others = df.drop(columns=selected_columns)\n",
    "\n",
    "\n",
    "# # Convert the DataFrame to percentage improvement relative to base case\n",
    "# df_relative = df.subtract(df.loc[base_case_index], axis=1)\n",
    "\n",
    "# # Convert to percentage improvement\n",
    "# df_relative = df_relative.divide(df.loc[base_case_index], axis=1) * 100\n",
    "\n",
    "# # Note: If you want improvement as a fraction (i.e., 0.1 instead of 10 for 10% improvement), you can remove the \"* 100\" from above.\n",
    "# df_relative\n",
    "\n",
    "base_case_index=0\n",
    "# Convert the DataFrame to percentage improvement relative to base case\n",
    "df_relative = df_selected.loc[base_case_index].subtract(df_selected)\n",
    "\n",
    "# Convert to percentage improvement\n",
    "df_relative = df_relative.divide(df_selected.loc[base_case_index], axis=1) * 100\n",
    "\n",
    "# concatenate the dataframes along the column axis\n",
    "df_relative = pd.concat([df_others, df_relative], axis=1)\n",
    "df_relative.to_json(f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/WACA-OCSVM-relative_df.json\")\n",
    "df_relative\n",
    "\n",
    "df = pd.read_json(f'./{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "# df = pd.read_json('path_to_file.json')\n",
    "df\n",
    "[eval(i) for i in df.columns.tolist()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
