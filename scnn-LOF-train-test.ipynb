{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "EXP_PATH_NAME=\"SCNN-LOF\"\n",
    "RIVAL_PATH_NAME=\"WACA-LOF\"\n",
    "joblib.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "EER: 0.333, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.400, Threshold: 0.200 <-- Worse case\n",
      "EER: 0.167, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.333, Threshold: 1.000 <-- Worse case\n",
      "--------------------\u001b[32mUtility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mPreprocessing utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mNeural Networks utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "EER: 0.333, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.400, Threshold: 0.200 <-- Worse case\n",
      "EER: 0.167, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.333, Threshold: 1.000 <-- Worse case\n",
      "--------------------\u001b[32mUtility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mPreprocessing utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mWACA utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mClassification utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "Numpy Seed was set to: 567\n",
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade pip\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import gc\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import dataclasses\n",
    "from sklearn.svm import OneClassSVM\n",
    "from dataclasses import asdict\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import random\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold # Feature selector\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Global utitlity functions are in separate notebook\n",
    "%run ./Classification_utility-functions.ipynb\n",
    "%run ./SEED-CONSTANTS.ipynb\n",
    "\n",
    "\n",
    "np.random.seed(SEED)\n",
    "print(f\"Numpy Seed was set to: {SEED}\")\n",
    "\n",
    "print(\"Setup Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__dir__()\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class ExperimentParameters:\n",
    "    \"\"\"Contains all relevant parameters to run an experiment.\"\"\"\n",
    "\n",
    "    name: str  # Name of Parameter set. Used as identifier for charts etc.\n",
    "    frequency: int\n",
    "    max_subjects: int\n",
    "    max_test_subjects: int\n",
    "        \n",
    "    user_ids: list\n",
    "    num_sample_points_per_exp: int\n",
    "    exp_begin_cutoff_idx: int\n",
    "    exp_end_cutoff_idx: int\n",
    "        \n",
    "    \n",
    "    seconds_per_subject_train: float\n",
    "    seconds_per_subject_test: float\n",
    "    window_size: int  # After resampling\n",
    "    ocsvm_step_width: int  # After resampling\n",
    "    scaler: str  # StandardScaler, MinMaxScaler, Normalizer, MaxAbsScaler, RobustScaler, PowerTransformer\n",
    "    scaler_scope: str  # {\"subject\", \"session\"}\n",
    "    scaler_global: bool  # fit transform scale on all data (True) or fit on training only (False)\n",
    "    ocsvm_kernel: str # ocsvm kernel\n",
    "    ocsvm_nu: float  # Best value found in random search, used for final model\n",
    "    ocsvm_gamma: float  # Best value found in random search, used for final model\n",
    "    feature_cols: list  # Columns used as features\n",
    "    exclude_subjects: list  # Don't load data from those users\n",
    "        \n",
    "    # Calculated values\n",
    "    def __post_init__(self):\n",
    "        # HDF key of table:\n",
    "        self.table_name = f\"sensors_{self.frequency}hz\"\n",
    "\n",
    "        \n",
    "\n",
    "# INSTANCES\n",
    "# ===========================================================\n",
    "\n",
    "# NAIVE_APPROACH\n",
    "# -----------------------------------------------------------\n",
    "NAIVE_MINMAX_OCSVM = ExperimentParameters(\n",
    "    name=\"NAIVE-MINMAX_OCSVM\",\n",
    "    frequency=100,\n",
    "    max_subjects=29,\n",
    "    max_test_subjects=10,\n",
    "    user_ids = [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 28, 29, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49],\n",
    "    num_sample_points_per_exp=21000,\n",
    "    exp_begin_cutoff_idx=500,\n",
    "    exp_end_cutoff_idx=-500,\n",
    "    seconds_per_subject_train=210,\n",
    "    seconds_per_subject_test=210,    \n",
    "    window_size=250,\n",
    "    ocsvm_step_width=250,\n",
    "    scaler=\"minmax\",\n",
    "    scaler_scope=\"subject\",\n",
    "    scaler_global=True,\n",
    "    ocsvm_kernel=\"rbf\",\n",
    "    ocsvm_nu=None,\n",
    "    ocsvm_gamma=None,\n",
    "    feature_cols=[\n",
    "        \"x_a\",\n",
    "        \"y_a\",\n",
    "        \"z_a\",\n",
    "        \"x_g\",\n",
    "        \"y_g\",\n",
    "        \"z_g\",\n",
    "    ],\n",
    "    exclude_subjects=[],\n",
    ")\n",
    "\n",
    "# VALID_APPROACH\n",
    "# -----------------------------------------------------------\n",
    "VALID_MINMAX_OCSVM = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-MINMAX-OCSVM\",\n",
    "    scaler_global=False,\n",
    "    ocsvm_nu=0.165,\n",
    "    ocsvm_gamma=0.039,\n",
    ")\n",
    "\n",
    "# NAIVE_ROBUST_APPROACH\n",
    "# -----------------------------------------------------------\n",
    "NAIVE_ROBUST_OCSVM = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"NAIVE-ROBUST-OCSVM\",\n",
    "    scaler=\"robust\",\n",
    "    scaler_global=True,\n",
    "    ocsvm_nu=0.153,\n",
    "    ocsvm_gamma=0.091,  # below median, selected by chart\n",
    ")\n",
    "\n",
    "# ROBUST_APPROACH (VALID)\n",
    "# -----------------------------------------------------------\n",
    "VALID_ROBUST_OCSVM_125 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=125\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "\n",
    "VALID_ROBUST_OCSVM_250 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=250\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_500 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=500\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_750 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=750\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_1000 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=1000\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_1250 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=1250\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_1500 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=1500\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_1750 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=1750\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_2000 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=2000\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "# NORMALIZER_APPROACH (VALID)\n",
    "# -----------------------------------------------------------\n",
    "VALID_NORMALIZER_OCSVM = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-NORMALIZER-OCSVM\",\n",
    "    scaler=\"Normalizer\",\n",
    "    scaler_global=False,\n",
    "    ocsvm_nu=0.074,\n",
    "    ocsvm_gamma= 0.029,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "P = VALID_ROBUST_OCSVM_2000\n",
    "P.ocsvm_step_width = int(P.window_size * .5)\n",
    "P.classifier=\"LOF\"\n",
    "\n",
    "P.train_cores=1 # 20 cores for every user and 1 core for the nested crossval function\n",
    "P.test_cores=2 # 10 cores for every user and 2 for the nested crossval function\n",
    "# P = VALID_NORMALIZER_OCSVM\n",
    "\n",
    "param_dist = {\n",
    "    'model__n_neighbors': np.arange(1, 20),\n",
    "    'model__contamination': np.linspace(0.0001, 0.1, 50),\n",
    "    #'metric': ['minkowski', 'manhattan', 'chebyshev', 'hamming', 'cosine']\n",
    "}\n",
    "P.LOF_metric=\"minkowski\"\n",
    "P.p=2\n",
    "\n",
    "P.scaler_clip=False\n",
    "P.is_NN=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>VALID-ROBUST-OCSVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frequency</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_subjects</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_test_subjects</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_ids</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_sample_points_per_exp</th>\n",
       "      <td>21000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp_begin_cutoff_idx</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp_end_cutoff_idx</th>\n",
       "      <td>-500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seconds_per_subject_train</th>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seconds_per_subject_test</th>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>window_size</th>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocsvm_step_width</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler</th>\n",
       "      <td>RobustScaler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler_scope</th>\n",
       "      <td>subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler_global</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocsvm_kernel</th>\n",
       "      <td>rbf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocsvm_nu</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocsvm_gamma</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_cols</th>\n",
       "      <td>[x_a, y_a, z_a, x_g, y_g, z_g]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exclude_subjects</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       Value\n",
       "name                                                      VALID-ROBUST-OCSVM\n",
       "frequency                                                                100\n",
       "max_subjects                                                              29\n",
       "max_test_subjects                                                         10\n",
       "user_ids                   [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 2...\n",
       "num_sample_points_per_exp                                              21000\n",
       "exp_begin_cutoff_idx                                                     500\n",
       "exp_end_cutoff_idx                                                      -500\n",
       "seconds_per_subject_train                                                210\n",
       "seconds_per_subject_test                                                 210\n",
       "window_size                                                             2000\n",
       "ocsvm_step_width                                                        1000\n",
       "scaler                                                          RobustScaler\n",
       "scaler_scope                                                         subject\n",
       "scaler_global                                                          False\n",
       "ocsvm_kernel                                                             rbf\n",
       "ocsvm_nu                                                                None\n",
       "ocsvm_gamma                                                             None\n",
       "feature_cols                                  [x_a, y_a, z_a, x_g, y_g, z_g]\n",
       "exclude_subjects                                                          []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils_ppp(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_scorer(utils_eer, greater_is_better=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils_eer_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading exp1 data:\n",
      "1) accel_count: 28388, gyro_count: 31997\n",
      "2) accel_count: 26010, gyro_count: 28954\n",
      "3) accel_count: 28227, gyro_count: 31814\n",
      "4) accel_count: 24860, gyro_count: 26105\n",
      "5) accel_count: 24270, gyro_count: 24347\n",
      "6) accel_count: 25012, gyro_count: 25060\n",
      "7) accel_count: 25301, gyro_count: 25382\n",
      "8) accel_count: 21975, gyro_count: 21658\n",
      "19) accel_count: 24110, gyro_count: 25050\n",
      "21) accel_count: 24326, gyro_count: 23809\n",
      "22) accel_count: 29123, gyro_count: 28724\n",
      "26) accel_count: 23148, gyro_count: 24291\n",
      "27) accel_count: 24299, gyro_count: 23589\n",
      "28) accel_count: 23807, gyro_count: 24523\n",
      "29) accel_count: 24030, gyro_count: 23457\n",
      "35) accel_count: 24388, gyro_count: 23673\n",
      "36) accel_count: 24228, gyro_count: 24208\n",
      "37) accel_count: 31945, gyro_count: 31816\n",
      "38) accel_count: 22135, gyro_count: 22327\n",
      "39) accel_count: 23573, gyro_count: 23459\n",
      "40) accel_count: 23057, gyro_count: 24296\n",
      "41) accel_count: 24102, gyro_count: 23681\n",
      "42) accel_count: 24074, gyro_count: 24328\n",
      "43) accel_count: 22631, gyro_count: 23835\n",
      "44) accel_count: 24473, gyro_count: 23749\n",
      "45) accel_count: 23974, gyro_count: 23229\n",
      "46) accel_count: 23614, gyro_count: 23827\n",
      "48) accel_count: 22828, gyro_count: 23904\n",
      "49) accel_count: 24183, gyro_count: 24633\n",
      "Loading exp2 data:\n",
      "1) accel_count: 24049, gyro_count: 26943\n",
      "2) accel_count: 24468, gyro_count: 27667\n",
      "3) accel_count: 24611, gyro_count: 27000\n",
      "4) accel_count: 24972, gyro_count: 26798\n",
      "5) accel_count: 23573, gyro_count: 23372\n",
      "6) accel_count: 23800, gyro_count: 23890\n",
      "7) accel_count: 23347, gyro_count: 24145\n",
      "8) accel_count: 22947, gyro_count: 22660\n",
      "19) accel_count: 26156, gyro_count: 25815\n",
      "21) accel_count: 23566, gyro_count: 24408\n",
      "22) accel_count: 23844, gyro_count: 24589\n",
      "26) accel_count: 23179, gyro_count: 23925\n",
      "27) accel_count: 25109, gyro_count: 25820\n",
      "28) accel_count: 23133, gyro_count: 24028\n",
      "29) accel_count: 23180, gyro_count: 24314\n",
      "35) accel_count: 23299, gyro_count: 23854\n",
      "36) accel_count: 25497, gyro_count: 25059\n",
      "37) accel_count: 25994, gyro_count: 25232\n",
      "38) accel_count: 21164, gyro_count: 21182\n",
      "39) accel_count: 24214, gyro_count: 23585\n",
      "40) accel_count: 23944, gyro_count: 23170\n",
      "41) accel_count: 23193, gyro_count: 24111\n",
      "42) accel_count: 26505, gyro_count: 25697\n",
      "43) accel_count: 22690, gyro_count: 23981\n",
      "44) accel_count: 23002, gyro_count: 23829\n",
      "45) accel_count: 23978, gyro_count: 23350\n",
      "46) accel_count: 21128, gyro_count: 21848\n",
      "48) accel_count: 27996, gyro_count: 27205\n",
      "49) accel_count: 23061, gyro_count: 24129\n"
     ]
    }
   ],
   "source": [
    "#include 47 later\n",
    "# user_ids = [9]\n",
    "df_exps_dict = load_data_frames(P.user_ids, P.exp_begin_cutoff_idx, P.exp_end_cutoff_idx, P.num_sample_points_per_exp)\n",
    "raw_dfList_exp1, raw_dfList_exp2 = df_exps_dict['dfList_exp1'], df_exps_dict['dfList_exp2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading exp1 data:\n",
      "47) accel_count: 22777, gyro_count: 22226\n",
      "Loading exp2 data:\n",
      "47) accel_count: 17718, gyro_count: 18353\n"
     ]
    }
   ],
   "source": [
    "num_sample_points_per_exp_user_47 = 18000\n",
    "df_exps_dict_user_47 = load_data_frames([47], P.exp_begin_cutoff_idx, P.exp_end_cutoff_idx, num_sample_points_per_exp_user_47)\n",
    "dfList_exp1_user_47, dfList_exp2_user_47 = df_exps_dict_user_47['dfList_exp1'], df_exps_dict_user_47['dfList_exp2']\n",
    "\n",
    "raw_dfList_exp1_user_47 = dfList_exp1_user_47\n",
    "raw_dfList_exp2_user_47 = dfList_exp2_user_47"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Butterworth frequency Cut-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Naive Approach\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butterworth\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for cut_off_freq in tqdm(list(range(48, 50))):\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "\n",
    "    \n",
    "    ffted_dfList_exp1 = get_ffted_dfList(raw_dfList_exp1, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    \n",
    "    ffted_dfList_exp1_user_47 = get_ffted_dfList(raw_dfList_exp1_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": ffted_dfList_exp1,\n",
    "                \"dfList_exp2\": ffted_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": ffted_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": ffted_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.cut_off_freq] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                                    extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                                    param_dist=param_dist)\n",
    "        \n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq: {P.cut_off_freq}\\n\")\n",
    "        f.write(EER_df_train_dict[P.cut_off_freq].to_string())\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reseting experiment params successful!\n",
      "train_set: {0, 1, 2, 3, 4, 5, 6, 9, 10, 12, 14, 15, 16, 18, 19, 22, 23, 24, 25, 28}\n",
      "test_set: {7, 8, 11, 13, 17, 20, 21, 26, 27, 29}\n",
      "cut_off_freq: 33\n",
      "clip=False_experiments_results/SCNN_trained_models/SCNN-standalone/1.2/SCNN_3_1_conv_1_dense_arg_dict_default/125/SCNN-standalone-1.2-SCNN_3_1_conv_1_dense_arg_dict_default-125-deep_feature_extractor-Butter33-cv0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 04:07:06.793721: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-08-03 04:07:06.793791: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n",
      "2023-08-03 04:07:06.794267: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "clip=False_experiments_results/SCNN_trained_models/SCNN-standalone/1.2/SCNN_3_1_conv_1_dense_arg_dict_default/250/SCNN-standalone-1.2-SCNN_3_1_conv_1_dense_arg_dict_default-250-deep_feature_extractor-Butter33-cv0\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "clip=False_experiments_results/SCNN_trained_models/SCNN-standalone/1.2/SCNN_3_1_conv_1_dense_arg_dict_default/500/SCNN-standalone-1.2-SCNN_3_1_conv_1_dense_arg_dict_default-500-deep_feature_extractor-Butter33-cv0\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "clip=False_experiments_results/SCNN_trained_models/SCNN-standalone/1.2/SCNN_3_1_conv_1_dense_arg_dict_default/750/SCNN-standalone-1.2-SCNN_3_1_conv_1_dense_arg_dict_default-750-deep_feature_extractor-Butter33-cv0\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "clip=False_experiments_results/SCNN_trained_models/SCNN-standalone/1.2/SCNN_3_1_conv_1_dense_arg_dict_default/1000/SCNN-standalone-1.2-SCNN_3_1_conv_1_dense_arg_dict_default-1000-deep_feature_extractor-Butter33-cv0\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "clip=False_experiments_results/SCNN_trained_models/SCNN-standalone/1.2/SCNN_3_2_conv_1_dense_arg_dict_default/125/SCNN-standalone-1.2-SCNN_3_2_conv_1_dense_arg_dict_default-125-deep_feature_extractor-Butter33-cv0\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "clip=False_experiments_results/SCNN_trained_models/SCNN-standalone/1.2/SCNN_3_2_conv_1_dense_arg_dict_default/250/SCNN-standalone-1.2-SCNN_3_2_conv_1_dense_arg_dict_default-250-deep_feature_extractor-Butter33-cv0\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "clip=False_experiments_results/SCNN_trained_models/SCNN-standalone/1.2/SCNN_3_2_conv_1_dense_arg_dict_default/500/SCNN-standalone-1.2-SCNN_3_2_conv_1_dense_arg_dict_default-500-deep_feature_extractor-Butter33-cv0\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "clip=False_experiments_results/SCNN_trained_models/SCNN-standalone/1.2/SCNN_3_2_conv_1_dense_arg_dict_default/750/SCNN-standalone-1.2-SCNN_3_2_conv_1_dense_arg_dict_default-750-deep_feature_extractor-Butter33-cv0\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "clip=False_experiments_results/SCNN_trained_models/SCNN-standalone/1.2/SCNN_3_2_conv_1_dense_arg_dict_default/1000/SCNN-standalone-1.2-SCNN_3_2_conv_1_dense_arg_dict_default-1000-deep_feature_extractor-Butter33-cv0\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "clip=False_experiments_results/SCNN_trained_models/SCNN-standalone/1.2/SCNN_1_3_conv_1_dense_arg_dict_default/125/SCNN-standalone-1.2-SCNN_1_3_conv_1_dense_arg_dict_default-125-deep_feature_extractor-Butter33-cv0\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "clip=False_experiments_results/SCNN_trained_models/SCNN-standalone/1.2/SCNN_1_3_conv_1_dense_arg_dict_default/250/SCNN-standalone-1.2-SCNN_1_3_conv_1_dense_arg_dict_default-250-deep_feature_extractor-Butter33-cv0\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "clip=False_experiments_results/SCNN_trained_models/SCNN-standalone/1.2/SCNN_1_3_conv_1_dense_arg_dict_default/500/SCNN-standalone-1.2-SCNN_1_3_conv_1_dense_arg_dict_default-500-deep_feature_extractor-Butter33-cv0\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "clip=False_experiments_results/SCNN_trained_models/SCNN-standalone/1.2/SCNN_1_3_conv_1_dense_arg_dict_default/750/SCNN-standalone-1.2-SCNN_1_3_conv_1_dense_arg_dict_default-750-deep_feature_extractor-Butter33-cv0\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "clip=False_experiments_results/SCNN_trained_models/SCNN-standalone/1.2/SCNN_1_3_conv_1_dense_arg_dict_default/1000/SCNN-standalone-1.2-SCNN_1_3_conv_1_dense_arg_dict_default-1000-deep_feature_extractor-Butter33-cv0\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "clip=False_experiments_results/SCNN_trained_models/SCNN-standalone/1.2/SCNN_3_123_conv_1_dense_arg_dict_default/125/SCNN-standalone-1.2-SCNN_3_123_conv_1_dense_arg_dict_default-125-deep_feature_extractor-Butter33-cv0\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "clip=False_experiments_results/SCNN_trained_models/SCNN-standalone/1.2/SCNN_3_123_conv_1_dense_arg_dict_default/250/SCNN-standalone-1.2-SCNN_3_123_conv_1_dense_arg_dict_default-250-deep_feature_extractor-Butter33-cv0\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "clip=False_experiments_results/SCNN_trained_models/SCNN-standalone/1.2/SCNN_3_123_conv_1_dense_arg_dict_default/500/SCNN-standalone-1.2-SCNN_3_123_conv_1_dense_arg_dict_default-500-deep_feature_extractor-Butter33-cv0\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "clip=False_experiments_results/SCNN_trained_models/SCNN-standalone/1.2/SCNN_3_123_conv_1_dense_arg_dict_default/750/SCNN-standalone-1.2-SCNN_3_123_conv_1_dense_arg_dict_default-750-deep_feature_extractor-Butter33-cv0\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "clip=False_experiments_results/SCNN_trained_models/SCNN-standalone/1.2/SCNN_3_123_conv_1_dense_arg_dict_default/1000/SCNN-standalone-1.2-SCNN_3_123_conv_1_dense_arg_dict_default-1000-deep_feature_extractor-Butter33-cv0\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "train_set: {0, 1, 2, 3, 4, 5, 6, 9, 10, 12, 14, 15, 16, 18, 19, 22, 23, 24, 25, 28}\n",
      "test_set: {7, 8, 11, 13, 17, 20, 21, 26, 27, 29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 125\n",
      "NN preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         1.0000002  0.99999994 1.         0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.99999994 0.99999994 1.         1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  1.         0.9999998  0.99999994 1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.        1.        1.0000001 0.9999998 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.9999999  1.0000001  0.9999999  1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 0.99999994 1.         0.99999994 0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.0000001  1.0000002  0.99999994 0.99999994 1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001 0.9999998 0.9999998 1.0000001 1.        0.9999998]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.9999998  1.         0.9999998  0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  0.9999999  0.99999994 1.         0.9999999  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.         0.99999994 1.0000002  0.9999998  1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.0000001  0.9999998  1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  0.99999994 0.99999994 1.         0.9999998  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         0.99999994 1.0000001  0.9999999  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.        1.0000001 0.9999999 0.9999999 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.9999999  1.0000001  0.99999994 1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         1.0000001  1.         0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.9999998  1.         0.9999999  0.99999994 1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999 1.        1.0000001 1.        1.        0.9999999]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.         1.         1.0000001  0.99999994 1.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 04:08:54.262300: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MakeSCNNExpDicOwner Time:  2.0739223044365644\n",
      "MakeSCNNXExpDicUnknown Time:  243.34117383882403\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 20/20 [00:00<00:00, 333.74it/s]\n",
      "\n",
      " 33%|███▎      | 1/3 [01:01<02:03, 61.91s/it]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 20/20 [00:00<00:00, 17429.06it/s]\n",
      "\n",
      " 67%|██████▋   | 2/3 [01:58<00:59, 59.03s/it]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 20/20 [00:00<00:00, 19226.70it/s]\n",
      "\n",
      "100%|██████████| 3/3 [02:50<00:00, 56.76s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 20/20 [00:00<00:00, 17249.86it/s]\n",
      "\n",
      " 20%|██        | 1/5 [00:03<00:15,  3.76s/it]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 20/20 [00:00<00:00, 12715.79it/s]\n",
      "\n",
      " 40%|████      | 2/5 [00:07<00:10,  3.63s/it]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 20/20 [00:00<00:00, 14155.60it/s]\n",
      "\n",
      " 60%|██████    | 3/5 [00:10<00:07,  3.60s/it]\u001b[A\n",
      "\n",
      "run: 3: 100%|██████████| 20/20 [00:00<00:00, 13582.59it/s]\n",
      "\n",
      " 80%|████████  | 4/5 [00:14<00:03,  3.62s/it]\u001b[A\n",
      "\n",
      "run: 4: 100%|██████████| 20/20 [00:00<00:00, 9243.65it/s]\n",
      "\n",
      "100%|██████████| 5/5 [00:17<00:00,  3.59s/it]\u001b[A\n",
      " 20%|██        | 1/5 [08:25<33:43, 505.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 250\n",
      "NN preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         1.0000002  0.99999994 1.         0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.99999994 0.99999994 1.         1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  1.         0.9999998  0.99999994 1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.        1.        1.0000001 0.9999998 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.9999999  1.0000001  0.9999999  1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 0.99999994 1.         0.99999994 0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.0000001  1.0000002  0.99999994 0.99999994 1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001 0.9999998 0.9999998 1.0000001 1.        0.9999998]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.9999998  1.         0.9999998  0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  0.9999999  0.99999994 1.         0.9999999  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.         0.99999994 1.0000002  0.9999998  1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.0000001  0.9999998  1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  0.99999994 0.99999994 1.         0.9999998  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         0.99999994 1.0000001  0.9999999  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.        1.0000001 0.9999999 0.9999999 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.9999999  1.0000001  0.99999994 1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         1.0000001  1.         0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.9999998  1.         0.9999999  0.99999994 1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999 1.        1.0000001 1.        1.        0.9999999]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.         1.         1.0000001  0.99999994 1.        ]\n",
      "MakeSCNNExpDicOwner Time:  1.683199079707265\n",
      "MakeSCNNXExpDicUnknown Time:  132.17435762565583\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 20/20 [00:00<00:00, 14960.96it/s]\n",
      "\n",
      " 33%|███▎      | 1/3 [00:19<00:39, 19.82s/it]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 20/20 [00:00<00:00, 13092.88it/s]\n",
      "\n",
      " 67%|██████▋   | 2/3 [00:39<00:19, 19.99s/it]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 20/20 [00:00<00:00, 13402.47it/s]\n",
      "\n",
      "100%|██████████| 3/3 [01:00<00:00, 20.05s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 20/20 [00:00<00:00, 16703.72it/s]\n",
      "\n",
      " 20%|██        | 1/5 [00:01<00:07,  1.95s/it]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 20/20 [00:00<00:00, 14070.12it/s]\n",
      "\n",
      " 40%|████      | 2/5 [00:03<00:05,  1.92s/it]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 20/20 [00:00<00:00, 15036.04it/s]\n",
      "\n",
      " 60%|██████    | 3/5 [00:05<00:03,  1.88s/it]\u001b[A\n",
      "\n",
      "run: 3: 100%|██████████| 20/20 [00:00<00:00, 11568.90it/s]\n",
      "\n",
      " 80%|████████  | 4/5 [00:07<00:01,  1.88s/it]\u001b[A\n",
      "\n",
      "run: 4: 100%|██████████| 20/20 [00:00<00:00, 17039.63it/s]\n",
      "\n",
      "100%|██████████| 5/5 [00:09<00:00,  1.88s/it]\u001b[A\n",
      " 40%|████      | 2/5 [12:26<17:28, 349.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 500\n",
      "NN preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         1.0000002  0.99999994 1.         0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.99999994 0.99999994 1.         1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  1.         0.9999998  0.99999994 1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.        1.        1.0000001 0.9999998 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.9999999  1.0000001  0.9999999  1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 0.99999994 1.         0.99999994 0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.0000001  1.0000002  0.99999994 0.99999994 1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001 0.9999998 0.9999998 1.0000001 1.        0.9999998]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.9999998  1.         0.9999998  0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  0.9999999  0.99999994 1.         0.9999999  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.         0.99999994 1.0000002  0.9999998  1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.0000001  0.9999998  1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  0.99999994 0.99999994 1.         0.9999998  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         0.99999994 1.0000001  0.9999999  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.        1.0000001 0.9999999 0.9999999 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.9999999  1.0000001  0.99999994 1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         1.0000001  1.         0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.9999998  1.         0.9999999  0.99999994 1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999 1.        1.0000001 1.        1.        0.9999999]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.         1.         1.0000001  0.99999994 1.        ]\n",
      "MakeSCNNExpDicOwner Time:  1.4478312861174345\n",
      "MakeSCNNXExpDicUnknown Time:  77.82721701171249\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 20/20 [00:00<00:00, 6090.62it/s]\n",
      "\n",
      " 33%|███▎      | 1/3 [00:10<00:21, 10.85s/it]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 20/20 [00:00<00:00, 14310.15it/s]\n",
      "\n",
      " 67%|██████▋   | 2/3 [00:21<00:10, 10.83s/it]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 20/20 [00:00<00:00, 17382.11it/s]\n",
      "\n",
      "100%|██████████| 3/3 [00:32<00:00, 10.86s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 20/20 [00:00<00:00, 15560.39it/s]\n",
      "\n",
      " 20%|██        | 1/5 [00:00<00:03,  1.05it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 20/20 [00:00<00:00, 12565.32it/s]\n",
      "\n",
      " 40%|████      | 2/5 [00:01<00:02,  1.05it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 20/20 [00:00<00:00, 16571.73it/s]\n",
      "\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.06it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|██████████| 20/20 [00:00<00:00, 18808.54it/s]\n",
      "\n",
      " 80%|████████  | 4/5 [00:03<00:00,  1.07it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|██████████| 20/20 [00:00<00:00, 20301.57it/s]\n",
      "\n",
      "100%|██████████| 5/5 [00:04<00:00,  1.06it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [14:41<08:23, 251.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 750\n",
      "NN preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         1.0000002  0.99999994 1.         0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.99999994 0.99999994 1.         1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  1.         0.9999998  0.99999994 1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.        1.        1.0000001 0.9999998 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.9999999  1.0000001  0.9999999  1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 0.99999994 1.         0.99999994 0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.0000001  1.0000002  0.99999994 0.99999994 1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001 0.9999998 0.9999998 1.0000001 1.        0.9999998]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.9999998  1.         0.9999998  0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  0.9999999  0.99999994 1.         0.9999999  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.         0.99999994 1.0000002  0.9999998  1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.0000001  0.9999998  1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  0.99999994 0.99999994 1.         0.9999998  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         0.99999994 1.0000001  0.9999999  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.        1.0000001 0.9999999 0.9999999 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.9999999  1.0000001  0.99999994 1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         1.0000001  1.         0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.9999998  1.         0.9999999  0.99999994 1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999 1.        1.0000001 1.        1.        0.9999999]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.         1.         1.0000001  0.99999994 1.        ]\n",
      "MakeSCNNExpDicOwner Time:  1.3243335634469986\n",
      "MakeSCNNXExpDicUnknown Time:  58.108322826214135\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 20/20 [00:00<00:00, 13673.36it/s]\n",
      "\n",
      " 33%|███▎      | 1/3 [00:08<00:17,  8.84s/it]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 20/20 [00:00<00:00, 12642.97it/s]\n",
      "\n",
      " 67%|██████▋   | 2/3 [00:17<00:08,  8.88s/it]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 20/20 [00:00<00:00, 15172.02it/s]\n",
      "\n",
      "100%|██████████| 3/3 [00:26<00:00,  8.88s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 20/20 [00:00<00:00, 23205.00it/s]\n",
      "\n",
      " 20%|██        | 1/5 [00:00<00:03,  1.18it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 20/20 [00:00<00:00, 11259.88it/s]\n",
      "\n",
      " 40%|████      | 2/5 [00:01<00:02,  1.20it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 20/20 [00:00<00:00, 13368.30it/s]\n",
      "\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.23it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|██████████| 20/20 [00:00<00:00, 16200.48it/s]\n",
      "\n",
      " 80%|████████  | 4/5 [00:03<00:00,  1.23it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|██████████| 20/20 [00:00<00:00, 23051.96it/s]\n",
      "\n",
      "100%|██████████| 5/5 [00:04<00:00,  1.22it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [16:23<03:12, 192.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1000\n",
      "NN preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         1.0000002  0.99999994 1.         0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.99999994 0.99999994 1.         1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  1.         0.9999998  0.99999994 1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.        1.        1.0000001 0.9999998 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.9999999  1.0000001  0.9999999  1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 0.99999994 1.         0.99999994 0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.0000001  1.0000002  0.99999994 0.99999994 1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001 0.9999998 0.9999998 1.0000001 1.        0.9999998]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.9999998  1.         0.9999998  0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  0.9999999  0.99999994 1.         0.9999999  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.         0.99999994 1.0000002  0.9999998  1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.0000001  0.9999998  1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  0.99999994 0.99999994 1.         0.9999998  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         0.99999994 1.0000001  0.9999999  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.        1.0000001 0.9999999 0.9999999 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.9999999  1.0000001  0.99999994 1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         1.0000001  1.         0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.9999998  1.         0.9999999  0.99999994 1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999 1.        1.0000001 1.        1.        0.9999999]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.         1.         1.0000001  0.99999994 1.        ]\n",
      "MakeSCNNExpDicOwner Time:  1.2935024360194802\n",
      "MakeSCNNXExpDicUnknown Time:  50.61618238501251\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 20/20 [00:00<00:00, 14103.24it/s]\n",
      "\n",
      " 33%|███▎      | 1/3 [00:08<00:16,  8.03s/it]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 20/20 [00:00<00:00, 13974.03it/s]\n",
      "\n",
      " 67%|██████▋   | 2/3 [00:16<00:08,  8.00s/it]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 20/20 [00:00<00:00, 13374.69it/s]\n",
      "\n",
      "100%|██████████| 3/3 [00:23<00:00,  8.00s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 20/20 [00:00<00:00, 18351.80it/s]\n",
      "/tmp/ipykernel_255184/1897070050.py:65: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.49it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 20/20 [00:00<00:00, 15488.57it/s]\n",
      "/tmp/ipykernel_255184/1897070050.py:65: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\n",
      " 40%|████      | 2/5 [00:01<00:02,  1.47it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 20/20 [00:00<00:00, 15302.09it/s]\n",
      "/tmp/ipykernel_255184/1897070050.py:65: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.42it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|██████████| 20/20 [00:00<00:00, 12941.39it/s]\n",
      "/tmp/ipykernel_255184/1897070050.py:65: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.42it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|██████████| 20/20 [00:00<00:00, 12717.72it/s]\n",
      "/tmp/ipykernel_255184/1897070050.py:65: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.43it/s]\u001b[A\n",
      "100%|██████████| 5/5 [17:52<00:00, 214.57s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 125\n",
      "NN preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.0000001 0.9999999 1.        1.0000001 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.99999994 1.0000001  1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         0.9999999  0.9999999  1.         1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  1.         0.9999998  1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.99999994 0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.9999998  1.         1.         1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.9999998  1.         1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.9999998  1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000002  1.0000001  1.0000002  1.         0.9999998  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.99999994 1.         0.99999994 0.9999999  0.99999994]\n",
      "len(X_exp1_dict_user_47[47]): 337\n",
      "len_exp2_user_47: 289\n",
      "MakeSCNNExpDicOwner Time:  0.9462045412510633\n",
      "MakeSCNNXExpDicUnknown Time:  61.07225513737649\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 10/10 [00:00<00:00, 6082.23it/s]\n",
      "\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.51it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 10/10 [00:00<00:00, 9457.28it/s]\n",
      "\n",
      " 40%|████      | 2/5 [00:01<00:02,  1.40it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 10/10 [00:00<00:00, 7250.31it/s]\n",
      "\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.42it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|██████████| 10/10 [00:00<00:00, 8208.03it/s]\n",
      "\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.47it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|██████████| 10/10 [00:00<00:00, 12752.52it/s]\n",
      "\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.47it/s]\u001b[A\n",
      " 20%|██        | 1/5 [01:41<06:47, 101.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 250\n",
      "NN preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.0000001 0.9999999 1.        1.0000001 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.99999994 1.0000001  1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         0.9999999  0.9999999  1.         1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  1.         0.9999998  1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.99999994 0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.9999998  1.         1.         1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.9999998  1.         1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.9999998  1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000002  1.0000001  1.0000002  1.         0.9999998  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.99999994 1.         0.99999994 0.9999999  0.99999994]\n",
      "len(X_exp1_dict_user_47[47]): 167\n",
      "len_exp2_user_47: 143\n",
      "MakeSCNNExpDicOwner Time:  0.7514129523187876\n",
      "MakeSCNNXExpDicUnknown Time:  32.44302166439593\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 10/10 [00:00<00:00, 3107.35it/s]\n",
      "\n",
      " 20%|██        | 1/5 [00:00<00:01,  2.47it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 10/10 [00:00<00:00, 9438.13it/s]\n",
      "\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.66it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 10/10 [00:00<00:00, 6373.35it/s]\n",
      "\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.64it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|██████████| 10/10 [00:00<00:00, 7757.17it/s]\n",
      "\n",
      " 80%|████████  | 4/5 [00:01<00:00,  2.62it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|██████████| 10/10 [00:00<00:00, 8653.40it/s]\n",
      "\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.61it/s]\u001b[A\n",
      " 40%|████      | 2/5 [02:35<03:39, 73.33s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 500\n",
      "NN preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.0000001 0.9999999 1.        1.0000001 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.99999994 1.0000001  1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         0.9999999  0.9999999  1.         1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  1.         0.9999998  1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.99999994 0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.9999998  1.         1.         1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.9999998  1.         1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.9999998  1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000002  1.0000001  1.0000002  1.         0.9999998  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.99999994 1.         0.99999994 0.9999999  0.99999994]\n",
      "len(X_exp1_dict_user_47[47]): 83\n",
      "len_exp2_user_47: 71\n",
      "MakeSCNNExpDicOwner Time:  0.6349666360765696\n",
      "MakeSCNNXExpDicUnknown Time:  20.101588831283152\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 10/10 [00:00<00:00, 7136.81it/s]\n",
      "\n",
      " 20%|██        | 1/5 [00:00<00:01,  3.60it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 10/10 [00:00<00:00, 7541.00it/s]\n",
      "\n",
      " 40%|████      | 2/5 [00:00<00:00,  3.50it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 10/10 [00:00<00:00, 6803.41it/s]\n",
      "\n",
      " 60%|██████    | 3/5 [00:00<00:00,  3.57it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|██████████| 10/10 [00:00<00:00, 10262.55it/s]\n",
      "\n",
      " 80%|████████  | 4/5 [00:01<00:00,  3.50it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|██████████| 10/10 [00:00<00:00, 6866.90it/s]\n",
      "\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.57it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [03:06<01:48, 54.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 750\n",
      "NN preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.0000001 0.9999999 1.        1.0000001 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.99999994 1.0000001  1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         0.9999999  0.9999999  1.         1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  1.         0.9999998  1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.99999994 0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.9999998  1.         1.         1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.9999998  1.         1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.9999998  1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000002  1.0000001  1.0000002  1.         0.9999998  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.99999994 1.         0.99999994 0.9999999  0.99999994]\n",
      "len(X_exp1_dict_user_47[47]): 55\n",
      "len_exp2_user_47: 47\n",
      "MakeSCNNExpDicOwner Time:  0.6332390531897545\n",
      "MakeSCNNXExpDicUnknown Time:  14.558258620090783\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 10/10 [00:00<00:00, 7337.83it/s]\n",
      "\n",
      " 20%|██        | 1/5 [00:00<00:01,  3.85it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 10/10 [00:00<00:00, 9448.76it/s]\n",
      "\n",
      " 40%|████      | 2/5 [00:00<00:00,  3.97it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 10/10 [00:00<00:00, 6692.68it/s]\n",
      "\n",
      " 60%|██████    | 3/5 [00:00<00:00,  4.01it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|██████████| 10/10 [00:00<00:00, 7851.56it/s]\n",
      "\n",
      " 80%|████████  | 4/5 [00:00<00:00,  4.06it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|██████████| 10/10 [00:00<00:00, 10911.30it/s]\n",
      "\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.00it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [03:29<00:41, 41.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1000\n",
      "NN preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.0000001 0.9999999 1.        1.0000001 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.99999994 1.0000001  1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         0.9999999  0.9999999  1.         1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  1.         0.9999998  1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.99999994 0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.9999998  1.         1.         1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.9999998  1.         1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.9999998  1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000002  1.0000001  1.0000002  1.         0.9999998  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.99999994 1.         0.99999994 0.9999999  0.99999994]\n",
      "len(X_exp1_dict_user_47[47]): 41\n",
      "len_exp2_user_47: 35\n",
      "MakeSCNNExpDicOwner Time:  0.6005590027198195\n",
      "MakeSCNNXExpDicUnknown Time:  12.408456578850746\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 10/10 [00:00<00:00, 10376.80it/s]\n",
      "\n",
      " 20%|██        | 1/5 [00:00<00:00,  4.25it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 10/10 [00:00<00:00, 6015.06it/s]\n",
      "\n",
      " 40%|████      | 2/5 [00:00<00:00,  4.30it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 10/10 [00:00<00:00, 8586.09it/s]\n",
      "\n",
      " 60%|██████    | 3/5 [00:00<00:00,  4.32it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|██████████| 10/10 [00:00<00:00, 8858.09it/s]\n",
      "\n",
      " 80%|████████  | 4/5 [00:00<00:00,  4.40it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|██████████| 10/10 [00:00<00:00, 7419.61it/s]\n",
      "\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.34it/s]\u001b[A\n",
      "100%|██████████| 5/5 [03:48<00:00, 45.66s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 125\n",
      "NN preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         1.0000002  0.99999994 1.         0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.99999994 0.99999994 1.         1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  1.         0.9999998  0.99999994 1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.        1.        1.0000001 0.9999998 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.9999999  1.0000001  0.9999999  1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 0.99999994 1.         0.99999994 0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.0000001  1.0000002  0.99999994 0.99999994 1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001 0.9999998 0.9999998 1.0000001 1.        0.9999998]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.9999998  1.         0.9999998  0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  0.9999999  0.99999994 1.         0.9999999  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.         0.99999994 1.0000002  0.9999998  1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.0000001  0.9999998  1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  0.99999994 0.99999994 1.         0.9999998  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         0.99999994 1.0000001  0.9999999  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.        1.0000001 0.9999999 0.9999999 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.9999999  1.0000001  0.99999994 1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         1.0000001  1.         0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.9999998  1.         0.9999999  0.99999994 1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999 1.        1.0000001 1.        1.        0.9999999]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.         1.         1.0000001  0.99999994 1.        ]\n",
      "MakeSCNNExpDicOwner Time:  2.2834598179906607\n",
      "MakeSCNNXExpDicUnknown Time:  247.79129867441952\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 20/20 [00:00<00:00, 5257.01it/s]\n",
      "\n",
      " 33%|███▎      | 1/3 [01:00<02:00, 60.40s/it]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 20/20 [00:00<00:00, 14588.88it/s]\n",
      "\n",
      " 67%|██████▋   | 2/3 [01:55<00:57, 57.38s/it]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 20/20 [00:00<00:00, 18363.85it/s]\n",
      "\n",
      "100%|██████████| 3/3 [02:47<00:00, 55.67s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 20/20 [00:00<00:00, 16156.80it/s]\n",
      "\n",
      " 20%|██        | 1/5 [00:03<00:15,  3.99s/it]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 20/20 [00:00<00:00, 16994.75it/s]\n",
      "\n",
      " 40%|████      | 2/5 [00:07<00:11,  3.79s/it]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 20/20 [00:00<00:00, 13298.36it/s]\n",
      "\n",
      " 60%|██████    | 3/5 [00:11<00:07,  3.71s/it]\u001b[A\n",
      "\n",
      "run: 3: 100%|██████████| 20/20 [00:00<00:00, 11226.72it/s]\n",
      "\n",
      " 80%|████████  | 4/5 [00:14<00:03,  3.65s/it]\u001b[A\n",
      "\n",
      "run: 4: 100%|██████████| 20/20 [00:00<00:00, 11342.09it/s]\n",
      "\n",
      "100%|██████████| 5/5 [00:18<00:00,  3.68s/it]\u001b[A\n",
      " 20%|██        | 1/5 [08:28<33:53, 508.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 250\n",
      "NN preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         1.0000002  0.99999994 1.         0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.99999994 0.99999994 1.         1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  1.         0.9999998  0.99999994 1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.        1.        1.0000001 0.9999998 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.9999999  1.0000001  0.9999999  1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 0.99999994 1.         0.99999994 0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.0000001  1.0000002  0.99999994 0.99999994 1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001 0.9999998 0.9999998 1.0000001 1.        0.9999998]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.9999998  1.         0.9999998  0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  0.9999999  0.99999994 1.         0.9999999  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.         0.99999994 1.0000002  0.9999998  1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.0000001  0.9999998  1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  0.99999994 0.99999994 1.         0.9999998  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         0.99999994 1.0000001  0.9999999  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.        1.0000001 0.9999999 0.9999999 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.9999999  1.0000001  0.99999994 1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         1.0000001  1.         0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.9999998  1.         0.9999999  0.99999994 1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999 1.        1.0000001 1.        1.        0.9999999]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.         1.         1.0000001  0.99999994 1.        ]\n",
      "MakeSCNNExpDicOwner Time:  1.867125646211207\n",
      "MakeSCNNXExpDicUnknown Time:  134.48140674550086\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 20/20 [00:00<00:00, 8320.38it/s]\n",
      "\n",
      " 33%|███▎      | 1/3 [00:19<00:39, 19.79s/it]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 20/20 [00:00<00:00, 21754.69it/s]\n",
      "\n",
      " 67%|██████▋   | 2/3 [00:39<00:19, 19.76s/it]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 20/20 [00:00<00:00, 18477.11it/s]\n",
      "\n",
      "100%|██████████| 3/3 [00:59<00:00, 19.97s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 20/20 [00:00<00:00, 17509.10it/s]\n",
      "\n",
      " 20%|██        | 1/5 [00:01<00:07,  1.98s/it]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 20/20 [00:00<00:00, 14337.05it/s]\n",
      "\n",
      " 40%|████      | 2/5 [00:03<00:05,  1.96s/it]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 20/20 [00:00<00:00, 11213.22it/s]\n",
      "\n",
      " 60%|██████    | 3/5 [00:05<00:03,  1.91s/it]\u001b[A\n",
      "\n",
      "run: 3: 100%|██████████| 20/20 [00:00<00:00, 13747.31it/s]\n",
      "\n",
      " 80%|████████  | 4/5 [00:07<00:01,  1.88s/it]\u001b[A\n",
      "\n",
      "run: 4: 100%|██████████| 20/20 [00:00<00:00, 11178.85it/s]\n",
      "\n",
      "100%|██████████| 5/5 [00:09<00:00,  1.89s/it]\u001b[A\n",
      " 40%|████      | 2/5 [12:30<17:35, 351.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 500\n",
      "NN preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         1.0000002  0.99999994 1.         0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.99999994 0.99999994 1.         1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  1.         0.9999998  0.99999994 1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.        1.        1.0000001 0.9999998 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.9999999  1.0000001  0.9999999  1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 0.99999994 1.         0.99999994 0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.0000001  1.0000002  0.99999994 0.99999994 1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001 0.9999998 0.9999998 1.0000001 1.        0.9999998]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.9999998  1.         0.9999998  0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  0.9999999  0.99999994 1.         0.9999999  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.         0.99999994 1.0000002  0.9999998  1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.0000001  0.9999998  1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  0.99999994 0.99999994 1.         0.9999998  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         0.99999994 1.0000001  0.9999999  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.        1.0000001 0.9999999 0.9999999 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.9999999  1.0000001  0.99999994 1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         1.0000001  1.         0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.9999998  1.         0.9999999  0.99999994 1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999 1.        1.0000001 1.        1.        0.9999999]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.         1.         1.0000001  0.99999994 1.        ]\n",
      "MakeSCNNExpDicOwner Time:  1.5638361116871238\n",
      "MakeSCNNXExpDicUnknown Time:  79.02201850060374\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 20/20 [00:00<00:00, 6293.03it/s]\n",
      "\n",
      " 33%|███▎      | 1/3 [00:10<00:21, 10.95s/it]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 20/20 [00:00<00:00, 12953.38it/s]\n",
      "\n",
      " 67%|██████▋   | 2/3 [00:21<00:10, 10.95s/it]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 20/20 [00:00<00:00, 11753.69it/s]\n",
      "\n",
      "100%|██████████| 3/3 [00:32<00:00, 10.94s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 20/20 [00:00<00:00, 14093.76it/s]\n",
      "\n",
      " 20%|██        | 1/5 [00:00<00:03,  1.06it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 20/20 [00:00<00:00, 18436.50it/s]\n",
      "\n",
      " 40%|████      | 2/5 [00:01<00:02,  1.03it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 20/20 [00:00<00:00, 10508.09it/s]\n",
      "\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.05it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|██████████| 20/20 [00:00<00:00, 13883.83it/s]\n",
      "\n",
      " 80%|████████  | 4/5 [00:03<00:00,  1.06it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|██████████| 20/20 [00:00<00:00, 14963.62it/s]\n",
      "\n",
      "100%|██████████| 5/5 [00:04<00:00,  1.06it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [14:47<08:27, 253.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 750\n",
      "NN preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         1.0000002  0.99999994 1.         0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.99999994 0.99999994 1.         1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  1.         0.9999998  0.99999994 1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.        1.        1.0000001 0.9999998 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.9999999  1.0000001  0.9999999  1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 0.99999994 1.         0.99999994 0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.0000001  1.0000002  0.99999994 0.99999994 1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001 0.9999998 0.9999998 1.0000001 1.        0.9999998]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.9999998  1.         0.9999998  0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  0.9999999  0.99999994 1.         0.9999999  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.         0.99999994 1.0000002  0.9999998  1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.0000001  0.9999998  1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  0.99999994 0.99999994 1.         0.9999998  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         0.99999994 1.0000001  0.9999999  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.        1.0000001 0.9999999 0.9999999 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.9999999  1.0000001  0.99999994 1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         1.0000001  1.         0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.9999998  1.         0.9999999  0.99999994 1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999 1.        1.0000001 1.        1.        0.9999999]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.         1.         1.0000001  0.99999994 1.        ]\n",
      "MakeSCNNExpDicOwner Time:  1.435048682615161\n",
      "MakeSCNNXExpDicUnknown Time:  58.93310449179262\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 20/20 [00:00<00:00, 12242.57it/s]\n",
      "\n",
      " 33%|███▎      | 1/3 [00:08<00:17,  8.84s/it]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 20/20 [00:00<00:00, 18753.87it/s]\n",
      "\n",
      " 67%|██████▋   | 2/3 [00:17<00:08,  8.83s/it]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 20/20 [00:00<00:00, 21692.81it/s]\n",
      "\n",
      "100%|██████████| 3/3 [00:26<00:00,  8.78s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 20/20 [00:00<00:00, 16800.74it/s]\n",
      "\n",
      " 20%|██        | 1/5 [00:00<00:03,  1.17it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 20/20 [00:00<00:00, 17296.10it/s]\n",
      "\n",
      " 40%|████      | 2/5 [00:01<00:02,  1.19it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 20/20 [00:00<00:00, 14535.80it/s]\n",
      "\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.21it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|██████████| 20/20 [00:00<00:00, 13580.39it/s]\n",
      "\n",
      " 80%|████████  | 4/5 [00:03<00:00,  1.22it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|██████████| 20/20 [00:00<00:00, 13886.12it/s]\n",
      "\n",
      "100%|██████████| 5/5 [00:04<00:00,  1.20it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [16:30<03:14, 194.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1000\n",
      "NN preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         1.0000002  0.99999994 1.         0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.99999994 0.99999994 1.         1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  1.         0.9999998  0.99999994 1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.        1.        1.0000001 0.9999998 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.9999999  1.0000001  0.9999999  1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 0.99999994 1.         0.99999994 0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.0000001  1.0000002  0.99999994 0.99999994 1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001 0.9999998 0.9999998 1.0000001 1.        0.9999998]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.9999998  1.         0.9999998  0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  0.9999999  0.99999994 1.         0.9999999  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.         0.99999994 1.0000002  0.9999998  1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.0000001  0.9999998  1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  0.99999994 0.99999994 1.         0.9999998  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         0.99999994 1.0000001  0.9999999  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.        1.0000001 0.9999999 0.9999999 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.9999999  1.0000001  0.99999994 1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         1.0000001  1.         0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.9999998  1.         0.9999999  0.99999994 1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999 1.        1.0000001 1.        1.        0.9999999]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.         1.         1.0000001  0.99999994 1.        ]\n",
      "MakeSCNNExpDicOwner Time:  1.42898144479841\n",
      "MakeSCNNXExpDicUnknown Time:  51.47616907581687\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 20/20 [00:00<00:00, 17349.76it/s]\n",
      "\n",
      " 33%|███▎      | 1/3 [00:07<00:15,  7.93s/it]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 20/20 [00:00<00:00, 14548.40it/s]\n",
      "\n",
      " 67%|██████▋   | 2/3 [00:16<00:08,  8.06s/it]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 20/20 [00:00<00:00, 12939.39it/s]\n",
      "\n",
      "100%|██████████| 3/3 [00:24<00:00,  8.07s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 20/20 [00:00<00:00, 18141.45it/s]\n",
      "\n",
      " 20%|██        | 1/5 [00:00<00:03,  1.29it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 20/20 [00:00<00:00, 14327.26it/s]\n",
      "\n",
      " 40%|████      | 2/5 [00:01<00:02,  1.30it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 20/20 [00:00<00:00, 21487.21it/s]\n",
      "\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.34it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|██████████| 20/20 [00:00<00:00, 14126.99it/s]\n",
      "\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.37it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|██████████| 20/20 [00:00<00:00, 21178.01it/s]\n",
      "\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.35it/s]\u001b[A\n",
      "100%|██████████| 5/5 [18:01<00:00, 216.24s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 125\n",
      "NN preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.0000001 0.9999999 1.        1.0000001 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.99999994 1.0000001  1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         0.9999999  0.9999999  1.         1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  1.         0.9999998  1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.99999994 0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.9999998  1.         1.         1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.9999998  1.         1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.9999998  1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000002  1.0000001  1.0000002  1.         0.9999998  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.99999994 1.         0.99999994 0.9999999  0.99999994]\n",
      "len(X_exp1_dict_user_47[47]): 337\n",
      "len_exp2_user_47: 289\n",
      "MakeSCNNExpDicOwner Time:  1.012353296391666\n",
      "MakeSCNNXExpDicUnknown Time:  62.67061629425734\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 10/10 [00:00<00:00, 4439.36it/s]\n",
      "\n",
      " 20%|██        | 1/5 [00:00<00:03,  1.28it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 10/10 [00:00<00:00, 5263.28it/s]\n",
      "\n",
      " 40%|████      | 2/5 [00:01<00:02,  1.41it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 10/10 [00:00<00:00, 8360.18it/s]\n",
      "\n",
      " 60%|██████    | 3/5 [00:02<00:01,  1.41it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|██████████| 10/10 [00:00<00:00, 6820.01it/s]\n",
      "\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.40it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|██████████| 10/10 [00:00<00:00, 8535.42it/s]\n",
      "\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.42it/s]\u001b[A\n",
      " 20%|██        | 1/5 [01:43<06:55, 103.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 250\n",
      "NN preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.0000001 0.9999999 1.        1.0000001 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.99999994 1.0000001  1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         0.9999999  0.9999999  1.         1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  1.         0.9999998  1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.99999994 0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.9999998  1.         1.         1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.9999998  1.         1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.9999998  1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000002  1.0000001  1.0000002  1.         0.9999998  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.99999994 1.         0.99999994 0.9999999  0.99999994]\n",
      "len(X_exp1_dict_user_47[47]): 167\n",
      "len_exp2_user_47: 143\n",
      "MakeSCNNExpDicOwner Time:  0.8550648540258408\n",
      "MakeSCNNXExpDicUnknown Time:  33.21302902139723\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 10/10 [00:00<00:00, 3039.13it/s]\n",
      "\n",
      " 20%|██        | 1/5 [00:00<00:01,  2.36it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 10/10 [00:00<00:00, 8504.27it/s]\n",
      "\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.46it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 10/10 [00:00<00:00, 5206.43it/s]\n",
      "\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.51it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|██████████| 10/10 [00:00<00:00, 6066.39it/s]\n",
      "\n",
      " 80%|████████  | 4/5 [00:01<00:00,  2.55it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|██████████| 10/10 [00:00<00:00, 8656.97it/s]\n",
      "\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.53it/s]\u001b[A\n",
      " 40%|████      | 2/5 [02:38<03:44, 74.74s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 500\n",
      "NN preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.0000001 0.9999999 1.        1.0000001 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.99999994 1.0000001  1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         0.9999999  0.9999999  1.         1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  1.         0.9999998  1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.99999994 0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.9999998  1.         1.         1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.9999998  1.         1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.9999998  1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000002  1.0000001  1.0000002  1.         0.9999998  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.99999994 1.         0.99999994 0.9999999  0.99999994]\n",
      "len(X_exp1_dict_user_47[47]): 83\n",
      "len_exp2_user_47: 71\n",
      "MakeSCNNExpDicOwner Time:  0.7249470530077815\n",
      "MakeSCNNXExpDicUnknown Time:  19.36979872826487\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 10/10 [00:00<00:00, 8640.92it/s]\n",
      "\n",
      " 20%|██        | 1/5 [00:00<00:01,  3.45it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 10/10 [00:00<00:00, 8540.63it/s]\n",
      "\n",
      " 40%|████      | 2/5 [00:00<00:00,  3.57it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 10/10 [00:00<00:00, 7616.31it/s]\n",
      "\n",
      " 60%|██████    | 3/5 [00:00<00:00,  3.49it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|██████████| 10/10 [00:00<00:00, 7423.55it/s]\n",
      "\n",
      " 80%|████████  | 4/5 [00:01<00:00,  3.54it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|██████████| 10/10 [00:00<00:00, 7788.87it/s]\n",
      "\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.55it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [03:08<01:49, 54.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 750\n",
      "NN preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.0000001 0.9999999 1.        1.0000001 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.99999994 1.0000001  1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         0.9999999  0.9999999  1.         1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  1.         0.9999998  1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.99999994 0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.9999998  1.         1.         1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.9999998  1.         1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.9999998  1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000002  1.0000001  1.0000002  1.         0.9999998  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.99999994 1.         0.99999994 0.9999999  0.99999994]\n",
      "len(X_exp1_dict_user_47[47]): 55\n",
      "len_exp2_user_47: 47\n",
      "MakeSCNNExpDicOwner Time:  0.6344783343374729\n",
      "MakeSCNNXExpDicUnknown Time:  15.684475477784872\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 10/10 [00:00<00:00, 8981.38it/s]\n",
      "\n",
      " 20%|██        | 1/5 [00:00<00:01,  3.84it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 10/10 [00:00<00:00, 7262.86it/s]\n",
      "\n",
      " 40%|████      | 2/5 [00:00<00:00,  3.89it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 10/10 [00:00<00:00, 6912.17it/s]\n",
      "\n",
      " 60%|██████    | 3/5 [00:00<00:00,  3.91it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|██████████| 10/10 [00:00<00:00, 7010.37it/s]\n",
      "\n",
      " 80%|████████  | 4/5 [00:01<00:00,  3.97it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|██████████| 10/10 [00:00<00:00, 14112.73it/s]\n",
      "\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.88it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [03:32<00:42, 42.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1000\n",
      "NN preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.0000001 0.9999999 1.        1.0000001 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.99999994 1.0000001  1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         0.9999999  0.9999999  1.         1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  1.         0.9999998  1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.99999994 0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.9999998  1.         1.         1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.9999998  1.         1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         0.9999998  1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000002  1.0000001  1.0000002  1.         0.9999998  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.99999994 1.         0.99999994 0.9999999  0.99999994]\n",
      "len(X_exp1_dict_user_47[47]): 41\n",
      "len_exp2_user_47: 35\n",
      "MakeSCNNExpDicOwner Time:  0.62574119027704\n",
      "MakeSCNNXExpDicUnknown Time:  12.568344140425324\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|██████████| 10/10 [00:00<00:00, 9416.94it/s]\n",
      "\n",
      " 20%|██        | 1/5 [00:00<00:00,  4.23it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|██████████| 10/10 [00:00<00:00, 6340.60it/s]\n",
      "\n",
      " 40%|████      | 2/5 [00:00<00:00,  4.22it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|██████████| 10/10 [00:00<00:00, 7451.24it/s]\n",
      "\n",
      " 60%|██████    | 3/5 [00:00<00:00,  4.27it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|██████████| 10/10 [00:00<00:00, 9633.22it/s]\n",
      "\n",
      " 80%|████████  | 4/5 [00:00<00:00,  4.26it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|██████████| 10/10 [00:00<00:00, 8217.68it/s]\n",
      "\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.25it/s]\u001b[A\n",
      "100%|██████████| 5/5 [03:52<00:00, 46.42s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 125\n",
      "NN preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         1.0000002  0.99999994 1.         0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.99999994 0.99999994 1.         1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  1.         0.9999998  0.99999994 1.         1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.        1.        1.0000001 0.9999998 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999  0.9999999  1.0000001  0.9999999  1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 0.99999994 1.         0.99999994 0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.0000001  1.0000002  0.99999994 0.99999994 1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001 0.9999998 0.9999998 1.0000001 1.        0.9999998]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.9999998  1.         0.9999998  0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  0.9999999  0.99999994 1.         0.9999999  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.         0.99999994 1.0000002  0.9999998  1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.0000001  0.9999998  1.         0.99999994 1.        ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.0000001  0.99999994 0.99999994 1.         0.9999998  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 1.         0.99999994 1.0000001  0.9999999  0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.        1.        1.0000001 0.9999999 0.9999999 1.       ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.9999999  1.0000001  0.99999994 1.         0.99999994]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         0.99999994 1.         1.0000001  1.         0.9999999 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.99999994 0.9999998  1.         0.9999999  0.99999994 1.0000001 ]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[0.9999999 1.        1.0000001 1.        1.        0.9999999]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[1.         1.         1.         1.0000001  0.99999994 1.        ]\n",
      "MakeSCNNExpDicOwner Time:  2.3246253980323672\n"
     ]
    }
   ],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "P.smoothing = \"Butterworth\"\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "\n",
    "time_of_execution = time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "clf_type=\"standalone\"\n",
    "\n",
    "\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "\n",
    "\n",
    "\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    \n",
    "    train_set, test_set=THREE_FOLD_CV[cv_fold_idx]\n",
    "    print(f\"train_set: {train_set}\")\n",
    "    print(f\"test_set: {test_set}\")\n",
    "    \n",
    "    #--------------butter33----------------------\n",
    "    P.cut_off_freq=33\n",
    "\n",
    "    P.Butter_per_win_argdict={\n",
    "        \"filter_order\": P.filter_order,\n",
    "        \"cut_off_freq\": P.cut_off_freq,\n",
    "        \"sampling_freq\": P.sampling_freq,\n",
    "        \"filtfilt\": P.filtfilt,\n",
    "        }   \n",
    "\n",
    "\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "\n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "\n",
    "    #--------------butter33----------------------\n",
    "\n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": ffted_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": ffted_dfList_exp2_user_47\n",
    "    }\n",
    "        \n",
    "        \n",
    "\n",
    "    exp_deep_archi_extractors_dict=get_exp_deep_feature_extractors_dict(exp_num=EXP_NUM_1_2, \n",
    "                                                                        model_architectures=list(TRAINING_CONFIG_CV_DICT[\"Butter\"][cv_fold_idx]['1000'].keys()), \n",
    "                                                                        exp_path_name=EXP_PATH_NAME,\n",
    "                                                                        window_size_lst=WINDOW_SIZE_LST_NN,\n",
    "                                                                        variant_suffix=f\"Butter33-cv{cv_fold_idx}\")\n",
    "        \n",
    "\n",
    "    key_column= [\"cut_off_freq\"]\n",
    "    # -----CV_FOLD-------\n",
    "    # for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_OCSVM_LOF_IF_NN(\\\n",
    "                                    cv_fold_idx=cv_fold_idx, \n",
    "                                    cv_sets=THREE_FOLD_CV, \n",
    "                                    dfList_dict=dfList_dict, \n",
    "                                    window_size_lst=WINDOW_SIZE_LST_NN, \n",
    "                                    exp_config=P, \n",
    "                                    exp_deep_archi_extractors_dict=exp_deep_archi_extractors_dict, \n",
    "                                    overlap=OVERLAP, \n",
    "                                    param_dist=param_dist, \n",
    "                                    train_file_name=train_file_name, \n",
    "                                    test_file_name=test_file_name, \n",
    "                                    preprocessing_params=P.cut_off_freq,\n",
    "                                    key_column=key_column,\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    del exp_deep_archi_extractors_dict\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butterworth\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P.Butter_per_win_argdict={\n",
    "    \"filter_order\": P.filter_order,\n",
    "    \"cut_off_freq\": None,\n",
    "    \"sampling_freq\": P.sampling_freq,\n",
    "    \"filtfilt\": P.filtfilt,\n",
    "    }\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for cut_off_freq in tqdm(list(range(48, 50))):\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.Butter_per_win_argdict[\"cut_off_freq\"]=cut_off_freq\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "\n",
    "    \n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    \n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": ffted_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": ffted_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.cut_off_freq] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                                    extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                                    param_dist=param_dist)\n",
    "        \n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq: {P.cut_off_freq}\\n\")\n",
    "        f.write(EER_df_train_dict[P.cut_off_freq].to_string())\n",
    "\n",
    "\n",
    "\n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Butterworth frequency Cut-off + EMA span\n",
    "## 2.1 Naive Approach\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_2)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butter+EMA\"\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "indices = list(range(1, 50))\n",
    "mesh = np.array(np.meshgrid(indices, indices))\n",
    "index_pairs = mesh.T.reshape(-1, 2)\n",
    "\n",
    "print(f\"total cut_off_span_pairs: {index_pairs.shape}, choice_num: {choice_num}\")\n",
    "cut_off_span_pairs = index_pairs[np.random.choice(index_pairs.shape[0], size=CHOICE_NUM_PAIRS, replace=False), :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for key_pair in tqdm(cut_off_span_pairs):\n",
    "    \n",
    "    key_pair = tuple(key_pair)\n",
    "    cut_off_freq, span = key_pair[0], key_pair[1]\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.span=span\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "    print(f\"span: {P.span}\")\n",
    "\n",
    "    \n",
    "    ffted_dfList_exp1 = get_ffted_dfList(raw_dfList_exp1, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    EMAed_dfList_exp1 = get_EMAed_dfList(ffted_dfList_exp1, span=P.span)\n",
    "    EMAed_dfList_exp2 = get_EMAed_dfList(ffted_dfList_exp2, span=P.span)\n",
    "    \n",
    "    ffted_dfList_exp1_user_47 = get_ffted_dfList(raw_dfList_exp1_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    EMAed_dfList_exp1_user_47 = get_EMAed_dfList(ffted_dfList_exp1_user_47, span=P.span)\n",
    "    EMAed_dfList_exp2_user_47 = get_EMAed_dfList(ffted_dfList_exp2_user_47, span=P.span)\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": EMAed_dfList_exp1,\n",
    "                \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": EMAed_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[key_pair] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                      extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                      param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq, span: {key_pair}\\n\")\n",
    "        f.write(EER_df_train_dict[key_pair].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\", \"EMA_span\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_2)\n",
    "P.smoothing = \"Butter+EMA\"\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "\n",
    "time_of_execution = time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "clf_type=\"standalone\"\n",
    "\n",
    "\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "\n",
    "\n",
    "\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    \n",
    "    train_set, test_set=THREE_FOLD_CV[cv_fold_idx]\n",
    "    print(f\"train_set: {train_set}\")\n",
    "    print(f\"test_set: {test_set}\")\n",
    "    \n",
    "    #--------------butter33-EMA20----------------------\n",
    "\n",
    "    P.cut_off_freq=33\n",
    "    P.span=20\n",
    "\n",
    "    P.Butter_per_win_argdict={\n",
    "        \"filter_order\": P.filter_order,\n",
    "        \"cut_off_freq\": P.cut_off_freq,\n",
    "        \"sampling_freq\": P.sampling_freq,\n",
    "        \"filtfilt\": P.filtfilt,\n",
    "        }   \n",
    "\n",
    "    P.EMA_per_win_span=P.span\n",
    "\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}, EMA span: {P.span}\")\n",
    "\n",
    "\n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    EMAed_dfList_exp2 = get_EMAed_dfList(ffted_dfList_exp2, span=P.span)\n",
    "\n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    EMAed_dfList_exp2_user_47 = get_EMAed_dfList(ffted_dfList_exp2_user_47, span=P.span)\n",
    "\n",
    "    #--------------butter33-EMA20----------------------\n",
    "\n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "    }\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    exp_deep_archi_extractors_dict=get_exp_deep_feature_extractors_dict(exp_num=EXP_NUM_2_2, \n",
    "                                                                        model_architectures=list(TRAINING_CONFIG_CV_DICT[\"Butter-EMA\"][cv_fold_idx]['1000'].keys()), \n",
    "                                                                        exp_path_name=EXP_PATH_NAME,\n",
    "                                                                        window_size_lst=WINDOW_SIZE_LST_NN,\n",
    "                                                                        variant_suffix=f\"Butter33-EMA20-cv{cv_fold_idx}\")\n",
    "        \n",
    "\n",
    "    key_column= [\"cut_off_freq\", \"EMA_span\"]\n",
    "    # -----CV_FOLD-------\n",
    "    # for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_OCSVM_LOF_IF_NN(\\\n",
    "                                    cv_fold_idx=cv_fold_idx, \n",
    "                                    cv_sets=THREE_FOLD_CV, \n",
    "                                    dfList_dict=dfList_dict, \n",
    "                                    window_size_lst=WINDOW_SIZE_LST_NN, \n",
    "                                    exp_config=P, \n",
    "                                    exp_deep_archi_extractors_dict=exp_deep_archi_extractors_dict, \n",
    "                                    overlap=OVERLAP, \n",
    "                                    param_dist=param_dist, \n",
    "                                    train_file_name=train_file_name, \n",
    "                                    test_file_name=test_file_name, \n",
    "                                    preprocessing_params=(P.cut_off_freq, P.EMA_per_win_span),\n",
    "                                    key_column=key_column,\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    del exp_deep_archi_extractors_dict\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_2)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butter+EMA\"\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "indices = list(range(1, 50))\n",
    "mesh = np.array(np.meshgrid(indices, indices))\n",
    "index_pairs = mesh.T.reshape(-1, 2)\n",
    "\n",
    "print(f\"total cut_off_span_pairs: {index_pairs.shape}, choice_num: {choice_num}\")\n",
    "cut_off_span_pairs = index_pairs[np.random.choice(index_pairs.shape[0], size=CHOICE_NUM_PAIRS, replace=False), :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P.Butter_per_win_argdict={\n",
    "    \"filter_order\": P.filter_order,\n",
    "    \"cut_off_freq\": None,\n",
    "    \"sampling_freq\": P.sampling_freq,\n",
    "    \"filtfilt\": P.filtfilt,\n",
    "    }\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for key_pair in tqdm(cut_off_span_pairs):\n",
    "    key_pair = tuple(key_pair)\n",
    "    cut_off_freq, span = key_pair[0], key_pair[1]\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.Butter_per_win_argdict[\"cut_off_freq\"]=cut_off_freq\n",
    "    \n",
    "    \n",
    "    P.span=span\n",
    "    P.EMA_per_win_span=span\n",
    "\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "    print(f\"span: {P.span}\")\n",
    "\n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    EMAed_dfList_exp2 = get_EMAed_dfList(ffted_dfList_exp2, span=P.span)\n",
    "    \n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    EMAed_dfList_exp2_user_47 = get_EMAed_dfList(ffted_dfList_exp2_user_47, span=P.span)\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[key_pair] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                      extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                      param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq, span: {key_pair}\\n\")\n",
    "        f.write(EER_df_train_dict[key_pair].to_string())\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\", \"EMA_span\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. EMA span\n",
    "## 3.1 Naive Approach\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"EMA\"\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for span in tqdm(list(range(48, 50))):\n",
    "    P.span=span\n",
    "    print(f\"EMA span: {P.span}\")\n",
    "\n",
    "    \n",
    "    EMAed_dfList_exp1 = get_EMAed_dfList(raw_dfList_exp1, span=P.span)\n",
    "    EMAed_dfList_exp2 = get_EMAed_dfList(raw_dfList_exp2, span=P.span)\n",
    "    \n",
    "    EMAed_dfList_exp1_user_47 = get_EMAed_dfList(raw_dfList_exp1_user_47, span=P.span)\n",
    "    EMAed_dfList_exp2_user_47 = get_EMAed_dfList(raw_dfList_exp2_user_47, span=P.span)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": EMAed_dfList_exp1,\n",
    "                \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": EMAed_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.span] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                    extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                    param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\EMA span: {P.span}\\n\")\n",
    "        f.write(EER_df_train_dict[P.span].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"EMA_span\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"EMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for span in tqdm(list(range(48, 50))):\n",
    "    P.span=span\n",
    "    print(f\"EMA span: {P.span}\")\n",
    "\n",
    "    P.EMA_per_win_span=P.span\n",
    "\n",
    "    EMAed_dfList_exp2 = get_EMAed_dfList(raw_dfList_exp2, span=P.span)\n",
    "    \n",
    "    EMAed_dfList_exp2_user_47 = get_EMAed_dfList(raw_dfList_exp2_user_47, span=P.span)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.span] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                    extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                    param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\EMA span: {P.span}\\n\")\n",
    "        f.write(EER_df_train_dict[P.span].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"EMA_span\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SMA winsize\n",
    "## 4.1 Naive Approach\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"SMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for winsize in tqdm(list(range(48, 50))):\n",
    "    P.winsize=winsize\n",
    "    print(f\"SMA winsize: {P.winsize}\")\n",
    "\n",
    "\n",
    "    SMAed_dfList_exp1 = get_SMAed_dfList(raw_dfList_exp1, winsize=P.winsize)\n",
    "    SMAed_dfList_exp2 = get_SMAed_dfList(raw_dfList_exp2, winsize=P.winsize)\n",
    "    \n",
    "    SMAed_dfList_exp1_user_47 = get_SMAed_dfList(raw_dfList_exp1_user_47, winsize=P.winsize)\n",
    "    SMAed_dfList_exp2_user_47 = get_SMAed_dfList(raw_dfList_exp2_user_47, winsize=P.winsize)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": SMAed_dfList_exp1,\n",
    "                \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": SMAed_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.winsize] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                       extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                       param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\SMA winsize: {P.winsize}\\n\")\n",
    "        f.write(EER_df_train_dict[P.winsize].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"SMA_winsize\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"SMA\"\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for winsize in tqdm(list(range(48, 50))):\n",
    "    P.winsize=winsize\n",
    "    P.SMA_per_win_winsize=P.winsize\n",
    "\n",
    "    print(f\"SMA winsize: {P.winsize}\")\n",
    "\n",
    "\n",
    "    SMAed_dfList_exp2 = get_SMAed_dfList(raw_dfList_exp2, winsize=P.winsize)\n",
    "    \n",
    "    SMAed_dfList_exp2_user_47 = get_SMAed_dfList(raw_dfList_exp2_user_47, winsize=P.winsize)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.winsize] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                       extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                       param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\SMA winsize: {P.winsize}\\n\")\n",
    "        f.write(EER_df_train_dict[P.winsize].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"SMA_winsize\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Butterworth frequency Cut-off + SMA winsize\n",
    "## 5.1 Naive Approach\n",
    "### Optimizing and Testin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_5)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butter+SMA\"\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "indices = list(range(1, 50))\n",
    "mesh = np.array(np.meshgrid(indices, indices))\n",
    "index_pairs = mesh.T.reshape(-1, 2)\n",
    "\n",
    "print(f\"total cut_off_winsize_pairs: {index_pairs.shape}, choice_num: {choice_num}\")\n",
    "cut_off_winsize_pairs = index_pairs[np.random.choice(index_pairs.shape[0], size=CHOICE_NUM_PAIRS, replace=False), :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for key_pair in tqdm(cut_off_winsize_pairs):\n",
    "    \n",
    "    key_pair = tuple(key_pair)\n",
    "    cut_off_freq, winsize = key_pair[0], key_pair[1]\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.winsize=winsize\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "    print(f\"winsize: {P.winsize}\")\n",
    "\n",
    "    \n",
    "    ffted_dfList_exp1 = get_ffted_dfList(raw_dfList_exp1, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    SMAed_dfList_exp1 = get_SMAed_dfList(ffted_dfList_exp1, winsize=P.winsize)\n",
    "    SMAed_dfList_exp2 = get_SMAed_dfList(ffted_dfList_exp2, winsize=P.winsize)\n",
    "    \n",
    "    ffted_dfList_exp1_user_47 = get_ffted_dfList(raw_dfList_exp1_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    SMAed_dfList_exp1_user_47 = get_SMAed_dfList(ffted_dfList_exp1_user_47, winsize=P.winsize)\n",
    "    SMAed_dfList_exp2_user_47 = get_SMAed_dfList(ffted_dfList_exp2_user_47, winsize=P.winsize)\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": SMAed_dfList_exp1,\n",
    "                \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": SMAed_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[key_pair] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                      extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                      param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq, winsize: {key_pair}\\n\")\n",
    "        f.write(EER_df_train_dict[key_pair].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\", \"SMA_winsize\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_5)\n",
    "P.smoothing = \"Butter+SMA\"\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "\n",
    "time_of_execution = time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "\n",
    "clf_type=\"standalone\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    \n",
    "    train_set, test_set=THREE_FOLD_CV[cv_fold_idx]\n",
    "    print(f\"train_set: {train_set}\")\n",
    "    print(f\"test_set: {test_set}\")\n",
    "    \n",
    "    # --------------butter33-SMA20----------------------\n",
    "    P.winsize=20\n",
    "    P.cut_off_freq=33\n",
    "\n",
    "    P.Butter_per_win_argdict={\n",
    "        \"filter_order\": P.filter_order,\n",
    "        \"cut_off_freq\": P.cut_off_freq,\n",
    "        \"sampling_freq\": P.sampling_freq,\n",
    "        \"filtfilt\": P.filtfilt,\n",
    "         }   \n",
    "    P.SMA_per_win_winsize=P.winsize\n",
    "\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}, winsize: {P.winsize}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    SMAed_dfList_exp2 = get_SMAed_dfList(ffted_dfList_exp2, winsize=P.winsize)\n",
    "\n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    SMAed_dfList_exp2_user_47 = get_SMAed_dfList(ffted_dfList_exp2_user_47, winsize=P.winsize)\n",
    "\n",
    "\n",
    "    # --------------butter33-SMA20----------------------\n",
    "\n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "    }\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    exp_deep_archi_extractors_dict=get_exp_deep_feature_extractors_dict(exp_num=EXP_NUM_5_2, \n",
    "                                                                        model_architectures=list(TRAINING_CONFIG_CV_DICT[\"Butter-SMA\"][cv_fold_idx]['1000'].keys()), \n",
    "                                                                        exp_path_name=EXP_PATH_NAME,\n",
    "                                                                        window_size_lst=WINDOW_SIZE_LST_NN,\n",
    "                                                                        variant_suffix=f\"Butter33-SMA20-cv{cv_fold_idx}\")\n",
    "        \n",
    "\n",
    "    key_column= [\"cut_off_freq\", \"SMA_winsize\"]\n",
    "    # -----CV_FOLD-------\n",
    "    # for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_OCSVM_LOF_IF_NN(\\\n",
    "                                    cv_fold_idx=cv_fold_idx, \n",
    "                                    cv_sets=THREE_FOLD_CV, \n",
    "                                    dfList_dict=dfList_dict, \n",
    "                                    window_size_lst=WINDOW_SIZE_LST_NN, \n",
    "                                    exp_config=P, \n",
    "                                    exp_deep_archi_extractors_dict=exp_deep_archi_extractors_dict, \n",
    "                                    overlap=OVERLAP, \n",
    "                                    param_dist=param_dist, \n",
    "                                    train_file_name=train_file_name, \n",
    "                                    test_file_name=test_file_name, \n",
    "                                    preprocessing_params=(P.cut_off_freq, P.SMA_per_win_winsize),\n",
    "                                    key_column=key_column,\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    del exp_deep_archi_extractors_dict\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_5)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butter+SMA\"\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "indices = list(range(1, 50))\n",
    "mesh = np.array(np.meshgrid(indices, indices))\n",
    "index_pairs = mesh.T.reshape(-1, 2)\n",
    "\n",
    "print(f\"total cut_off_winsize_pairs: {index_pairs.shape}, choice_num: {choice_num}\")\n",
    "cut_off_winsize_pairs = index_pairs[np.random.choice(index_pairs.shape[0], size=CHOICE_NUM_PAIRS, replace=False), :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P.Butter_per_win_argdict={\n",
    "    \"filter_order\": P.filter_order,\n",
    "    \"cut_off_freq\": None,\n",
    "    \"sampling_freq\": P.sampling_freq,\n",
    "    \"filtfilt\": P.filtfilt,\n",
    "    }\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for key_pair in tqdm(cut_off_winsize_pairs):\n",
    "    \n",
    "    key_pair = tuple(key_pair)\n",
    "    cut_off_freq, winsize = key_pair[0], key_pair[1]\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.Butter_per_win_argdict[\"cut_off_freq\"]=P.cut_off_freq\n",
    "    P.winsize=winsize\n",
    "    P.SMA_per_win_winsize=P.winsize\n",
    "    \n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "    print(f\"winsize: {P.winsize}\")\n",
    "\n",
    "    \n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    SMAed_dfList_exp2 = get_SMAed_dfList(ffted_dfList_exp2, winsize=P.winsize)\n",
    "    \n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    SMAed_dfList_exp2_user_47 = get_SMAed_dfList(ffted_dfList_exp2_user_47, winsize=P.winsize)\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[key_pair] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                      extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                      param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq, winsize: {key_pair}\\n\")\n",
    "        f.write(EER_df_train_dict[key_pair].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\", \"SMA_winsize\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. The effect of Varying Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "\n",
    "\n",
    "\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/overlap_Mean_EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/overlap_Mean_EER_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for overlap in tqdm(list(range(1, 2))):\n",
    "    overlap*=0.01\n",
    "    max_window_size=2000\n",
    "    step_width = int(max_window_size * (1-overlap))\n",
    "    max_num_windows=max(len(getIndices(sampleSize=max_window_size, step=step_width, numSamplePoints=P.num_sample_points_per_exp)), 100)\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": raw_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": raw_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[overlap] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                     extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                     param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\overlap: {overlap}\\n\")\n",
    "        f.write(EER_df_train_dict[overlap].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"overlap\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"6\" halign=\"left\"></th>\n",
       "      <th colspan=\"5\" halign=\"left\">Window Size</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean rank</th>\n",
       "      <th>scnn_archi_name</th>\n",
       "      <th>cut_off_freq</th>\n",
       "      <th>EMA_span</th>\n",
       "      <th>SMA_winsize</th>\n",
       "      <th>type</th>\n",
       "      <th>125</th>\n",
       "      <th>250</th>\n",
       "      <th>500</th>\n",
       "      <th>750</th>\n",
       "      <th>1000</th>\n",
       "      <th>mean</th>\n",
       "      <th>variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>SCNN_1_3_conv_1_dense_arg_dict_default</td>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.179794</td>\n",
       "      <td>0.137370</td>\n",
       "      <td>0.100089</td>\n",
       "      <td>0.092929</td>\n",
       "      <td>0.080759</td>\n",
       "      <td>0.118188</td>\n",
       "      <td>0.001632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>SCNN_3_123_conv_1_dense_arg_dict_default</td>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.147538</td>\n",
       "      <td>0.110690</td>\n",
       "      <td>0.065016</td>\n",
       "      <td>0.052929</td>\n",
       "      <td>0.071861</td>\n",
       "      <td>0.089607</td>\n",
       "      <td>0.001516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>SCNN_3_1_conv_1_dense_arg_dict_default</td>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.149461</td>\n",
       "      <td>0.101918</td>\n",
       "      <td>0.073516</td>\n",
       "      <td>0.068855</td>\n",
       "      <td>0.043857</td>\n",
       "      <td>0.087522</td>\n",
       "      <td>0.001624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>SCNN_3_2_conv_1_dense_arg_dict_default</td>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.153017</td>\n",
       "      <td>0.147594</td>\n",
       "      <td>0.094467</td>\n",
       "      <td>0.086599</td>\n",
       "      <td>0.066712</td>\n",
       "      <td>0.109678</td>\n",
       "      <td>0.001481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>SCNN_1_3_conv_1_dense_arg_dict_default</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.152583</td>\n",
       "      <td>0.103116</td>\n",
       "      <td>0.072289</td>\n",
       "      <td>0.077576</td>\n",
       "      <td>0.072945</td>\n",
       "      <td>0.095702</td>\n",
       "      <td>0.001171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>SCNN_3_123_conv_1_dense_arg_dict_default</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.157440</td>\n",
       "      <td>0.103260</td>\n",
       "      <td>0.061490</td>\n",
       "      <td>0.052828</td>\n",
       "      <td>0.043993</td>\n",
       "      <td>0.083802</td>\n",
       "      <td>0.002211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>SCNN_3_1_conv_1_dense_arg_dict_default</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.156748</td>\n",
       "      <td>0.089643</td>\n",
       "      <td>0.068005</td>\n",
       "      <td>0.051313</td>\n",
       "      <td>0.040605</td>\n",
       "      <td>0.081263</td>\n",
       "      <td>0.002124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>SCNN_3_2_conv_1_dense_arg_dict_default</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.148220</td>\n",
       "      <td>0.095664</td>\n",
       "      <td>0.070080</td>\n",
       "      <td>0.061414</td>\n",
       "      <td>0.052078</td>\n",
       "      <td>0.085491</td>\n",
       "      <td>0.001493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>SCNN_1_3_conv_1_dense_arg_dict_default</td>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>20</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.144411</td>\n",
       "      <td>0.106143</td>\n",
       "      <td>0.076417</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.050587</td>\n",
       "      <td>0.093512</td>\n",
       "      <td>0.001224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>SCNN_3_123_conv_1_dense_arg_dict_default</td>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>20</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.160369</td>\n",
       "      <td>0.090131</td>\n",
       "      <td>0.069946</td>\n",
       "      <td>0.057879</td>\n",
       "      <td>0.051355</td>\n",
       "      <td>0.085936</td>\n",
       "      <td>0.001949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>SCNN_3_1_conv_1_dense_arg_dict_default</td>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>20</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.156583</td>\n",
       "      <td>0.095365</td>\n",
       "      <td>0.069143</td>\n",
       "      <td>0.055185</td>\n",
       "      <td>0.054517</td>\n",
       "      <td>0.086159</td>\n",
       "      <td>0.001824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7</td>\n",
       "      <td>SCNN_3_2_conv_1_dense_arg_dict_default</td>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>20</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.145714</td>\n",
       "      <td>0.110778</td>\n",
       "      <td>0.072713</td>\n",
       "      <td>0.061044</td>\n",
       "      <td>0.053026</td>\n",
       "      <td>0.088655</td>\n",
       "      <td>0.001508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              \\\n",
       "   mean rank                           scnn_archi_name cut_off_freq EMA_span   \n",
       "0         12    SCNN_1_3_conv_1_dense_arg_dict_default           33        -   \n",
       "1          8  SCNN_3_123_conv_1_dense_arg_dict_default           33        -   \n",
       "2          6    SCNN_3_1_conv_1_dense_arg_dict_default           33        -   \n",
       "3         11    SCNN_3_2_conv_1_dense_arg_dict_default           33        -   \n",
       "4         10    SCNN_1_3_conv_1_dense_arg_dict_default           33       20   \n",
       "5          2  SCNN_3_123_conv_1_dense_arg_dict_default           33       20   \n",
       "6          1    SCNN_3_1_conv_1_dense_arg_dict_default           33       20   \n",
       "7          3    SCNN_3_2_conv_1_dense_arg_dict_default           33       20   \n",
       "8          9    SCNN_1_3_conv_1_dense_arg_dict_default           33        -   \n",
       "9          4  SCNN_3_123_conv_1_dense_arg_dict_default           33        -   \n",
       "10         5    SCNN_3_1_conv_1_dense_arg_dict_default           33        -   \n",
       "11         7    SCNN_3_2_conv_1_dense_arg_dict_default           33        -   \n",
       "\n",
       "                     Window Size                                          \\\n",
       "   SMA_winsize  type         125       250       500       750      1000   \n",
       "0            -  Real    0.179794  0.137370  0.100089  0.092929  0.080759   \n",
       "1            -  Real    0.147538  0.110690  0.065016  0.052929  0.071861   \n",
       "2            -  Real    0.149461  0.101918  0.073516  0.068855  0.043857   \n",
       "3            -  Real    0.153017  0.147594  0.094467  0.086599  0.066712   \n",
       "4            -  Real    0.152583  0.103116  0.072289  0.077576  0.072945   \n",
       "5            -  Real    0.157440  0.103260  0.061490  0.052828  0.043993   \n",
       "6            -  Real    0.156748  0.089643  0.068005  0.051313  0.040605   \n",
       "7            -  Real    0.148220  0.095664  0.070080  0.061414  0.052078   \n",
       "8           20  Real    0.144411  0.106143  0.076417  0.090000  0.050587   \n",
       "9           20  Real    0.160369  0.090131  0.069946  0.057879  0.051355   \n",
       "10          20  Real    0.156583  0.095365  0.069143  0.055185  0.054517   \n",
       "11          20  Real    0.145714  0.110778  0.072713  0.061044  0.053026   \n",
       "\n",
       "                        \n",
       "        mean  variance  \n",
       "0   0.118188  0.001632  \n",
       "1   0.089607  0.001516  \n",
       "2   0.087522  0.001624  \n",
       "3   0.109678  0.001481  \n",
       "4   0.095702  0.001171  \n",
       "5   0.083802  0.002211  \n",
       "6   0.081263  0.002124  \n",
       "7   0.085491  0.001493  \n",
       "8   0.093512  0.001224  \n",
       "9   0.085936  0.001949  \n",
       "10  0.086159  0.001824  \n",
       "11  0.088655  0.001508  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=return_and_save_final_result_df_as_json_NN(final_exp_results_path=FINAL_EXP_RESULTS_PATH, exp_path_name=EXP_PATH_NAME, window_size_lst=WINDOW_SIZE_LST_NN)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"6\" halign=\"left\"></th>\n",
       "      <th colspan=\"5\" halign=\"left\">Window Size</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean rank</th>\n",
       "      <th>scnn_archi_name</th>\n",
       "      <th>cut_off_freq</th>\n",
       "      <th>EMA_span</th>\n",
       "      <th>SMA_winsize</th>\n",
       "      <th>type</th>\n",
       "      <th>125</th>\n",
       "      <th>250</th>\n",
       "      <th>500</th>\n",
       "      <th>750</th>\n",
       "      <th>1000</th>\n",
       "      <th>mean</th>\n",
       "      <th>variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>SCNN_1_3_conv_1_dense_arg_dict_default</td>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>SCNN_3_123_conv_1_dense_arg_dict_default</td>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>17.940646</td>\n",
       "      <td>19.422021</td>\n",
       "      <td>35.042354</td>\n",
       "      <td>43.043478</td>\n",
       "      <td>11.017897</td>\n",
       "      <td>24.183072</td>\n",
       "      <td>7.111830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>SCNN_3_1_conv_1_dense_arg_dict_default</td>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>16.870931</td>\n",
       "      <td>25.807233</td>\n",
       "      <td>26.549264</td>\n",
       "      <td>25.905797</td>\n",
       "      <td>45.693512</td>\n",
       "      <td>25.947219</td>\n",
       "      <td>0.533492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>SCNN_3_2_conv_1_dense_arg_dict_default</td>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>14.893487</td>\n",
       "      <td>-7.442686</td>\n",
       "      <td>5.617477</td>\n",
       "      <td>6.811594</td>\n",
       "      <td>17.393736</td>\n",
       "      <td>7.200899</td>\n",
       "      <td>9.247688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>SCNN_1_3_conv_1_dense_arg_dict_default</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>15.134937</td>\n",
       "      <td>24.935421</td>\n",
       "      <td>27.775301</td>\n",
       "      <td>16.521739</td>\n",
       "      <td>9.675615</td>\n",
       "      <td>19.026084</td>\n",
       "      <td>28.252270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>SCNN_3_123_conv_1_dense_arg_dict_default</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>12.433143</td>\n",
       "      <td>24.830481</td>\n",
       "      <td>38.564423</td>\n",
       "      <td>43.152174</td>\n",
       "      <td>45.525727</td>\n",
       "      <td>29.094172</td>\n",
       "      <td>-35.427915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>SCNN_3_1_conv_1_dense_arg_dict_default</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>12.818240</td>\n",
       "      <td>34.743300</td>\n",
       "      <td>32.055283</td>\n",
       "      <td>44.782609</td>\n",
       "      <td>49.720358</td>\n",
       "      <td>31.242833</td>\n",
       "      <td>-30.080712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>SCNN_3_2_conv_1_dense_arg_dict_default</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>17.561661</td>\n",
       "      <td>30.360026</td>\n",
       "      <td>29.982167</td>\n",
       "      <td>33.913043</td>\n",
       "      <td>35.514541</td>\n",
       "      <td>27.665270</td>\n",
       "      <td>8.542436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>SCNN_1_3_conv_1_dense_arg_dict_default</td>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>20</td>\n",
       "      <td>Real</td>\n",
       "      <td>19.679697</td>\n",
       "      <td>22.731676</td>\n",
       "      <td>23.651360</td>\n",
       "      <td>3.152174</td>\n",
       "      <td>37.360179</td>\n",
       "      <td>20.879027</td>\n",
       "      <td>24.998915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>SCNN_3_123_conv_1_dense_arg_dict_default</td>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>20</td>\n",
       "      <td>Real</td>\n",
       "      <td>10.804120</td>\n",
       "      <td>34.388118</td>\n",
       "      <td>30.115916</td>\n",
       "      <td>37.717391</td>\n",
       "      <td>36.409396</td>\n",
       "      <td>27.288852</td>\n",
       "      <td>-19.400537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>SCNN_3_1_conv_1_dense_arg_dict_default</td>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>20</td>\n",
       "      <td>Real</td>\n",
       "      <td>12.909930</td>\n",
       "      <td>30.577979</td>\n",
       "      <td>30.918413</td>\n",
       "      <td>40.615942</td>\n",
       "      <td>32.494407</td>\n",
       "      <td>27.100554</td>\n",
       "      <td>-11.710090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7</td>\n",
       "      <td>SCNN_3_2_conv_1_dense_arg_dict_default</td>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>20</td>\n",
       "      <td>Real</td>\n",
       "      <td>18.955347</td>\n",
       "      <td>19.357443</td>\n",
       "      <td>27.351761</td>\n",
       "      <td>34.311594</td>\n",
       "      <td>34.340045</td>\n",
       "      <td>24.988298</td>\n",
       "      <td>7.637537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              \\\n",
       "   mean rank                           scnn_archi_name cut_off_freq EMA_span   \n",
       "0         12    SCNN_1_3_conv_1_dense_arg_dict_default           33        -   \n",
       "1          8  SCNN_3_123_conv_1_dense_arg_dict_default           33        -   \n",
       "2          6    SCNN_3_1_conv_1_dense_arg_dict_default           33        -   \n",
       "3         11    SCNN_3_2_conv_1_dense_arg_dict_default           33        -   \n",
       "4         10    SCNN_1_3_conv_1_dense_arg_dict_default           33       20   \n",
       "5          2  SCNN_3_123_conv_1_dense_arg_dict_default           33       20   \n",
       "6          1    SCNN_3_1_conv_1_dense_arg_dict_default           33       20   \n",
       "7          3    SCNN_3_2_conv_1_dense_arg_dict_default           33       20   \n",
       "8          9    SCNN_1_3_conv_1_dense_arg_dict_default           33        -   \n",
       "9          4  SCNN_3_123_conv_1_dense_arg_dict_default           33        -   \n",
       "10         5    SCNN_3_1_conv_1_dense_arg_dict_default           33        -   \n",
       "11         7    SCNN_3_2_conv_1_dense_arg_dict_default           33        -   \n",
       "\n",
       "                     Window Size                                              \\\n",
       "   SMA_winsize  type         125        250        500        750       1000   \n",
       "0            -  Real    0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "1            -  Real   17.940646  19.422021  35.042354  43.043478  11.017897   \n",
       "2            -  Real   16.870931  25.807233  26.549264  25.905797  45.693512   \n",
       "3            -  Real   14.893487  -7.442686   5.617477   6.811594  17.393736   \n",
       "4            -  Real   15.134937  24.935421  27.775301  16.521739   9.675615   \n",
       "5            -  Real   12.433143  24.830481  38.564423  43.152174  45.525727   \n",
       "6            -  Real   12.818240  34.743300  32.055283  44.782609  49.720358   \n",
       "7            -  Real   17.561661  30.360026  29.982167  33.913043  35.514541   \n",
       "8           20  Real   19.679697  22.731676  23.651360   3.152174  37.360179   \n",
       "9           20  Real   10.804120  34.388118  30.115916  37.717391  36.409396   \n",
       "10          20  Real   12.909930  30.577979  30.918413  40.615942  32.494407   \n",
       "11          20  Real   18.955347  19.357443  27.351761  34.311594  34.340045   \n",
       "\n",
       "                          \n",
       "         mean   variance  \n",
       "0    0.000000   0.000000  \n",
       "1   24.183072   7.111830  \n",
       "2   25.947219   0.533492  \n",
       "3    7.200899   9.247688  \n",
       "4   19.026084  28.252270  \n",
       "5   29.094172 -35.427915  \n",
       "6   31.242833 -30.080712  \n",
       "7   27.665270   8.542436  \n",
       "8   20.879027  24.998915  \n",
       "9   27.288852 -19.400537  \n",
       "10  27.100554 -11.710090  \n",
       "11  24.988298   7.637537  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_df=return_and_save_final_relative_result_df_as_json(df, base_case_index=0, final_exp_results_path=FINAL_EXP_RESULTS_PATH, exp_path_name=EXP_PATH_NAME, window_size_lst=WINDOW_SIZE_LST_NN)\n",
    "relative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.style.hide(axis='index').to_latex()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df.style.hide(axis='index').to_latex()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
