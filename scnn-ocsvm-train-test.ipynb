{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "EXP_PATH_NAME=\"SCNN-OCSVM\"\n",
    "RIVAL_PATH_NAME=\"WACA-OCSVM\"\n",
    "joblib.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "EER: 0.333, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.400, Threshold: 0.200 <-- Worse case\n",
      "EER: 0.167, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.333, Threshold: 1.000 <-- Worse case\n",
      "--------------------\u001b[32mUtility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mPreprocessing utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mNeural Networks utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "EER: 0.333, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.400, Threshold: 0.200 <-- Worse case\n",
      "EER: 0.167, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.333, Threshold: 1.000 <-- Worse case\n",
      "--------------------\u001b[32mUtility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mPreprocessing utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mWACA utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mClassification utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "Numpy Seed was set to: 567\n",
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade pip\n",
    "\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import dataclasses\n",
    "from sklearn.svm import OneClassSVM\n",
    "from dataclasses import asdict\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import random\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold # Feature selector\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Global utitlity functions are in separate notebook\n",
    "%run ./Classification_utility-functions.ipynb\n",
    "%run ./SEED-CONSTANTS.ipynb\n",
    "\n",
    "WINDOW_SIZE_LST=WINDOW_SIZE_LST\n",
    "np.random.seed(SEED)\n",
    "print(f\"Numpy Seed was set to: {SEED}\")\n",
    "\n",
    "print(\"Setup Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__dir__()\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class ExperimentParameters:\n",
    "    \"\"\"Contains all relevant parameters to run an experiment.\"\"\"\n",
    "\n",
    "    name: str  # Name of Parameter set. Used as identifier for charts etc.\n",
    "    frequency: int\n",
    "    max_subjects: int\n",
    "    max_test_subjects: int\n",
    "        \n",
    "    user_ids: list\n",
    "    num_sample_points_per_exp: int\n",
    "    exp_begin_cutoff_idx: int\n",
    "    exp_end_cutoff_idx: int\n",
    "        \n",
    "    \n",
    "    seconds_per_subject_train: float\n",
    "    seconds_per_subject_test: float\n",
    "    window_size: int  # After resampling\n",
    "    ocsvm_step_width: int  # After resampling\n",
    "    scaler: str  # StandardScaler, MinMaxScaler, Normalizer, MaxAbsScaler, RobustScaler, PowerTransformer\n",
    "    scaler_scope: str  # {\"subject\", \"session\"}\n",
    "    scaler_global: bool  # fit transform scale on all data (True) or fit on training only (False)\n",
    "    ocsvm_kernel: str # ocsvm kernel\n",
    "    ocsvm_nu: float  # Best value found in random search, used for final model\n",
    "    ocsvm_gamma: float  # Best value found in random search, used for final model\n",
    "    feature_cols: list  # Columns used as features\n",
    "    exclude_subjects: list  # Don't load data from those users\n",
    "        \n",
    "    # Calculated values\n",
    "    def __post_init__(self):\n",
    "        # HDF key of table:\n",
    "        self.table_name = f\"sensors_{self.frequency}hz\"\n",
    "\n",
    "        \n",
    "\n",
    "# INSTANCES\n",
    "# ===========================================================\n",
    "\n",
    "# NAIVE_APPROACH\n",
    "# -----------------------------------------------------------\n",
    "NAIVE_MINMAX_OCSVM = ExperimentParameters(\n",
    "    name=\"NAIVE-MINMAX_OCSVM\",\n",
    "    frequency=100,\n",
    "    max_subjects=29,\n",
    "    max_test_subjects=10,\n",
    "    user_ids = [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 28, 29, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49],\n",
    "    num_sample_points_per_exp=21000,\n",
    "    exp_begin_cutoff_idx=500,\n",
    "    exp_end_cutoff_idx=-500,\n",
    "    seconds_per_subject_train=210,\n",
    "    seconds_per_subject_test=210,    \n",
    "    window_size=250,\n",
    "    ocsvm_step_width=250,\n",
    "    scaler=\"minmax\",\n",
    "    scaler_scope=\"subject\",\n",
    "    scaler_global=True,\n",
    "    ocsvm_kernel=\"rbf\",\n",
    "    ocsvm_nu=None,\n",
    "    ocsvm_gamma=None,\n",
    "    feature_cols=[\n",
    "        \"x_a\",\n",
    "        \"y_a\",\n",
    "        \"z_a\",\n",
    "        \"x_g\",\n",
    "        \"y_g\",\n",
    "        \"z_g\",\n",
    "    ],\n",
    "    exclude_subjects=[],\n",
    ")\n",
    "\n",
    "# VALID_APPROACH\n",
    "# -----------------------------------------------------------\n",
    "VALID_MINMAX_OCSVM = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-MINMAX-OCSVM\",\n",
    "    scaler_global=False,\n",
    "    ocsvm_nu=0.165,\n",
    "    ocsvm_gamma=0.039,\n",
    ")\n",
    "\n",
    "# NAIVE_ROBUST_APPROACH\n",
    "# -----------------------------------------------------------\n",
    "NAIVE_ROBUST_OCSVM = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"NAIVE-ROBUST-OCSVM\",\n",
    "    scaler=\"robust\",\n",
    "    scaler_global=True,\n",
    "    ocsvm_nu=0.153,\n",
    "    ocsvm_gamma=0.091,  # below median, selected by chart\n",
    ")\n",
    "\n",
    "# ROBUST_APPROACH (VALID)\n",
    "# -----------------------------------------------------------\n",
    "VALID_ROBUST_OCSVM_125 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=125\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "\n",
    "VALID_ROBUST_OCSVM_250 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=250\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_500 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=500\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_750 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=750\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_1000 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=1000\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_1250 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=1250\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_1500 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=1500\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_1750 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=1750\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_2000 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=2000\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "# NORMALIZER_APPROACH (VALID)\n",
    "# -----------------------------------------------------------\n",
    "VALID_NORMALIZER_OCSVM = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-NORMALIZER-OCSVM\",\n",
    "    scaler=\"Normalizer\",\n",
    "    scaler_global=False,\n",
    "    ocsvm_nu=0.074,\n",
    "    ocsvm_gamma= 0.029,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\"></th>\n",
       "      <th colspan=\"9\" halign=\"left\">Window Size</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean rank</th>\n",
       "      <th>cut_off_freq</th>\n",
       "      <th>EMA_span</th>\n",
       "      <th>SMA_winsize</th>\n",
       "      <th>type</th>\n",
       "      <th>125</th>\n",
       "      <th>250</th>\n",
       "      <th>500</th>\n",
       "      <th>750</th>\n",
       "      <th>1000</th>\n",
       "      <th>1250</th>\n",
       "      <th>1500</th>\n",
       "      <th>1750</th>\n",
       "      <th>2000</th>\n",
       "      <th>mean</th>\n",
       "      <th>variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Naive</td>\n",
       "      <td>0.227313</td>\n",
       "      <td>0.171836</td>\n",
       "      <td>0.141258</td>\n",
       "      <td>0.139333</td>\n",
       "      <td>0.131003</td>\n",
       "      <td>0.125799</td>\n",
       "      <td>0.115267</td>\n",
       "      <td>0.111014</td>\n",
       "      <td>0.093000</td>\n",
       "      <td>0.139536</td>\n",
       "      <td>0.001573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.226914</td>\n",
       "      <td>0.170020</td>\n",
       "      <td>0.146292</td>\n",
       "      <td>0.141475</td>\n",
       "      <td>0.127317</td>\n",
       "      <td>0.127153</td>\n",
       "      <td>0.117160</td>\n",
       "      <td>0.112222</td>\n",
       "      <td>0.090389</td>\n",
       "      <td>0.139882</td>\n",
       "      <td>0.001572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>-</td>\n",
       "      <td>25</td>\n",
       "      <td>-</td>\n",
       "      <td>Naive</td>\n",
       "      <td>0.192994</td>\n",
       "      <td>0.158703</td>\n",
       "      <td>0.142343</td>\n",
       "      <td>0.153697</td>\n",
       "      <td>0.140488</td>\n",
       "      <td>0.136146</td>\n",
       "      <td>0.155267</td>\n",
       "      <td>0.136812</td>\n",
       "      <td>0.151167</td>\n",
       "      <td>0.151957</td>\n",
       "      <td>0.000306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>-</td>\n",
       "      <td>9</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.362315</td>\n",
       "      <td>0.309108</td>\n",
       "      <td>0.259384</td>\n",
       "      <td>0.226525</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.193125</td>\n",
       "      <td>0.189506</td>\n",
       "      <td>0.173382</td>\n",
       "      <td>0.171278</td>\n",
       "      <td>0.233847</td>\n",
       "      <td>0.004283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>41</td>\n",
       "      <td>Naive</td>\n",
       "      <td>0.196284</td>\n",
       "      <td>0.157013</td>\n",
       "      <td>0.149103</td>\n",
       "      <td>0.146646</td>\n",
       "      <td>0.149404</td>\n",
       "      <td>0.142569</td>\n",
       "      <td>0.149465</td>\n",
       "      <td>0.153333</td>\n",
       "      <td>0.152778</td>\n",
       "      <td>0.155177</td>\n",
       "      <td>0.000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.397188</td>\n",
       "      <td>0.357325</td>\n",
       "      <td>0.298996</td>\n",
       "      <td>0.253192</td>\n",
       "      <td>0.242033</td>\n",
       "      <td>0.221389</td>\n",
       "      <td>0.213580</td>\n",
       "      <td>0.182802</td>\n",
       "      <td>0.184500</td>\n",
       "      <td>0.261223</td>\n",
       "      <td>0.005684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>10</td>\n",
       "      <td>-</td>\n",
       "      <td>Naive</td>\n",
       "      <td>0.213821</td>\n",
       "      <td>0.164012</td>\n",
       "      <td>0.146345</td>\n",
       "      <td>0.141535</td>\n",
       "      <td>0.134824</td>\n",
       "      <td>0.117049</td>\n",
       "      <td>0.110988</td>\n",
       "      <td>0.107729</td>\n",
       "      <td>0.114778</td>\n",
       "      <td>0.139009</td>\n",
       "      <td>0.001142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.291194</td>\n",
       "      <td>0.220732</td>\n",
       "      <td>0.175596</td>\n",
       "      <td>0.164606</td>\n",
       "      <td>0.154146</td>\n",
       "      <td>0.157569</td>\n",
       "      <td>0.128642</td>\n",
       "      <td>0.115894</td>\n",
       "      <td>0.117333</td>\n",
       "      <td>0.169524</td>\n",
       "      <td>0.003142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>-</td>\n",
       "      <td>41</td>\n",
       "      <td>Naive</td>\n",
       "      <td>0.225110</td>\n",
       "      <td>0.174325</td>\n",
       "      <td>0.153025</td>\n",
       "      <td>0.147010</td>\n",
       "      <td>0.138320</td>\n",
       "      <td>0.139410</td>\n",
       "      <td>0.127531</td>\n",
       "      <td>0.128937</td>\n",
       "      <td>0.132056</td>\n",
       "      <td>0.151747</td>\n",
       "      <td>0.000968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.318503</td>\n",
       "      <td>0.235023</td>\n",
       "      <td>0.188969</td>\n",
       "      <td>0.172444</td>\n",
       "      <td>0.161192</td>\n",
       "      <td>0.154618</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.123575</td>\n",
       "      <td>0.134222</td>\n",
       "      <td>0.179916</td>\n",
       "      <td>0.003882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Window Size            \\\n",
       "  mean rank cut_off_freq EMA_span SMA_winsize   type         125       250   \n",
       "0         2           41        -           -  Naive    0.227313  0.171836   \n",
       "1         3           49        -           -   Real    0.226914  0.170020   \n",
       "2         5            -       25           -  Naive    0.192994  0.158703   \n",
       "3         9            -        9           -   Real    0.362315  0.309108   \n",
       "4         6            -        -          41  Naive    0.196284  0.157013   \n",
       "5        10            -        -           1   Real    0.397188  0.357325   \n",
       "6         1           37       10           -  Naive    0.213821  0.164012   \n",
       "7         7           16        2           -   Real    0.291194  0.220732   \n",
       "8         4           39        -          41  Naive    0.225110  0.174325   \n",
       "9         8           20        -           4   Real    0.318503  0.235023   \n",
       "\n",
       "                                                                         \\\n",
       "        500       750      1000      1250      1500      1750      2000   \n",
       "0  0.141258  0.139333  0.131003  0.125799  0.115267  0.111014  0.093000   \n",
       "1  0.146292  0.141475  0.127317  0.127153  0.117160  0.112222  0.090389   \n",
       "2  0.142343  0.153697  0.140488  0.136146  0.155267  0.136812  0.151167   \n",
       "3  0.259384  0.226525  0.220000  0.193125  0.189506  0.173382  0.171278   \n",
       "4  0.149103  0.146646  0.149404  0.142569  0.149465  0.153333  0.152778   \n",
       "5  0.298996  0.253192  0.242033  0.221389  0.213580  0.182802  0.184500   \n",
       "6  0.146345  0.141535  0.134824  0.117049  0.110988  0.107729  0.114778   \n",
       "7  0.175596  0.164606  0.154146  0.157569  0.128642  0.115894  0.117333   \n",
       "8  0.153025  0.147010  0.138320  0.139410  0.127531  0.128937  0.132056   \n",
       "9  0.188969  0.172444  0.161192  0.154618  0.130700  0.123575  0.134222   \n",
       "\n",
       "                       \n",
       "       mean  variance  \n",
       "0  0.139536  0.001573  \n",
       "1  0.139882  0.001572  \n",
       "2  0.151957  0.000306  \n",
       "3  0.233847  0.004283  \n",
       "4  0.155177  0.000255  \n",
       "5  0.261223  0.005684  \n",
       "6  0.139009  0.001142  \n",
       "7  0.169524  0.003142  \n",
       "8  0.151747  0.000968  \n",
       "9  0.179916  0.003882  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = VALID_ROBUST_OCSVM_2000\n",
    "P.ocsvm_step_width = int(P.window_size * .5)\n",
    "P.classifier=\"OCSVM\"\n",
    "P.ocsvm_kernel = \"rbf\"\n",
    "P.train_cores=1 # 20 cores for every user and 1 core for the nested crossval function\n",
    "P.test_cores=2 # 10 cores for every user and 2 for the nested crossval function\n",
    "\n",
    "\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{RIVAL_PATH_NAME}/{RIVAL_PATH_NAME}-df.json\"\n",
    "RIVAL_DF = pd.read_json(test_file_name)\n",
    "RIVAL_DF\n",
    "# Convert the string representation of tuples back to tuples\n",
    "idx_tuples = [eval(i) for i in RIVAL_DF.columns.tolist()]\n",
    "\n",
    "# # Convert list of tuples back to a MultiIndex\n",
    "multiindex_columns = pd.MultiIndex.from_tuples(idx_tuples)\n",
    "\n",
    "RIVAL_DF.columns = multiindex_columns\n",
    "RIVAL_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window_size</th>\n",
       "      <th>step_width</th>\n",
       "      <th>Mean_EER</th>\n",
       "      <th>median_nu</th>\n",
       "      <th>median_gamma</th>\n",
       "      <th>cut_off_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125</td>\n",
       "      <td>62</td>\n",
       "      <td>0.227313</td>\n",
       "      <td>0.035779</td>\n",
       "      <td>0.655129</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250</td>\n",
       "      <td>125</td>\n",
       "      <td>0.171836</td>\n",
       "      <td>0.055147</td>\n",
       "      <td>0.372759</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>250</td>\n",
       "      <td>0.141258</td>\n",
       "      <td>0.041895</td>\n",
       "      <td>0.372759</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>750</td>\n",
       "      <td>375</td>\n",
       "      <td>0.139333</td>\n",
       "      <td>0.034759</td>\n",
       "      <td>0.120679</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>0.131003</td>\n",
       "      <td>0.035779</td>\n",
       "      <td>0.068665</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1250</td>\n",
       "      <td>625</td>\n",
       "      <td>0.125799</td>\n",
       "      <td>0.040876</td>\n",
       "      <td>0.068665</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1500</td>\n",
       "      <td>750</td>\n",
       "      <td>0.115267</td>\n",
       "      <td>0.033740</td>\n",
       "      <td>0.068665</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1750</td>\n",
       "      <td>875</td>\n",
       "      <td>0.111014</td>\n",
       "      <td>0.031701</td>\n",
       "      <td>0.068665</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.093000</td>\n",
       "      <td>0.034759</td>\n",
       "      <td>0.068665</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   window_size  step_width  Mean_EER  median_nu  median_gamma  cut_off_freq\n",
       "0          125          62  0.227313   0.035779      0.655129            41\n",
       "1          250         125  0.171836   0.055147      0.372759            41\n",
       "2          500         250  0.141258   0.041895      0.372759            41\n",
       "3          750         375  0.139333   0.034759      0.120679            41\n",
       "4         1000         500  0.131003   0.035779      0.068665            41\n",
       "5         1250         625  0.125799   0.040876      0.068665            41\n",
       "6         1500         750  0.115267   0.033740      0.068665            41\n",
       "7         1750         875  0.111014   0.031701      0.068665            41\n",
       "8         2000        1000  0.093000   0.034759      0.068665            41"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.smoothing = \"Butterworth\"\n",
    "\n",
    "\n",
    "#range(1000, 2001, 250)#\n",
    "preprocessing_method=\"Naive\"\n",
    "rival_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{RIVAL_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "rival_test_hyperparameters_df = pd.read_json(rival_test_hyperparameters_file_name)\n",
    "rival_test_hyperparameters_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>VALID-ROBUST-OCSVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frequency</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_subjects</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_test_subjects</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_ids</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_sample_points_per_exp</th>\n",
       "      <td>21000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp_begin_cutoff_idx</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp_end_cutoff_idx</th>\n",
       "      <td>-500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seconds_per_subject_train</th>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seconds_per_subject_test</th>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>window_size</th>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocsvm_step_width</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler</th>\n",
       "      <td>RobustScaler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler_scope</th>\n",
       "      <td>subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler_global</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocsvm_kernel</th>\n",
       "      <td>rbf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocsvm_nu</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocsvm_gamma</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_cols</th>\n",
       "      <td>[x_a, y_a, z_a, x_g, y_g, z_g]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exclude_subjects</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       Value\n",
       "name                                                      VALID-ROBUST-OCSVM\n",
       "frequency                                                                100\n",
       "max_subjects                                                              29\n",
       "max_test_subjects                                                         10\n",
       "user_ids                   [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 2...\n",
       "num_sample_points_per_exp                                              21000\n",
       "exp_begin_cutoff_idx                                                     500\n",
       "exp_end_cutoff_idx                                                      -500\n",
       "seconds_per_subject_train                                                210\n",
       "seconds_per_subject_test                                                 210\n",
       "window_size                                                             2000\n",
       "ocsvm_step_width                                                        1000\n",
       "scaler                                                          RobustScaler\n",
       "scaler_scope                                                         subject\n",
       "scaler_global                                                          False\n",
       "ocsvm_kernel                                                             rbf\n",
       "ocsvm_nu                                                                None\n",
       "ocsvm_gamma                                                             None\n",
       "feature_cols                                  [x_a, y_a, z_a, x_g, y_g, z_g]\n",
       "exclude_subjects                                                          []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils_ppp(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_scorer(utils_eer, greater_is_better=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils_eer_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading exp1 data:\n",
      "1) accel_count: 28388, gyro_count: 31997\n",
      "2) accel_count: 26010, gyro_count: 28954\n",
      "3) accel_count: 28227, gyro_count: 31814\n",
      "4) accel_count: 24860, gyro_count: 26105\n",
      "5) accel_count: 24270, gyro_count: 24347\n",
      "6) accel_count: 25012, gyro_count: 25060\n",
      "7) accel_count: 25301, gyro_count: 25382\n",
      "8) accel_count: 21975, gyro_count: 21658\n",
      "19) accel_count: 24110, gyro_count: 25050\n",
      "21) accel_count: 24326, gyro_count: 23809\n",
      "22) accel_count: 29123, gyro_count: 28724\n",
      "26) accel_count: 23148, gyro_count: 24291\n",
      "27) accel_count: 24299, gyro_count: 23589\n",
      "28) accel_count: 23807, gyro_count: 24523\n",
      "29) accel_count: 24030, gyro_count: 23457\n",
      "35) accel_count: 24388, gyro_count: 23673\n",
      "36) accel_count: 24228, gyro_count: 24208\n",
      "37) accel_count: 31945, gyro_count: 31816\n",
      "38) accel_count: 22135, gyro_count: 22327\n",
      "39) accel_count: 23573, gyro_count: 23459\n",
      "40) accel_count: 23057, gyro_count: 24296\n",
      "41) accel_count: 24102, gyro_count: 23681\n",
      "42) accel_count: 24074, gyro_count: 24328\n",
      "43) accel_count: 22631, gyro_count: 23835\n",
      "44) accel_count: 24473, gyro_count: 23749\n",
      "45) accel_count: 23974, gyro_count: 23229\n",
      "46) accel_count: 23614, gyro_count: 23827\n",
      "48) accel_count: 22828, gyro_count: 23904\n",
      "49) accel_count: 24183, gyro_count: 24633\n",
      "Loading exp2 data:\n",
      "1) accel_count: 24049, gyro_count: 26943\n",
      "2) accel_count: 24468, gyro_count: 27667\n",
      "3) accel_count: 24611, gyro_count: 27000\n",
      "4) accel_count: 24972, gyro_count: 26798\n",
      "5) accel_count: 23573, gyro_count: 23372\n",
      "6) accel_count: 23800, gyro_count: 23890\n",
      "7) accel_count: 23347, gyro_count: 24145\n",
      "8) accel_count: 22947, gyro_count: 22660\n",
      "19) accel_count: 26156, gyro_count: 25815\n",
      "21) accel_count: 23566, gyro_count: 24408\n",
      "22) accel_count: 23844, gyro_count: 24589\n",
      "26) accel_count: 23179, gyro_count: 23925\n",
      "27) accel_count: 25109, gyro_count: 25820\n",
      "28) accel_count: 23133, gyro_count: 24028\n",
      "29) accel_count: 23180, gyro_count: 24314\n",
      "35) accel_count: 23299, gyro_count: 23854\n",
      "36) accel_count: 25497, gyro_count: 25059\n",
      "37) accel_count: 25994, gyro_count: 25232\n",
      "38) accel_count: 21164, gyro_count: 21182\n",
      "39) accel_count: 24214, gyro_count: 23585\n",
      "40) accel_count: 23944, gyro_count: 23170\n",
      "41) accel_count: 23193, gyro_count: 24111\n",
      "42) accel_count: 26505, gyro_count: 25697\n",
      "43) accel_count: 22690, gyro_count: 23981\n",
      "44) accel_count: 23002, gyro_count: 23829\n",
      "45) accel_count: 23978, gyro_count: 23350\n",
      "46) accel_count: 21128, gyro_count: 21848\n",
      "48) accel_count: 27996, gyro_count: 27205\n",
      "49) accel_count: 23061, gyro_count: 24129\n"
     ]
    }
   ],
   "source": [
    "#include 47 later\n",
    "# user_ids = [9]\n",
    "df_exps_dict = load_data_frames(P.user_ids, P.exp_begin_cutoff_idx, P.exp_end_cutoff_idx, P.num_sample_points_per_exp)\n",
    "raw_dfList_exp1, raw_dfList_exp2 = df_exps_dict['dfList_exp1'], df_exps_dict['dfList_exp2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set: [7, 24, 8, 11, 13, 19, 28, 21, 26, 3, 20, 22, 6, 25, 16, 1, 17, 27, 5]\n",
      "test_set: [0, 12, 14, 9, 18, 23, 2, 15, 10, 4]\n",
      "train_set: [7, 24, 8, 11, 13, 19, 28, 21, 26, 3, 20, 22, 6, 25, 16, 1, 17, 27, 5]\n",
      "test_set: [0, 12, 14, 9, 18, 23, 2, 15, 10, 4]\n"
     ]
    }
   ],
   "source": [
    "randomized_data_idx = list(range(len(P.user_ids)))\n",
    "random.Random(SEED).shuffle(randomized_data_idx)\n",
    "split_idx = 2 * (len(randomized_data_idx)//3) + 1\n",
    "train_set = randomized_data_idx[: split_idx]\n",
    "test_set = randomized_data_idx[split_idx: ]\n",
    "# train_set = randomized_data_idx\n",
    "print(f\"train_set: {train_set}\\ntest_set: {test_set}\")\n",
    "# train_set = test_set\n",
    "# test_set = train_set\n",
    "print(f\"train_set: {train_set}\\ntest_set: {test_set}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading exp1 data:\n",
      "47) accel_count: 22777, gyro_count: 22226\n",
      "Loading exp2 data:\n",
      "47) accel_count: 17718, gyro_count: 18353\n"
     ]
    }
   ],
   "source": [
    "num_sample_points_per_exp_user_47 = 18000\n",
    "df_exps_dict_user_47 = load_data_frames([47], P.exp_begin_cutoff_idx, P.exp_end_cutoff_idx, num_sample_points_per_exp_user_47)\n",
    "dfList_exp1_user_47, dfList_exp2_user_47 = df_exps_dict_user_47['dfList_exp1'], df_exps_dict_user_47['dfList_exp2']\n",
    "\n",
    "raw_dfList_exp1_user_47 = dfList_exp1_user_47\n",
    "raw_dfList_exp2_user_47 = dfList_exp2_user_47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set: [7, 24, 8, 11, 13, 19, 28, 21, 26, 3, 20, 22, 6, 25, 16, 1, 17, 27, 5]\n"
     ]
    }
   ],
   "source": [
    "print(f\"train_set: {train_set}\")\n",
    "# print(f\"X_exp1_train_dic: {X_exp1_train_dic.keys()}\")\n",
    "# print(f\"X_exp2_train_dic: {X_exp2_train_dic.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_set: [0, 12, 14, 9, 18, 23, 2, 15, 10, 4]\n"
     ]
    }
   ],
   "source": [
    "print(f\"test_set: {test_set}\")\n",
    "# print(f\"X_exp1_test_dic: {X_exp1_test_dic.keys()}\")\n",
    "# print(f\"X_exp2_test_dic: {X_exp2_test_dic.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[125, 250, 500, 750, 1000, 1250, 1500, 1750, 2000]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WINDOW_SIZE_LST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_classifier_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m SAVE_PATH_NAME\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# loss_func = lambda y_true, dist: k_contrastive_loss(y_true, dist, arg_dict[\"contrastive_loss_margin\"])\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel_classifier_type\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_classifier_type' is not defined"
     ]
    }
   ],
   "source": [
    "int('121')\n",
    "SAVE_PATH_NAME\n",
    "# loss_func = lambda y_true, dist: k_contrastive_loss(y_true, dist, arg_dict[\"contrastive_loss_margin\"])\n",
    "model_classifier_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.extract_learnt_features(X_exp1_dic, X_exp2_dic, fitted_scaler_classifier_exp2_dic, deep_feature_model, verbose=1)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_learnt_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exp_deep_feature_extractors_dict(exp_num, model_architectures= list(TRAINING_CONFIG_DICT['2000'].keys())):\n",
    "    exp_deep_feature_extractors_dict={}\n",
    "\n",
    "    model_classifier_type=EXP_PATH_NAME\n",
    "    base_path=f\"final_experiments_results/{SAVE_PATH_NAME}/{model_classifier_type}/{exp_num}\"\n",
    "    for archi_name in model_architectures:\n",
    "        # archi_name=\"SCNN_3_1_conv_1_dense_arg_dict_default\"\n",
    "        path_to_archi_name=base_path+f\"/{archi_name}\"\n",
    "        archi_win_dict={}\n",
    "        for winsize in WINDOW_SIZE_LST:\n",
    "            final_path=path_to_archi_name+f\"/{winsize}\"\n",
    "            deep_feature_extractor_path=final_path+f\"/{model_classifier_type}-{exp_num}-{archi_name}-{winsize}-deep_feature_extractor\"\n",
    "            print(deep_feature_extractor_path)\n",
    "            \n",
    "            deep_feature_model=load_model(deep_feature_extractor_path)#, custom_objects={'loss_func': loss_func})\n",
    "            \n",
    "            deep_feature_extractor = partial_func(extract_learnt_features, deep_feature_model=deep_feature_model)\n",
    "\n",
    "            archi_win_dict[winsize]=deep_feature_extractor\n",
    "\n",
    "        exp_deep_feature_extractors_dict[archi_name]=archi_win_dict\n",
    "        \n",
    "    return exp_deep_feature_extractors_dict\n",
    "\n",
    "# exp_deep_feature_extractors_dict=get_exp_deep_feature_extractors_dict(exp_num=EXP_NUM_1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to suppress TensorFlow warnings\n",
    "def suppress_tf_warnings():\n",
    "    warnings.filterwarnings('ignore', category=UserWarning)\n",
    "    tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Call the function before loading the model\n",
    "suppress_tf_warnings()\n",
    "\n",
    "def enable_tf_warnings():\n",
    "    warnings.filterwarnings('default', category=UserWarning)\n",
    "    tf.get_logger().setLevel('WARN')\n",
    "\n",
    "# Call the function to enable warnings\n",
    "# enable_tf_warnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TESTING_CONFIG_DICT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mTESTING_CONFIG_DICT\u001b[49m\u001b[38;5;241m.\u001b[39mitems()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TESTING_CONFIG_DICT' is not defined"
     ]
    }
   ],
   "source": [
    "TESTING_CONFIG_DICT.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_exp1_dict, X_train_exp2_dict, fitted_scaler_SNN_train_exp2_dict=get_raw_windows(raw_dfList_exp1, raw_dfList_exp2, 1750, 1750//2, train_set, \n",
    "                                                                    P.scaler, P.num_sample_points_per_exp, EMA_per_win_span=None, SMA_per_win_winsize=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 6)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_exp1_dict[7][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SCNN_1_3_conv_1_dense_arg_dict_default': {...},\n",
       " 'SCNN_3_123_conv_1_dense_arg_dict_default': {...},\n",
       " 'SCNN_3_1_conv_1_dense_arg_dict_default': {...},\n",
       " 'SCNN_1_2_conv_1_dense_arg_dict_default': {...},\n",
       " 'SCNN_1_1_conv_1_dense_arg_dict_default': {...},\n",
       " 'SCNN_1_2_conv_2_dense_arg_dict_default': {...},\n",
       " 'SCNN_1_5_conv_1_dense_arg_dict_default': {...},\n",
       " 'SCNN_4_1234_conv_1_dense_arg_dict_default': {...}}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TESTING_CONFIG_DICT[\"SCNN_1_3_conv_1_dense_arg_dict_default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([X_train_exp1_dict[7][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model_name, model in TESTING_CONFIG_DICT.items():\n",
    "#     print(model[1750].predict(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# archi_name=\"SCNN_1_2_conv_1_dense_arg_dict_default\"\n",
    "# base_path=f\"final_experiments_results/{SAVE_PATH_NAME}/{model_classifier_type}/{EXP_NUM_1_1}\"\n",
    "# path_to_archi_name=base_path+f\"/{archi_name}\"\n",
    "# final_path=path_to_archi_name+f\"/{winsize_str}\"\n",
    "# deep_feature_extractor_path=final_path+f\"/{model_classifier_type}-{EXP_NUM_1_1}-{archi_name}-{winsize_str}-deep_feature_extractor\"\n",
    "# model=load_model(deep_feature_extractor_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path=f\"final_experiments_results/{SAVE_PATH_NAME}/{EXP_PATH_NAME}/{EXP_NUM_1_1}\"\n",
    "path_to_archi_name=base_path+f\"/{archi_name}\"\n",
    "final_path=path_to_archi_name+f\"/{winsize}\"\n",
    "    \n",
    "    if not os.path.isdir(base_path):\n",
    "        raise Exception(f\"Base path does not exist: {base_path}\")\n",
    "        \n",
    "    if not os.path.isdir(path_to_archi_name):\n",
    "        os.mkdir(path_to_archi_name)\n",
    "\n",
    "    if not os.path.isdir(final_path):\n",
    "        os.mkdir(final_path)\n",
    "        \n",
    "    model.save(final_path+f\"/{model_classifier_type}-{exp_num}-{archi_name}-{winsize}-model\")\n",
    "    deep_feature_model = extract_deep_feature_extactor(model)\n",
    "    deep_feature_model.save(final_path+f\"/{model_classifier_type}-{exp_num}-{archi_name}-{winsize}-deep_feature_extractor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def correct_archi_extractor_dict_key_names(exp_deep_archi_extractors_dict):\n",
    "#     items=list(exp_deep_archi_extractors_dict.items())\n",
    "#     for key, val in items:\n",
    "#         exp_deep_archi_extractors_dict[key[:-len(\"_arg_dict_default\")]]=val\n",
    "#         # del exp_deep_archi_extractors_dict[key]\n",
    "#     return exp_deep_archi_extractors_dict\n",
    "# exp_deep_archi_extractors_dict=correct_archi_extractor_dict_key_names(exp_deep_feature_extractors_dict)\n",
    "# exp_deep_archi_extractors_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_deep_archi_extractors_dict.keys()\n",
    "WINDOW_SIZE_LST = [125, 250, 500, 750, 1000, 1250, 1500, 1750, 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_tf_warnings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WACA-OCSVM'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RIVAL_PATH_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cut_off_freq: -99999\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window_size</th>\n",
       "      <th>step_width</th>\n",
       "      <th>Mean_EER</th>\n",
       "      <th>median_nu</th>\n",
       "      <th>median_gamma</th>\n",
       "      <th>cut_off_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125</td>\n",
       "      <td>62</td>\n",
       "      <td>0.231879</td>\n",
       "      <td>0.042914</td>\n",
       "      <td>0.655129</td>\n",
       "      <td>-99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250</td>\n",
       "      <td>125</td>\n",
       "      <td>0.178423</td>\n",
       "      <td>0.040876</td>\n",
       "      <td>0.655129</td>\n",
       "      <td>-99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>250</td>\n",
       "      <td>0.152316</td>\n",
       "      <td>0.040876</td>\n",
       "      <td>0.372759</td>\n",
       "      <td>-99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>750</td>\n",
       "      <td>375</td>\n",
       "      <td>0.146990</td>\n",
       "      <td>0.035779</td>\n",
       "      <td>0.120679</td>\n",
       "      <td>-99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>0.133523</td>\n",
       "      <td>0.034759</td>\n",
       "      <td>0.120679</td>\n",
       "      <td>-99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1250</td>\n",
       "      <td>625</td>\n",
       "      <td>0.128229</td>\n",
       "      <td>0.034759</td>\n",
       "      <td>0.068665</td>\n",
       "      <td>-99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1500</td>\n",
       "      <td>750</td>\n",
       "      <td>0.137243</td>\n",
       "      <td>0.034759</td>\n",
       "      <td>0.030650</td>\n",
       "      <td>-99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1750</td>\n",
       "      <td>875</td>\n",
       "      <td>0.108889</td>\n",
       "      <td>0.035779</td>\n",
       "      <td>0.068665</td>\n",
       "      <td>-99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.106611</td>\n",
       "      <td>0.035779</td>\n",
       "      <td>0.039069</td>\n",
       "      <td>-99999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   window_size  step_width  Mean_EER  median_nu  median_gamma  cut_off_freq\n",
       "0          125          62  0.231879   0.042914      0.655129        -99999\n",
       "1          250         125  0.178423   0.040876      0.655129        -99999\n",
       "2          500         250  0.152316   0.040876      0.372759        -99999\n",
       "3          750         375  0.146990   0.035779      0.120679        -99999\n",
       "4         1000         500  0.133523   0.034759      0.120679        -99999\n",
       "5         1250         625  0.128229   0.034759      0.068665        -99999\n",
       "6         1500         750  0.137243   0.034759      0.030650        -99999\n",
       "7         1750         875  0.108889   0.035779      0.068665        -99999\n",
       "8         2000        1000  0.106611   0.035779      0.039069        -99999"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rival_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{RIVAL_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "rival_test_hyperparameters_df = pd.read_json(rival_test_hyperparameters_file_name)\n",
    "\n",
    "P.cut_off_freq=rival_test_hyperparameters_df[\"cut_off_freq\"][0]\n",
    "print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "rival_test_hyperparameters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/0/SCNN_1_3_conv_1_dense_arg_dict_default/125/SCNN-OCSVM-0-SCNN_1_3_conv_1_dense_arg_dict_default-125-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/0/SCNN_1_3_conv_1_dense_arg_dict_default/250/SCNN-OCSVM-0-SCNN_1_3_conv_1_dense_arg_dict_default-250-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/0/SCNN_1_3_conv_1_dense_arg_dict_default/500/SCNN-OCSVM-0-SCNN_1_3_conv_1_dense_arg_dict_default-500-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/0/SCNN_1_3_conv_1_dense_arg_dict_default/750/SCNN-OCSVM-0-SCNN_1_3_conv_1_dense_arg_dict_default-750-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/0/SCNN_1_3_conv_1_dense_arg_dict_default/1000/SCNN-OCSVM-0-SCNN_1_3_conv_1_dense_arg_dict_default-1000-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/0/SCNN_1_3_conv_1_dense_arg_dict_default/1250/SCNN-OCSVM-0-SCNN_1_3_conv_1_dense_arg_dict_default-1250-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/0/SCNN_1_3_conv_1_dense_arg_dict_default/1500/SCNN-OCSVM-0-SCNN_1_3_conv_1_dense_arg_dict_default-1500-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/0/SCNN_1_3_conv_1_dense_arg_dict_default/1750/SCNN-OCSVM-0-SCNN_1_3_conv_1_dense_arg_dict_default-1750-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/0/SCNN_1_3_conv_1_dense_arg_dict_default/2000/SCNN-OCSVM-0-SCNN_1_3_conv_1_dense_arg_dict_default-2000-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'SCNN_1_3_conv_1_dense_arg_dict_default': {125: functools.partial(<function extract_learnt_features at 0x7f41c94283a0>, deep_feature_model=<keras.engine.functional.Functional object at 0x7f2b577db520>),\n",
       "  250: functools.partial(<function extract_learnt_features at 0x7f41c94283a0>, deep_feature_model=<keras.engine.functional.Functional object at 0x7f2b4f609f10>),\n",
       "  500: functools.partial(<function extract_learnt_features at 0x7f41c94283a0>, deep_feature_model=<keras.engine.functional.Functional object at 0x7f2b468dc1f0>),\n",
       "  750: functools.partial(<function extract_learnt_features at 0x7f41c94283a0>, deep_feature_model=<keras.engine.functional.Functional object at 0x7f2b4a9fad60>),\n",
       "  1000: functools.partial(<function extract_learnt_features at 0x7f41c94283a0>, deep_feature_model=<keras.engine.functional.Functional object at 0x7f2b4a9fd6d0>),\n",
       "  1250: functools.partial(<function extract_learnt_features at 0x7f41c94283a0>, deep_feature_model=<keras.engine.functional.Functional object at 0x7f2b54be0f70>),\n",
       "  1500: functools.partial(<function extract_learnt_features at 0x7f41c94283a0>, deep_feature_model=<keras.engine.functional.Functional object at 0x7f37a44fa940>),\n",
       "  1750: functools.partial(<function extract_learnt_features at 0x7f41c94283a0>, deep_feature_model=<keras.engine.functional.Functional object at 0x7f2b54d9c700>),\n",
       "  2000: functools.partial(<function extract_learnt_features at 0x7f41c94283a0>, deep_feature_model=<keras.engine.functional.Functional object at 0x7f2b55a44d30>)}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_exp_deep_feature_extractors_dict(EXP_NUM_0, model_architectures= [list(TRAINING_CONFIG_DICT['2000'].keys())[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. No Smoothing\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reseting experiment params successful!\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/0/SCNN_1_3_conv_1_dense_arg_dict_default/125/SCNN-OCSVM-0-SCNN_1_3_conv_1_dense_arg_dict_default-125-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/0/SCNN_1_3_conv_1_dense_arg_dict_default/250/SCNN-OCSVM-0-SCNN_1_3_conv_1_dense_arg_dict_default-250-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/0/SCNN_1_3_conv_1_dense_arg_dict_default/500/SCNN-OCSVM-0-SCNN_1_3_conv_1_dense_arg_dict_default-500-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/0/SCNN_1_3_conv_1_dense_arg_dict_default/750/SCNN-OCSVM-0-SCNN_1_3_conv_1_dense_arg_dict_default-750-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/0/SCNN_1_3_conv_1_dense_arg_dict_default/1000/SCNN-OCSVM-0-SCNN_1_3_conv_1_dense_arg_dict_default-1000-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/0/SCNN_1_3_conv_1_dense_arg_dict_default/1250/SCNN-OCSVM-0-SCNN_1_3_conv_1_dense_arg_dict_default-1250-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/0/SCNN_1_3_conv_1_dense_arg_dict_default/1500/SCNN-OCSVM-0-SCNN_1_3_conv_1_dense_arg_dict_default-1500-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/0/SCNN_1_3_conv_1_dense_arg_dict_default/1750/SCNN-OCSVM-0-SCNN_1_3_conv_1_dense_arg_dict_default-1750-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/0/SCNN_1_3_conv_1_dense_arg_dict_default/2000/SCNN-OCSVM-0-SCNN_1_3_conv_1_dense_arg_dict_default-2000-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "train_set: [7, 24, 8, 11, 13, 19, 28, 21, 26, 3, 20, 22, 6, 25, 16, 1, 17, 27, 5, 47]\n",
      "test_set: [0, 12, 14, 9, 18, 23, 2, 15, 10, 4]\n",
      "cut_off_freq: -99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 125\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeSCNNExpDicOwner Time:  1.3784460476599634\n",
      "MakeSCNNXExpDicUnknown Time:  36.01882671285421\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 3487.99it/s]\n",
      "\n",
      " 20%|        | 1/5 [00:07<00:30,  7.61s/it]\u001b[A\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 7665.03it/s]\n",
      "\n",
      " 40%|      | 2/5 [00:08<00:11,  3.72s/it]\u001b[A\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 9218.25it/s]\n",
      "\n",
      " 60%|    | 3/5 [00:09<00:04,  2.49s/it]\u001b[A\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 8504.27it/s]\n",
      "\n",
      " 80%|  | 4/5 [00:10<00:01,  1.76s/it]\u001b[A\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 10713.42it/s]\n",
      "\n",
      "100%|| 5/5 [00:10<00:00,  2.15s/it]\u001b[A\n",
      " 11%|         | 1/9 [00:49<06:33, 49.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 250\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeSCNNExpDicOwner Time:  1.41061616782099\n",
      "MakeSCNNXExpDicUnknown Time:  21.557730227708817\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 10645.44it/s]\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:01,  3.24it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 17105.64it/s]\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:00,  3.17it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 11459.85it/s]\n",
      "\n",
      " 60%|    | 3/5 [00:00<00:00,  3.21it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 5765.37it/s]\n",
      "\n",
      " 80%|  | 4/5 [00:01<00:00,  3.17it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 6685.22it/s]\n",
      "\n",
      "100%|| 5/5 [00:01<00:00,  3.15it/s]\u001b[A\n",
      " 22%|       | 2/9 [01:14<04:05, 35.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 500\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeSCNNExpDicOwner Time:  1.007868583779782\n",
      "MakeSCNNXExpDicUnknown Time:  13.988841319922358\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 9086.45it/s]\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:01,  3.50it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 10691.57it/s]\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:00,  3.76it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 11181.83it/s]\n",
      "\n",
      " 60%|    | 3/5 [00:00<00:00,  4.05it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 7432.76it/s]\n",
      "\n",
      " 80%|  | 4/5 [00:00<00:00,  4.18it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 9185.95it/s]\n",
      "\n",
      "100%|| 5/5 [00:01<00:00,  4.02it/s]\u001b[A\n",
      " 33%|      | 3/9 [01:30<02:39, 26.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 750\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeSCNNExpDicOwner Time:  1.0117722088471055\n",
      "MakeSCNNXExpDicUnknown Time:  11.65586586901918\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 8996.79it/s]\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:00,  4.62it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 9112.11it/s]\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:00,  4.61it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 8782.04it/s]\n",
      "\n",
      " 60%|    | 3/5 [00:00<00:00,  4.62it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 11478.66it/s]\n",
      "\n",
      " 80%|  | 4/5 [00:00<00:00,  4.54it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 4932.73it/s]\n",
      "\n",
      "100%|| 5/5 [00:01<00:00,  4.54it/s]\u001b[A\n",
      " 44%|     | 4/9 [01:44<01:48, 21.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1000\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeSCNNExpDicOwner Time:  1.0058970707468688\n",
      "MakeSCNNXExpDicUnknown Time:  11.08208082895726\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 10557.02it/s]\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:01,  3.74it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 3059.30it/s]\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:00,  4.43it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 16611.10it/s]\n",
      "\n",
      " 60%|    | 3/5 [00:00<00:00,  4.69it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 12479.33it/s]\n",
      "\n",
      " 80%|  | 4/5 [00:00<00:00,  4.83it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 7870.71it/s]\n",
      "\n",
      "100%|| 5/5 [00:01<00:00,  4.69it/s]\u001b[A\n",
      " 56%|    | 5/9 [01:58<01:14, 18.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1250\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeSCNNExpDicOwner Time:  0.9358006520196795\n",
      "MakeSCNNXExpDicUnknown Time:  10.244010021910071\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 14227.63it/s]\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:00,  4.79it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 5602.86it/s]\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:00,  4.62it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 7876.63it/s]\n",
      "\n",
      " 60%|    | 3/5 [00:00<00:00,  4.82it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 8398.69it/s]\n",
      "\n",
      " 80%|  | 4/5 [00:00<00:00,  4.87it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 5726.01it/s]\n",
      "\n",
      "100%|| 5/5 [00:01<00:00,  4.83it/s]\u001b[A\n",
      " 67%|   | 6/9 [02:10<00:49, 16.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1500\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeSCNNExpDicOwner Time:  0.9502140879631042\n",
      "MakeSCNNXExpDicUnknown Time:  10.025906547904015\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 9628.80it/s]\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:00,  4.93it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 10519.95it/s]\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:00,  5.09it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 10594.35it/s]\n",
      "\n",
      " 60%|    | 3/5 [00:00<00:00,  4.99it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 10143.42it/s]\n",
      "\n",
      " 80%|  | 4/5 [00:00<00:00,  5.01it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 6395.71it/s]\n",
      "\n",
      "100%|| 5/5 [00:00<00:00,  5.01it/s]\u001b[A\n",
      " 78%|  | 7/9 [02:22<00:30, 15.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1750\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeSCNNExpDicOwner Time:  0.882440104149282\n",
      "MakeSCNNXExpDicUnknown Time:  10.21187306707725\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 9279.43it/s]\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:00,  4.84it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 11599.29it/s]\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:00,  5.27it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 6638.66it/s]\n",
      "\n",
      " 60%|    | 3/5 [00:00<00:00,  5.05it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 14378.83it/s]\n",
      "\n",
      " 80%|  | 4/5 [00:00<00:00,  5.20it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 7851.56it/s]\n",
      "\n",
      "100%|| 5/5 [00:00<00:00,  5.11it/s]\u001b[A\n",
      " 89%| | 8/9 [02:35<00:14, 14.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 2000\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeSCNNExpDicOwner Time:  0.9358865288086236\n",
      "MakeSCNNXExpDicUnknown Time:  10.004068272188306\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 10300.35it/s]\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:00,  4.73it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 8505.99it/s]\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:00,  4.87it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 6859.04it/s]\n",
      "\n",
      " 60%|    | 3/5 [00:00<00:00,  5.06it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 8103.37it/s]\n",
      "\n",
      " 80%|  | 4/5 [00:00<00:00,  5.04it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 9345.60it/s]\n",
      "\n",
      "100%|| 5/5 [00:00<00:00,  5.02it/s]\u001b[A\n",
      "100%|| 9/9 [02:47<00:00, 18.58s/it]\n"
     ]
    }
   ],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "# exp_deep_archi_extractors_dict=get_exp_deep_feature_extractors_dict(exp_num=EXP_NUM_0)\n",
    "exp_deep_archi_extractors_dict=get_exp_deep_feature_extractors_dict(EXP_NUM_0, model_architectures= [list(TRAINING_CONFIG_DICT['2000'].keys())[0]])\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = None\n",
    "\n",
    "preprocessing_method=None\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "\n",
    "EER_df_test_dict={}\n",
    "for scnn_archi_name, exp_deep_feature_extractors_dict in exp_deep_archi_extractors_dict.items():\n",
    "\n",
    "    rival_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{RIVAL_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "    rival_test_hyperparameters_df = pd.read_json(rival_test_hyperparameters_file_name)\n",
    "\n",
    "    P.cut_off_freq=rival_test_hyperparameters_df[\"cut_off_freq\"][0]\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "\n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": raw_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": raw_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "    test_dict_key=scnn_archi_name\n",
    "    EER_df_test_dict[test_dict_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, \n",
    "                                                                                                          test_set=test_set, exp_config=P, \n",
    "                                                                                                          extract_features_func_dict=exp_deep_feature_extractors_dict, \n",
    "                                                                                                          overlap=OVERLAP, \n",
    "                                                                                                          best_param_df=rival_test_hyperparameters_df)\n",
    "    with open(test_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"test_dict_key: {test_dict_key}\\n\")\n",
    "        f.write(EER_df_test_dict[test_dict_key].to_string())\n",
    "        \n",
    "    \n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"scnn_archi_name\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exp_deep_archi_extractors_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexp_deep_archi_extractors_dict\u001b[49m\u001b[38;5;241m.\u001b[39mitems()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exp_deep_archi_extractors_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# exp_deep_archi_extractors_dict.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Butterworth frequency Cut-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Naive Approach\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reseting experiment params successful!\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_1_3_conv_1_dense_arg_dict_default/125/SCNN-OCSVM-1.1-SCNN_1_3_conv_1_dense_arg_dict_default-125-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_1_3_conv_1_dense_arg_dict_default/250/SCNN-OCSVM-1.1-SCNN_1_3_conv_1_dense_arg_dict_default-250-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_1_3_conv_1_dense_arg_dict_default/500/SCNN-OCSVM-1.1-SCNN_1_3_conv_1_dense_arg_dict_default-500-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_1_3_conv_1_dense_arg_dict_default/750/SCNN-OCSVM-1.1-SCNN_1_3_conv_1_dense_arg_dict_default-750-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_1_3_conv_1_dense_arg_dict_default/1000/SCNN-OCSVM-1.1-SCNN_1_3_conv_1_dense_arg_dict_default-1000-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_1_3_conv_1_dense_arg_dict_default/1250/SCNN-OCSVM-1.1-SCNN_1_3_conv_1_dense_arg_dict_default-1250-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_1_3_conv_1_dense_arg_dict_default/1500/SCNN-OCSVM-1.1-SCNN_1_3_conv_1_dense_arg_dict_default-1500-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_1_3_conv_1_dense_arg_dict_default/1750/SCNN-OCSVM-1.1-SCNN_1_3_conv_1_dense_arg_dict_default-1750-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_1_3_conv_1_dense_arg_dict_default/2000/SCNN-OCSVM-1.1-SCNN_1_3_conv_1_dense_arg_dict_default-2000-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "train_set: [7, 24, 8, 11, 13, 19, 28, 21, 26, 3, 20, 22, 6, 25, 16, 1, 17, 27, 5, 47]\n",
      "test_set: [0, 12, 14, 9, 18, 23, 2, 15, 10, 4]\n",
      "cut_off_freq: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 125\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeSCNNExpDicOwner Time:  1.3732085549272597\n",
      "MakeSCNNXExpDicUnknown Time:  36.0077731590718\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 7898.88it/s]\n",
      "\n",
      " 20%|        | 1/5 [00:07<00:30,  7.55s/it]\u001b[A\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 8481.91it/s]\n",
      "\n",
      " 40%|      | 2/5 [00:08<00:11,  3.67s/it]\u001b[A\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 8391.96it/s]\n",
      "\n",
      " 60%|    | 3/5 [00:09<00:04,  2.44s/it]\u001b[A\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 7949.78it/s]\n",
      "\n",
      " 80%|  | 4/5 [00:10<00:01,  1.76s/it]\u001b[A\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 7850.09it/s]\n",
      "\n",
      "100%|| 5/5 [00:10<00:00,  2.14s/it]\u001b[A\n",
      " 11%|         | 1/9 [00:49<06:32, 49.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 250\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeSCNNExpDicOwner Time:  1.1335460939444602\n",
      "MakeSCNNXExpDicUnknown Time:  21.330917451065034\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 9946.18it/s]\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:01,  3.10it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 8931.65it/s]\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:00,  3.27it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 10724.38it/s]\n",
      "\n",
      " 60%|    | 3/5 [00:00<00:00,  3.23it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 9043.35it/s]\n",
      "\n",
      " 80%|  | 4/5 [00:01<00:00,  3.22it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 8093.99it/s]\n",
      "\n",
      "100%|| 5/5 [00:01<00:00,  3.23it/s]\u001b[A\n",
      " 22%|       | 2/9 [01:15<04:12, 36.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 500\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeSCNNExpDicOwner Time:  0.9136877139098942\n",
      "MakeSCNNXExpDicUnknown Time:  14.052313982043415\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 8447.74it/s]\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:01,  3.93it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 14737.54it/s]\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:00,  4.08it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 4515.83it/s]\n",
      "\n",
      " 60%|    | 3/5 [00:00<00:00,  3.99it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 9740.60it/s]\n",
      "\n",
      " 80%|  | 4/5 [00:00<00:00,  4.10it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 8375.21it/s]\n",
      "\n",
      "100%|| 5/5 [00:01<00:00,  4.02it/s]\u001b[A\n",
      " 33%|      | 3/9 [01:32<02:42, 27.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 750\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeSCNNExpDicOwner Time:  0.8777186991646886\n",
      "MakeSCNNXExpDicUnknown Time:  11.4912491729483\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 16033.27it/s]\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:00,  4.61it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 7403.89it/s]\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:00,  4.66it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 8298.98it/s]\n",
      "\n",
      " 60%|    | 3/5 [00:00<00:00,  4.64it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 13066.37it/s]\n",
      "\n",
      " 80%|  | 4/5 [00:00<00:00,  4.73it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 6405.47it/s]\n",
      "\n",
      "100%|| 5/5 [00:01<00:00,  4.68it/s]\u001b[A\n",
      " 44%|     | 4/9 [01:46<01:49, 21.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1000\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeSCNNExpDicOwner Time:  0.94317776709795\n",
      "MakeSCNNXExpDicUnknown Time:  11.129078991245478\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 9076.62it/s]\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:00,  4.36it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 10240.00it/s]\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:00,  4.59it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 9852.72it/s]\n",
      "\n",
      " 60%|    | 3/5 [00:00<00:00,  4.72it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 12569.09it/s]\n",
      "\n",
      " 80%|  | 4/5 [00:00<00:00,  4.76it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 8899.44it/s]\n",
      "\n",
      "100%|| 5/5 [00:01<00:00,  4.76it/s]\u001b[A\n",
      " 56%|    | 5/9 [01:59<01:15, 18.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1250\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeSCNNExpDicOwner Time:  0.965620064176619\n",
      "MakeSCNNXExpDicUnknown Time:  10.210061935242265\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 12038.76it/s]\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:00,  4.95it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 11567.30it/s]\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:00,  5.06it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 9495.82it/s]\n",
      "\n",
      " 60%|    | 3/5 [00:00<00:00,  4.96it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 11169.92it/s]\n",
      "\n",
      " 80%|  | 4/5 [00:00<00:00,  4.92it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 11268.95it/s]\n",
      "\n",
      "100%|| 5/5 [00:01<00:00,  4.91it/s]\u001b[A\n",
      " 67%|   | 6/9 [02:12<00:49, 16.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1500\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeSCNNExpDicOwner Time:  0.9002588903531432\n",
      "MakeSCNNXExpDicUnknown Time:  10.257131960242987\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 10333.34it/s]\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:00,  5.32it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 8813.41it/s]\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:00,  5.42it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 9381.13it/s]\n",
      "\n",
      " 60%|    | 3/5 [00:00<00:00,  5.25it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 10462.22it/s]\n",
      "\n",
      " 80%|  | 4/5 [00:00<00:00,  5.18it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 11939.38it/s]\n",
      "\n",
      "100%|| 5/5 [00:00<00:00,  5.21it/s]\u001b[A\n",
      " 78%|  | 7/9 [02:24<00:30, 15.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1750\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeSCNNExpDicOwner Time:  0.9454913157969713\n",
      "MakeSCNNXExpDicUnknown Time:  9.923396124970168\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 8876.83it/s]\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:00,  4.91it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 6709.81it/s]\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:00,  5.02it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 9018.07it/s]\n",
      "\n",
      " 60%|    | 3/5 [00:00<00:00,  5.08it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 8217.68it/s]\n",
      "\n",
      " 80%|  | 4/5 [00:00<00:00,  5.15it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 5919.98it/s]\n",
      "\n",
      "100%|| 5/5 [00:00<00:00,  5.08it/s]\u001b[A\n",
      " 89%| | 8/9 [02:36<00:14, 14.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 2000\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeSCNNExpDicOwner Time:  0.9188729957677424\n",
      "MakeSCNNXExpDicUnknown Time:  10.006054670084268\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 8845.01it/s]\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:00,  5.07it/s]\u001b[A\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 10580.99it/s]\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:00,  5.19it/s]\u001b[A\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 14222.80it/s]\n",
      "\n",
      " 60%|    | 3/5 [00:00<00:00,  5.23it/s]\u001b[A\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 10200.16it/s]\n",
      "\n",
      " 80%|  | 4/5 [00:00<00:00,  5.12it/s]\u001b[A\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 9825.03it/s]\n",
      "\n",
      "100%|| 5/5 [00:00<00:00,  5.07it/s]\u001b[A\n",
      "100%|| 9/9 [02:48<00:00, 18.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns must be same length as key\n",
      "the number of columns in key_column must match the number of elements of dict key!\n",
      "len(key_column): 1 != len(key): 2\n",
      "type(key_column): <class 'list'>, type(key): <class 'tuple'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#-------\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#-------\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# key_column= test_dict_key#[\"cut_off_freq\"]\u001b[39;00m\n\u001b[1;32m     70\u001b[0m EER_df_test_dict_df\u001b[38;5;241m=\u001b[39mmake_raw_exp_df_results(EER_df_test_dict, key_column)\n\u001b[0;32m---> 71\u001b[0m eer_per_window_size_col_df \u001b[38;5;241m=\u001b[39m\u001b[43mmake_eer_per_window_size_col_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEER_df_test_dict_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size_lst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWINDOW_SIZE_LST\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m EER_df_test_dict_df\u001b[38;5;241m.\u001b[39mto_json(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_file_name[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_raw_df.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     74\u001b[0m eer_per_window_size_col_df\u001b[38;5;241m.\u001b[39mto_json(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_file_name[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_eer_per_window_size_col_df.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/tmp/ipykernel_94/1680989987.py:2\u001b[0m, in \u001b[0;36mmake_eer_per_window_size_col_df\u001b[0;34m(df, key_column, window_size_lst)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_eer_per_window_size_col_df\u001b[39m(df, key_column, window_size_lst):\n\u001b[0;32m----> 2\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m()\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Calculate the mean and variance of EER for each unique Key\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     agg_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(key_column)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean_EER\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39magg([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvar\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "# exp_deep_archi_extractors_dict=get_exp_deep_feature_extractors_dict(exp_num=EXP_NUM_1_1)\n",
    "exp_deep_archi_extractors_dict=get_exp_deep_feature_extractors_dict(EXP_NUM_1_1, model_architectures= [list(TRAINING_CONFIG_DICT['2000'].keys())[0]])\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butterworth\"\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "overlap=0.5\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for scnn_archi_name, exp_deep_feature_extractors_dict in exp_deep_archi_extractors_dict.items():\n",
    "\n",
    "    rival_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{RIVAL_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "    rival_test_hyperparameters_df = pd.read_json(rival_test_hyperparameters_file_name)\n",
    "\n",
    "    P.cut_off_freq=rival_test_hyperparameters_df[\"cut_off_freq\"][0]\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "\n",
    "    ffted_dfList_exp1 = get_ffted_dfList(raw_dfList_exp1, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    ffted_dfList_exp1_user_47 = get_ffted_dfList(raw_dfList_exp1_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "\n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": ffted_dfList_exp1,\n",
    "                \"dfList_exp2\": ffted_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": ffted_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": ffted_dfList_exp2_user_47\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    min_key=rival_test_hyperparameters_df[\"cut_off_freq\"][0]\n",
    "    test_dict_key=min_key, scnn_archi_name\n",
    "    EER_df_test_dict[test_dict_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, \n",
    "                                                                                                          test_set=test_set, exp_config=P, \n",
    "                                                                                                          extract_features_func_dict=exp_deep_feature_extractors_dict, \n",
    "                                                                                                          overlap=OVERLAP, \n",
    "                                                                                                          best_param_df=rival_test_hyperparameters_df)\n",
    "    with open(test_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"cut_off_freq: {test_dict_key}\\n\")\n",
    "        f.write(EER_df_test_dict[test_dict_key].to_string())\n",
    "        \n",
    "    \n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\", \"scnn_archi_name\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dict_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m key_column\u001b[38;5;241m=\u001b[39m \u001b[43mtest_dict_key\u001b[49m\u001b[38;5;66;03m#[\"cut_off_freq\"]\u001b[39;00m\n\u001b[1;32m      2\u001b[0m EER_df_test_dict_df\u001b[38;5;241m=\u001b[39mmake_raw_exp_df_results(EER_df_test_dict, key_column)\n\u001b[1;32m      3\u001b[0m eer_per_window_size_col_df \u001b[38;5;241m=\u001b[39mmake_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst\u001b[38;5;241m=\u001b[39mWINDOW_SIZE_LST)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_dict_key' is not defined"
     ]
    }
   ],
   "source": [
    "key_column= test_dict_key#[\"cut_off_freq\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['SCNN_1_3_conv_1_dense_arg_dict_default', 'SCNN_3_123_conv_1_dense_arg_dict_default', 'SCNN_3_1_conv_1_dense_arg_dict_default', 'SCNN_1_2_conv_1_dense_arg_dict_default', 'SCNN_1_1_conv_1_dense_arg_dict_default', 'SCNN_1_2_conv_2_dense_arg_dict_default', 'SCNN_1_5_conv_1_dense_arg_dict_default', 'SCNN_4_1234_conv_1_dense_arg_dict_default'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 1227\n"
     ]
    }
   ],
   "source": [
    "exp_deep_archi_extractors_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1750\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:02<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "window_size_lst=WINDOW_SIZE_LST\n",
    "exp_config=P\n",
    "extract_features_func_dict=exp_deep_archi_extractors_dict[\"SCNN_1_3_conv_1_dense_arg_dict_default\"]\n",
    "dfList_exp1=dfList_dict[\"dfList_exp1\"]\n",
    "dfList_exp2=dfList_dict[\"dfList_exp2\"]\n",
    "EER_distro_df_dict = {}\n",
    "\n",
    "if exp_config.classifier==\"OCSVM\":\n",
    "    columns = ['window_size', \"step_width\", \"Mean_EER\", \"median_nu\", \"median_gamma\"]\n",
    "\n",
    "elif exp_config.classifier==\"IF\":\n",
    "    columns = ['window_size', \"step_width\", \"Mean_EER\", \"median_n_estimators\", \"median_max_samples\", \"median_contamination\", \"median_max_features\"]\n",
    "\n",
    "elif exp_config.classifier==\"LOF\":\n",
    "    columns = ['window_size', \"step_width\", \"Mean_EER\", \"median_n_neighbors\", \"median_contamination\"]\n",
    "\n",
    "else:\n",
    "    raise Exception(\"Unspecified classifier\")\n",
    "\n",
    "Mean_EER_df = pd.DataFrame(columns = columns)\n",
    "for window_size in tqdm(window_size_lst):\n",
    "    print(f\"window_size: {window_size}\")\n",
    "    step_width = int(window_size * (1-overlap))\n",
    "\n",
    "\n",
    "    X_test_exp1_dict, X_test_exp2_dict, fitted_scaler_test_exp2_dict=get_raw_windows(dfList_exp1, dfList_exp2, window_size, step_width, test_set, exp_config.scaler, \n",
    "                                                                                     exp_config.num_sample_points_per_exp, EMA_per_win_span=exp_config.EMA_per_win_span, \n",
    "                                                                                     SMA_per_win_winsize=exp_config.SMA_per_win_winsize,\n",
    "                                                                                     Butter_per_win_argdict=exp_config.Butter_per_win_argdict, \n",
    "                                                                                     verbose=0)\n",
    "\n",
    "\n",
    "    extract_features_func=extract_features_func_dict[window_size]\n",
    "\n",
    "    X_exp_test_dic = extract_features_func(X_test_exp1_dict, X_test_exp2_dict, fitted_scaler_test_exp2_dict)\n",
    "    \n",
    "X_exp_test_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1750, 6)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_exp1_dict[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 12, 14, 9, 18, 23, 2, 15, 10, 4])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_exp1_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_exp1_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_1_3_conv_1_dense_arg_dict_default/1750'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path=f\"final_experiments_results/{SAVE_PATH_NAME}/SCNN-OCSVM/{EXP_NUM_1_1}\"\n",
    "archi_name = list(TRAINING_CONFIG_DICT['2000'].keys())[0]\n",
    "# archi_name=\"SCNN_3_1_conv_1_dense_arg_dict_default\"\n",
    "path_to_archi_name=base_path+f\"/{archi_name}\"\n",
    "archi_win_dict={}\n",
    "final_path=path_to_archi_name+f\"/{1750}\"\n",
    "final_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = lambda y_true, dist: k_contrastive_loss(y_true, dist, arg_dict[\"contrastive_loss_margin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_1_3_conv_1_dense_arg_dict_default/1750/SCNN-OCSVM-1.1-SCNN_1_3_conv_1_dense_arg_dict_default-1750-deep_feature_extractor\n",
      "ERROR! Session/line number was not unique in database. History logging moved to new session 1233\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "deep_feature_extractor_path=final_path+f\"/SCNN-OCSVM-{EXP_NUM_1_1}-{archi_name}-{1750}-deep_feature_extractor\"\n",
    "print(deep_feature_extractor_path)\n",
    "\n",
    "deep_feature_model=load_model(deep_feature_extractor_path)# custom_objects={'loss_func': loss_func})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.483646  , 0.48248574, 0.48391485, 0.5176617 , 0.48232555,\n",
       "        0.3948905 , 0.6571675 , 0.48332578, 0.51537776, 0.3830801 ,\n",
       "        0.48369867, 0.51706755, 0.6734431 , 0.48458332, 0.48752847,\n",
       "        0.48411527, 0.40387475, 0.518302  , 0.5168323 , 0.48319304,\n",
       "        0.5119249 , 0.36883786, 0.51656306, 0.48535657, 0.5160692 ,\n",
       "        0.49645782, 0.6151099 , 0.3447324 , 0.5164715 , 0.5170238 ,\n",
       "        0.48302537, 0.50929   , 0.48378727, 0.48427698, 0.37200788,\n",
       "        0.4824282 , 0.5118454 , 0.5513184 , 0.5161465 , 0.52022976,\n",
       "        0.51684374, 0.48289943, 0.5146176 , 0.3277162 , 0.5170986 ,\n",
       "        0.51055324, 0.49970394, 0.48305914, 0.4838405 , 0.48391843,\n",
       "        0.5158296 , 0.48297504, 0.48362806, 0.48273656, 0.5167648 ,\n",
       "        0.4758899 , 0.5162947 , 0.48710927, 0.5125867 , 0.51675326,\n",
       "        0.65629625, 0.5182427 , 0.6739007 , 0.665079  , 0.48348317,\n",
       "        0.4852233 , 0.48333368, 0.3298231 , 0.48392308, 0.33389822,\n",
       "        0.5162636 , 0.5160936 , 0.5179101 , 0.48428842, 0.51652765,\n",
       "        0.48321196, 0.51659805, 0.48325184, 0.51740557, 0.4842739 ,\n",
       "        0.4824667 , 0.4836565 , 0.4834537 , 0.5144378 ]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_feature_model.predict(np.array([X_test_exp1_dict[0][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type dict).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mExtractDeepFeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_exp1_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep_feature_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/ipykernel_97/2623984813.py:32\u001b[0m, in \u001b[0;36mExtractDeepFeatures\u001b[0;34m(X_exp, deep_feature_model)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mExtractDeepFeatures\u001b[39m(X_exp, deep_feature_model):\n\u001b[1;32m     30\u001b[0m         \n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# if X_exp and deep_feature_model:\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeep_feature_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_exp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1720\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1714\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   1715\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing Model.predict with \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1716\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultiWorkerDistributionStrategy or TPUStrategy and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1717\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoShardPolicy.FILE might lead to out-of-order result\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1718\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Consider setting it to AutoShardPolicy.DATA.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1720\u001b[0m data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/data_adapter.py:1383\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_coordinator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1382\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/data_adapter.py:1138\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1135\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution_value \u001b[38;5;241m=\u001b[39m steps_per_execution\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1137\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1152\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/data_adapter.py:230\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    220\u001b[0m              x,\n\u001b[1;32m    221\u001b[0m              y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m              shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    228\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    229\u001b[0m   \u001b[38;5;28msuper\u001b[39m(TensorLikeDataAdapter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(x, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 230\u001b[0m   x, y, sample_weights \u001b[38;5;241m=\u001b[39m \u001b[43m_process_tensorlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m   sample_weight_modes \u001b[38;5;241m=\u001b[39m broadcast_sample_weight_modes(\n\u001b[1;32m    232\u001b[0m       sample_weights, sample_weight_modes)\n\u001b[1;32m    234\u001b[0m   \u001b[38;5;66;03m# If sample_weights are not specified for an output use 1.0 as weights.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/data_adapter.py:1031\u001b[0m, in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n\u001b[1;32m   1029\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m-> 1031\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_convert_numpy_and_scipy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mlist_to_tuple(inputs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/nest.py:869\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/nest.py:869\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/data_adapter.py:1026\u001b[0m, in \u001b[0;36m_process_tensorlike.<locals>._convert_numpy_and_scipy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1024\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, np\u001b[38;5;241m.\u001b[39mfloating):\n\u001b[1;32m   1025\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mfloatx()\n\u001b[0;32m-> 1026\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_scipy_sparse(x):\n\u001b[1;32m   1028\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m    208\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m    209\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1430\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[1;32m   1369\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1370\u001b[0m   \u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[1;32m   1371\u001b[0m \n\u001b[1;32m   1372\u001b[0m \u001b[38;5;124;03m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;124;03m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1430\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1431\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1436\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_tensor_v2\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1435\u001b[0m   \u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1436\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1440\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_hint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1441\u001b[0m \u001b[43m      \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py:163\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1566\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1561\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor did not convert to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1562\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe preferred dtype: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1563\u001b[0m                       (ret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype, preferred_dtype\u001b[38;5;241m.\u001b[39mbase_dtype))\n\u001b[1;32m   1565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1566\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1569\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:52\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m as_ref  \u001b[38;5;66;03m# Unused.\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:271\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    176\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:283\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    282\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 283\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m g \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[1;32m    286\u001b[0m tensor_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:308\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    307\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:106\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    105\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type dict)."
     ]
    }
   ],
   "source": [
    "ExtractDeepFeatures([X_test_exp1_dict], deep_feature_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_learnt_features(X_test_exp1_dict, X_test_exp2_dict, fitted_scaler_test_exp2_dict, deep_feature_model, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MakeDeepXExpDic(X_test_exp1_dict, X_test_exp2_dict, fitted_scaler_test_exp2_dict, \n",
    "                    X_test_exp1_dict, X_test_exp2_dict, fitted_scaler_test_exp2_dict, deep_feature_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 1234\n"
     ]
    }
   ],
   "source": [
    "extract_features_func(X_test_exp1_dict, X_test_exp2_dict, fitted_scaler_test_exp2_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "exp_deep_feature_extractors_dict=get_exp_deep_feature_extractors_dict(exp_num=EXP_NUM_1_2)\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butterworth\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P.Butter_per_win_argdict={\n",
    "    \"filter_order\": P.filter_order,\n",
    "    \"cut_off_freq\": None,\n",
    "    \"sampling_freq\": P.sampling_freq,\n",
    "    \"filtfilt\": P.filtfilt,\n",
    "    }\n",
    "\n",
    "overlap=0.5\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for scnn_archi_name, exp_deep_feature_extractors_dict in exp_deep_archi_extractors_dict.items():\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.Butter_per_win_argdict[\"cut_off_freq\"]=cut_off_freq\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "\n",
    "    \n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    \n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": ffted_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": ffted_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.cut_off_freq] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                                    extract_features_func_dict=extract_WACA_features, overlap=OVERLAP, \n",
    "                                                                                                    param_dist=param_dist)\n",
    "        \n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq: {P.cut_off_freq}\\n\")\n",
    "        f.write(EER_df_train_dict[P.cut_off_freq].to_string())\n",
    "\n",
    "\n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=extract_WACA_features, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Butterworth frequency Cut-off + EMA span\n",
    "## 2.1 Naive Approach\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_2)\n",
    "\n",
    "exp_deep_feature_extractors_dict=get_exp_deep_feature_extractors_dict(exp_num=EXP_NUM_2_1)\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butter+EMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "    \n",
    "indices = list(range(1, 50))\n",
    "mesh = np.array(np.meshgrid(indices, indices))\n",
    "index_pairs = mesh.T.reshape(-1, 2)\n",
    "\n",
    "print(f\"total cut_off_span_pairs: {index_pairs.shape}, choice_num: {choice_num}\")\n",
    "cut_off_span_pairs = index_pairs[np.random.choice(index_pairs.shape[0], size=CHOICE_NUM_PAIRS, replace=False), :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "overlap=0.5\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for key_pair in tqdm(cut_off_span_pairs):\n",
    "    \n",
    "    key_pair = tuple(key_pair)\n",
    "    cut_off_freq, span = key_pair[0], key_pair[1]\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.span=span\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "    print(f\"span: {P.span}\")\n",
    "\n",
    "    \n",
    "    ffted_dfList_exp1 = get_ffted_dfList(raw_dfList_exp1, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    EMAed_dfList_exp1 = get_EMAed_dfList(ffted_dfList_exp1, span=P.span)\n",
    "    EMAed_dfList_exp2 = get_EMAed_dfList(ffted_dfList_exp2, span=P.span)\n",
    "    \n",
    "    ffted_dfList_exp1_user_47 = get_ffted_dfList(raw_dfList_exp1_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    EMAed_dfList_exp1_user_47 = get_EMAed_dfList(ffted_dfList_exp1_user_47, span=P.span)\n",
    "    EMAed_dfList_exp2_user_47 = get_EMAed_dfList(ffted_dfList_exp2_user_47, span=P.span)\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": EMAed_dfList_exp1,\n",
    "                \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": EMAed_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[key_pair] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                      extract_features_func_dict=extract_WACA_features, overlap=OVERLAP, \n",
    "                                                                                      param_dist=param_dist)\n",
    "        \n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq, span: {key_pair}\\n\")\n",
    "        f.write(EER_df_train_dict[key_pair].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=extract_WACA_features, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------  \n",
    "#-------\n",
    "key_column= [\"cut_off_freq\", \"EMA_span\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_2)\n",
    "\n",
    "exp_deep_feature_extractors_dict=get_exp_deep_feature_extractors_dict(exp_num=EXP_NUM_2_2)\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butter+EMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "    \n",
    "indices = list(range(1, 50))\n",
    "mesh = np.array(np.meshgrid(indices, indices))\n",
    "index_pairs = mesh.T.reshape(-1, 2)\n",
    "\n",
    "print(f\"total cut_off_span_pairs: {index_pairs.shape}, choice_num: {choice_num}\")\n",
    "cut_off_span_pairs = index_pairs[np.random.choice(index_pairs.shape[0], size=CHOICE_NUM_PAIRS, replace=False), :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P.Butter_per_win_argdict={\n",
    "    \"filter_order\": P.filter_order,\n",
    "    \"cut_off_freq\": None,\n",
    "    \"sampling_freq\": P.sampling_freq,\n",
    "    \"filtfilt\": P.filtfilt,\n",
    "    }\n",
    "\n",
    "overlap=0.5\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for key_pair in tqdm(cut_off_span_pairs):\n",
    "    key_pair = tuple(key_pair)\n",
    "    cut_off_freq, span = key_pair[0], key_pair[1]\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.Butter_per_win_argdict[\"cut_off_freq\"]=cut_off_freq\n",
    "    \n",
    "    \n",
    "    P.span=span\n",
    "    P.EMA_per_win_span=span\n",
    "\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "    print(f\"span: {P.span}\")\n",
    "\n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    EMAed_dfList_exp2 = get_EMAed_dfList(ffted_dfList_exp2, span=P.span)\n",
    "    \n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    EMAed_dfList_exp2_user_47 = get_EMAed_dfList(ffted_dfList_exp2_user_47, span=P.span)\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[key_pair] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                      extract_features_func_dict=extract_WACA_features, overlap=OVERLAP, \n",
    "                                                                                      param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq, span: {key_pair}\\n\")\n",
    "        f.write(EER_df_train_dict[key_pair].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=extract_WACA_features, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\", \"EMA_span\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. EMA span\n",
    "## 3.1 Naive Approach\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "exp_deep_feature_extractors_dict=get_exp_deep_feature_extractors_dict(exp_num=EXP_NUM_3_1)\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"EMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "overlap=0.5\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for span in tqdm(list(range(1, 50, 23))):\n",
    "    P.span=span\n",
    "    print(f\"EMA span: {P.span}\")\n",
    "\n",
    "    \n",
    "    EMAed_dfList_exp1 = get_EMAed_dfList(raw_dfList_exp1, span=P.span)\n",
    "    EMAed_dfList_exp2 = get_EMAed_dfList(raw_dfList_exp2, span=P.span)\n",
    "    \n",
    "    EMAed_dfList_exp1_user_47 = get_EMAed_dfList(raw_dfList_exp1_user_47, span=P.span)\n",
    "    EMAed_dfList_exp2_user_47 = get_EMAed_dfList(raw_dfList_exp2_user_47, span=P.span)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": EMAed_dfList_exp1,\n",
    "                \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": EMAed_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.span] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                    extract_features_func_dict=extract_WACA_features, overlap=OVERLAP, \n",
    "                                                                                    param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\EMA span: {P.span}\\n\")\n",
    "        f.write(EER_df_train_dict[P.span].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=extract_WACA_features, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"EMA_span\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "exp_deep_feature_extractors_dict=get_exp_deep_feature_extractors_dict(exp_num=EXP_NUM_3_2)\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"EMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    " \n",
    "\n",
    "\n",
    "overlap=0.5\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for span in tqdm(list(range(1, 50, 23))):\n",
    "    P.span=span\n",
    "    print(f\"EMA span: {P.span}\")\n",
    "\n",
    "    P.EMA_per_win_span=P.span\n",
    "\n",
    "    EMAed_dfList_exp2 = get_EMAed_dfList(raw_dfList_exp2, span=P.span)\n",
    "    \n",
    "    EMAed_dfList_exp2_user_47 = get_EMAed_dfList(raw_dfList_exp2_user_47, span=P.span)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.span] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                    extract_features_func_dict=extract_WACA_features, overlap=OVERLAP, \n",
    "                                                                                    param_dist=param_dist)\n",
    "        \n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\EMA span: {P.span}\\n\")\n",
    "        f.write(EER_df_train_dict[P.span].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=extract_WACA_features, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"EMA_span\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SMA winsize\n",
    "## 4.1 Naive Approach\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "exp_deep_feature_extractors_dict=get_exp_deep_feature_extractors_dict(exp_num=EXP_NUM_4_1)\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"SMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "overlap=0.5\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for winsize in tqdm(list(range(1, 50, 23))):\n",
    "    P.winsize=winsize\n",
    "    print(f\"SMA winsize: {P.winsize}\")\n",
    "\n",
    "\n",
    "    SMAed_dfList_exp1 = get_SMAed_dfList(raw_dfList_exp1, winsize=P.winsize)\n",
    "    SMAed_dfList_exp2 = get_SMAed_dfList(raw_dfList_exp2, winsize=P.winsize)\n",
    "    \n",
    "    SMAed_dfList_exp1_user_47 = get_SMAed_dfList(raw_dfList_exp1_user_47, winsize=P.winsize)\n",
    "    SMAed_dfList_exp2_user_47 = get_SMAed_dfList(raw_dfList_exp2_user_47, winsize=P.winsize)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": SMAed_dfList_exp1,\n",
    "                \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": SMAed_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.winsize] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                       extract_features_func_dict=extract_WACA_features, overlap=OVERLAP, \n",
    "                                                                                       param_dist=param_dist)\n",
    "        \n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\SMA winsize: {P.winsize}\\n\")\n",
    "        f.write(EER_df_train_dict[P.winsize].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=extract_WACA_features, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"SMA_winsize\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "exp_deep_feature_extractors_dict=get_exp_deep_feature_extractors_dict(exp_num=EXP_NUM_4_2)\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"SMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "overlap=0.5\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for winsize in tqdm(list(range(1, 50, 23))):\n",
    "    P.winsize=winsize\n",
    "    P.SMA_per_win_winsize=P.winsize\n",
    "\n",
    "    print(f\"SMA winsize: {P.winsize}\")\n",
    "\n",
    "\n",
    "    SMAed_dfList_exp2 = get_SMAed_dfList(raw_dfList_exp2, winsize=P.winsize)\n",
    "    \n",
    "    SMAed_dfList_exp2_user_47 = get_SMAed_dfList(raw_dfList_exp2_user_47, winsize=P.winsize)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.winsize] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                       extract_features_func_dict=extract_WACA_features, overlap=OVERLAP, \n",
    "                                                                                       param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\SMA winsize: {P.winsize}\\n\")\n",
    "        f.write(EER_df_train_dict[P.winsize].to_string())\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=extract_WACA_features, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"SMA_winsize\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Butterworth frequency Cut-off + SMA winsize\n",
    "## 5.1 Naive Approach\n",
    "### Optimizing and Testin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_5)\n",
    "\n",
    "exp_deep_feature_extractors_dict=get_exp_deep_feature_extractors_dict(exp_num=EXP_NUM_5_1)\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butter+SMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "    \n",
    "indices = list(range(1, 50))\n",
    "mesh = np.array(np.meshgrid(indices, indices))\n",
    "index_pairs = mesh.T.reshape(-1, 2)\n",
    "\n",
    "print(f\"total cut_off_winsize_pairs: {index_pairs.shape}, choice_num: {choice_num}\")\n",
    "cut_off_winsize_pairs = index_pairs[np.random.choice(index_pairs.shape[0], size=CHOICE_NUM_PAIRS, replace=False), :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "overlap=0.5\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for key_pair in tqdm(cut_off_winsize_pairs):\n",
    "    \n",
    "    key_pair = tuple(key_pair)\n",
    "    cut_off_freq, winsize = key_pair[0], key_pair[1]\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.winsize=winsize\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "    print(f\"winsize: {P.winsize}\")\n",
    "\n",
    "    \n",
    "    ffted_dfList_exp1 = get_ffted_dfList(raw_dfList_exp1, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    SMAed_dfList_exp1 = get_SMAed_dfList(ffted_dfList_exp1, winsize=P.winsize)\n",
    "    SMAed_dfList_exp2 = get_SMAed_dfList(ffted_dfList_exp2, winsize=P.winsize)\n",
    "    \n",
    "    ffted_dfList_exp1_user_47 = get_ffted_dfList(raw_dfList_exp1_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    SMAed_dfList_exp1_user_47 = get_SMAed_dfList(ffted_dfList_exp1_user_47, winsize=P.winsize)\n",
    "    SMAed_dfList_exp2_user_47 = get_SMAed_dfList(ffted_dfList_exp2_user_47, winsize=P.winsize)\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": SMAed_dfList_exp1,\n",
    "                \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": SMAed_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[key_pair] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                      extract_features_func_dict=extract_WACA_features, overlap=OVERLAP, \n",
    "                                                                                      param_dist=param_dist)\n",
    "\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq, winsize: {key_pair}\\n\")\n",
    "        f.write(EER_df_train_dict[key_pair].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=extract_WACA_features, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\", \"SMA_winsize\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(key_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_5)\n",
    "\n",
    "exp_deep_feature_extractors_dict=get_exp_deep_feature_extractors_dict(exp_num=EXP_NUM_5_2)\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butter+SMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "    \n",
    "indices = list(range(1, 50))\n",
    "mesh = np.array(np.meshgrid(indices, indices))\n",
    "index_pairs = mesh.T.reshape(-1, 2)\n",
    "\n",
    "print(f\"total cut_off_winsize_pairs: {index_pairs.shape}, choice_num: {choice_num}\")\n",
    "cut_off_winsize_pairs = index_pairs[np.random.choice(index_pairs.shape[0], size=CHOICE_NUM_PAIRS, replace=False), :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P.Butter_per_win_argdict={\n",
    "    \"filter_order\": P.filter_order,\n",
    "    \"cut_off_freq\": None,\n",
    "    \"sampling_freq\": P.sampling_freq,\n",
    "    \"filtfilt\": P.filtfilt,\n",
    "    }\n",
    "\n",
    "overlap=0.5\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for key_pair in tqdm(cut_off_winsize_pairs):\n",
    "    \n",
    "    key_pair = tuple(key_pair)\n",
    "    cut_off_freq, winsize = key_pair[0], key_pair[1]\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.Butter_per_win_argdict[\"cut_off_freq\"]=P.cut_off_freq\n",
    "    P.winsize=winsize\n",
    "    P.SMA_per_win_winsize=P.winsize\n",
    "    \n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "    print(f\"winsize: {P.winsize}\")\n",
    "\n",
    "    \n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    SMAed_dfList_exp2 = get_SMAed_dfList(ffted_dfList_exp2, winsize=P.winsize)\n",
    "    \n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    SMAed_dfList_exp2_user_47 = get_SMAed_dfList(ffted_dfList_exp2_user_47, winsize=P.winsize)\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[key_pair] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                      extract_features_func_dict=extract_WACA_features, overlap=OVERLAP, \n",
    "                                                                                      param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq, winsize: {key_pair}\\n\")\n",
    "        f.write(EER_df_train_dict[key_pair].to_string())\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=extract_WACA_features, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\", \"SMA_winsize\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. The effect of Varying Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "\n",
    "\n",
    "\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/overlap_Mean_EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/overlap_Mean_EER_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for overlap in tqdm(list(range(50, 52))):\n",
    "    overlap*=0.01\n",
    "    max_window_size=2000\n",
    "    step_width = int(max_window_size * (1-overlap))\n",
    "    max_num_windows=min(len(getIndices(sampleSize=max_window_size, step=step_width, numSamplePoints=P.num_sample_points_per_exp)), 20)\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": raw_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": raw_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[overlap] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                     extract_features_func_dict=extract_WACA_features, overlap=OVERLAP, \n",
    "                                                                                     param_dist=param_dist)\n",
    "        \n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\overlap: {overlap}\\n\")\n",
    "        f.write(EER_df_train_dict[overlap].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=extract_WACA_features, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"overlap\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=return_and_save_final_result_df_as_json(final_exp_results_path=FINAL_EXP_RESULTS_PATH, exp_path_name=EXP_PATH_NAME, window_size_lst=WINDOW_SIZE_LST)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df=return_and_save_final_relative_result_df_as_json(df, base_case_index=0, final_exp_results_path=FINAL_EXP_RESULTS_PATH, exp_path_name=EXP_PATH_NAME, \n",
    "                                                             window_size_lst=WINDOW_SIZE_LST)\n",
    "relative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.style.hide(axis='index').to_latex()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df.style.hide(axis='index').to_latex()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
