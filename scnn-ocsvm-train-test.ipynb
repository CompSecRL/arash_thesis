{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "EXP_PATH_NAME=\"SCNN-OCSVM\"\n",
    "RIVAL_PATH_NAME=\"WACA-OCSVM\"\n",
    "joblib.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "EER: 0.333, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.400, Threshold: 0.200 <-- Worse case\n",
      "EER: 0.167, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.333, Threshold: 1.000 <-- Worse case\n",
      "--------------------\u001b[32mUtility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mPreprocessing utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mNeural Networks utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "EER: 0.333, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.400, Threshold: 0.200 <-- Worse case\n",
      "EER: 0.167, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.333, Threshold: 1.000 <-- Worse case\n",
      "--------------------\u001b[32mUtility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mPreprocessing utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mWACA utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mClassification utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "Numpy Seed was set to: 567\n",
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade pip\n",
    "\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import dataclasses\n",
    "from sklearn.svm import OneClassSVM\n",
    "from dataclasses import asdict\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import random\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold # Feature selector\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Global utitlity functions are in separate notebook\n",
    "%run ./Classification_utility-functions.ipynb\n",
    "%run ./SEED-CONSTANTS.ipynb\n",
    "\n",
    "np.random.seed(SEED)\n",
    "print(f\"Numpy Seed was set to: {SEED}\")\n",
    "\n",
    "print(\"Setup Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.2'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__dir__()\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class ExperimentParameters:\n",
    "    \"\"\"Contains all relevant parameters to run an experiment.\"\"\"\n",
    "\n",
    "    name: str  # Name of Parameter set. Used as identifier for charts etc.\n",
    "    frequency: int\n",
    "    max_subjects: int\n",
    "    max_test_subjects: int\n",
    "        \n",
    "    user_ids: list\n",
    "    num_sample_points_per_exp: int\n",
    "    exp_begin_cutoff_idx: int\n",
    "    exp_end_cutoff_idx: int\n",
    "        \n",
    "    \n",
    "    seconds_per_subject_train: float\n",
    "    seconds_per_subject_test: float\n",
    "    window_size: int  # After resampling\n",
    "    ocsvm_step_width: int  # After resampling\n",
    "    scaler: str  # StandardScaler, MinMaxScaler, Normalizer, MaxAbsScaler, RobustScaler, PowerTransformer\n",
    "    scaler_scope: str  # {\"subject\", \"session\"}\n",
    "    scaler_global: bool  # fit transform scale on all data (True) or fit on training only (False)\n",
    "    ocsvm_kernel: str # ocsvm kernel\n",
    "    ocsvm_nu: float  # Best value found in random search, used for final model\n",
    "    ocsvm_gamma: float  # Best value found in random search, used for final model\n",
    "    feature_cols: list  # Columns used as features\n",
    "    exclude_subjects: list  # Don't load data from those users\n",
    "        \n",
    "    # Calculated values\n",
    "    def __post_init__(self):\n",
    "        # HDF key of table:\n",
    "        self.table_name = f\"sensors_{self.frequency}hz\"\n",
    "\n",
    "        \n",
    "\n",
    "# INSTANCES\n",
    "# ===========================================================\n",
    "\n",
    "# NAIVE_APPROACH\n",
    "# -----------------------------------------------------------\n",
    "NAIVE_MINMAX_OCSVM = ExperimentParameters(\n",
    "    name=\"NAIVE-MINMAX_OCSVM\",\n",
    "    frequency=100,\n",
    "    max_subjects=29,\n",
    "    max_test_subjects=10,\n",
    "    user_ids = [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 28, 29, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49],\n",
    "    num_sample_points_per_exp=21000,\n",
    "    exp_begin_cutoff_idx=500,\n",
    "    exp_end_cutoff_idx=-500,\n",
    "    seconds_per_subject_train=210,\n",
    "    seconds_per_subject_test=210,    \n",
    "    window_size=250,\n",
    "    ocsvm_step_width=250,\n",
    "    scaler=\"minmax\",\n",
    "    scaler_scope=\"subject\",\n",
    "    scaler_global=True,\n",
    "    ocsvm_kernel=\"rbf\",\n",
    "    ocsvm_nu=None,\n",
    "    ocsvm_gamma=None,\n",
    "    feature_cols=[\n",
    "        \"x_a\",\n",
    "        \"y_a\",\n",
    "        \"z_a\",\n",
    "        \"x_g\",\n",
    "        \"y_g\",\n",
    "        \"z_g\",\n",
    "    ],\n",
    "    exclude_subjects=[],\n",
    ")\n",
    "\n",
    "# VALID_APPROACH\n",
    "# -----------------------------------------------------------\n",
    "VALID_MINMAX_OCSVM = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-MINMAX-OCSVM\",\n",
    "    scaler_global=False,\n",
    "    ocsvm_nu=0.165,\n",
    "    ocsvm_gamma=0.039,\n",
    ")\n",
    "\n",
    "# NAIVE_ROBUST_APPROACH\n",
    "# -----------------------------------------------------------\n",
    "NAIVE_ROBUST_OCSVM = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"NAIVE-ROBUST-OCSVM\",\n",
    "    scaler=\"robust\",\n",
    "    scaler_global=True,\n",
    "    ocsvm_nu=0.153,\n",
    "    ocsvm_gamma=0.091,  # below median, selected by chart\n",
    ")\n",
    "\n",
    "# ROBUST_APPROACH (VALID)\n",
    "# -----------------------------------------------------------\n",
    "VALID_ROBUST_OCSVM_125 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=125\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "\n",
    "VALID_ROBUST_OCSVM_250 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=250\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_500 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=500\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_750 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=750\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_1000 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=1000\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_1250 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=1250\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_1500 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=1500\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_1750 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=1750\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_2000 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=2000\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "# NORMALIZER_APPROACH (VALID)\n",
    "# -----------------------------------------------------------\n",
    "VALID_NORMALIZER_OCSVM = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-NORMALIZER-OCSVM\",\n",
    "    scaler=\"Normalizer\",\n",
    "    scaler_global=False,\n",
    "    ocsvm_nu=0.074,\n",
    "    ocsvm_gamma= 0.029,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\"></th>\n",
       "      <th colspan=\"9\" halign=\"left\">Window Size</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean rank</th>\n",
       "      <th>cut_off_freq</th>\n",
       "      <th>EMA_span</th>\n",
       "      <th>SMA_winsize</th>\n",
       "      <th>type</th>\n",
       "      <th>125</th>\n",
       "      <th>250</th>\n",
       "      <th>500</th>\n",
       "      <th>750</th>\n",
       "      <th>1000</th>\n",
       "      <th>1250</th>\n",
       "      <th>1500</th>\n",
       "      <th>1750</th>\n",
       "      <th>2000</th>\n",
       "      <th>mean</th>\n",
       "      <th>variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Naive</td>\n",
       "      <td>0.227313</td>\n",
       "      <td>0.171836</td>\n",
       "      <td>0.141258</td>\n",
       "      <td>0.139333</td>\n",
       "      <td>0.131003</td>\n",
       "      <td>0.125799</td>\n",
       "      <td>0.115267</td>\n",
       "      <td>0.111014</td>\n",
       "      <td>0.093000</td>\n",
       "      <td>0.139536</td>\n",
       "      <td>0.001573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.226914</td>\n",
       "      <td>0.170020</td>\n",
       "      <td>0.146292</td>\n",
       "      <td>0.141475</td>\n",
       "      <td>0.127317</td>\n",
       "      <td>0.127153</td>\n",
       "      <td>0.117160</td>\n",
       "      <td>0.112222</td>\n",
       "      <td>0.090389</td>\n",
       "      <td>0.139882</td>\n",
       "      <td>0.001572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>-</td>\n",
       "      <td>25</td>\n",
       "      <td>-</td>\n",
       "      <td>Naive</td>\n",
       "      <td>0.192994</td>\n",
       "      <td>0.158703</td>\n",
       "      <td>0.142343</td>\n",
       "      <td>0.153697</td>\n",
       "      <td>0.140488</td>\n",
       "      <td>0.136146</td>\n",
       "      <td>0.155267</td>\n",
       "      <td>0.136812</td>\n",
       "      <td>0.151167</td>\n",
       "      <td>0.151957</td>\n",
       "      <td>0.000306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>-</td>\n",
       "      <td>9</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.362315</td>\n",
       "      <td>0.309108</td>\n",
       "      <td>0.259384</td>\n",
       "      <td>0.226525</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.193125</td>\n",
       "      <td>0.189506</td>\n",
       "      <td>0.173382</td>\n",
       "      <td>0.171278</td>\n",
       "      <td>0.233847</td>\n",
       "      <td>0.004283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>41</td>\n",
       "      <td>Naive</td>\n",
       "      <td>0.196284</td>\n",
       "      <td>0.157013</td>\n",
       "      <td>0.149103</td>\n",
       "      <td>0.146646</td>\n",
       "      <td>0.149404</td>\n",
       "      <td>0.142569</td>\n",
       "      <td>0.149465</td>\n",
       "      <td>0.153333</td>\n",
       "      <td>0.152778</td>\n",
       "      <td>0.155177</td>\n",
       "      <td>0.000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.397188</td>\n",
       "      <td>0.357325</td>\n",
       "      <td>0.298996</td>\n",
       "      <td>0.253192</td>\n",
       "      <td>0.242033</td>\n",
       "      <td>0.221389</td>\n",
       "      <td>0.213580</td>\n",
       "      <td>0.182802</td>\n",
       "      <td>0.184500</td>\n",
       "      <td>0.261223</td>\n",
       "      <td>0.005684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>10</td>\n",
       "      <td>-</td>\n",
       "      <td>Naive</td>\n",
       "      <td>0.213821</td>\n",
       "      <td>0.164012</td>\n",
       "      <td>0.146345</td>\n",
       "      <td>0.141535</td>\n",
       "      <td>0.134824</td>\n",
       "      <td>0.117049</td>\n",
       "      <td>0.110988</td>\n",
       "      <td>0.107729</td>\n",
       "      <td>0.114778</td>\n",
       "      <td>0.139009</td>\n",
       "      <td>0.001142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.291194</td>\n",
       "      <td>0.220732</td>\n",
       "      <td>0.175596</td>\n",
       "      <td>0.164606</td>\n",
       "      <td>0.154146</td>\n",
       "      <td>0.157569</td>\n",
       "      <td>0.128642</td>\n",
       "      <td>0.115894</td>\n",
       "      <td>0.117333</td>\n",
       "      <td>0.169524</td>\n",
       "      <td>0.003142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>-</td>\n",
       "      <td>41</td>\n",
       "      <td>Naive</td>\n",
       "      <td>0.225110</td>\n",
       "      <td>0.174325</td>\n",
       "      <td>0.153025</td>\n",
       "      <td>0.147010</td>\n",
       "      <td>0.138320</td>\n",
       "      <td>0.139410</td>\n",
       "      <td>0.127531</td>\n",
       "      <td>0.128937</td>\n",
       "      <td>0.132056</td>\n",
       "      <td>0.151747</td>\n",
       "      <td>0.000968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.318503</td>\n",
       "      <td>0.235023</td>\n",
       "      <td>0.188969</td>\n",
       "      <td>0.172444</td>\n",
       "      <td>0.161192</td>\n",
       "      <td>0.154618</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.123575</td>\n",
       "      <td>0.134222</td>\n",
       "      <td>0.179916</td>\n",
       "      <td>0.003882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Window Size            \\\n",
       "  mean rank cut_off_freq EMA_span SMA_winsize   type         125       250   \n",
       "0         2           41        -           -  Naive    0.227313  0.171836   \n",
       "1         3           49        -           -   Real    0.226914  0.170020   \n",
       "2         5            -       25           -  Naive    0.192994  0.158703   \n",
       "3         9            -        9           -   Real    0.362315  0.309108   \n",
       "4         6            -        -          41  Naive    0.196284  0.157013   \n",
       "5        10            -        -           1   Real    0.397188  0.357325   \n",
       "6         1           37       10           -  Naive    0.213821  0.164012   \n",
       "7         7           16        2           -   Real    0.291194  0.220732   \n",
       "8         4           39        -          41  Naive    0.225110  0.174325   \n",
       "9         8           20        -           4   Real    0.318503  0.235023   \n",
       "\n",
       "                                                                         \\\n",
       "        500       750      1000      1250      1500      1750      2000   \n",
       "0  0.141258  0.139333  0.131003  0.125799  0.115267  0.111014  0.093000   \n",
       "1  0.146292  0.141475  0.127317  0.127153  0.117160  0.112222  0.090389   \n",
       "2  0.142343  0.153697  0.140488  0.136146  0.155267  0.136812  0.151167   \n",
       "3  0.259384  0.226525  0.220000  0.193125  0.189506  0.173382  0.171278   \n",
       "4  0.149103  0.146646  0.149404  0.142569  0.149465  0.153333  0.152778   \n",
       "5  0.298996  0.253192  0.242033  0.221389  0.213580  0.182802  0.184500   \n",
       "6  0.146345  0.141535  0.134824  0.117049  0.110988  0.107729  0.114778   \n",
       "7  0.175596  0.164606  0.154146  0.157569  0.128642  0.115894  0.117333   \n",
       "8  0.153025  0.147010  0.138320  0.139410  0.127531  0.128937  0.132056   \n",
       "9  0.188969  0.172444  0.161192  0.154618  0.130700  0.123575  0.134222   \n",
       "\n",
       "                       \n",
       "       mean  variance  \n",
       "0  0.139536  0.001573  \n",
       "1  0.139882  0.001572  \n",
       "2  0.151957  0.000306  \n",
       "3  0.233847  0.004283  \n",
       "4  0.155177  0.000255  \n",
       "5  0.261223  0.005684  \n",
       "6  0.139009  0.001142  \n",
       "7  0.169524  0.003142  \n",
       "8  0.151747  0.000968  \n",
       "9  0.179916  0.003882  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = VALID_ROBUST_OCSVM_2000\n",
    "P.ocsvm_step_width = int(P.window_size * .5)\n",
    "P.classifier=\"OCSVM\"\n",
    "P.ocsvm_kernel = \"rbf\"\n",
    "P.train_cores=1 # 20 cores for every user and 1 core for the nested crossval function\n",
    "P.test_cores=2 # 10 cores for every user and 2 for the nested crossval function\n",
    "\n",
    "\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{RIVAL_PATH_NAME}/{RIVAL_PATH_NAME}-df.json\"\n",
    "RIVAL_DF = pd.read_json(test_file_name)\n",
    "RIVAL_DF\n",
    "# Convert the string representation of tuples back to tuples\n",
    "idx_tuples = [eval(i) for i in RIVAL_DF.columns.tolist()]\n",
    "\n",
    "# # Convert list of tuples back to a MultiIndex\n",
    "multiindex_columns = pd.MultiIndex.from_tuples(idx_tuples)\n",
    "\n",
    "RIVAL_DF.columns = multiindex_columns\n",
    "RIVAL_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window_size</th>\n",
       "      <th>step_width</th>\n",
       "      <th>Mean_EER</th>\n",
       "      <th>median_nu</th>\n",
       "      <th>median_gamma</th>\n",
       "      <th>cut_off_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125</td>\n",
       "      <td>62</td>\n",
       "      <td>0.227313</td>\n",
       "      <td>0.035779</td>\n",
       "      <td>0.655129</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250</td>\n",
       "      <td>125</td>\n",
       "      <td>0.171836</td>\n",
       "      <td>0.055147</td>\n",
       "      <td>0.372759</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>250</td>\n",
       "      <td>0.141258</td>\n",
       "      <td>0.041895</td>\n",
       "      <td>0.372759</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>750</td>\n",
       "      <td>375</td>\n",
       "      <td>0.139333</td>\n",
       "      <td>0.034759</td>\n",
       "      <td>0.120679</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>0.131003</td>\n",
       "      <td>0.035779</td>\n",
       "      <td>0.068665</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1250</td>\n",
       "      <td>625</td>\n",
       "      <td>0.125799</td>\n",
       "      <td>0.040876</td>\n",
       "      <td>0.068665</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1500</td>\n",
       "      <td>750</td>\n",
       "      <td>0.115267</td>\n",
       "      <td>0.033740</td>\n",
       "      <td>0.068665</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1750</td>\n",
       "      <td>875</td>\n",
       "      <td>0.111014</td>\n",
       "      <td>0.031701</td>\n",
       "      <td>0.068665</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.093000</td>\n",
       "      <td>0.034759</td>\n",
       "      <td>0.068665</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   window_size  step_width  Mean_EER  median_nu  median_gamma  cut_off_freq\n",
       "0          125          62  0.227313   0.035779      0.655129            41\n",
       "1          250         125  0.171836   0.055147      0.372759            41\n",
       "2          500         250  0.141258   0.041895      0.372759            41\n",
       "3          750         375  0.139333   0.034759      0.120679            41\n",
       "4         1000         500  0.131003   0.035779      0.068665            41\n",
       "5         1250         625  0.125799   0.040876      0.068665            41\n",
       "6         1500         750  0.115267   0.033740      0.068665            41\n",
       "7         1750         875  0.111014   0.031701      0.068665            41\n",
       "8         2000        1000  0.093000   0.034759      0.068665            41"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.smoothing = \"Butterworth\"\n",
    "\n",
    "\n",
    "#range(1000, 2001, 250)#\n",
    "preprocessing_method=\"Naive\"\n",
    "rival_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{RIVAL_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "rival_test_hyperparameters_df = pd.read_json(rival_test_hyperparameters_file_name)\n",
    "rival_test_hyperparameters_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils_ppp(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils_eer_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading exp1 data:\n",
      "1) accel_count: 28388, gyro_count: 31997\n",
      "2) accel_count: 26010, gyro_count: 28954\n",
      "3) accel_count: 28227, gyro_count: 31814\n",
      "4) accel_count: 24860, gyro_count: 26105\n",
      "5) accel_count: 24270, gyro_count: 24347\n",
      "6) accel_count: 25012, gyro_count: 25060\n",
      "7) accel_count: 25301, gyro_count: 25382\n",
      "8) accel_count: 21975, gyro_count: 21658\n",
      "19) accel_count: 24110, gyro_count: 25050\n",
      "21) accel_count: 24326, gyro_count: 23809\n",
      "22) accel_count: 29123, gyro_count: 28724\n",
      "26) accel_count: 23148, gyro_count: 24291\n",
      "27) accel_count: 24299, gyro_count: 23589\n",
      "28) accel_count: 23807, gyro_count: 24523\n",
      "29) accel_count: 24030, gyro_count: 23457\n",
      "35) accel_count: 24388, gyro_count: 23673\n",
      "36) accel_count: 24228, gyro_count: 24208\n",
      "37) accel_count: 31945, gyro_count: 31816\n",
      "38) accel_count: 22135, gyro_count: 22327\n",
      "39) accel_count: 23573, gyro_count: 23459\n",
      "40) accel_count: 23057, gyro_count: 24296\n",
      "41) accel_count: 24102, gyro_count: 23681\n",
      "42) accel_count: 24074, gyro_count: 24328\n",
      "43) accel_count: 22631, gyro_count: 23835\n",
      "44) accel_count: 24473, gyro_count: 23749\n",
      "45) accel_count: 23974, gyro_count: 23229\n",
      "46) accel_count: 23614, gyro_count: 23827\n",
      "48) accel_count: 22828, gyro_count: 23904\n",
      "49) accel_count: 24183, gyro_count: 24633\n",
      "Loading exp2 data:\n",
      "1) accel_count: 24049, gyro_count: 26943\n",
      "2) accel_count: 24468, gyro_count: 27667\n",
      "3) accel_count: 24611, gyro_count: 27000\n",
      "4) accel_count: 24972, gyro_count: 26798\n",
      "5) accel_count: 23573, gyro_count: 23372\n",
      "6) accel_count: 23800, gyro_count: 23890\n",
      "7) accel_count: 23347, gyro_count: 24145\n",
      "8) accel_count: 22947, gyro_count: 22660\n",
      "19) accel_count: 26156, gyro_count: 25815\n",
      "21) accel_count: 23566, gyro_count: 24408\n",
      "22) accel_count: 23844, gyro_count: 24589\n",
      "26) accel_count: 23179, gyro_count: 23925\n",
      "27) accel_count: 25109, gyro_count: 25820\n",
      "28) accel_count: 23133, gyro_count: 24028\n",
      "29) accel_count: 23180, gyro_count: 24314\n",
      "35) accel_count: 23299, gyro_count: 23854\n",
      "36) accel_count: 25497, gyro_count: 25059\n",
      "37) accel_count: 25994, gyro_count: 25232\n",
      "38) accel_count: 21164, gyro_count: 21182\n",
      "39) accel_count: 24214, gyro_count: 23585\n",
      "40) accel_count: 23944, gyro_count: 23170\n",
      "41) accel_count: 23193, gyro_count: 24111\n",
      "42) accel_count: 26505, gyro_count: 25697\n",
      "43) accel_count: 22690, gyro_count: 23981\n",
      "44) accel_count: 23002, gyro_count: 23829\n",
      "45) accel_count: 23978, gyro_count: 23350\n",
      "46) accel_count: 21128, gyro_count: 21848\n",
      "48) accel_count: 27996, gyro_count: 27205\n",
      "49) accel_count: 23061, gyro_count: 24129\n"
     ]
    }
   ],
   "source": [
    "#include 47 later\n",
    "# user_ids = [9]\n",
    "df_exps_dict = load_data_frames(P.user_ids, P.exp_begin_cutoff_idx, P.exp_end_cutoff_idx, P.num_sample_points_per_exp)\n",
    "raw_dfList_exp1, raw_dfList_exp2 = df_exps_dict['dfList_exp1'], df_exps_dict['dfList_exp2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set: [7, 24, 8, 11, 13, 19, 28, 21, 26, 3, 20, 22, 6, 25, 16, 1, 17, 27, 5]\n",
      "test_set: [0, 12, 14, 9, 18, 23, 2, 15, 10, 4]\n",
      "train_set: [7, 24, 8, 11, 13, 19, 28, 21, 26, 3, 20, 22, 6, 25, 16, 1, 17, 27, 5]\n",
      "test_set: [0, 12, 14, 9, 18, 23, 2, 15, 10, 4]\n"
     ]
    }
   ],
   "source": [
    "randomized_data_idx = list(range(len(P.user_ids)))\n",
    "random.Random(SEED).shuffle(randomized_data_idx)\n",
    "split_idx = 2 * (len(randomized_data_idx)//3) + 1\n",
    "train_set = randomized_data_idx[: split_idx]\n",
    "test_set = randomized_data_idx[split_idx: ]\n",
    "# train_set = randomized_data_idx\n",
    "print(f\"train_set: {train_set}\\ntest_set: {test_set}\")\n",
    "# train_set = test_set\n",
    "# test_set = train_set\n",
    "print(f\"train_set: {train_set}\\ntest_set: {test_set}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading exp1 data:\n",
      "47) accel_count: 22777, gyro_count: 22226\n",
      "Loading exp2 data:\n",
      "47) accel_count: 17718, gyro_count: 18353\n"
     ]
    }
   ],
   "source": [
    "num_sample_points_per_exp_user_47 = 18000\n",
    "df_exps_dict_user_47 = load_data_frames([47], P.exp_begin_cutoff_idx, P.exp_end_cutoff_idx, num_sample_points_per_exp_user_47)\n",
    "dfList_exp1_user_47, dfList_exp2_user_47 = df_exps_dict_user_47['dfList_exp1'], df_exps_dict_user_47['dfList_exp2']\n",
    "\n",
    "raw_dfList_exp1_user_47 = dfList_exp1_user_47\n",
    "raw_dfList_exp2_user_47 = dfList_exp2_user_47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"train_set: {train_set}\")\n",
    "# print(f\"X_exp1_train_dic: {X_exp1_train_dic.keys()}\")\n",
    "# print(f\"X_exp2_train_dic: {X_exp2_train_dic.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"test_set: {test_set}\")\n",
    "# print(f\"X_exp1_test_dic: {X_exp1_test_dic.keys()}\")\n",
    "# print(f\"X_exp2_test_dic: {X_exp2_test_dic.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "int('121')\n",
    "SAVE_PATH_NAME\n",
    "loss_func = lambda y_true, dist: k_contrastive_loss(y_true, dist, arg_dict[\"contrastive_loss_margin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_1_3_conv_1_dense_arg_dict_default/1750/SCNN-OCSVM-1.1-SCNN_1_3_conv_1_dense_arg_dict_default-1750-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_1_3_conv_1_dense_arg_dict_default/2000/SCNN-OCSVM-1.1-SCNN_1_3_conv_1_dense_arg_dict_default-2000-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_3_123_conv_1_dense_arg_dict_default/1750/SCNN-OCSVM-1.1-SCNN_3_123_conv_1_dense_arg_dict_default-1750-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_3_123_conv_1_dense_arg_dict_default/2000/SCNN-OCSVM-1.1-SCNN_3_123_conv_1_dense_arg_dict_default-2000-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_3_1_conv_1_dense_arg_dict_default/1750/SCNN-OCSVM-1.1-SCNN_3_1_conv_1_dense_arg_dict_default-1750-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_3_1_conv_1_dense_arg_dict_default/2000/SCNN-OCSVM-1.1-SCNN_3_1_conv_1_dense_arg_dict_default-2000-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_1_2_conv_1_dense_arg_dict_default/1750/SCNN-OCSVM-1.1-SCNN_1_2_conv_1_dense_arg_dict_default-1750-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_1_2_conv_1_dense_arg_dict_default/2000/SCNN-OCSVM-1.1-SCNN_1_2_conv_1_dense_arg_dict_default-2000-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_1_1_conv_1_dense_arg_dict_default/1750/SCNN-OCSVM-1.1-SCNN_1_1_conv_1_dense_arg_dict_default-1750-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_1_1_conv_1_dense_arg_dict_default/2000/SCNN-OCSVM-1.1-SCNN_1_1_conv_1_dense_arg_dict_default-2000-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_1_2_conv_2_dense_arg_dict_default/1750/SCNN-OCSVM-1.1-SCNN_1_2_conv_2_dense_arg_dict_default-1750-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_1_2_conv_2_dense_arg_dict_default/2000/SCNN-OCSVM-1.1-SCNN_1_2_conv_2_dense_arg_dict_default-2000-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_1_5_conv_1_dense_arg_dict_default/1750/SCNN-OCSVM-1.1-SCNN_1_5_conv_1_dense_arg_dict_default-1750-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_1_5_conv_1_dense_arg_dict_default/2000/SCNN-OCSVM-1.1-SCNN_1_5_conv_1_dense_arg_dict_default-2000-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_4_1234_conv_1_dense_arg_dict_default/1750/SCNN-OCSVM-1.1-SCNN_4_1234_conv_1_dense_arg_dict_default-1750-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "final_experiments_results/SCNN_trained_models/SCNN-OCSVM/1.1/SCNN_4_1234_conv_1_dense_arg_dict_default/2000/SCNN-OCSVM-1.1-SCNN_4_1234_conv_1_dense_arg_dict_default-2000-deep_feature_extractor\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'SCNN_1_3_conv_1_dense_arg_dict_default': {1750: <keras.engine.functional.Functional at 0x7fd62e900880>,\n",
       "  2000: <keras.engine.functional.Functional at 0x7fd62d5f8160>},\n",
       " 'SCNN_3_123_conv_1_dense_arg_dict_default': {1750: <keras.engine.functional.Functional at 0x7fd62dd11970>,\n",
       "  2000: <keras.engine.functional.Functional at 0x7fd62dd11640>},\n",
       " 'SCNN_3_1_conv_1_dense_arg_dict_default': {1750: <keras.engine.functional.Functional at 0x7fd62edec040>,\n",
       "  2000: <keras.engine.functional.Functional at 0x7fd62f1090a0>},\n",
       " 'SCNN_1_2_conv_1_dense_arg_dict_default': {1750: <keras.engine.functional.Functional at 0x7fd62e025bb0>,\n",
       "  2000: <keras.engine.functional.Functional at 0x7fd62ccfaf10>},\n",
       " 'SCNN_1_1_conv_1_dense_arg_dict_default': {1750: <keras.engine.functional.Functional at 0x7fd62cc46f70>,\n",
       "  2000: <keras.engine.functional.Functional at 0x7fd62c1928b0>},\n",
       " 'SCNN_1_2_conv_2_dense_arg_dict_default': {1750: <keras.engine.functional.Functional at 0x7fd60ba84bb0>,\n",
       "  2000: <keras.engine.functional.Functional at 0x7fd60b63ea60>},\n",
       " 'SCNN_1_5_conv_1_dense_arg_dict_default': {1750: <keras.engine.functional.Functional at 0x7fd60a5b8130>,\n",
       "  2000: <keras.engine.functional.Functional at 0x7fd6098c8700>},\n",
       " 'SCNN_4_1234_conv_1_dense_arg_dict_default': {1750: <keras.engine.functional.Functional at 0x7fd608bd27c0>,\n",
       "  2000: <keras.engine.functional.Functional at 0x7fd5e77734f0>}}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_architectures = list(TRAINING_CONFIG_DICT['2000'].keys())\n",
    "TESTING_CONFIG_DICT={}\n",
    "\n",
    "model_classifier_type=EXP_PATH_NAME\n",
    "base_path=f\"final_experiments_results/{SAVE_PATH_NAME}/{model_classifier_type}/{EXP_NUM_1_1}\"\n",
    "for archi_name in model_architectures:\n",
    "    # archi_name=\"SCNN_3_1_conv_1_dense_arg_dict_default\"\n",
    "    path_to_archi_name=base_path+f\"/{archi_name}\"\n",
    "    archi_win_dict={}\n",
    "    for winsize_str in list(TRAINING_CONFIG_DICT.keys())[-2:]:\n",
    "        final_path=path_to_archi_name+f\"/{winsize_str}\"\n",
    "        deep_feature_extractor_path=final_path+f\"/{model_classifier_type}-{EXP_NUM_1_1}-{archi_name}-{winsize_str}-deep_feature_extractor\"\n",
    "        print(deep_feature_extractor_path)\n",
    "        archi_win_dict[int(winsize_str)]=load_model(deep_feature_extractor_path, custom_objects={'loss_func': loss_func})\n",
    "        \n",
    "    TESTING_CONFIG_DICT[archi_name]=archi_win_dict\n",
    "\n",
    "TESTING_CONFIG_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_exp1_dict, X_train_exp2_dict, fitted_scaler_SNN_train_exp2_dict=get_raw_windows(raw_dfList_exp1, raw_dfList_exp2, 1750, 1750//2, train_set, \n",
    "                                                                    P.scaler, P.num_sample_points_per_exp, EMA_per_win_span=None, SMA_per_win_winsize=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 6)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_exp1_dict[7][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SCNN_1_3_conv_1_dense_arg_dict_default': {...},\n",
       " 'SCNN_3_123_conv_1_dense_arg_dict_default': {...},\n",
       " 'SCNN_3_1_conv_1_dense_arg_dict_default': {...},\n",
       " 'SCNN_1_2_conv_1_dense_arg_dict_default': {...},\n",
       " 'SCNN_1_1_conv_1_dense_arg_dict_default': {...},\n",
       " 'SCNN_1_2_conv_2_dense_arg_dict_default': {...},\n",
       " 'SCNN_1_5_conv_1_dense_arg_dict_default': {...},\n",
       " 'SCNN_4_1234_conv_1_dense_arg_dict_default': {...}}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TESTING_CONFIG_DICT[\"SCNN_1_3_conv_1_dense_arg_dict_default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([X_train_exp1_dict[7][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5740009  0.6722793  0.5910726  0.35238677 0.67490584 0.41873464\n",
      "  0.41862527 0.68550205 0.4903324  0.3954168  0.62073976 0.3625825\n",
      "  0.5475633  0.56418186 0.50144356 0.57056206 0.40080127 0.47761008\n",
      "  0.33118108 0.59106636 0.46235883 0.5707219  0.4189213  0.53638446\n",
      "  0.37229556 0.5791994  0.4715442  0.61087537 0.3761476  0.44408736\n",
      "  0.68833035 0.48037443 0.61874944 0.5496584  0.46162724 0.62916857\n",
      "  0.5288415  0.5880092  0.41918623 0.47109902 0.42755282 0.5964268\n",
      "  0.46997038 0.49072525 0.43724227 0.4994222  0.39486378 0.6835028\n",
      "  0.60120344 0.545115   0.38591582 0.643959   0.50861263 0.6626936\n",
      "  0.36119074 0.56632775 0.44539332 0.5142644  0.58677447 0.31033707\n",
      "  0.45429698 0.41801888 0.35567385 0.44557536 0.5955584  0.5181858\n",
      "  0.67627335 0.6089235  0.67423767 0.39229286 0.45297152 0.38871533\n",
      "  0.3058741  0.53217405 0.3184663  0.6212817  0.42456594 0.6536396\n",
      "  0.35598078 0.571648   0.6520068  0.61606836 0.56198716 0.40527037]]\n",
      "[[0.3807883  0.7223638  0.5718766  0.5867678  0.55146706 0.62841713\n",
      "  0.34881818 0.5400437  0.23594254 0.33474147 0.62666565 0.45812863\n",
      "  0.38958064 0.8050964  0.45527652 0.5439107  0.5205728  0.41213763\n",
      "  0.4614293  0.46888155 0.439844   0.51166236 0.5827022  0.4168124\n",
      "  0.51385593 0.4953349  0.52158123 0.43375358 0.56377983 0.6089432\n",
      "  0.27714354 0.6983918  0.524098   0.48630613 0.56585574 0.47750002\n",
      "  0.50508964 0.5425631  0.71005297 0.6200379  0.6321353  0.52977204\n",
      "  0.55510116 0.20725271 0.3694567  0.45642552 0.15615773 0.32777917\n",
      "  0.5993495  0.5110311  0.38361385 0.38324782 0.6308217  0.46108148\n",
      "  0.7009079  0.62260234 0.5797795  0.51045525 0.5471644  0.55551445\n",
      "  0.6124314  0.578067   0.32356885 0.6124521  0.40603474 0.53900194\n",
      "  0.6139676  0.43211523 0.8046893  0.49141315 0.41200334 0.524605\n",
      "  0.49013874 0.5780455  0.68456244 0.40380257 0.59642434 0.17659843\n",
      "  0.54392225 0.33528805 0.32407147 0.5639768  0.2450456  0.53926426]]\n",
      "[[0.596495   0.65779155 0.48765633 0.52004635 0.5733435  0.32585582\n",
      "  0.41473377 0.7836391  0.32661867 0.44101864 0.59325033 0.2982694\n",
      "  0.45108673 0.43434164 0.48094288 0.692995   0.59312516 0.48801407\n",
      "  0.22498953 0.6706653  0.48527077 0.5541266  0.5396388  0.45135307\n",
      "  0.42089456 0.46714437 0.3918767  0.5459421  0.48454633 0.5974581\n",
      "  0.6642558  0.53802866 0.60302424 0.49246073 0.5622304  0.5199504\n",
      "  0.5434186  0.53246504 0.5321908  0.4680774  0.38038486 0.6030522\n",
      "  0.45840427 0.42264616 0.51251227 0.43343443 0.49786043 0.7011734\n",
      "  0.56485677 0.408074   0.44047377 0.5268134  0.27240387 0.72596437\n",
      "  0.20669389 0.5475411  0.52970624 0.3695467  0.62694025 0.34097534\n",
      "  0.5438646  0.5272485  0.44281486 0.40518764 0.45686573 0.5113125\n",
      "  0.71001333 0.58752584 0.44898918 0.2775526  0.46943218 0.54373866\n",
      "  0.2582557  0.60325253 0.25588435 0.4048309  0.5134719  0.47205386\n",
      "  0.37420398 0.52670175 0.5783577  0.45720986 0.6380919  0.50242835]]\n",
      "[[0.7173693  0.32572973 0.53952754 0.5369345  0.31784332 0.62993133\n",
      "  0.54589057 0.6439389  0.7126089  0.3567858  0.58630127 0.72088367\n",
      "  0.27192533 0.5038558  0.32875448 0.5904294  0.5870823  0.3591838\n",
      "  0.3594431  0.4669248  0.26667088 0.34767973 0.63541716 0.51589125\n",
      "  0.5290389  0.4090923  0.7091601  0.686702   0.28650323 0.39647663\n",
      "  0.35405147 0.50469863 0.45568696 0.54663587 0.46151614 0.3941061\n",
      "  0.64372516 0.592936   0.3778362  0.57570165 0.29065418 0.4301151\n",
      "  0.5476691  0.40111238 0.39290798 0.65976727 0.39824393 0.3876475\n",
      "  0.5925417  0.2979343  0.370309   0.51468366 0.6876462  0.35081095\n",
      "  0.3283835  0.59697723 0.30335253 0.5268831  0.63442206 0.37074098\n",
      "  0.5881939  0.6409123  0.30771917 0.36465693 0.51828814 0.30910736\n",
      "  0.28080046 0.5035529  0.6493919  0.5391748  0.4413332  0.5306859\n",
      "  0.39221096 0.6972712  0.68233943 0.30854267 0.64722365 0.35474083\n",
      "  0.6074976  0.33592033 0.46928385 0.490443   0.46455932 0.62039673]]\n",
      "[[0.49783853 0.35965484 0.34184754 0.45681193 0.73069704 0.5491996\n",
      "  0.3954043  0.6393242  0.7127742  0.5936797  0.41735792 0.6154935\n",
      "  0.44688594 0.3697761  0.41409087 0.48503822 0.58913416 0.5503595\n",
      "  0.315452   0.31788927 0.43184313 0.38377595 0.5979083  0.2653517\n",
      "  0.52243596 0.33239284 0.26420516 0.42592776 0.48959938 0.30321753\n",
      "  0.3811521  0.5949553  0.2962224  0.7005085  0.672995   0.522211\n",
      "  0.45192507 0.7165824  0.40713942 0.5029639  0.61404103 0.56073695\n",
      "  0.4644858  0.5855654  0.4089954  0.31664547 0.5769584  0.5530923\n",
      "  0.41354206 0.39738753 0.4849138  0.3664858  0.52825874 0.4876733\n",
      "  0.5560074  0.70238346 0.32188493 0.58367    0.42716163 0.48286143\n",
      "  0.5416151  0.4401084  0.4873799  0.3655306  0.3085624  0.43537074\n",
      "  0.57536376 0.55915636 0.50767493 0.5543046  0.6329453  0.51566976\n",
      "  0.36513233 0.30565667 0.2878912  0.52060574 0.55737525 0.5866329\n",
      "  0.38113195 0.6527929  0.42859536 0.66116565 0.4967317  0.4435267 ]]\n",
      "[[0.57958925 0.5792661  0.5617313  0.47323212 0.6609555  0.61160636\n",
      "  0.57407856 0.4288926  0.45848465 0.38086092 0.38250843 0.4307955\n",
      "  0.41918963 0.44247577 0.42528912 0.49022442 0.48694867 0.38891304\n",
      "  0.38120344 0.5653929  0.58718354 0.5492049  0.5415069  0.54876333\n",
      "  0.5493625  0.43838826 0.4315213  0.41607335 0.2999661  0.55387455\n",
      "  0.49997836 0.557619  ]]\n",
      "[[0.6287272  0.39012852 0.6509299  0.6380622  0.5931228  0.60779977\n",
      "  0.6271567  0.59839314 0.61006653 0.35285196 0.61249524 0.3615806\n",
      "  0.6133812  0.6685946  0.65960693 0.38057286 0.35219496 0.3622948\n",
      "  0.614712   0.6266955  0.3676002  0.3822556  0.37496853 0.6419511\n",
      "  0.6026592  0.5993069  0.38907194 0.45369422 0.6268635  0.38683328\n",
      "  0.595886   0.6262822  0.62215406 0.38902617 0.43945736 0.6108676\n",
      "  0.37318915 0.4092731  0.5389551  0.59415376 0.64593863 0.40215448\n",
      "  0.44147235 0.62158334 0.36694467 0.61071485 0.37082124 0.55921274\n",
      "  0.37712994 0.62248224 0.61604744 0.55859214 0.5956501  0.35680467\n",
      "  0.40842274 0.60444516 0.36916023 0.6283831  0.6339698  0.46469122\n",
      "  0.61786157 0.43596676 0.3910786  0.6439662  0.38433975 0.37981018\n",
      "  0.56022364 0.43157282 0.38519233 0.6432746  0.6069455  0.34922826\n",
      "  0.5185184  0.6347942  0.6260458  0.38859433 0.572457   0.47021404\n",
      "  0.42564783 0.34566152 0.51404184 0.5860841  0.58332694 0.38539425]]\n",
      "[[0.36262587 0.23920864 0.7307327  0.4180873  0.44875705 0.18786436\n",
      "  0.47271556 0.7027956  0.5441028  0.30184883 0.42361248 0.21424094\n",
      "  0.8064377  0.4788306  0.48454106 0.5611434  0.6026398  0.32997054\n",
      "  0.62771565 0.34499094 0.5386294  0.5296221  0.692751   0.5056189\n",
      "  0.5878853  0.2246438  0.62062484 0.45125106 0.768724   0.40552086\n",
      "  0.40758508 0.62757635 0.6197487  0.6854532  0.71699584 0.44336\n",
      "  0.25366098 0.7839749  0.52926135 0.3749581  0.15370491 0.43967554\n",
      "  0.7065428  0.29052466 0.13244316 0.31659502 0.75787926 0.6081036\n",
      "  0.5349561  0.81372833 0.68204105 0.5742996  0.586186   0.53832674\n",
      "  0.4334248  0.7932931  0.64686435 0.4683485  0.28196287 0.6992742\n",
      "  0.501569   0.2496776  0.4593058  0.506331   0.53351045 0.39434898\n",
      "  0.759814   0.47733733 0.5109651  0.2507274  0.5254915  0.56860876\n",
      "  0.7535163  0.7131491  0.23101097 0.54127157 0.5984242  0.58417654\n",
      "  0.4844707  0.67301005 0.4689648  0.5733315  0.6190387  0.32094014]]\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in TESTING_CONFIG_DICT.items():\n",
    "    print(model[1750].predict(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A merge layer should be called on a list of inputs.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [85]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m final_path\u001b[38;5;241m=\u001b[39mpath_to_archi_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwinsize_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m deep_feature_extractor_path\u001b[38;5;241m=\u001b[39mfinal_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_classifier_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEXP_NUM_1_1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchi_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwinsize_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-deep_feature_extractor\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m model\u001b[38;5;241m=\u001b[39m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep_feature_extractor_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/saving/save.py:205\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    203\u001b[0m       filepath \u001b[38;5;241m=\u001b[39m path_to_string(filepath)\n\u001b[1;32m    204\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaved_model_load\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnable to load model. Filepath is not an hdf5 file (or h5py is not \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavailable) or SavedModel.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/saving/saved_model/load.py:143\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, compile, options)\u001b[0m\n\u001b[1;32m    140\u001b[0m loaded \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39mload_partial(path, nodes_to_load, options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Finalize the loaded layers and remove the extra tracked dependencies.\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m \u001b[43mkeras_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinalize_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m keras_loader\u001b[38;5;241m.\u001b[39mdel_tracking()\n\u001b[1;32m    146\u001b[0m model \u001b[38;5;241m=\u001b[39m loaded[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/saving/saved_model/load.py:644\u001b[0m, in \u001b[0;36mKerasObjectLoader.finalize_objects\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    641\u001b[0m _finalize_config_layers(layers_revived_from_config)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;66;03m# Initialize graph networks, now that layer dependencies have been resolved.\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reconstruct_all_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/saving/saved_model/load.py:663\u001b[0m, in \u001b[0;36mKerasObjectLoader._reconstruct_all_models\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    661\u001b[0m   all_initialized_models\u001b[38;5;241m.\u001b[39madd(model_id)\n\u001b[1;32m    662\u001b[0m   model, layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_layer_dependencies[model_id]\n\u001b[0;32m--> 663\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reconstruct_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m   _finalize_config_layers([model])\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_initialized_models \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_layer_dependencies\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m    667\u001b[0m   \u001b[38;5;66;03m# This should not happen.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/saving/saved_model/load.py:708\u001b[0m, in \u001b[0;36mKerasObjectLoader._reconstruct_model\u001b[0;34m(self, model_id, model, layers)\u001b[0m\n\u001b[1;32m    705\u001b[0m       model\u001b[38;5;241m.\u001b[39mbuild(input_shapes)\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Reconstruct functional model\u001b[39;00m\n\u001b[1;32m    707\u001b[0m   (inputs, outputs,\n\u001b[0;32m--> 708\u001b[0m    created_layers) \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruct_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m       \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreated_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    710\u001b[0m   model\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(inputs, outputs, name\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    711\u001b[0m   functional_lib\u001b[38;5;241m.\u001b[39mconnect_ancillary_layers(model, created_layers)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/functional.py:1283\u001b[0m, in \u001b[0;36mreconstruct_from_config\u001b[0;34m(config, custom_objects, created_layers)\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m unprocessed_nodes:\n\u001b[1;32m   1282\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m node_data \u001b[38;5;129;01min\u001b[39;00m unprocessed_nodes\u001b[38;5;241m.\u001b[39mpop(layer):\n\u001b[0;32m-> 1283\u001b[0m         \u001b[43mprocess_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m input_tensors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1286\u001b[0m output_tensors \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/functional.py:1231\u001b[0m, in \u001b[0;36mreconstruct_from_config.<locals>.process_node\u001b[0;34m(layer, node_data)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m layer\u001b[38;5;241m.\u001b[39m_preserve_input_structure_in_config:\n\u001b[1;32m   1229\u001b[0m   input_tensors \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1230\u001b[0m       base_layer_utils\u001b[38;5;241m.\u001b[39munnest_if_single_tensor(input_tensors))\n\u001b[0;32m-> 1231\u001b[0m output_tensors \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;66;03m# Update node index map.\u001b[39;00m\n\u001b[1;32m   1234\u001b[0m output_index \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(output_tensors)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_keras_history\u001b[38;5;241m.\u001b[39mnode_index\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:976\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;66;03m# Functional Model construction mode is invoked when `Layer`s are called on\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# symbolic `KerasTensor`s, i.e.:\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# >> inputs = tf.keras.Input(10)\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# >> outputs = MyLayer()(inputs)  # Functional construction mode.\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# >> model = tf.keras.Model(inputs, outputs)\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _in_functional_construction_mode(\u001b[38;5;28mself\u001b[39m, inputs, args, kwargs, input_list):\n\u001b[0;32m--> 976\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_functional_construction_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# Maintains info about the `Layer.call` stack.\u001b[39;00m\n\u001b[1;32m    980\u001b[0m call_context \u001b[38;5;241m=\u001b[39m base_layer_utils\u001b[38;5;241m.\u001b[39mcall_context()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:1114\u001b[0m, in \u001b[0;36mLayer._functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1109\u001b[0m     training_arg_passed_by_framework \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m call_context\u001b[38;5;241m.\u001b[39menter(\n\u001b[1;32m   1112\u001b[0m     layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, inputs\u001b[38;5;241m=\u001b[39minputs, build_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39mtraining_value):\n\u001b[1;32m   1113\u001b[0m   \u001b[38;5;66;03m# Check input assumptions set after layer building, e.g. input shape.\u001b[39;00m\n\u001b[0;32m-> 1114\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_keras_tensor_symbolic_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1117\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA layer\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms `call` method should return a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1119\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTensor or a list of Tensors, not None \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1120\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(layer: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:848\u001b[0m, in \u001b[0;36mLayer._keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(keras_tensor\u001b[38;5;241m.\u001b[39mKerasTensor, output_signature)\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 848\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_output_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_masks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:888\u001b[0m, in \u001b[0;36mLayer._infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_build(inputs)\n\u001b[1;32m    887\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs)\n\u001b[0;32m--> 888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_mask_metadata(inputs, outputs, input_masks,\n\u001b[1;32m    892\u001b[0m                         build_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/layers/merge.py:117\u001b[0m, in \u001b[0;36m_Merge.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m    116\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA merge layer should be called on a list of inputs.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    118\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reshape_required:\n\u001b[1;32m    119\u001b[0m     reshaped_inputs \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mValueError\u001b[0m: A merge layer should be called on a list of inputs."
     ]
    }
   ],
   "source": [
    "archi_name=\"SCNN_1_2_conv_1_dense_arg_dict_default\"\n",
    "base_path=f\"final_experiments_results/{SAVE_PATH_NAME}/{model_classifier_type}/{EXP_NUM_1_1}\"\n",
    "path_to_archi_name=base_path+f\"/{archi_name}\"\n",
    "final_path=path_to_archi_name+f\"/{winsize_str}\"\n",
    "deep_feature_extractor_path=final_path+f\"/{model_classifier_type}-{EXP_NUM_1_1}-{archi_name}-{winsize_str}-deep_feature_extractor\"\n",
    "model=load_model(deep_feature_extractor_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "left_inputs (InputLayer)     [(None, 1750, 6)]         0         \n",
      "_________________________________________________________________\n",
      "basemodel (Functional)       (None, 84)                166116    \n",
      "=================================================================\n",
      "Total params: 166,116\n",
      "Trainable params: 165,052\n",
      "Non-trainable params: 1,064\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path=f\"final_experiments_results/{SAVE_PATH_NAME}/{EXP_PATH_NAME}/{EXP_NUM_1_1}\"\n",
    "path_to_archi_name=base_path+f\"/{archi_name}\"\n",
    "final_path=path_to_archi_name+f\"/{winsize}\"\n",
    "    \n",
    "    if not os.path.isdir(base_path):\n",
    "        raise Exception(f\"Base path does not exist: {base_path}\")\n",
    "        \n",
    "    if not os.path.isdir(path_to_archi_name):\n",
    "        os.mkdir(path_to_archi_name)\n",
    "\n",
    "    if not os.path.isdir(final_path):\n",
    "        os.mkdir(final_path)\n",
    "        \n",
    "    model.save(final_path+f\"/{model_classifier_type}-{exp_num}-{archi_name}-{winsize}-model\")\n",
    "    deep_feature_model = extract_deep_feature_extactor(model)\n",
    "    deep_feature_model.save(final_path+f\"/{model_classifier_type}-{exp_num}-{archi_name}-{winsize}-deep_feature_extractor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Butterworth frequency Cut-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Naive Approach\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butterworth\"\n",
    "\n",
    "\n",
    "window_size_lst = [1750, 2000]#[125, 250, 500, 750, 1000, 1250, 1500, 1750, 2000]#range(1000, 2001, 250)#\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "\n",
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "\n",
    "overlap=0.5\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "\n",
    "    \n",
    "rival_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{RIVAL_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "rival_test_hyperparameters_df = pd.read_json(rival_test_hyperparameters_file_name)\n",
    "\n",
    "P.cut_off_freq=rival_test_hyperparameters_df[\"cut_off_freq\"][0]\n",
    "print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "\n",
    "ffted_dfList_exp1 = get_ffted_dfList(raw_dfList_exp1, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "ffted_dfList_exp1_user_47 = get_ffted_dfList(raw_dfList_exp1_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": ffted_dfList_exp1,\n",
    "            \"dfList_exp2\": ffted_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": ffted_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": ffted_dfList_exp2_user_47\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "min_key=rival_test_hyperparameters_df[\"cut_off_freq\"][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func=deep_feature_extractor, overlap=overlap, \n",
    "                                                                                   best_param_df=rival_test_hyperparameters_df)\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"cut_off_freq: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "        \n",
    "# mean_EER_test_dict={}\n",
    "# for key in EER_df_test_dict:\n",
    "#     mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "\n",
    "\n",
    "# l = list(mean_EER_test_dict.items())\n",
    "# l.sort(key=lambda i: i[1])\n",
    "\n",
    "# with open(test_file_name, \"a\") as f:\n",
    "#     f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "#     f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "#     for i in range(len(l)):\n",
    "#         f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butterworth\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P.Butter_per_win_argdict={\n",
    "    \"filter_order\": P.filter_order,\n",
    "    \"cut_off_freq\": None,\n",
    "    \"sampling_freq\": P.sampling_freq,\n",
    "    \"filtfilt\": P.filtfilt,\n",
    "    }\n",
    "\n",
    "overlap=0.5\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for cut_off_freq in tqdm(list(range(1, 50, 23))):\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.Butter_per_win_argdict[\"cut_off_freq\"]=cut_off_freq\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "\n",
    "    \n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    \n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": ffted_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": ffted_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.cut_off_freq] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst, train_set=train_set, exp_config=P, \n",
    "                                                                                                    extract_features_func=extract_WACA_features, overlap=overlap, \n",
    "                                                                                                    param_dist=param_dist)\n",
    "        \n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq: {P.cut_off_freq}\\n\")\n",
    "        f.write(EER_df_train_dict[P.cut_off_freq].to_string())\n",
    "\n",
    "\n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func=extract_WACA_features, overlap=overlap, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Butterworth frequency Cut-off + EMA span\n",
    "## 2.1 Naive Approach\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butter+EMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "    \n",
    "indices = list(range(1, 50))\n",
    "mesh = np.array(np.meshgrid(indices, indices))\n",
    "index_pairs = mesh.T.reshape(-1, 2)\n",
    "choice_num=3#13#240\n",
    "print(f\"total cut_off_span_pairs: {index_pairs.shape}, choice_num: {choice_num}\")\n",
    "cut_off_span_pairs = index_pairs[np.random.choice(index_pairs.shape[0], size=choice_num, replace=False), :]\n",
    "\n",
    "\n",
    "\n",
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "overlap=0.5\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for key_pair in tqdm(cut_off_span_pairs):\n",
    "    \n",
    "    key_pair = tuple(key_pair)\n",
    "    cut_off_freq, span = key_pair[0], key_pair[1]\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.span=span\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "    print(f\"span: {P.span}\")\n",
    "\n",
    "    \n",
    "    ffted_dfList_exp1 = get_ffted_dfList(raw_dfList_exp1, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    EMAed_dfList_exp1 = get_EMAed_dfList(ffted_dfList_exp1, span=P.span)\n",
    "    EMAed_dfList_exp2 = get_EMAed_dfList(ffted_dfList_exp2, span=P.span)\n",
    "    \n",
    "    ffted_dfList_exp1_user_47 = get_ffted_dfList(raw_dfList_exp1_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    EMAed_dfList_exp1_user_47 = get_EMAed_dfList(ffted_dfList_exp1_user_47, span=P.span)\n",
    "    EMAed_dfList_exp2_user_47 = get_EMAed_dfList(ffted_dfList_exp2_user_47, span=P.span)\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": EMAed_dfList_exp1,\n",
    "                \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": EMAed_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[key_pair] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst, train_set=train_set, exp_config=P, \n",
    "                                                                                      extract_features_func=extract_WACA_features, overlap=overlap, \n",
    "                                                                                      param_dist=param_dist)\n",
    "        \n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq, span: {key_pair}\\n\")\n",
    "        f.write(EER_df_train_dict[key_pair].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func=extract_WACA_features, overlap=overlap, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------  \n",
    "#-------\n",
    "key_column= [\"cut_off_freq\", \"EMA_span\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butter+EMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "    \n",
    "indices = list(range(1, 50))\n",
    "mesh = np.array(np.meshgrid(indices, indices))\n",
    "index_pairs = mesh.T.reshape(-1, 2)\n",
    "choice_num=3#13#240\n",
    "print(f\"total cut_off_span_pairs: {index_pairs.shape}, choice_num: {choice_num}\")\n",
    "cut_off_span_pairs = index_pairs[np.random.choice(index_pairs.shape[0], size=choice_num, replace=False), :]\n",
    "\n",
    "\n",
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P.Butter_per_win_argdict={\n",
    "    \"filter_order\": P.filter_order,\n",
    "    \"cut_off_freq\": None,\n",
    "    \"sampling_freq\": P.sampling_freq,\n",
    "    \"filtfilt\": P.filtfilt,\n",
    "    }\n",
    "\n",
    "overlap=0.5\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for key_pair in tqdm(cut_off_span_pairs):\n",
    "    key_pair = tuple(key_pair)\n",
    "    cut_off_freq, span = key_pair[0], key_pair[1]\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.Butter_per_win_argdict[\"cut_off_freq\"]=cut_off_freq\n",
    "    \n",
    "    \n",
    "    P.span=span\n",
    "    P.EMA_per_win_span=span\n",
    "\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "    print(f\"span: {P.span}\")\n",
    "\n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    EMAed_dfList_exp2 = get_EMAed_dfList(ffted_dfList_exp2, span=P.span)\n",
    "    \n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    EMAed_dfList_exp2_user_47 = get_EMAed_dfList(ffted_dfList_exp2_user_47, span=P.span)\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[key_pair] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst, train_set=train_set, exp_config=P, \n",
    "                                                                                      extract_features_func=extract_WACA_features, overlap=overlap, \n",
    "                                                                                      param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq, span: {key_pair}\\n\")\n",
    "        f.write(EER_df_train_dict[key_pair].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func=extract_WACA_features, overlap=overlap, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\", \"EMA_span\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. EMA span\n",
    "## 3.1 Naive Approach\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"EMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "    \n",
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "\n",
    "\n",
    "overlap=0.5\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for span in tqdm(list(range(1, 50, 23))):\n",
    "    P.span=span\n",
    "    print(f\"EMA span: {P.span}\")\n",
    "\n",
    "    \n",
    "    EMAed_dfList_exp1 = get_EMAed_dfList(raw_dfList_exp1, span=P.span)\n",
    "    EMAed_dfList_exp2 = get_EMAed_dfList(raw_dfList_exp2, span=P.span)\n",
    "    \n",
    "    EMAed_dfList_exp1_user_47 = get_EMAed_dfList(raw_dfList_exp1_user_47, span=P.span)\n",
    "    EMAed_dfList_exp2_user_47 = get_EMAed_dfList(raw_dfList_exp2_user_47, span=P.span)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": EMAed_dfList_exp1,\n",
    "                \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": EMAed_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.span] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst, train_set=train_set, exp_config=P, \n",
    "                                                                                    extract_features_func=extract_WACA_features, overlap=overlap, \n",
    "                                                                                    param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\EMA span: {P.span}\\n\")\n",
    "        f.write(EER_df_train_dict[P.span].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func=extract_WACA_features, overlap=overlap, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"EMA_span\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"EMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "init_experiment_params(exp_config=P)\n",
    " \n",
    "\n",
    "\n",
    "overlap=0.5\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for span in tqdm(list(range(1, 50, 23))):\n",
    "    P.span=span\n",
    "    print(f\"EMA span: {P.span}\")\n",
    "\n",
    "    P.EMA_per_win_span=P.span\n",
    "\n",
    "    EMAed_dfList_exp2 = get_EMAed_dfList(raw_dfList_exp2, span=P.span)\n",
    "    \n",
    "    EMAed_dfList_exp2_user_47 = get_EMAed_dfList(raw_dfList_exp2_user_47, span=P.span)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.span] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst, train_set=train_set, exp_config=P, \n",
    "                                                                                    extract_features_func=extract_WACA_features, overlap=overlap, \n",
    "                                                                                    param_dist=param_dist)\n",
    "        \n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\EMA span: {P.span}\\n\")\n",
    "        f.write(EER_df_train_dict[P.span].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func=extract_WACA_features, overlap=overlap, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"EMA_span\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SMA winsize\n",
    "## 4.1 Naive Approach\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"SMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "    \n",
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "\n",
    "\n",
    "overlap=0.5\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for winsize in tqdm(list(range(1, 50, 23))):\n",
    "    P.winsize=winsize\n",
    "    print(f\"SMA winsize: {P.winsize}\")\n",
    "\n",
    "\n",
    "    SMAed_dfList_exp1 = get_SMAed_dfList(raw_dfList_exp1, winsize=P.winsize)\n",
    "    SMAed_dfList_exp2 = get_SMAed_dfList(raw_dfList_exp2, winsize=P.winsize)\n",
    "    \n",
    "    SMAed_dfList_exp1_user_47 = get_SMAed_dfList(raw_dfList_exp1_user_47, winsize=P.winsize)\n",
    "    SMAed_dfList_exp2_user_47 = get_SMAed_dfList(raw_dfList_exp2_user_47, winsize=P.winsize)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": SMAed_dfList_exp1,\n",
    "                \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": SMAed_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.winsize] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst, train_set=train_set, exp_config=P, \n",
    "                                                                                       extract_features_func=extract_WACA_features, overlap=overlap, \n",
    "                                                                                       param_dist=param_dist)\n",
    "        \n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\SMA winsize: {P.winsize}\\n\")\n",
    "        f.write(EER_df_train_dict[P.winsize].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func=extract_WACA_features, overlap=overlap, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"SMA_winsize\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"SMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "    \n",
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "\n",
    "\n",
    "overlap=0.5\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for winsize in tqdm(list(range(1, 50, 23))):\n",
    "    P.winsize=winsize\n",
    "    P.SMA_per_win_winsize=P.winsize\n",
    "\n",
    "    print(f\"SMA winsize: {P.winsize}\")\n",
    "\n",
    "\n",
    "    SMAed_dfList_exp2 = get_SMAed_dfList(raw_dfList_exp2, winsize=P.winsize)\n",
    "    \n",
    "    SMAed_dfList_exp2_user_47 = get_SMAed_dfList(raw_dfList_exp2_user_47, winsize=P.winsize)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.winsize] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst, train_set=train_set, exp_config=P, \n",
    "                                                                                       extract_features_func=extract_WACA_features, overlap=overlap, \n",
    "                                                                                       param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\SMA winsize: {P.winsize}\\n\")\n",
    "        f.write(EER_df_train_dict[P.winsize].to_string())\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func=extract_WACA_features, overlap=overlap, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"SMA_winsize\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Butterworth frequency Cut-off + SMA winsize\n",
    "## 5.1 Naive Approach\n",
    "### Optimizing and Testin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butter+SMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "    \n",
    "indices = list(range(1, 50))\n",
    "mesh = np.array(np.meshgrid(indices, indices))\n",
    "index_pairs = mesh.T.reshape(-1, 2)\n",
    "choice_num=3#13#240\n",
    "print(f\"total cut_off_winsize_pairs: {index_pairs.shape}, choice_num: {choice_num}\")\n",
    "cut_off_winsize_pairs = index_pairs[np.random.choice(index_pairs.shape[0], size=choice_num, replace=False), :]\n",
    "\n",
    "\n",
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "overlap=0.5\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for key_pair in tqdm(cut_off_winsize_pairs):\n",
    "    \n",
    "    key_pair = tuple(key_pair)\n",
    "    cut_off_freq, winsize = key_pair[0], key_pair[1]\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.winsize=winsize\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "    print(f\"winsize: {P.winsize}\")\n",
    "\n",
    "    \n",
    "    ffted_dfList_exp1 = get_ffted_dfList(raw_dfList_exp1, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    SMAed_dfList_exp1 = get_SMAed_dfList(ffted_dfList_exp1, winsize=P.winsize)\n",
    "    SMAed_dfList_exp2 = get_SMAed_dfList(ffted_dfList_exp2, winsize=P.winsize)\n",
    "    \n",
    "    ffted_dfList_exp1_user_47 = get_ffted_dfList(raw_dfList_exp1_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    SMAed_dfList_exp1_user_47 = get_SMAed_dfList(ffted_dfList_exp1_user_47, winsize=P.winsize)\n",
    "    SMAed_dfList_exp2_user_47 = get_SMAed_dfList(ffted_dfList_exp2_user_47, winsize=P.winsize)\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": SMAed_dfList_exp1,\n",
    "                \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": SMAed_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[key_pair] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst, train_set=train_set, exp_config=P, \n",
    "                                                                                      extract_features_func=extract_WACA_features, overlap=overlap, \n",
    "                                                                                      param_dist=param_dist)\n",
    "\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq, winsize: {key_pair}\\n\")\n",
    "        f.write(EER_df_train_dict[key_pair].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func=extract_WACA_features, overlap=overlap, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\", \"SMA_winsize\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(key_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butter+SMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "    \n",
    "indices = list(range(1, 50))\n",
    "mesh = np.array(np.meshgrid(indices, indices))\n",
    "index_pairs = mesh.T.reshape(-1, 2)\n",
    "choice_num=3#13#240\n",
    "print(f\"total cut_off_winsize_pairs: {index_pairs.shape}, choice_num: {choice_num}\")\n",
    "cut_off_winsize_pairs = index_pairs[np.random.choice(index_pairs.shape[0], size=choice_num, replace=False), :]\n",
    "\n",
    "\n",
    "\n",
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "\n",
    "\n",
    "P.Butter_per_win_argdict={\n",
    "    \"filter_order\": P.filter_order,\n",
    "    \"cut_off_freq\": None,\n",
    "    \"sampling_freq\": P.sampling_freq,\n",
    "    \"filtfilt\": P.filtfilt,\n",
    "    }\n",
    "\n",
    "overlap=0.5\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for key_pair in tqdm(cut_off_winsize_pairs):\n",
    "    \n",
    "    key_pair = tuple(key_pair)\n",
    "    cut_off_freq, winsize = key_pair[0], key_pair[1]\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.Butter_per_win_argdict[\"cut_off_freq\"]=P.cut_off_freq\n",
    "    P.winsize=winsize\n",
    "    P.SMA_per_win_winsize=P.winsize\n",
    "    \n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "    print(f\"winsize: {P.winsize}\")\n",
    "\n",
    "    \n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    SMAed_dfList_exp2 = get_SMAed_dfList(ffted_dfList_exp2, winsize=P.winsize)\n",
    "    \n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    SMAed_dfList_exp2_user_47 = get_SMAed_dfList(ffted_dfList_exp2_user_47, winsize=P.winsize)\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[key_pair] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst, train_set=train_set, exp_config=P, \n",
    "                                                                                      extract_features_func=extract_WACA_features, overlap=overlap, \n",
    "                                                                                      param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq, winsize: {key_pair}\\n\")\n",
    "        f.write(EER_df_train_dict[key_pair].to_string())\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func=extract_WACA_features, overlap=overlap, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\", \"SMA_winsize\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. The effect of Varying Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "\n",
    "\n",
    "\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/overlap_Mean_EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/overlap_Mean_EER_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for overlap in tqdm(list(range(50, 52))):\n",
    "    overlap*=0.01\n",
    "    max_window_size=2000\n",
    "    step_width = int(max_window_size * (1-overlap))\n",
    "    max_num_windows=max(len(getIndices(sampleSize=max_window_size, step=step_width, numSamplePoints=P.num_sample_points_per_exp)), 100)\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": raw_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": raw_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[overlap] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst, train_set=train_set, exp_config=P, \n",
    "                                                                                     extract_features_func=extract_WACA_features, overlap=overlap, \n",
    "                                                                                     param_dist=param_dist)\n",
    "        \n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\overlap: {overlap}\\n\")\n",
    "        f.write(EER_df_train_dict[overlap].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func=extract_WACA_features, overlap=overlap, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"overlap\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=return_and_save_final_result_df_as_json(final_exp_results_path=FINAL_EXP_RESULTS_PATH, exp_path_name=EXP_PATH_NAME, window_size_lst=window_size_lst)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df=return_and_save_final_relative_result_df_as_json(df, base_case_index=0, final_exp_results_path=FINAL_EXP_RESULTS_PATH, exp_path_name=EXP_PATH_NAME, window_size_lst=window_size_lst)\n",
    "relative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.style.hide(axis='index').to_latex()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df.style.hide(axis='index').to_latex()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
