{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "3ae86968-b273-4857-a4fd-537b178d854b",
    "_uuid": "bcec77b5-0738-4d15-8aad-f6b59b3ac023",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "EER: 0.333, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.400, Threshold: 0.200 <-- Worse case\n",
      "EER: 0.167, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.333, Threshold: 1.000 <-- Worse case\n",
      "--------------------\u001b[32mUtility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mPreprocessing utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade pip\n",
    "\n",
    "TEST_MODE = 0 # Testing macro\n",
    "\n",
    "import os\n",
    "import numpy as np # linear algebraf\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import iqr\n",
    "from scipy.stats import median_abs_deviation\n",
    "from scipy.stats import mode\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "%run ./Preprocessing_utility-functions.ipynb\n",
    "%run ./SEED-CONSTANTS.ipynb\n",
    "\n",
    "\n",
    "print(f\"\\x1b[32mSEED: {SEED}\\x1b[0m\")\n",
    "# Global utitlity functions are in separate notebook\n",
    "if TEST_MODE: print(\"setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.10\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MakeWACAXExpDic(X_exp1_train_dic, X_exp2_train_dic, fitted_scaler_classifier_exp2_train_dic, X_exp1_test_dic, X_exp2_test_dic, fitted_scaler_classifier_exp2_test_dic):\n",
    "    X_exp_train_dic = MakeWACAXExpDicOwner(X_exp2_train_dic, scaler_clip=False, scaler_type=\"MinMaxScaler\")\n",
    "    X_exp_train_dic = MakeWACAXExpDicUnknown(X_exp1_train_dic, X_exp_train_dic, fitted_raw_scaler_dict=fitted_scaler_classifier_exp2_train_dic)\n",
    "    X_exp_test_dic = MakeWACAXExpDicOwner(X_exp2_test_dic, scaler_clip=False, scaler_type=\"MinMaxScaler\")\n",
    "    X_exp_test_dic = MakeWACAXExpDicUnknown(X_exp1_test_dic, X_exp_test_dic, fitted_raw_scaler_dict=fitted_scaler_classifier_exp2_test_dic)\n",
    "    \n",
    "    return X_exp_train_dic, X_exp_test_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "  \n",
    "\n",
    "def extract_WACA_features(X_exp1_dic, X_exp2_dic, fitted_scaler_classifier_exp2_dic, scaler_clip, verbose=1):\n",
    "    start = timeit.default_timer()\n",
    "    X_exp_dic = MakeWACAXExpDicOwner(X_exp2_dic, scaler_clip=scaler_clip, scaler_type=\"MinMaxScaler\")\n",
    "    stop = timeit.default_timer()\n",
    "    print('MakeWACAXExpDicOwner Time: ', stop - start)\n",
    "\n",
    "    start = timeit.default_timer()\n",
    "    X_exp_dic = MakeWACAXExpDicUnknown(X_exp1_dic, X_exp_dic, fitted_raw_scaler_dict=fitted_scaler_classifier_exp2_dic)\n",
    "    stop = timeit.default_timer()\n",
    "    print('MakeWACAXExpDicUnknown Time: ', stop - start)\n",
    "    \n",
    "    if verbose: print(\"Done extracting features\")\n",
    "    return X_exp_dic\n",
    "\n",
    "EXTRACT_WACA_features_DICT={winsize: extract_WACA_features for winsize in WINDOW_SIZE_LST}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{125: <function __main__.extract_WACA_features(X_exp1_dic, X_exp2_dic, fitted_scaler_classifier_exp2_dic, verbose=1)>,\n",
       " 250: <function __main__.extract_WACA_features(X_exp1_dic, X_exp2_dic, fitted_scaler_classifier_exp2_dic, verbose=1)>,\n",
       " 500: <function __main__.extract_WACA_features(X_exp1_dic, X_exp2_dic, fitted_scaler_classifier_exp2_dic, verbose=1)>,\n",
       " 750: <function __main__.extract_WACA_features(X_exp1_dic, X_exp2_dic, fitted_scaler_classifier_exp2_dic, verbose=1)>,\n",
       " 1000: <function __main__.extract_WACA_features(X_exp1_dic, X_exp2_dic, fitted_scaler_classifier_exp2_dic, verbose=1)>,\n",
       " 1250: <function __main__.extract_WACA_features(X_exp1_dic, X_exp2_dic, fitted_scaler_classifier_exp2_dic, verbose=1)>,\n",
       " 1500: <function __main__.extract_WACA_features(X_exp1_dic, X_exp2_dic, fitted_scaler_classifier_exp2_dic, verbose=1)>,\n",
       " 1750: <function __main__.extract_WACA_features(X_exp1_dic, X_exp2_dic, fitted_scaler_classifier_exp2_dic, verbose=1)>,\n",
       " 2000: <function __main__.extract_WACA_features(X_exp1_dic, X_exp2_dic, fitted_scaler_classifier_exp2_dic, verbose=1)>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXTRACT_WACA_features_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "4054a4f7-9d44-4c4d-865d-b30fc17e48db",
    "_uuid": "e31d6f97-ed7c-4146-bec0-ba773f823c37",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def spectral_energy(x):\n",
    "    '''\n",
    "    spectral_energy according to Parseval's theorem\n",
    "    '''\n",
    "    # should i keep using rfft or just fft\n",
    "    return (1/len(x)) * np.sum(np.abs(np.fft.rfft(x))**2)\n",
    "\n",
    "def window_entropy(x, n_bins=13):\n",
    "    \n",
    "    heights, bins = np.histogram(x, bins=n_bins)\n",
    "    heights = heights/sum(heights)\n",
    "    window_entropy=entropy(heights, base=2, axis=0)\n",
    "    \n",
    "    return window_entropy\n",
    "\n",
    "def average_absolute_difference_peaks(peaks, window_size):\n",
    "    \n",
    "    if len(peaks) == 0:\n",
    "        return window_size\n",
    "    elif len(peaks) == 1:\n",
    "        return window_size\n",
    "    \n",
    "    n = len(peaks) * (len(peaks)-1)\n",
    "    \n",
    "    return np.abs(peaks[:, None] - peaks[None, :]).ravel().sum()/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "77192541-b078-4a38-9a98-29b1868f61b2",
    "_uuid": "d0eaad53-bd24-45f3-8261-1e1f4a244ce5",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def signal_to_encoding(signal_df, freq=100):\n",
    "    dic = {}\n",
    "\n",
    "#     print(\"mean calculation started\")\n",
    "    dic['mean_x_a'] = np.mean(signal_df['x_a'])\n",
    "    dic['mean_y_a'] = np.mean(signal_df['y_a'])\n",
    "    dic['mean_z_a'] = np.mean(signal_df['z_a'])\n",
    "    dic['mean_x_g'] = np.mean(signal_df['x_g'])\n",
    "    dic['mean_y_g'] = np.mean(signal_df['y_g'])\n",
    "    dic['mean_z_g'] = np.mean(signal_df['z_g'])\n",
    "#     print(\"mean calculation ended\")\n",
    "    \n",
    "#     print(\"median calculation started\")\n",
    "    dic['median_x_a'] = np.median(signal_df['x_a'])\n",
    "    dic['median_y_a'] = np.median(signal_df['y_a'])\n",
    "    dic['median_z_a'] = np.median(signal_df['z_a'])\n",
    "    dic['median_x_g'] = np.median(signal_df['x_g'])\n",
    "    dic['median_y_g'] = np.median(signal_df['y_g'])\n",
    "    dic['median_z_g'] = np.median(signal_df['z_g'])\n",
    "#     print(\"median calculation ended\")\n",
    "    \n",
    "#     print(\"var calculation started\")\n",
    "    dic['var_x_a'] = np.var(signal_df['x_a'])\n",
    "    dic['var_y_a'] = np.var(signal_df['y_a'])\n",
    "    dic['var_z_a'] = np.var(signal_df['z_a'])\n",
    "    dic['var_x_g'] = np.var(signal_df['x_g'])\n",
    "    dic['var_y_g'] = np.var(signal_df['y_g'])\n",
    "    dic['var_z_g'] = np.var(signal_df['z_g'])\n",
    "#     print(\"var calculation ended\")\n",
    "\n",
    "#     print(\"avg absolute difference of peaks calculation started\")\n",
    "    peaks_x_a, _ = find_peaks(signal_df['x_a'], height=None)\n",
    "    peaks_y_a, _ = find_peaks(signal_df['y_a'], height=None)\n",
    "    peaks_z_a, _ = find_peaks(signal_df['z_a'], height=None)\n",
    "    peaks_x_g, _ = find_peaks(signal_df['x_g'], height=None)\n",
    "    peaks_y_g, _ = find_peaks(signal_df['y_g'], height=None)\n",
    "    peaks_z_g, _ = find_peaks(signal_df['z_g'], height=None)\n",
    "    \n",
    "    window_size=len(signal_df['x_a'])\n",
    "    dic['aadp_x_a'] = average_absolute_difference_peaks(peaks=peaks_x_a, window_size=window_size)\n",
    "    dic['aadp_y_a'] = average_absolute_difference_peaks(peaks=peaks_y_a, window_size=window_size)\n",
    "    dic['aadp_z_a'] = average_absolute_difference_peaks(peaks=peaks_z_a, window_size=window_size)\n",
    "    dic['aadp_x_g'] = average_absolute_difference_peaks(peaks=peaks_x_g, window_size=window_size)\n",
    "    dic['aadp_y_g'] = average_absolute_difference_peaks(peaks=peaks_y_g, window_size=window_size)\n",
    "    dic['aadp_z_g'] = average_absolute_difference_peaks(peaks=peaks_z_g, window_size=window_size)\n",
    "    \n",
    "\n",
    "    \n",
    "#     print(\"range calculation started\")\n",
    "    dic['ptp_x_a'] = np.ptp(signal_df['x_a'])\n",
    "    dic['ptp_y_a'] = np.ptp(signal_df['y_a'])\n",
    "    dic['ptp_z_a'] = np.ptp(signal_df['z_a'])\n",
    "    dic['ptp_x_g'] = np.ptp(signal_df['x_g'])\n",
    "    dic['ptp_y_g'] = np.ptp(signal_df['y_g'])\n",
    "    dic['ptp_z_g'] = np.ptp(signal_df['z_g'])\n",
    "#     print(\"range calculation ended\")\n",
    "    \n",
    "#     print(\"mode calculation started\")\n",
    "    dic['mode_x_a'] = mode(signal_df['x_a'])[0][0]\n",
    "    dic['mode_y_a'] = mode(signal_df['y_a'])[0][0]\n",
    "    dic['mode_z_a'] = mode(signal_df['z_a'])[0][0]\n",
    "    dic['mode_x_g'] = mode(signal_df['x_g'])[0][0]\n",
    "    dic['mode_y_g'] = mode(signal_df['y_g'])[0][0]\n",
    "    dic['mode_z_g'] = mode(signal_df['z_g'])[0][0]\n",
    "#     print(\"mode calculation ended\")\n",
    "    \n",
    "#     print(\"cov calculation started\")\n",
    "# seem to require 2 axes according to waca pattent\n",
    "    # dic['cov_x_a'] = np.cov(signal_df['x_a']) * 1\n",
    "    # dic['cov_y_a'] = np.cov(signal_df['y_a']) * 1\n",
    "    # dic['cov_z_a'] = np.cov(signal_df['z_a']) * 1\n",
    "    # dic['cov_x_g'] = np.cov(signal_df['x_g']) * 1\n",
    "    # dic['cov_y_g'] = np.cov(signal_df['y_g']) * 1\n",
    "    # dic['cov_z_g'] = np.cov(signal_df['z_g']) * 1\n",
    "    dic['cov_xy_a'] = np.cov(signal_df['x_a'], signal_df['y_a'])[0][1]\n",
    "    dic['cov_yz_a'] = np.cov(signal_df['y_a'], signal_df['z_a'])[0][1]\n",
    "    dic['cov_xz_a'] = np.cov(signal_df['x_a'], signal_df['z_a'])[0][1]\n",
    "    dic['cov_xy_g'] = np.cov(signal_df['x_g'], signal_df['y_g'])[0][1]\n",
    "    dic['cov_yz_g'] = np.cov(signal_df['y_g'], signal_df['z_g'])[0][1]\n",
    "    dic['cov_xz_g'] = np.cov(signal_df['x_g'], signal_df['z_g'])[0][1]\n",
    "    \n",
    "#     print(\"cov calculation ended\")\n",
    "    \n",
    "#     print(\"mean absolute deviation calculation started\")\n",
    "    dic['mad_x_a'] = median_abs_deviation(signal_df['x_a'])\n",
    "    dic['mad_y_a'] = median_abs_deviation(signal_df['y_a'])\n",
    "    dic['mad_z_a'] = median_abs_deviation(signal_df['z_a'])\n",
    "    dic['mad_x_g'] = median_abs_deviation(signal_df['x_g'])\n",
    "    dic['mad_y_g'] = median_abs_deviation(signal_df['y_g'])\n",
    "    dic['mad_z_g'] = median_abs_deviation(signal_df['z_g'])\n",
    "#     print(\"mean absolute deviation calculation ended\")\n",
    "    \n",
    "#     print(\"inter-quartile range calculation started\")\n",
    "    dic['iqr_x_a'] = iqr(signal_df['x_a'])\n",
    "    dic['iqr_y_a'] = iqr(signal_df['y_a'])\n",
    "    dic['iqr_z_a'] = iqr(signal_df['z_a'])\n",
    "    dic['iqr_x_g'] = iqr(signal_df['x_g'])\n",
    "    dic['iqr_y_g'] = iqr(signal_df['y_g'])\n",
    "    dic['iqr_z_g'] = iqr(signal_df['z_g'])\n",
    "#     print(\"inter-quartile range calculation ended\")\n",
    "    \n",
    "#     print(\"correlation calculation started\")\n",
    "    dic['correlate_xy_a'] = np.corrcoef(signal_df['x_a'], signal_df['y_a'])[0][1]\n",
    "    dic['correlate_yz_a'] = np.corrcoef(signal_df['y_a'], signal_df['z_a'])[0][1]\n",
    "    dic['correlate_xz_a'] = np.corrcoef(signal_df['x_a'], signal_df['z_a'])[0][1]\n",
    "    dic['correlate_xy_g'] = np.corrcoef(signal_df['x_g'], signal_df['y_g'])[0][1]\n",
    "    dic['correlate_yz_g'] = np.corrcoef(signal_df['y_g'], signal_df['z_g'])[0][1]\n",
    "    dic['correlate_xz_g'] = np.corrcoef(signal_df['x_g'], signal_df['z_g'])[0][1]\n",
    "#     print(\"correlation calculation ended\")\n",
    "    \n",
    "#     print(\"skew calculation started\")\n",
    "    dic['skew_x_a'] = skew(signal_df['x_a'])\n",
    "    dic['skew_y_a'] = skew(signal_df['y_a'])\n",
    "    dic['skew_z_a'] = skew(signal_df['z_a'])\n",
    "    dic['skew_x_g'] = skew(signal_df['x_g'])\n",
    "    dic['skew_y_g'] = skew(signal_df['y_g'])\n",
    "    dic['skew_z_g'] = skew(signal_df['z_g'])\n",
    "#     print(\"skew calculation ended\")\n",
    "    \n",
    "#     print(\"kurtosis calculation started\")\n",
    "    dic['kurtosis_x_a'] = kurtosis(signal_df['x_a'])\n",
    "    dic['kurtosis_y_a'] = kurtosis(signal_df['y_a'])\n",
    "    dic['kurtosis_z_a'] = kurtosis(signal_df['z_a'])\n",
    "    dic['kurtosis_x_g'] = kurtosis(signal_df['x_g'])\n",
    "    dic['kurtosis_y_g'] = kurtosis(signal_df['y_g'])\n",
    "    dic['kurtosis_z_g'] = kurtosis(signal_df['z_g'])\n",
    "#     print(\"kurtosis calculation ended\")\n",
    "    \n",
    "    \n",
    "#     print(\"spectral energy calculation started\")\n",
    "    dic['spectral_energy_x_a'] = spectral_energy(signal_df['x_a'])\n",
    "    dic['spectral_energy_y_a'] = spectral_energy(signal_df['y_a'])\n",
    "    dic['spectral_energy_z_a'] = spectral_energy(signal_df['z_a'])\n",
    "    dic['spectral_energy_x_g'] = spectral_energy(signal_df['x_g'])\n",
    "    dic['spectral_energy_y_g'] = spectral_energy(signal_df['y_g'])\n",
    "    dic['spectral_energy_z_g'] = spectral_energy(signal_df['z_g'])\n",
    "#     print(\"spectral energy calculation ended\")\n",
    "\n",
    "    # print(\"entropy calculation started\")\n",
    "\n",
    "    dic['entropy_x_a'] = window_entropy(signal_df['x_a'])\n",
    "    dic['entropy_y_a'] = window_entropy(signal_df['y_a'])\n",
    "    dic['entropy_z_a'] = window_entropy(signal_df['z_a'])\n",
    "    dic['entropy_x_g'] = window_entropy(signal_df['x_g'])\n",
    "    dic['entropy_y_g'] = window_entropy(signal_df['y_g'])\n",
    "    dic['entropy_z_g'] = window_entropy(signal_df['z_g'])\n",
    "    \n",
    "    # print(\"entropy calculation ended\")\n",
    "\n",
    "\n",
    "    \n",
    "    vector = [dic['mean_x_a'], \n",
    "              dic['mean_y_a'],\n",
    "              dic['mean_z_a'],\n",
    "              dic['mean_x_g'],\n",
    "              dic['mean_y_g'],\n",
    "              dic['mean_z_g'],\n",
    "              \n",
    "              dic['median_x_a'],\n",
    "              dic['median_y_a'],\n",
    "              dic['median_z_a'],\n",
    "              dic['median_x_g'],\n",
    "              dic['median_y_g'],\n",
    "              dic['median_z_g'],\n",
    "              \n",
    "              dic['var_x_a'],\n",
    "              dic['var_y_a'],\n",
    "              dic['var_z_a'],\n",
    "              dic['var_x_g'],\n",
    "              dic['var_y_g'],\n",
    "              dic['var_z_g'],\n",
    "              \n",
    "              dic['aadp_x_a'],\n",
    "              dic['aadp_y_a'],\n",
    "              dic['aadp_z_a'],\n",
    "              dic['aadp_x_g'],\n",
    "              dic['aadp_y_g'],\n",
    "              dic['aadp_z_g'],\n",
    "              \n",
    "              dic['ptp_x_a'],\n",
    "              dic['ptp_y_a'],\n",
    "              dic['ptp_z_a'],\n",
    "              dic['ptp_x_g'],\n",
    "              dic['ptp_y_g'],\n",
    "              dic['ptp_z_g'],\n",
    "              \n",
    "              dic['mode_x_a'],\n",
    "              dic['mode_y_a'],\n",
    "              dic['mode_z_a'],\n",
    "              dic['mode_x_g'],\n",
    "              dic['mode_y_g'],\n",
    "              dic['mode_z_g'],\n",
    "              \n",
    "              dic['cov_xy_a'],\n",
    "              dic['cov_yz_a'],\n",
    "              dic['cov_xz_a'],\n",
    "              dic['cov_xy_g'],\n",
    "              dic['cov_yz_g'],\n",
    "              dic['cov_xz_g'],\n",
    "              \n",
    "              dic['mad_x_a'],\n",
    "              dic['mad_y_a'],\n",
    "              dic['mad_z_a'],\n",
    "              dic['mad_x_g'],\n",
    "              dic['mad_y_g'],\n",
    "              dic['mad_z_g'],\n",
    "              \n",
    "              dic['iqr_x_a'],\n",
    "              dic['iqr_y_a'],\n",
    "              dic['iqr_z_a'],\n",
    "              dic['iqr_x_g'],\n",
    "              dic['iqr_y_g'],\n",
    "              dic['iqr_z_g'],\n",
    "              \n",
    "              dic['correlate_xy_a'],\n",
    "              dic['correlate_yz_a'],\n",
    "              dic['correlate_xz_a'],\n",
    "              dic['correlate_xy_g'],\n",
    "              dic['correlate_yz_g'],\n",
    "              dic['correlate_xz_g'],\n",
    "              \n",
    "              dic['skew_x_a'],\n",
    "              dic['skew_y_a'],\n",
    "              dic['skew_z_a'],\n",
    "              dic['skew_x_g'],\n",
    "              dic['skew_y_g'],\n",
    "              dic['skew_z_g'],\n",
    "              \n",
    "              dic['kurtosis_x_a'],\n",
    "              dic['kurtosis_y_a'],\n",
    "              dic['kurtosis_z_a'],\n",
    "              dic['kurtosis_x_g'],\n",
    "              dic['kurtosis_y_g'],\n",
    "              dic['kurtosis_z_g'],\n",
    "              \n",
    "              dic['spectral_energy_x_a'],\n",
    "              dic['spectral_energy_y_a'],\n",
    "              dic['spectral_energy_z_a'],\n",
    "              dic['spectral_energy_x_g'],\n",
    "              dic['spectral_energy_y_g'],\n",
    "              dic['spectral_energy_z_g'],\n",
    "              \n",
    "              dic['entropy_x_a'],\n",
    "              dic['entropy_y_a'],\n",
    "              dic['entropy_z_a'],\n",
    "              dic['entropy_x_g'],\n",
    "              dic['entropy_y_g'],\n",
    "              dic['entropy_z_g']\n",
    "             ]\n",
    "    \n",
    "    \n",
    "    return dic, np.array(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0092c407-5812-43bd-a3a9-f78bfadff3b7",
    "_uuid": "35621051-b220-428d-b505-85ef106f8ce6"
   },
   "source": [
    "# Split Dataset for Valid/Test  \n",
    "In two splits: one used during hyperparameter optimization, and one used during testing.\n",
    "\n",
    "The split is done along the subjects: All sessions of a single subject will either be in the validation split or in the testing split, never in both.\n",
    "\n",
    "They did a 30 60 split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "92d8caee-6b8e-45c7-b272-010046daec00",
    "_uuid": "0c7969f6-43b9-4d95-9253-76d5d040d30a"
   },
   "source": [
    "# Reshaping Raw Features.\n",
    "We have our own function of windows for this. Do this for both training and testing.\n",
    "\n",
    "# Extracting time and frequency based features.\n",
    "Again, we have a function for this. Do this for both training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "11bbc563-4576-4588-a73c-44936653c13b",
    "_uuid": "b9984fbd-a13f-4d5d-ad57-8b0c41aafe1d"
   },
   "source": [
    "# Hyperparameter Optimization \n",
    "\n",
    "I do not find any reaqsonable explaination how to use a cross-validation as we are talking about anomaly detection.\n",
    "\n",
    "I am using the experiment 1 data as train, and experiment 2 data as validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d09c240f-d000-4840-880c-260436115209",
    "_uuid": "aca85e4e-ed2f-4c54-bf0f-2f938ac45a07"
   },
   "source": [
    "# Using SVM in a real-world Scenario with multiple genuine users and intruders\n",
    "Source: https://datascience.stackexchange.com/questions/23623/what-is-the-best-way-to-classify-data-not-belonging-to-set-of-classes\n",
    "\n",
    "Stage 1: \n",
    "    Use one-class SVM to assign those images that do not belong to the set of predefined classes as the 9-th class.\n",
    "\n",
    "Stage 2:\n",
    "    For those images that passes through your filter, let the multi-class SVM assign them to one of the 8 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79b10ce2-42b2-4156-a1df-633bf52c05e6",
    "_uuid": "bf0780fa-46ed-4d13-90ee-3640fe7cc4aa"
   },
   "source": [
    "Loading data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeWACAXExpDicOwner(X_exp_reg_df_dict, scaler_clip, scaler_type=\"MinMaxScaler\"):\n",
    "    '''k\n",
    "    ???\n",
    "    return \n",
    "    X_exp_dic\n",
    "    dfLists are of the same size.\n",
    "    '''\n",
    "        \n",
    "    return MakeXExpDicOwner(X_exp_reg_df_dict, scaler_clip=scaler_clip, scaler_type=\"MinMaxScaler\",\n",
    "                            feature_extractor=WACA_feature_extractor, feature_extractor_transformer=transform_user_WACA_windows)\n",
    "    \n",
    "\n",
    "    \n",
    "def MakeWACAXExpDicUnknown(X_exp_unknown_df_dict, X_exp_dic, fitted_raw_scaler_dict):\n",
    "    '''k\n",
    "    ???\n",
    "    return \n",
    "    X_exp_dic\n",
    "    dfLists are of the same size.\n",
    "    '''  \n",
    "        \n",
    "    return MakeXExpDicUnknown(X_exp_unknown_df_dict, X_exp_dic, fitted_raw_scaler_dict, \n",
    "                              feature_extractor=WACA_feature_extractor, feature_extractor_transformer=transform_user_WACA_windows)\n",
    "\n",
    "\n",
    "def WACA_feature_extractor(X_exp):\n",
    "    a = []\n",
    "    \n",
    "    for window in X_exp:\n",
    "        a.append(signal_to_encoding(window)[1])\n",
    "        \n",
    "    return np.array(a)\n",
    "\n",
    "\n",
    "\n",
    "def transform_user_WACA_windows(X_exp, fitted_scaler):\n",
    "    \n",
    "    \n",
    "    transformed_X_exp = []\n",
    "    \n",
    "#     print(X_exp[0].shape)\n",
    "    for window in X_exp:\n",
    "        if len(window.shape) == 1:\n",
    "            window = window.reshape(1, -1)\n",
    "        scaled_array = fitted_scaler.transform(window)\n",
    "        transformed_X_exp.append(scaled_array.reshape(-1))\n",
    "        \n",
    "    return np.array(transformed_X_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\u001b[32mWACA utility functions imported\u001b[0m--------------------\n"
     ]
    }
   ],
   "source": [
    "print(20*'-' + \"\\x1b[32mWACA utility functions imported\\x1b[0m\" + 20*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
