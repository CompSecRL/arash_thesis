{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a24f672d-e0a3-4e6d-981f-e9ac9a91c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "CORES = -1\n",
    "SEED = 567\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f653b00-c8d6-40b2-b5d3-791c7ccdde9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "EER: 0.333, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.400, Threshold: 0.200 <-- Worse case\n",
      "EER: 0.167, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.333, Threshold: 1.000 <-- Worse case\n",
      "--------------------\u001b[32mUtility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mPreprocessing utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mNeural Networks utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "EER: 0.333, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.400, Threshold: 0.200 <-- Worse case\n",
      "EER: 0.167, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.333, Threshold: 1.000 <-- Worse case\n",
      "--------------------\u001b[32mUtility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mPreprocessing utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mWACA utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mClassification utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "Seed was set to: 567\n",
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install python-docx\n",
    "# !pip install antropy\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import dataclasses\n",
    "import math as math\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import cross_validate, RandomizedSearchCV\n",
    "import statsmodels.stats.api as sms\n",
    "from tqdm.auto import tqdm\n",
    "from dataclasses import asdict\n",
    "from sklearn import svm\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import random\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_curve, accuracy_score, make_scorer, auc\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold # Feature selector\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Input,\n",
    "    Conv1D,\n",
    "    MaxPooling1D,\n",
    "    Flatten,\n",
    "    Lambda,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    "    GlobalAveragePooling1D,\n",
    "    Activation\n",
    ")\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Ftrl, Nadam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import concatenate as keras_concat\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import iqr\n",
    "from scipy.stats import median_absolute_deviation\n",
    "from scipy.stats import mode\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.signal import peak_widths\n",
    "# from scipy.special import entr\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, MaxAbsScaler, RobustScaler, PowerTransformer\n",
    "get_new_scaler_dict = {\"StandardScaler\": StandardScaler, \"MinMaxScaler\": MinMaxScaler, \"Normalizer\": Normalizer, \n",
    "                       \"MaxAbsScaler\": MaxAbsScaler, \"RobustScaler\": RobustScaler, \"PowerTransformer\": PowerTransformer}\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import auc\n",
    "# import antropy as ant\n",
    "import time\n",
    "# import docx\n",
    "\n",
    "# Global utitlity functions are in separate notebook\n",
    "# Global utitlity functions are in separate notebook\n",
    "%run ./Classification_utility-functions.ipynb\n",
    "%run ./SEED-CONSTANTS.ipynb\n",
    "\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "print(f\"Seed was set to: {SEED}\")\n",
    "\n",
    "\n",
    "print(\"Setup Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9cc83fb-4d1e-4675-90bc-f8a762f11dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class ExperimentParameters:\n",
    "    \"\"\"Contains all relevant parameters to run an experiment.\"\"\"\n",
    "\n",
    "    name: str  # Name of Experiments Parameter set. Used as identifier for charts etc.\n",
    "\n",
    "    # Data / Splitting:\n",
    "    frequency: int\n",
    "    feature_cols: list  # Columns used as features\n",
    "    max_subjects: int\n",
    "    user_ids: list\n",
    "    num_sample_points_per_exp: int\n",
    "    exp_begin_cutoff_idx: int\n",
    "    exp_end_cutoff_idx: int\n",
    "        \n",
    "#     exclude_subjects: list  # Don't load data from those users\n",
    "    n_valid_train_subjects: int\n",
    "    n_valid_test_subjects: int\n",
    "    n_test_train_subjects: int\n",
    "    n_test_test_subjects: int\n",
    "    seconds_per_subject_train: float\n",
    "    seconds_per_subject_test: float\n",
    "    task_types: list  # Limit scenarios to [1, 3, 5] for sitting or [2, 4, 6] for walking, or don't limit (None)\n",
    "\n",
    "    # Reshaping\n",
    "    window_size: int  # After resampling\n",
    "    nn_step_width: int\n",
    "    ocsvm_step_width: int\n",
    "\n",
    "    # Normalization\n",
    "    scaler: str  # {\"std\", \"robust\", \"minmax\"}\n",
    "    scaler_scope: str  # {\"subject\", \"session\"}\n",
    "    scaler_global: bool  # scale training and testing sets at once (True), or fit scaler on training only (False)\n",
    "\n",
    "    # Siamese Network\n",
    "    max_pairs_per_session: int  # Max. number of pairs per session\n",
    "    margin: float  # Contrastive Loss Margin\n",
    "    model_variant: str  # {\"1d\", \"2d\"} Type of architecture\n",
    "    filters: list  # List of length 4, containing number of filters for conv layers\n",
    "    epochs_best: int  # Train epochs to for final model\n",
    "    epochs_max: int\n",
    "    batch_size: int\n",
    "    optimizer: str  # Optimizer to use for Siamese Network\n",
    "    optimizer_lr: float  # Learning Rate\n",
    "    optimizer_decay: float\n",
    "\n",
    "    # OCSVM\n",
    "    ocsvm_kernel: str # ocsvm kernel\n",
    "    ocsvm_nu: float  # Best value found in random search, used for final model\n",
    "    ocsvm_gamma: float  # Best value found in random search, used for final model\n",
    "\n",
    "    # Calculated values\n",
    "    def __post_init__(self):\n",
    "        # HDF key of table:\n",
    "        self.table_name = f\"sensors_{self.frequency}hz\"\n",
    "\n",
    "        # Number of samples per _session_ used for training:\n",
    "        self.samples_per_subject_train = math.ceil(\n",
    "            (self.seconds_per_subject_train * 100)\n",
    "            / (100 / self.frequency)\n",
    "            / self.window_size\n",
    "        )\n",
    "\n",
    "        # Number of samples per _session_ used for testing:\n",
    "        self.samples_per_subject_test = math.ceil(\n",
    "            (self.seconds_per_subject_test * 100)\n",
    "            / (100 / self.frequency)\n",
    "            / self.window_size\n",
    "        )\n",
    "\n",
    "\n",
    "# INSTANCES\n",
    "# ===========================================================\n",
    "\n",
    "# NAIVE_MINMAX (2D Filters)\n",
    "# -----------------------------------------------------------\n",
    "NAIVE_MINMAX_2D = ExperimentParameters(\n",
    "    name=\"NAIVE-MINMAX-2D\",\n",
    "#     # Data / Splitting\n",
    "    frequency=100,\n",
    "    feature_cols=[\n",
    "        \"acc_x\",\n",
    "        \"acc_y\",\n",
    "        \"acc_z\",\n",
    "        \"gyr_x\",\n",
    "        \"gyr_y\",\n",
    "        \"gyr_z\"\n",
    "    ],\n",
    "    max_subjects=29,\n",
    "    user_ids = [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 28, 29, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49],\n",
    "    num_sample_points_per_exp=21000,\n",
    "    exp_begin_cutoff_idx=500,\n",
    "    exp_end_cutoff_idx=-500,\n",
    "    n_valid_train_subjects=40,\n",
    "    n_valid_test_subjects=10,\n",
    "    n_test_train_subjects=10,\n",
    "    n_test_test_subjects=30,\n",
    "    seconds_per_subject_train=67.5,\n",
    "    seconds_per_subject_test=67.5,\n",
    "    task_types=None,\n",
    "    # Reshaping\n",
    "    window_size=100 * 1,  #1 sec\n",
    "    nn_step_width=125,\n",
    "    ocsvm_step_width=125,\n",
    "    # Normalization\n",
    "    scaler=\"minmax\",\n",
    "    scaler_scope=\"subject\",\n",
    "    scaler_global=True,\n",
    "    # Siamese Network\n",
    "    model_variant=\"2d\",\n",
    "    filters=[32, 64, 128, 32],\n",
    "#     filters=[32, 64, 128, 64],\n",
    "    epochs_best=35,\n",
    "    epochs_max=40,\n",
    "    batch_size=200,\n",
    "    optimizer=\"sgd\",\n",
    "    optimizer_lr=0.01,\n",
    "    optimizer_decay=0,\n",
    "    max_pairs_per_session=60,  # => 4min\n",
    "    margin=0.2,\n",
    "    # OCSVM\n",
    "    ocsvm_kernel=\"rbf\",\n",
    "    ocsvm_nu=0.092,\n",
    "    ocsvm_gamma=1.151,\n",
    ")  # <END NAIVE_APPROACH>\n",
    "\n",
    "# VALID_MINMAX (2D)\n",
    "# -----------------------------------------------------------\n",
    "VALID_MINMAX_2D = dataclasses.replace(\n",
    "    NAIVE_MINMAX_2D,\n",
    "    name=\"VALID-MINMAX-2D\",\n",
    "    task_types=None,\n",
    "    scaler_global=False,\n",
    "    epochs_max=40,\n",
    "    ocsvm_nu=0.110,\n",
    "    ocsvm_gamma=59.636,\n",
    ")\n",
    "\n",
    "# NAIVE_ROBUST (2D)\n",
    "# -----------------------------------------------------------\n",
    "NAIVE_ROBUST_2D = dataclasses.replace(\n",
    "    NAIVE_MINMAX_2D,\n",
    "    name=\"NAIVE-ROBUST-2D\",\n",
    "    scaler=\"robust\",\n",
    "    optimizer=\"sgd\",\n",
    "    optimizer_lr=0.05, # Decreased, to avoid \"all zeros\" prediction\n",
    "    optimizer_decay=0.002,\n",
    "    epochs_best=5,\n",
    "    ocsvm_nu=0.214,\n",
    "    ocsvm_gamma=2.354,\n",
    ")\n",
    "\n",
    "# VALID_ROBUST (2D)\n",
    "# -----------------------------------------------------------\n",
    "VALID_ROBUST_2D = dataclasses.replace(\n",
    "    NAIVE_MINMAX_2D,\n",
    "    name=\"VALID-ROBUST-2D\",\n",
    "    scaler=\"robust\",\n",
    "    scaler_global=False,\n",
    "    epochs_best=6,\n",
    "    epochs_max=20,\n",
    "    optimizer=\"sgd\",\n",
    "    optimizer_lr=0.05,  # Decrease LR, to avoid \"all zeros\" prediction\n",
    "    optimizer_decay=0.002,\n",
    "    ocsvm_nu=0.190,\n",
    "    ocsvm_gamma=0.069,\n",
    ")\n",
    "\n",
    "# VALID_ROBUST (1D)\n",
    "# -----------------------------------------------------------\n",
    "VALID_ROBUST_1D = dataclasses.replace(\n",
    "    NAIVE_MINMAX_2D,\n",
    "    name=\"VALID-ROBUST-1D\",\n",
    "    scaler=\"robust\",\n",
    "    scaler_global=False,\n",
    "    model_variant=\"1d\", \n",
    "    filters=[32, 64, 128, 64],    \n",
    "    epochs_best=9,\n",
    "    epochs_max=20,\n",
    "    ocsvm_nu=0.156,\n",
    "    ocsvm_gamma=33.932,\n",
    ")\n",
    "\n",
    "# FCN_ROBUST (1D)\n",
    "# -----------------------------------------------------------\n",
    "VALID_FCN_ROBUST_125 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_2D,\n",
    "    name=\"VALID-FCN-ROBUST-FINAL\",\n",
    "    task_types=[1, 2],\n",
    "    feature_cols=[\"acc_x\", \"acc_y\", \"acc_z\", 'gyr_x', 'gyr_y', 'gyr_z'], \n",
    "    frequency=100,\n",
    "    window_size=125,\n",
    "\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    seconds_per_subject_train=60 * 10,\n",
    "    seconds_per_subject_test=60 * 10,\n",
    "    max_pairs_per_session=60 * 10,\n",
    "    model_variant=\"fcn\",\n",
    "    filters=[32, 64, 32],\n",
    "    optimizer=\"adam\",\n",
    "    optimizer_lr=0.00001,\n",
    "#     optimizer_lr=0.0001,\n",
    "#     optimizer_lr=0.001,\n",
    "#     optimizer_lr=0.01,\n",
    "    optimizer_decay=None,\n",
    "    batch_size=300,\n",
    "    margin=1,\n",
    "#     margin=.2,\n",
    "    epochs_best=40,\n",
    "    epochs_max=80,\n",
    "    ocsvm_nu=0.007,\n",
    "    ocsvm_gamma=0.002,\n",
    ")\n",
    "\n",
    "VALID_FCN_ROBUST_250 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_2D,\n",
    "    name=\"VALID-FCN-ROBUST-FINAL\",\n",
    "    task_types=[1, 2],\n",
    "    feature_cols=[\"acc_x\", \"acc_y\", \"acc_z\", 'gyr_x', 'gyr_y', 'gyr_z'], \n",
    "    frequency=100,\n",
    "    window_size=250,\n",
    "\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    seconds_per_subject_train=60 * 10,\n",
    "    seconds_per_subject_test=60 * 10,\n",
    "    max_pairs_per_session=60 * 10,\n",
    "    model_variant=\"fcn\",\n",
    "    filters=[32, 64, 32],\n",
    "    optimizer=\"adam\",\n",
    "    optimizer_lr=0.00001,\n",
    "#     optimizer_lr=0.0001,\n",
    "#     optimizer_lr=0.001,\n",
    "#     optimizer_lr=0.01,\n",
    "    optimizer_decay=None,\n",
    "    batch_size=300,\n",
    "    margin=1,\n",
    "#     margin=.2,\n",
    "    epochs_best=40,\n",
    "    epochs_max=80,\n",
    "    ocsvm_nu=0.007,\n",
    "    ocsvm_gamma=0.002,\n",
    ")\n",
    "\n",
    "VALID_FCN_ROBUST_500 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_2D,\n",
    "    name=\"VALID-FCN-ROBUST-FINAL\",\n",
    "    task_types=[1, 2],\n",
    "    feature_cols=[\"acc_x\", \"acc_y\", \"acc_z\", 'gyr_x', 'gyr_y', 'gyr_z'], \n",
    "    frequency=100,\n",
    "    window_size=500,\n",
    "\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    seconds_per_subject_train=60 * 10,\n",
    "    seconds_per_subject_test=60 * 10,\n",
    "    max_pairs_per_session=60 * 10,\n",
    "    model_variant=\"fcn\",\n",
    "    filters=[32, 64, 32],\n",
    "    optimizer=\"adam\",\n",
    "    optimizer_lr=0.00001,\n",
    "#     optimizer_lr=0.0001,\n",
    "#     optimizer_lr=0.001,\n",
    "#     optimizer_lr=0.01,\n",
    "    optimizer_decay=None,\n",
    "    batch_size=300,\n",
    "    margin=1,\n",
    "#     margin=.2,\n",
    "    epochs_best=40,\n",
    "    epochs_max=80,\n",
    "    ocsvm_nu=0.007,\n",
    "    ocsvm_gamma=0.002,\n",
    ")\n",
    "\n",
    "VALID_FCN_ROBUST_750 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_2D,\n",
    "    name=\"VALID-FCN-ROBUST-FINAL\",\n",
    "    task_types=[1, 2],\n",
    "    feature_cols=[\"acc_x\", \"acc_y\", \"acc_z\", 'gyr_x', 'gyr_y', 'gyr_z'], \n",
    "    frequency=100,\n",
    "    window_size=750,\n",
    "\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    seconds_per_subject_train=60 * 10,\n",
    "    seconds_per_subject_test=60 * 10,\n",
    "    max_pairs_per_session=60 * 10,\n",
    "    model_variant=\"fcn\",\n",
    "    filters=[32, 64, 32],\n",
    "    optimizer=\"adam\",\n",
    "    optimizer_lr=0.00001,\n",
    "#     optimizer_lr=0.0001,\n",
    "#     optimizer_lr=0.001,\n",
    "#     optimizer_lr=0.01,\n",
    "    optimizer_decay=None,\n",
    "    batch_size=300,\n",
    "    margin=1,\n",
    "#     margin=.2,\n",
    "    epochs_best=40,\n",
    "    epochs_max=80,\n",
    "    ocsvm_nu=0.007,\n",
    "    ocsvm_gamma=0.002,\n",
    ")\n",
    "\n",
    "VALID_FCN_ROBUST_1000 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_2D,\n",
    "    name=\"VALID-FCN-ROBUST-FINAL\",\n",
    "    task_types=[1, 2],\n",
    "    feature_cols=[\"acc_x\", \"acc_y\", \"acc_z\", 'gyr_x', 'gyr_y', 'gyr_z'], \n",
    "    frequency=100,\n",
    "    window_size=1000,\n",
    "\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    seconds_per_subject_train=60 * 10,\n",
    "    seconds_per_subject_test=60 * 10,\n",
    "    max_pairs_per_session=60 * 10,\n",
    "    model_variant=\"fcn\",\n",
    "    filters=[32, 64, 32],\n",
    "    optimizer=\"adam\",\n",
    "    optimizer_lr=0.00001,\n",
    "#     optimizer_lr=0.0001,\n",
    "#     optimizer_lr=0.001,\n",
    "#     optimizer_lr=0.01,\n",
    "    optimizer_decay=None,\n",
    "    batch_size=300,\n",
    "    margin=1,\n",
    "#     margin=.2,\n",
    "    epochs_best=40,\n",
    "    epochs_max=80,\n",
    "    ocsvm_nu=0.007,\n",
    "    ocsvm_gamma=0.002,\n",
    ")\n",
    "\n",
    "VALID_FCN_ROBUST_1250 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_2D,\n",
    "    name=\"VALID-FCN-ROBUST-FINAL\",\n",
    "    task_types=[1, 2],\n",
    "    feature_cols=[\"acc_x\", \"acc_y\", \"acc_z\", 'gyr_x', 'gyr_y', 'gyr_z'], \n",
    "    frequency=100,\n",
    "    window_size=1250,\n",
    "\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    seconds_per_subject_train=60 * 10,\n",
    "    seconds_per_subject_test=60 * 10,\n",
    "    max_pairs_per_session=60 * 10,\n",
    "    model_variant=\"fcn\",\n",
    "    filters=[32, 64, 32],\n",
    "    optimizer=\"adam\",\n",
    "    optimizer_lr=0.00001,\n",
    "#     optimizer_lr=0.0001,\n",
    "#     optimizer_lr=0.001,\n",
    "#     optimizer_lr=0.01,\n",
    "    optimizer_decay=None,\n",
    "    batch_size=300,\n",
    "    margin=1,\n",
    "#     margin=.2,\n",
    "    epochs_best=40,\n",
    "    epochs_max=80,\n",
    "    ocsvm_nu=0.007,\n",
    "    ocsvm_gamma=0.002,\n",
    ")\n",
    "\n",
    "\n",
    "VALID_FCN_ROBUST_1500 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_2D,\n",
    "    name=\"VALID-FCN-ROBUST-FINAL\",\n",
    "    task_types=[1, 2],\n",
    "    feature_cols=[\"acc_x\", \"acc_y\", \"acc_z\", 'gyr_x', 'gyr_y', 'gyr_z'], \n",
    "    frequency=100,\n",
    "    window_size=1500,\n",
    "\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    seconds_per_subject_train=60 * 10,\n",
    "    seconds_per_subject_test=60 * 10,\n",
    "    max_pairs_per_session=60 * 10,\n",
    "    model_variant=\"fcn\",\n",
    "    filters=[32, 64, 32],\n",
    "    optimizer=\"adam\",\n",
    "    optimizer_lr=0.00001,\n",
    "#     optimizer_lr=0.0001,\n",
    "#     optimizer_lr=0.001,\n",
    "#     optimizer_lr=0.01,\n",
    "    optimizer_decay=None,\n",
    "    batch_size=300,\n",
    "    margin=1,\n",
    "#     margin=.2,\n",
    "    epochs_best=40,\n",
    "    epochs_max=80,\n",
    "    ocsvm_nu=0.007,\n",
    "    ocsvm_gamma=0.002,\n",
    ")\n",
    "\n",
    "VALID_FCN_ROBUST_1750 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_2D,\n",
    "    name=\"VALID-FCN-ROBUST-FINAL\",\n",
    "    task_types=[1, 2],\n",
    "    feature_cols=[\"acc_x\", \"acc_y\", \"acc_z\", 'gyr_x', 'gyr_y', 'gyr_z'], \n",
    "    frequency=100,\n",
    "    window_size=1750,\n",
    "\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    seconds_per_subject_train=60 * 10,\n",
    "    seconds_per_subject_test=60 * 10,\n",
    "    max_pairs_per_session=60 * 10,\n",
    "    model_variant=\"fcn\",\n",
    "    filters=[32, 64, 32],\n",
    "    optimizer=\"adam\",\n",
    "    optimizer_lr=0.00001,\n",
    "#     optimizer_lr=0.0001,\n",
    "#     optimizer_lr=0.001,\n",
    "#     optimizer_lr=0.01,\n",
    "    optimizer_decay=None,\n",
    "    batch_size=300,\n",
    "    margin=1,\n",
    "#     margin=.2,\n",
    "    epochs_best=40,\n",
    "    epochs_max=80,\n",
    "    ocsvm_nu=0.007,\n",
    "    ocsvm_gamma=0.002,\n",
    ")\n",
    "\n",
    "\n",
    "VALID_FCN_ROBUST_2000 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_2D,\n",
    "    name=\"VALID-FCN-ROBUST-FINAL\",\n",
    "    task_types=[1, 2],\n",
    "    feature_cols=[\"acc_x\", \"acc_y\", \"acc_z\", 'gyr_x', 'gyr_y', 'gyr_z'], \n",
    "    frequency=100,\n",
    "    window_size=2000,\n",
    "\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    seconds_per_subject_train=60 * 10,\n",
    "    seconds_per_subject_test=60 * 10,\n",
    "    max_pairs_per_session=60 * 10,\n",
    "    model_variant=\"fcn\",\n",
    "    filters=[32, 64, 32],\n",
    "    optimizer=\"adam\",\n",
    "    optimizer_lr=0.00001,\n",
    "#     optimizer_lr=0.0001,\n",
    "#     optimizer_lr=0.001,\n",
    "#     optimizer_lr=0.01,\n",
    "    optimizer_decay=None,\n",
    "    batch_size=300,\n",
    "    margin=1,\n",
    "#     margin=.2,\n",
    "    epochs_best=40,\n",
    "    epochs_max=80,\n",
    "    ocsvm_nu=0.007,\n",
    "    ocsvm_gamma=0.002,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6ea1959-5266-40c7-845d-45986195bf66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>VALID-FCN-ROBUST-FINAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frequency</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_cols</th>\n",
       "      <td>[acc_x, acc_y, acc_z, gyr_x, gyr_y, gyr_z]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_subjects</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_ids</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_sample_points_per_exp</th>\n",
       "      <td>21000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp_begin_cutoff_idx</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp_end_cutoff_idx</th>\n",
       "      <td>-500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_valid_train_subjects</th>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_valid_test_subjects</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_test_train_subjects</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_test_test_subjects</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seconds_per_subject_train</th>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seconds_per_subject_test</th>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task_types</th>\n",
       "      <td>[1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>window_size</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn_step_width</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocsvm_step_width</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler</th>\n",
       "      <td>RobustScaler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler_scope</th>\n",
       "      <td>subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler_global</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_pairs_per_session</th>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_variant</th>\n",
       "      <td>fcn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filters</th>\n",
       "      <td>[32, 64, 32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epochs_best</th>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epochs_max</th>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch_size</th>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimizer</th>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimizer_lr</th>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimizer_decay</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocsvm_kernel</th>\n",
       "      <td>rbf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocsvm_nu</th>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocsvm_gamma</th>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       Value\n",
       "name                                                  VALID-FCN-ROBUST-FINAL\n",
       "frequency                                                                100\n",
       "feature_cols                      [acc_x, acc_y, acc_z, gyr_x, gyr_y, gyr_z]\n",
       "max_subjects                                                              29\n",
       "user_ids                   [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 2...\n",
       "num_sample_points_per_exp                                              21000\n",
       "exp_begin_cutoff_idx                                                     500\n",
       "exp_end_cutoff_idx                                                      -500\n",
       "n_valid_train_subjects                                                    40\n",
       "n_valid_test_subjects                                                     10\n",
       "n_test_train_subjects                                                     10\n",
       "n_test_test_subjects                                                      30\n",
       "seconds_per_subject_train                                                600\n",
       "seconds_per_subject_test                                                 600\n",
       "task_types                                                            [1, 2]\n",
       "window_size                                                             1000\n",
       "nn_step_width                                                            500\n",
       "ocsvm_step_width                                                         500\n",
       "scaler                                                          RobustScaler\n",
       "scaler_scope                                                         subject\n",
       "scaler_global                                                          False\n",
       "max_pairs_per_session                                                    600\n",
       "margin                                                                     1\n",
       "model_variant                                                            fcn\n",
       "filters                                                         [32, 64, 32]\n",
       "epochs_best                                                               40\n",
       "epochs_max                                                                80\n",
       "batch_size                                                               300\n",
       "optimizer                                                               adam\n",
       "optimizer_lr                                                         0.00001\n",
       "optimizer_decay                                                         None\n",
       "ocsvm_kernel                                                             rbf\n",
       "ocsvm_nu                                                               0.007\n",
       "ocsvm_gamma                                                            0.002"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading exp1 data:\n",
      "1) accel_count: 28388, gyro_count: 31997\n",
      "2) accel_count: 26010, gyro_count: 28954\n",
      "3) accel_count: 28227, gyro_count: 31814\n",
      "4) accel_count: 24860, gyro_count: 26105\n",
      "5) accel_count: 24270, gyro_count: 24347\n",
      "6) accel_count: 25012, gyro_count: 25060\n",
      "7) accel_count: 25301, gyro_count: 25382\n",
      "8) accel_count: 21975, gyro_count: 21658\n",
      "19) accel_count: 24110, gyro_count: 25050\n",
      "21) accel_count: 24326, gyro_count: 23809\n",
      "22) accel_count: 29123, gyro_count: 28724\n",
      "26) accel_count: 23148, gyro_count: 24291\n",
      "27) accel_count: 24299, gyro_count: 23589\n",
      "28) accel_count: 23807, gyro_count: 24523\n",
      "29) accel_count: 24030, gyro_count: 23457\n",
      "35) accel_count: 24388, gyro_count: 23673\n",
      "36) accel_count: 24228, gyro_count: 24208\n",
      "37) accel_count: 31945, gyro_count: 31816\n",
      "38) accel_count: 22135, gyro_count: 22327\n",
      "39) accel_count: 23573, gyro_count: 23459\n",
      "40) accel_count: 23057, gyro_count: 24296\n",
      "41) accel_count: 24102, gyro_count: 23681\n",
      "42) accel_count: 24074, gyro_count: 24328\n",
      "43) accel_count: 22631, gyro_count: 23835\n",
      "44) accel_count: 24473, gyro_count: 23749\n",
      "45) accel_count: 23974, gyro_count: 23229\n",
      "46) accel_count: 23614, gyro_count: 23827\n",
      "48) accel_count: 22828, gyro_count: 23904\n",
      "49) accel_count: 24183, gyro_count: 24633\n",
      "Loading exp2 data:\n",
      "1) accel_count: 24049, gyro_count: 26943\n",
      "2) accel_count: 24468, gyro_count: 27667\n",
      "3) accel_count: 24611, gyro_count: 27000\n",
      "4) accel_count: 24972, gyro_count: 26798\n",
      "5) accel_count: 23573, gyro_count: 23372\n",
      "6) accel_count: 23800, gyro_count: 23890\n",
      "7) accel_count: 23347, gyro_count: 24145\n",
      "8) accel_count: 22947, gyro_count: 22660\n",
      "19) accel_count: 26156, gyro_count: 25815\n",
      "21) accel_count: 23566, gyro_count: 24408\n",
      "22) accel_count: 23844, gyro_count: 24589\n",
      "26) accel_count: 23179, gyro_count: 23925\n",
      "27) accel_count: 25109, gyro_count: 25820\n",
      "28) accel_count: 23133, gyro_count: 24028\n",
      "29) accel_count: 23180, gyro_count: 24314\n",
      "35) accel_count: 23299, gyro_count: 23854\n",
      "36) accel_count: 25497, gyro_count: 25059\n",
      "37) accel_count: 25994, gyro_count: 25232\n",
      "38) accel_count: 21164, gyro_count: 21182\n",
      "39) accel_count: 24214, gyro_count: 23585\n",
      "40) accel_count: 23944, gyro_count: 23170\n",
      "41) accel_count: 23193, gyro_count: 24111\n",
      "42) accel_count: 26505, gyro_count: 25697\n",
      "43) accel_count: 22690, gyro_count: 23981\n",
      "44) accel_count: 23002, gyro_count: 23829\n",
      "45) accel_count: 23978, gyro_count: 23350\n",
      "46) accel_count: 21128, gyro_count: 21848\n",
      "48) accel_count: 27996, gyro_count: 27205\n",
      "49) accel_count: 23061, gyro_count: 24129\n",
      "Loading exp1 data:\n",
      "47) accel_count: 22777, gyro_count: 22226\n",
      "Loading exp2 data:\n",
      "47) accel_count: 17718, gyro_count: 18353\n",
      "train_set: [7, 24, 8, 11, 13, 19, 28, 21, 26, 3, 20, 22, 6, 25, 16, 1, 17, 27, 5]\n",
      "test_set: [0, 12, 14, 9, 18, 23, 2, 15, 10, 4]\n"
     ]
    }
   ],
   "source": [
    "P = VALID_FCN_ROBUST_1000\n",
    "# utils_ppp(P)\n",
    "# P = VALID_ROBUST_1D\n",
    "# utils_ppp(P)\n",
    "# P = NAIVE_MINMAX_2D\n",
    "\n",
    "P.nn_step_width = int(P.window_size * .5)\n",
    "P.ocsvm_step_width = int(P.window_size * .5)\n",
    "P.scaler = 'RobustScaler'\n",
    "# P.scaler = 'MinMaxScaler'\n",
    "# P.scaler = 'Normalizer'\n",
    "# P.scaler = 'StandardScaler'\n",
    "\n",
    "utils_ppp(P)\n",
    "\n",
    "df_exps_dict = load_data_frames(P.user_ids, P.exp_begin_cutoff_idx, P.exp_end_cutoff_idx, P.num_sample_points_per_exp)\n",
    "dfList_exp1, dfList_exp2 = df_exps_dict['dfList_exp1'], df_exps_dict['dfList_exp2']\n",
    "\n",
    "raw_dfList_exp1 = dfList_exp1\n",
    "raw_dfList_exp2 = dfList_exp2\n",
    "\n",
    "# P.smoothing = \"FFT\"\n",
    "# P.cut_off_freq=25\n",
    "# ffted_dfList_exp1 = get_ffted_dfList(raw_dfList_exp1, cut_off_freq=P.cut_off_freq, filter_order=10)\n",
    "# ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=10)\n",
    "# raw_dfList_exp2[0]['EMA_x_a'][500:1000].plot()\n",
    "# ffted_dfList_exp2[0]['EMA_x_a'][500:1000].plot()\n",
    "# dfList_exp1 = ffted_dfList_exp1\n",
    "# dfList_exp2 = ffted_dfList_exp2\n",
    "\n",
    "# P.span=49\n",
    "# P.smoothing = \"FFT+EMA\"\n",
    "# # raw_dfList_exp1 = dfList_exp1\n",
    "# # raw_dfList_exp2 = dfList_exp2\n",
    "# P.cut_off_freq=42\n",
    "# ffted_dfList_exp1 = get_ffted_dfList(raw_dfList_exp1, cut_off_freq=P.cut_off_freq, filter_order=10)\n",
    "# ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=10)\n",
    "# ffted_dfList_exp2[0]['EMA_x_a'][500:1000].plot()\n",
    "# EMAed_dfList_exp1 = get_EMAed_dfList(ffted_dfList_exp1, span=P.span)\n",
    "# EMAed_dfList_exp2 = get_EMAed_dfList(ffted_dfList_exp2, span=P.span)\n",
    "# EMAed_dfList_exp2[0]['EMA_x_a'][500:1000].plot()\n",
    "# dfList_exp1 = EMAed_dfList_exp1\n",
    "# dfList_exp2 = EMAed_dfList_exp2\n",
    "\n",
    "num_sample_points_per_exp_user_47 = 18000\n",
    "df_exps_dict_user_47 = load_data_frames([47], P.exp_begin_cutoff_idx, P.exp_end_cutoff_idx, num_sample_points_per_exp_user_47)\n",
    "dfList_exp1_user_47, dfList_exp2_user_47 = df_exps_dict_user_47['dfList_exp1'], df_exps_dict_user_47['dfList_exp2']\n",
    "\n",
    "raw_dfList_exp1_user_47 = dfList_exp1_user_47\n",
    "raw_dfList_exp2_user_47 = dfList_exp2_user_47\n",
    "\n",
    "# ffted_dfList_exp1_user_47 = get_ffted_dfList(raw_dfList_exp1_user_47, cut_off_freq=P.cut_off_freq, filter_order=10)\n",
    "# ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=10)\n",
    "# ffted_dfList_exp2_user_47[0]['EMA_x_a'][500:1000].plot()\n",
    "# dfList_exp1_user_47 = ffted_dfList_exp1_user_47\n",
    "# dfList_exp2_user_47 = ffted_dfList_exp2_user_47\n",
    "\n",
    "\n",
    "# EMAed_dfList_exp1_user_47 = get_EMAed_dfList(ffted_dfList_exp1_user_47, span=P.span)\n",
    "# EMAed_dfList_exp2_user_47 = get_EMAed_dfList(ffted_dfList_exp2_user_47, span=P.span)\n",
    "# EMAed_dfList_exp2_user_47[0]['EMA_x_a'][500:1000].plot()\n",
    "# dfList_exp1_user_47 = EMAed_dfList_exp1_user_47\n",
    "# dfList_exp2_user_47 = EMAed_dfList_exp2_user_47\n",
    "\n",
    "\n",
    "randomized_data_idx = list(range(len(P.user_ids)))\n",
    "random.Random(SEED).shuffle(randomized_data_idx)\n",
    "split_idx = 2 * (len(randomized_data_idx)//3) + 1\n",
    "train_set = randomized_data_idx[: split_idx]\n",
    "test_set = randomized_data_idx[split_idx: ]\n",
    "print(f\"train_set: {train_set}\\ntest_set: {test_set}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c413872e-1513-474d-a050-8ed15d2127d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.model_variant = 'multi_head_fcn'\n",
    "P.tuning_metric=\"eer_val\"\n",
    "\n",
    "SCNN_1_5_conv_1_dense_arg_dict_default = {\n",
    "    \"input_dropout_streams\": [0.2],\n",
    "    \"filters_streams\": [[8, 16, 32, 64, 128]],\n",
    "    \"kernels_streams\": [[7, 7, 5, 5, 3]],\n",
    "    \"kernels_init_streams\": [[\"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\"]],\n",
    "    \"kernels_Max_Norm_constraint_streams\": [[3, 3, 3, 3, 3]],\n",
    "    \"conv_kernel_regularizer_streams\": [[\"l1\", \"l2\", \"l1\", \"l2\", \"l1\"]],\n",
    "    \"strides_streams\": [[1, 1, 1, 1, 1]],\n",
    "    \"paddings_streams\": [[\"causal\", \"causal\", \"causal\", \"causal\", \"causal\"]],\n",
    "    \"dropouts_streams\": [[0.1, 0.2, 0.3, 0.4, 0.5]],\n",
    "    \"activations_streams\": [['relu', 'relu', 'relu', 'relu', 'relu']],\n",
    "    \n",
    "    \"dense_layers\": [84],\n",
    "    \"dense_kernel_Max_Norm_constraints\":[3],\n",
    "    \"dense_kernel_regularizer\":[\"l1\"],\n",
    "    \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "    \"dense_dropouts\": [0.2], \n",
    "    \"dense_activations\": [\"sigmoid\"],\n",
    "\n",
    "    \n",
    "    \"loss_func_name\": \"k_contrastive_loss\",\n",
    "    \"contrastive_loss_margin\": 1.0,\n",
    "    \n",
    "    \"optimizer_name\": \"Adam\",\n",
    "    \"optimizer_lr\": 1e-3,\n",
    "    \"optimizer_decay\": None,\n",
    "    \n",
    "    \"batch_size\": 512,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "SCNN_3_1_conv_1_dense_arg_dict_default = {\n",
    "    \"input_dropout_streams\": [.2, .2, .2], #5**3\n",
    "    \"filters_streams\": [[64], [64], [64]], #6**3\n",
    "    \"kernels_streams\": [[7], [5], [3]], #4*3\n",
    "    \"kernels_init_streams\": [[\"glorot_uniform\"], [\"glorot_uniform\"], #8**3\n",
    "                             [\"glorot_uniform\"]],\n",
    "    \"kernels_Max_Norm_constraint_streams\": [[3], [3], [3]], #3**3\n",
    "    \"conv_kernel_regularizer_streams\": [[\"l1\"],[\"l2\"], [\"l1\"]],\n",
    "    \"strides_streams\": [[1], [1], [1]], #4**3\n",
    "    \"paddings_streams\": [[\"same\"], [\"same\"], [\"same\"]], #2*3\n",
    "    \"dropouts_streams\": [[.2], [.2], [.2]], #5**3\n",
    "    \"activations_streams\": [['relu'], ['relu'], ['relu']], # 8**3\n",
    "    \n",
    "    \"dense_layers\": [84],\n",
    "    \"dense_kernel_Max_Norm_constraints\":[3],\n",
    "    \"dense_kernel_regularizer\":[\"l1\"],\n",
    "    \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "    \"dense_dropouts\": [0], \n",
    "    \"dense_activations\": [\"sigmoid\"],\n",
    "    \n",
    "    \"loss_func_name\": \"k_contrastive_loss\",\n",
    "    \"contrastive_loss_margin\": 1.0,\n",
    "\n",
    "    \"optimizer_name\": \"Adam\",\n",
    "    \"optimizer_lr\": 1e-3,\n",
    "    \"optimizer_decay\": None,\n",
    "    \n",
    "    \"batch_size\": 512,\n",
    "    \n",
    "}\n",
    "\n",
    "SCNN_4_1_conv_1_dense_arg_dict_default = {\n",
    "    \"input_dropout_streams\": [.2, .2, .2, .2], #5**3\n",
    "    \"filters_streams\": [[64], [64], [64], [64]], #6**3\n",
    "    \"kernels_streams\": [[9], [7], [5], [3]], #4*3\n",
    "    \"kernels_init_streams\": [[\"glorot_uniform\"], [\"glorot_uniform\"], #8**3\n",
    "                             [\"glorot_uniform\"], [\"glorot_uniform\"]],\n",
    "    \"kernels_Max_Norm_constraint_streams\": [[3], [3], [3], [3]], #3**3\n",
    "    \"conv_kernel_regularizer_streams\": [[\"l1_l2\"],[\"l1\"], [\"l2\"], [\"l1\"]],\n",
    "    \"strides_streams\": [[1], [1], [1], [1]], #4**3\n",
    "    \"paddings_streams\": [[\"same\"], [\"same\"], [\"same\"], [\"same\"]], #2*3\n",
    "    \"dropouts_streams\": [[.2], [.2], [.2], [.2]], #5**3\n",
    "    \"activations_streams\": [['relu'], ['relu'], ['relu'], ['relu'], # 8**3\n",
    "    \n",
    "    \"dense_layers\": [84],\n",
    "    \"dense_kernel_Max_Norm_constraints\":[3],\n",
    "    \"dense_kernel_regularizer\":[\"l1\"],\n",
    "    \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "    \"dense_dropouts\": [0], \n",
    "    \"dense_activations\": [\"sigmoid\"],\n",
    "    \n",
    "    \"loss_func_name\": \"k_contrastive_loss\",\n",
    "    \"contrastive_loss_margin\": 1.0,\n",
    "\n",
    "    \"optimizer_name\": \"Adam\",\n",
    "    \"optimizer_lr\": 1e-3,\n",
    "    \"optimizer_decay\": None,\n",
    "    \n",
    "    \"batch_size\": 512,\n",
    "    \n",
    "}\n",
    "\n",
    "SCNN_5_1_conv_1_dense_arg_dict_default = {\n",
    "    \"input_dropout_streams\": [.2, .2, .2, .2, .2], #5**3\n",
    "    \"filters_streams\": [[64], [64], [64], [64], [64]], #6**3\n",
    "    \"kernels_streams\": [[11], [9], [7], [5], [3]], #4*3\n",
    "    \"kernels_init_streams\": [[\"glorot_uniform\"], [\"glorot_uniform\"], [\"glorot_uniform\"], #8**3\n",
    "                             [\"glorot_uniform\"], [\"glorot_uniform\"]],\n",
    "    \"kernels_Max_Norm_constraint_streams\": [[3], [3], [3], [3], [3]], #3**3\n",
    "    \"conv_kernel_regularizer_streams\": [[\"l1_l2\"], [\"l1_l2\"],[\"l1\"], [\"l2\"], [\"l1\"]],\n",
    "    \"strides_streams\": [[1], [1], [1], [1], [1]], #4**3\n",
    "    \"paddings_streams\": [[\"same\"], [\"same\"], [\"same\"], [\"same\"], [\"same\"]], #2*3\n",
    "    \"dropouts_streams\": [[.2], [.2], [.2], [.2], [.2]], #5**3\n",
    "    \"activations_streams\": [['relu'], ['relu'], ['relu'], ['relu'], ['relu']], # 8**3\n",
    "    \n",
    "    \"dense_layers\": [84],\n",
    "    \"dense_kernel_Max_Norm_constraints\":[3],\n",
    "    \"dense_kernel_regularizer\":[\"l1\"],\n",
    "    \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "    \"dense_dropouts\": [0], \n",
    "    \"dense_activations\": [\"sigmoid\"],\n",
    "    \n",
    "    \"loss_func_name\": \"k_contrastive_loss\",\n",
    "    \"contrastive_loss_margin\": 1.0,\n",
    "\n",
    "    \"optimizer_name\": \"Adam\",\n",
    "    \"optimizer_lr\": 1e-3,\n",
    "    \"optimizer_decay\": None,\n",
    "    \n",
    "    \"batch_size\": 512,\n",
    "    \n",
    "}\n",
    "\n",
    "SCNN_4_1234_conv_1_dense_arg_dict_default = {\n",
    "    \"input_dropout_streams\": [.2, .2, .2, .2], #5**3\n",
    "    \"filters_streams\": [[16], [16, 32], [16, 32, 64], [16, 32, 64, 128]], #6**3\n",
    "    \"kernels_streams\": [[3], [5, 3], [7, 5, 3], [9, 7, 5, 3]], #4*3\n",
    "    \"kernels_init_streams\": [[\"glorot_uniform\"], [\"glorot_uniform\", \"glorot_uniform\"], #8**3\n",
    "                             [\"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\"],\n",
    "                            [\"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\"]],\n",
    "    \"kernels_Max_Norm_constraint_streams\": [[3], [3, 3], [3, 3, 3], [3, 3, 3, 3]], #3**3\n",
    "    \"conv_kernel_regularizer_streams\": [[\"l1\"], [\"l2\",\"l1\"], [\"l1\",\"l2\", \"l1\"], [\"l2\", \"l1\", \"l2\", \"l1\"]],\n",
    "    \"strides_streams\": [[1], [1, 1], [1, 1, 1], [1, 1, 1, 1]], #4**3\n",
    "    \"paddings_streams\": [[\"causal\"], [\"causal\", \"causal\"], [\"causal\", \"causal\", \"causal\"], [\"causal\", \"causal\", \"causal\", \"causal\"]], #2*3\n",
    "    \"dropouts_streams\": [[.2], [.2, .3], [.2, .3, .4], [.2, .3, .4, .5]], #5**3\n",
    "    \"activations_streams\": [['relu'], ['relu', 'relu'], ['relu', 'relu', 'relu'], ['relu', 'relu', 'relu', 'relu']], # 8**3\n",
    "    \n",
    "    \"dense_layers\": [84],\n",
    "    \"dense_kernel_Max_Norm_constraints\":[3],\n",
    "    \"dense_kernel_regularizer\":[\"l1\"],\n",
    "    \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "    \"dense_dropouts\": [0], \n",
    "    \"dense_activations\": [\"sigmoid\"],\n",
    "    \n",
    "    \"loss_func_name\": \"k_contrastive_loss\",\n",
    "    \"contrastive_loss_margin\": 1.0,\n",
    "\n",
    "    \"optimizer_name\": \"Adam\",\n",
    "    \"optimizer_lr\": 1e-3,\n",
    "    \"optimizer_decay\": None,\n",
    "    \n",
    "    \"batch_size\": 512,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "SCNN_3_123_conv_1_dense_arg_dict_default = {\n",
    "    \"input_dropout_streams\": [.2, .2, .2], #5**3\n",
    "    \"filters_streams\": [[64], [64, 128], [64, 128, 256]], #6**3\n",
    "    \"kernels_streams\": [[3], [5, 3], [7, 5, 3]], #4*3\n",
    "    \"kernels_init_streams\": [[\"glorot_uniform\"], [\"glorot_uniform\", \"glorot_uniform\"], #8**3\n",
    "                             [\"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\"]],\n",
    "    \"kernels_Max_Norm_constraint_streams\": [[3], [3, 3], [3, 3, 3]], #3**3\n",
    "    \"conv_kernel_regularizer_streams\": [[\"l1\"], [\"l2\",\"l1\"], [\"l1\",\"l2\", \"l1\"]],\n",
    "    \"strides_streams\": [[1], [1, 1], [1, 1, 1]], #4**3\n",
    "    \"paddings_streams\": [[\"same\"], [\"same\", \"same\"], [\"same\", \"same\", \"same\"]], #2*3\n",
    "    \"dropouts_streams\": [[.4], [.3, .4], [.2, .3, .4]], #5**3\n",
    "    \"activations_streams\": [['relu'], ['relu', 'relu'], ['relu', 'relu', 'relu']], # 8**3\n",
    "    \n",
    "    \"dense_layers\": [84],\n",
    "    \"dense_kernel_Max_Norm_constraints\":[3],\n",
    "    \"dense_kernel_regularizer\":[\"l1\"],\n",
    "    \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "    \"dense_dropouts\": [0], \n",
    "    \"dense_activations\": [\"sigmoid\"],\n",
    "    \n",
    "    \"loss_func_name\": \"k_contrastive_loss\",\n",
    "    \"contrastive_loss_margin\": 1.0,\n",
    "\n",
    "    \"optimizer_name\": \"Adam\",\n",
    "    \"optimizer_lr\": 1e-3,\n",
    "    \"optimizer_decay\": None,\n",
    "    \n",
    "    \"batch_size\": 512,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "SCNN_1_3_conv_1_dense_arg_dict_default = {\n",
    "    \"input_dropout_streams\": [0.2],\n",
    "    \"filters_streams\": [[64, 128, 256]],\n",
    "    \"kernels_streams\": [[7, 5, 3]],\n",
    "    \"kernels_init_streams\": [[\"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\"]],\n",
    "    \"kernels_Max_Norm_constraint_streams\": [[3, 3, 3]],\n",
    "    \"conv_kernel_regularizer_streams\": [[\"l1\",\"l2\", \"l1\"]],\n",
    "    \"strides_streams\": [[1, 1, 1]],\n",
    "    \"paddings_streams\": [[\"same\", \"same\", \"same\"]],\n",
    "    \"dropouts_streams\": [[0.3, 0.4, 0.5]],\n",
    "    \"activations_streams\": [['relu', 'relu', 'relu']],\n",
    "    \n",
    "    \"dense_layers\": [84],\n",
    "    \"dense_kernel_Max_Norm_constraints\":[3],\n",
    "    \"dense_kernel_regularizer\":[\"l1\"],\n",
    "    \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "    \"dense_dropouts\": [0.2], \n",
    "    \"dense_activations\": [\"sigmoid\"],\n",
    "\n",
    "    \n",
    "    \"loss_func_name\": \"k_contrastive_loss\",\n",
    "    \"contrastive_loss_margin\": 1.0,\n",
    "    \n",
    "    \"optimizer_name\": \"Adam\",\n",
    "    \"optimizer_lr\": 1e-3,\n",
    "    \"optimizer_decay\": None,\n",
    "    \n",
    "    \"batch_size\": 512,\n",
    "\n",
    "}\n",
    "\n",
    "SCNN_1_2_conv_1_dense_arg_dict_default = {\n",
    "    \"input_dropout_streams\": [0.2],\n",
    "    \"filters_streams\": [[32, 64]],\n",
    "    \"kernels_streams\": [[5, 3]],\n",
    "    \"kernels_init_streams\": [[\"glorot_uniform\", \"glorot_uniform\"]],\n",
    "    \"kernels_Max_Norm_constraint_streams\": [[3, 3]],\n",
    "    \"conv_kernel_regularizer_streams\": [[\"l1\",\"l2\"]],\n",
    "    \"strides_streams\": [[1, 1]],\n",
    "    \"paddings_streams\": [[\"same\", \"same\"]],\n",
    "    \"dropouts_streams\": [[0.2, 0.3]],\n",
    "    \"activations_streams\": [['relu', 'relu']],\n",
    "    \n",
    "    \"dense_layers\": [84],\n",
    "    \"dense_kernel_Max_Norm_constraints\": [3],\n",
    "    \"dense_kernel_regularizer\":[\"l1\"],\n",
    "    \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "    \"dense_dropouts\": [0.2], \n",
    "    \"dense_activations\": [\"sigmoid\"],\n",
    "    \n",
    "    \"loss_func_name\": \"k_contrastive_loss\",\n",
    "    \"contrastive_loss_margin\": 1.0,\n",
    "    \n",
    "    \"optimizer_name\": \"Adam\",\n",
    "    \"optimizer_lr\": 1e-3,\n",
    "    \"optimizer_decay\": None,\n",
    "    \n",
    "    \"batch_size\": 512,\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "SCNN_1_1_conv_1_dense_arg_dict_default = {\n",
    "    \"input_dropout_streams\": [0.2],\n",
    "    \"filters_streams\": [[64]],\n",
    "    \"kernels_streams\": [[3]],\n",
    "    \"kernels_init_streams\": [[\"glorot_uniform\"]],\n",
    "    \"kernels_Max_Norm_constraint_streams\": [[3]],\n",
    "    \"strides_streams\": [[1]],\n",
    "    \"conv_kernel_regularizer_streams\": [[\"l1\"]],\n",
    "    \"paddings_streams\": [[\"same\"]],\n",
    "    \"dropouts_streams\": [[0.2]],\n",
    "    \"activations_streams\": [['relu']],\n",
    "    \n",
    "    \"dense_layers\": [84],\n",
    "    \"dense_kernel_Max_Norm_constraints\": [3],\n",
    "    \"dense_kernel_regularizer\":[\"l1\"],\n",
    "    \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "    \"dense_dropouts\": [0.2], \n",
    "    \"dense_activations\": [\"sigmoid\"],\n",
    "    \n",
    "    \"loss_func_name\": \"k_contrastive_loss\",\n",
    "    \"contrastive_loss_margin\": 1.0,\n",
    "    \n",
    "    \"optimizer_name\": \"Adam\",\n",
    "    \"optimizer_lr\": 1e-3,\n",
    "    \"optimizer_decay\": None,\n",
    "    \n",
    "    \"batch_size\": 512,\n",
    "\n",
    "}\n",
    "\n",
    "SCNN_1_2_conv_2_dense_arg_dict_default = {\n",
    "    \"input_dropout_streams\": [0.2],\n",
    "    \"filters_streams\": [[64, 128]],\n",
    "    \"kernels_streams\": [[5, 3]],\n",
    "    \"kernels_init_streams\": [[\"glorot_uniform\", \"glorot_uniform\"]],\n",
    "    \"kernels_Max_Norm_constraint_streams\": [[3, 3]],\n",
    "    \"conv_kernel_regularizer_streams\": [[\"l1\",\"l2\"]],\n",
    "    \"strides_streams\": [[1, 1]],\n",
    "    \"paddings_streams\": [[\"same\", \"same\"]],\n",
    "    \"dropouts_streams\": [[0.2, 0.3]],\n",
    "    \"activations_streams\": [['relu', 'relu']],\n",
    "    \n",
    "    \"dense_layers\": [32, 32],\n",
    "    \"dense_kernel_Max_Norm_constraints\": [3, 3],\n",
    "    \"dense_kernel_regularizer\":[\"l1\", \"l2\"],\n",
    "    \"dense_kernel_inits\": [\"glorot_uniform\", \"glorot_uniform\"],\n",
    "    \"dense_dropouts\": [0.2, 0.2], \n",
    "    \"dense_activations\": ['relu', \"sigmoid\"],\n",
    "    \n",
    "    \"loss_func_name\": \"k_contrastive_loss\",\n",
    "    \"contrastive_loss_margin\": 1.0,\n",
    "    \n",
    "    \"optimizer_name\": \"Adam\",\n",
    "    \"optimizer_lr\": 1e-3,\n",
    "    \"optimizer_decay\": None,\n",
    "    \n",
    "    \"batch_size\": 512,\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "pool_dict= {\n",
    "    \"num_filters_pool\": [8, 16, 32, 64, 128, 256],\n",
    "    \"kernel_size_pool\": [3, 5, 7, 9],\n",
    "    \"kernel_init_pool\": ['truncated_normal', 'orthogonal', 'uniform', 'lecun_normal', 'lecun_uniform', 'normal', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform'],\n",
    "    \"stride_pool\": [1 ,2 ,3 , 4],\n",
    "    \"padding_pool\": [\"valid\", \"same\", \"causal\"], # \"causal\" results in causal (dilated) convolutions, e.g. output[t] does not depend on input[t+1:]. \n",
    "    # Useful when modeling temporal data where the model should not violate the temporal order. See WaveNet: A Generative Model for Raw Audio, section 2.1.\n",
    "    \"dropout_pool\": [0, .1, .2, .3, .4, .5],\n",
    "    \"activation_pool\": ['softplus', 'softsign', 'relu', 'elu', 'gelu', 'selu', 'swish', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear', \"LeakyReLU\"],\n",
    "    \"num_dense_neuron_pool\": [8, 16, 32, 64, 84, 128, 256],\n",
    "    \"kernel_Max_Norm_constraints_pool\": [3, 4, 5],\n",
    "    \"kernel_regularizer_pool\": [\"l1\", \"l2\", \"l1_l2\"],\n",
    "    \n",
    "    \"optimizer_name_pool\": [\"Adam\", \"Nadam\" , \"RMSprop\", \"Adadelta\", \"Adagrad\", \"Adamax\"],#SGD\n",
    "    \"optimizer_lr_pool\": [1e-5, 5e-5 ,1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1],#1e-5, 5e-5, [1e-4, 2e-4, 3e-4, 4e-4, 5e-4], #\n",
    "    \"batch_size_pool\": [32, 64, 128, 256, 512, 1024],\n",
    "    \"contrastive_loss_margin_pool\": [.5, .6, .7, .8, .9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
    "    \n",
    "}\n",
    "# try model hyperparameter optimization using the updated method (reduceLROnPlateau) instead of early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70fca4d6-48f9-486a-8d54-8f8e02d6dc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_arch_dict = {\n",
    "                \"SCNN_1_3_conv_1_dense_arg_dict_default\": SCNN_1_3_conv_1_dense_arg_dict_default,\n",
    "                \"SCNN_3_123_conv_1_dense_arg_dict_default\": SCNN_3_123_conv_1_dense_arg_dict_default,\n",
    "                \"SCNN_3_1_conv_1_dense_arg_dict_default\": SCNN_3_1_conv_1_dense_arg_dict_default,\n",
    "                \"SCNN_1_2_conv_1_dense_arg_dict_default\": SCNN_1_2_conv_1_dense_arg_dict_default,\n",
    "                \"SCNN_1_1_conv_1_dense_arg_dict_default\": SCNN_1_1_conv_1_dense_arg_dict_default,\n",
    "                \"SCNN_1_2_conv_2_dense_arg_dict_default\": SCNN_1_2_conv_2_dense_arg_dict_default,\n",
    "                \"SCNN_1_5_conv_1_dense_arg_dict_default\": SCNN_1_5_conv_1_dense_arg_dict_default,\n",
    "                \"SCNN_4_1234_conv_1_dense_arg_dict_default\": SCNN_4_1234_conv_1_dense_arg_dict_default,\n",
    "}\n",
    "# cnn_arch_dict = {\n",
    "                # \"SCNN_simple_1_5_conv_1_dense_arg_dict_default\": SCNN_simple_1_5_conv_1_dense_arg_dict_default,\n",
    "                 # \"SCNN_1_5_conv_1_dense_arg_dict_default\": SCNN_1_5_conv_1_dense_arg_dict_default,\n",
    "                # \"SCNN_4_1234_conv_1_dense_arg_dict_default\": SCNN_4_1234_conv_1_dense_arg_dict_default}\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4112fe1-e78f-4aca-9a9a-319c2887cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_config_dict(desc=''):\n",
    "    dir_name = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    save_dir = f\"siamese_cnn_results_final\"\n",
    "    with open(f\"{save_dir}/results_dict_{desc}{dir_name}.json\", 'w') as file:\n",
    "            results_dict_json = json.dumps(results_dict)\n",
    "            file.write(results_dict_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74d74b47-bdd4-4776-804a-97a13b319048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)\n",
    "\n",
    "direct=\"results_dict_1750-SCNN_1_5_conv_1_dense_arg_dict_default20230321-201551\"\n",
    "with open(f\"siamese_cnn_results_final/{direct}.json\", 'r') as file:\n",
    "    results_dict=json.load(file)\n",
    "    \n",
    "def save_training_config_dict(desc=''):\n",
    "    dir_name = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    save_dir = f\"siamese_cnn_results_final\"\n",
    "    with open(f\"{save_dir}/results_dict_{desc}{dir_name}.json\", 'w') as file:\n",
    "            results_dict_json = json.dumps(results_dict)\n",
    "            file.write(results_dict_json)\n",
    "           \n",
    "P.smoothing=None\n",
    "P.cut_off_freq=None\n",
    "report_dict={    \n",
    "            # \"Training_samples\": str(np.unique(y_train, return_counts=True)),\n",
    "            # \"Validation_samples\": str(np.unique(y_valid, return_counts=True)),\n",
    "            \"smoothing\": P.smoothing,\n",
    "            # \"EMA_span\": P.span if \"EMA\" in P.smoothing else None ,\n",
    "            \"Butter_cut_off_freq\": P.cut_off_freq,\n",
    "            \"scaler\": P.scaler,\n",
    "    }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bb1ba23-3f56-4f35-9b71-447072e1f0c3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed was set to: 567\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 20\n",
      "len_exp2_user_47: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 13/13 [00:06<00:00,  1.98it/s]\n",
      "100%|| 7/7 [00:01<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:   10348, shape: (10348, 2000, 6), class balance: (array([0., 1.], dtype=float32), array([5148, 5200]))\n",
      "Validation samples: 5614, shape: (5614, 2000, 6), class balance: (array([0., 1.], dtype=float32), array([2814, 2800]))\n",
      "SCNN_1_3_conv_1_dense_arg_dict_default\n",
      "Using Model variant multi_head_fcn...\n",
      "0.0001\n",
      "{'input_dropout_streams': [0.2], 'filters_streams': [[64, 128, 256]], 'kernels_streams': [[7, 5, 3]], 'kernels_init_streams': [['glorot_uniform', 'glorot_uniform', 'glorot_uniform']], 'kernels_Max_Norm_constraint_streams': [[3, 3, 3]], 'conv_kernel_regularizer_streams': [['l1', 'l2', 'l1']], 'strides_streams': [[1, 1, 1]], 'paddings_streams': [['same', 'same', 'same']], 'dropouts_streams': [[0.3, 0.4, 0.5]], 'activations_streams': [['relu', 'relu', 'relu']], 'dense_layers': [84], 'dense_kernel_Max_Norm_constraints': [3], 'dense_kernel_regularizer': ['l1'], 'dense_kernel_inits': ['glorot_uniform'], 'dense_dropouts': [0.2], 'dense_activations': ['sigmoid'], 'loss_func_name': 'k_contrastive_loss', 'contrastive_loss_margin': 1.0, 'optimizer_name': 'Adam', 'optimizer_lr': 0.0001, 'optimizer_decay': None, 'batch_size': 64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 03:50:18.510831: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-31 03:50:19.570378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43490 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:65:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================[ Initial State ]================================"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 03:50:23.494250: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2023-05-31 03:50:24.972793: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8101\n",
      "2023-05-31 03:50:26.546328: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-05-31 03:50:26.547210: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-05-31 03:50:26.547228: W tensorflow/stream_executor/gpu/asm_compiler.cc:77] Couldn't get ptxas version string: Internal: Couldn't invoke ptxas --version\n",
      "2023-05-31 03:50:26.548727: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-05-31 03:50:26.548789: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2023-05-31 03:50:27.711522: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-05-31 03:50:27.711565: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN: roc_auc: 0.8373, eer: 0.2631, thres: 0.0494 => acc: 0.7370, f1: 0.7379\n",
      "\n",
      "\n",
      "VALID: roc_auc: 0.8855, eer: 0.1961, thres: 0.0661 => acc: 0.8041, f1: 0.8036\n",
      "\n",
      "Epoch 1/30\n",
      "162/162 [==============================] - 16s 84ms/step - loss: 45.6038 - val_loss: 35.9126\n",
      "================================[   Epoch 0   ]================================\n",
      "TRAIN: roc_auc: 0.8562, eer: 0.2315, thres: 0.2782 => acc: 0.7684, f1: 0.7692\n",
      "loss: 45.604, val_loss: 35.913\n",
      "\n",
      "VALID: roc_auc: 0.9032, eer: 0.1761, thres: 0.3751 => acc: 0.8240, f1: 0.8236\n",
      "loss: 45.604, val_loss: 35.913\n",
      "Epoch 2/30\n",
      "162/162 [==============================] - 13s 78ms/step - loss: 30.2301 - val_loss: 22.7552\n",
      "================================[   Epoch 1   ]================================\n",
      "TRAIN: roc_auc: 0.8274, eer: 0.2660, thres: 0.2152 => acc: 0.7342, f1: 0.7351\n",
      "loss: 30.230, val_loss: 22.755\n",
      "\n",
      "VALID: roc_auc: 0.8891, eer: 0.2058, thres: 0.2552 => acc: 0.7941, f1: 0.7936\n",
      "loss: 30.230, val_loss: 22.755\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    144\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model_func(arg_dict)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mMetricsCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_evaluate_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mloss_record_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_record_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_record_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_record_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m                               \u001b[49m\u001b[43msave_plots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_interm_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stoping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mReduceLROnPlateau_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mReduceLROnPlateau_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_epoch_log_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_epoch_log_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;66;43;03m#, reduce_lr],\u001b[39;49;00m\n\u001b[1;32m    159\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss_record_dict)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining History:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1230\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1227\u001b[0m   val_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m   1228\u001b[0m   epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n\u001b[0;32m-> 1230\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_logs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1231\u001b[0m training_logs \u001b[38;5;241m=\u001b[39m epoch_logs\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/callbacks.py:413\u001b[0m, in \u001b[0;36mCallbackList.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    411\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_logs(logs)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 413\u001b[0m   \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/ipykernel_74/772562099.py:555\u001b[0m, in \u001b[0;36mMetricsCallback.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_record_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(logs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y, desc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msets:\n\u001b[0;32m--> 555\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_metric_record_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# print(logs)\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mReduceLROnPlateau_args \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/tmp/ipykernel_74/772562099.py:581\u001b[0m, in \u001b[0;36mMetricsCallback.evaluate\u001b[0;34m(self, X, y, logs, desc, epoch, print_metric, save_metric_record_dict)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, logs, desc, epoch, print_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_metric_record_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[0;32m--> 581\u001b[0m     y_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m     y_score_neg \u001b[38;5;241m=\u001b[39m y_score \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# lower distance means closer to positive class\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;66;03m# Calc Metrics\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1751\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m   1750\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m-> 1751\u001b[0m   tmp_batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1753\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    882\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 885\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    888\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:924\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    922\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 924\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateful_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    926\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    927\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3036\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   3037\u001b[0m   (graph_function,\n\u001b[1;32m   3038\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1961\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1962\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1963\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1964\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1965\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m     args,\n\u001b[1;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1968\u001b[0m     executing_eagerly)\n\u001b[1;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    590\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 591\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    598\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    599\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    600\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    604\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# results_dict={}\n",
    "for window_size in [2000]: #125, 250, 500, 750, 1000, 1250, 1500, 1750, \n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    print(f\"Seed was set to: {SEED}\")\n",
    "    \n",
    "    if str(window_size) not in results_dict:\n",
    "        results_dict[str(window_size)]={}\n",
    "        \n",
    "    P.window_size=window_size\n",
    "    P.nn_step_width = int(P.window_size * .5)\n",
    "    P.ocsvm_step_width = int(P.window_size * .5)\n",
    "    P.scaler = 'RobustScaler'\n",
    "    \n",
    "\n",
    "    P.SMA_per_win_winsize=None\n",
    "    P.EMA_per_win_span=None\n",
    "    P.Butter_per_win_argdict=None\n",
    "    P.cut_off_freq=None\n",
    "    P.span=None\n",
    "\n",
    "    filter_order=10\n",
    "    sampling_freq=100\n",
    "    filtfilt=1\n",
    "    \n",
    "    # preparing train data\n",
    "    X_train_exp1_dict, X_train_exp2_dict, fitted_scaler_train_exp2_dict=get_raw_windows(dfList_exp1, dfList_exp2, window_size, step_width=P.nn_step_width, user_idx_set=train_set, scaler=P.scaler, \n",
    "                                                                                        num_sample_points_per_exp=P.num_sample_points_per_exp, \n",
    "                                                                                        EMA_per_win_span=P.EMA_per_win_span, \n",
    "                                                                                        SMA_per_win_winsize=P.SMA_per_win_winsize,\n",
    "                                                                                        Butter_per_win_argdict=P.Butter_per_win_argdict, \n",
    "                                                                                        verbose=0)\n",
    "\n",
    "    X_train_exp1_dict_user_47, X_train_exp2_dict_user_47, fitted_scaler_train_exp2_dict_user_47=get_raw_windows_user_47(dfList_exp1_user_47, dfList_exp2_user_47, \n",
    "                                                                                                                        window_size, step_width=P.nn_step_width, scaler=P.scaler, \n",
    "                                                                                                                        num_sample_points_per_exp=P.num_sample_points_per_exp, \n",
    "                                                                                                                        EMA_per_win_span=P.EMA_per_win_span, \n",
    "                                                                                                                        SMA_per_win_winsize=P.SMA_per_win_winsize,\n",
    "                                                                                                                        Butter_per_win_argdict=P.Butter_per_win_argdict, \n",
    "                                                                                                                        verbose=0)\n",
    "\n",
    "    X_train_exp1_dict, X_train_exp2_dict, fitted_scaler_train_exp2_dict=append_user_47_to_data(X_train_exp1_dict, X_train_exp2_dict, fitted_scaler_train_exp2_dict, P.user_ids, \n",
    "                                                                                               X_train_exp1_dict_user_47, X_train_exp2_dict_user_47, fitted_scaler_train_exp2_dict_user_47, \n",
    "                                                                                               verbose=0)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    spliter = 2*len(X_train_exp2_dict)//3\n",
    "    cnn_train_exp2 = {key: X_train_exp2_dict[key] for key in list(X_train_exp2_dict.keys())[:spliter]}\n",
    "    cnn_train_exp1 = {key: X_train_exp1_dict[key] for key in list(X_train_exp1_dict.keys())[:spliter]}\n",
    "    cnn_valid_exp2 = {key: X_train_exp2_dict[key] for key in list(X_train_exp2_dict.keys())[spliter:]}\n",
    "    cnn_valid_exp1 = {key: X_train_exp1_dict[key] for key in list(X_train_exp1_dict.keys())[spliter:]}\n",
    "\n",
    "\n",
    "    # why dont i get equal neg and pos pairs???\n",
    "    # num_pair_limit_train_2000 = 10348\n",
    "    # num_pair_limit_valid_2000 = 5614\n",
    "    # num_pair_limit_train_125 = 8*num_pair_limit_train_2000\n",
    "    # num_pair_limit_valid_125 = 8*num_pair_limit_valid_2000\n",
    "    num_samples=28000\n",
    "\n",
    "    train_pairs_dict = prep_X_y_pair(cnn_train_exp2, cnn_train_exp1, list(cnn_train_exp2.keys()), fitted_scaler_train_exp2_dict, num_pair_limit=(2*num_samples)//3)\n",
    "    X_train, y_train, X_train_distro_dic = train_pairs_dict[\"X\"], train_pairs_dict[\"y\"], train_pairs_dict[\"X_dic\"]\n",
    "\n",
    "    valid_pairs_dict = prep_X_y_pair(cnn_valid_exp2, cnn_valid_exp1, list(cnn_valid_exp2.keys()), fitted_scaler_train_exp2_dict, num_pair_limit=num_samples//3)\n",
    "    X_valid, y_valid, X_valid_distro_dic = valid_pairs_dict[\"X\"], valid_pairs_dict[\"y\"], valid_pairs_dict[\"X_dic\"]\n",
    "\n",
    "    # X_final_train, y_train = prep_X_y_pair(X_train_exp2_dict, X_train_exp1_dict, list(X_train_exp2_dict.keys()), fitted_scaler_train_exp2_dict, num_pair_limit=num_pair_limit_train_2000)\n",
    "\n",
    "    # 2D Filter Model needs flat 4th dimension\n",
    "    if P.model_variant == \"2d\":\n",
    "        X_train[0] = X_train[0].reshape((*X_train[0].shape, 1))\n",
    "        X_train[1] = X_train[1].reshape((*X_train[1].shape, 1))\n",
    "        X_valid[0] = X_valid[0].reshape((*X_valid[0].shape, 1))\n",
    "        X_valid[1] = X_valid[1].reshape((*X_valid[1].shape, 1))\n",
    "\n",
    "    print(\n",
    "        f\"Training samples:   {y_train.shape[0]}, shape: {X_train[0].shape},\"\n",
    "        + f\" class balance: {np.unique(y_train, return_counts=True)}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Validation samples: {y_valid.shape[0]}, shape: {X_valid[0].shape},\"\n",
    "        + f\" class balance: {np.unique(y_valid, return_counts=True)}\"\n",
    "    )\n",
    "    data_dict = {}\n",
    "    data_dict[\"X_valid\"], data_dict[\"y_valid\"], data_dict[\"X_train\"], data_dict[\"y_train\"] = X_valid, y_valid, X_train, y_train\n",
    "\n",
    "\n",
    "    X_left = np.concatenate([X_train[0], X_valid[0]])\n",
    "    X_right = np.concatenate([X_train[1], X_valid[1]])\n",
    "    y_train_valid = np.concatenate([y_train, y_valid])\n",
    "\n",
    "    X_left, X_right, y_train_valid = sklearn_shuffle(X_left, X_right, y_train_valid, random_state=SEED)\n",
    "\n",
    "    X_train_valid = [X_left, X_right]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    report_dict={    \n",
    "        \"Training_samples\": str(np.unique(y_train, return_counts=True)),\n",
    "        \"Validation_samples\": str(np.unique(y_valid, return_counts=True)),\n",
    "        \"smoothing\": P.smoothing if P.smoothing!=None else None,\n",
    "        # \"EMA_span\": P.span if \"EMA\" in P.smoothing else None ,\n",
    "        \"Butter_cut_off_freq\": P.cut_off_freq if P.cut_off_freq!=None else P.cut_off_freq,\n",
    "        \"scaler\": P.scaler,\n",
    "    }\n",
    "    \n",
    "    for arg_dict_name in cnn_arch_dict:\n",
    "        if arg_dict_name not in results_dict[str(window_size)]:\n",
    "            np.random.seed(SEED)\n",
    "            tf.random.set_seed(SEED)\n",
    "            print(arg_dict_name)\n",
    "            arg_dict=cnn_arch_dict[arg_dict_name]\n",
    "\n",
    "            \n",
    "            create_model_func = get_create_model_func(P.model_variant, P.window_size, P.feature_cols)\n",
    "\n",
    "            arg_dict[\"optimizer_lr\"]=0.001\n",
    "            \n",
    "            if 1250>window_size>750:\n",
    "                arg_dict[\"batch_size\"]=256\n",
    "                arg_dict[\"optimizer_lr\"]=0.001\n",
    "\n",
    "            elif  1500>=window_size>=1250:\n",
    "                arg_dict[\"batch_size\"]=128\n",
    "                arg_dict[\"optimizer_lr\"]=0.001\n",
    "                \n",
    "            elif window_size>1500:\n",
    "                arg_dict[\"batch_size\"]=64\n",
    "                arg_dict[\"optimizer_lr\"]=0.0001\n",
    "\n",
    "                \n",
    "            # arg_dict[\"optimizer_lr\"]=0.0001\n",
    "\n",
    "            lr_epoch_log_dict={}\n",
    "            ReduceLROnPlateau_args={'mointored_metric': \"val_loss\", \"factor\": 0.2, \"patience\": 3, \"verbose\": 1, \"min_lr\": 1e-6}#change val_loss to eer_val in next iter\n",
    "            print(arg_dict[\"optimizer_lr\"])\n",
    "            print(arg_dict)\n",
    "            loss_record_dict = {'loss': [], 'val_loss': []}\n",
    "            metric_record_dict = {}\n",
    "            model = create_model_func(arg_dict)\n",
    "\n",
    "            # Train\n",
    "            history = model.fit(\n",
    "                x=X_train,\n",
    "                y=y_train,\n",
    "                batch_size=arg_dict[\"batch_size\"],\n",
    "                epochs=30,\n",
    "                verbose=1,\n",
    "                validation_data=(X_valid, y_valid),\n",
    "                shuffle=True,\n",
    "                callbacks=[MetricsCallback((X_valid, y_valid, X_train, y_train), epoch_evaluate_freq=1, \n",
    "                                           loss_record_dict=loss_record_dict, metric_record_dict=metric_record_dict, \n",
    "                                           save_plots=True, print_interm_epochs=True, early_stoping=False,\n",
    "                                          ReduceLROnPlateau_args=ReduceLROnPlateau_args, lr_epoch_log_dict=lr_epoch_log_dict)]#, reduce_lr],\n",
    "            )\n",
    "            print(loss_record_dict)\n",
    "            print(\"Training History:\")\n",
    "            loss_fig = utils_plot_training_loss(loss_record_dict)\n",
    "\n",
    "            results_dict[str(window_size)][arg_dict_name]={\n",
    "                                                    \"lr_epoch_log_dict\": str(lr_epoch_log_dict),\n",
    "                                                    \"loss_record_dict\": loss_record_dict,\n",
    "                                                    \"metric_record_dict\": metric_record_dict,\n",
    "                                                    \"report_dict\": report_dict,\n",
    "                                                    \"ReduceLROnPlateau_args\": ReduceLROnPlateau_args,\n",
    "                                                    \"arg_dict\": arg_dict,\n",
    "            }\n",
    "\n",
    "            save_training_config_dict(f\"{window_size}-{arg_dict_name}\")\n",
    "            del model\n",
    "            del history\n",
    "            K.clear_session()\n",
    "            tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88b23aff-c731-49f0-8419-f72e253332ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d27b355-ca0a-44e6-9487-e2a4fe6af396",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2039210648.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [11]\u001b[0;36m\u001b[0m\n\u001b[0;31m    remove butter, i dont need we need it\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# run with hyperparams you found and compare against default\n",
    "# in the next run, switch from val_loss to eer_val\n",
    "remove butter, i dont need we need it\n",
    "maybe increase the cuttoff for loading dataframse to 5 magnitude\n",
    "# run on gyr, accel alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c61d5f18-86eb-4fe7-86b5-54d8a1ff3c31",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m history\n\u001b[1;32m      3\u001b[0m K\u001b[38;5;241m.\u001b[39mclear_session()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "del model\n",
    "del history\n",
    "K.clear_session()\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5abc7d-bada-4112-82ff-88e8aedb631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "results_dict[str(window_size)].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e38ce9-2919-45a7-a151-6237028e534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"siamese_cnn_results_final/results_dict_1250-SCNN_1_1_conv_1_dense_arg_dict_default20230315-091256.json\", 'r') as file:\n",
    "#     results_dict=json.load(file)\n",
    "def save_training_config_dict(desc=''):\n",
    "    dir_name = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    save_dir = f\"siamese_cnn_results_final\"\n",
    "    with open(f\"{save_dir}/results_dict_{desc}{dir_name}.json\", 'w') as file:\n",
    "            results_dict_json = json.dumps(results_dict)\n",
    "            file.write(results_dict_json)\n",
    "            \n",
    "            \n",
    "# save_training_config_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f3e236-6316-49bd-b223-b2ef9cf2e435",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pool = np.logspace(-5, -2, num=10)\n",
    "lr_pool=[1e-5]#, 2e-5]\n",
    "lr_epoch_log_dict={}\n",
    "ReduceLROnPlateau_err_mode={}\n",
    "ReduceLROnPlateau_args={'mointored_metric': \"val_loss\", \"factor\": 0.2, \"patience\": 0, \"verbose\": 1, \"min_lr\": 1e-8}\n",
    "ReduceLROnPlateau_args=ReduceLROnPlateau_args\n",
    "for lr in lr_pool:\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    arg_dict['optimizer_lr'] = lr\n",
    "    print(lr)\n",
    "    print(arg_dict)\n",
    "    loss_record_dict = {'loss': [], 'val_loss': []}\n",
    "    metric_record_dict = {}\n",
    "    model = create_model_func(arg_dict)\n",
    "\n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "        batch_size=arg_dict[\"batch_size\"],\n",
    "        epochs=200,\n",
    "        verbose=1,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        shuffle=True,\n",
    "        callbacks=[MetricsCallback((X_valid, y_valid, X_train, y_train), epoch_evaluate_freq=1, \n",
    "                                   loss_record_dict=loss_record_dict, metric_record_dict=metric_record_dict, \n",
    "                                   save_plots=True, print_interm_epochs=True, early_stoping=False,\n",
    "                                  ReduceLROnPlateau_args=ReduceLROnPlateau_args, lr_epoch_log_dict=lr_epoch_log_dict)]#, reduce_lr],\n",
    "    )\n",
    "    print(loss_record_dict)\n",
    "    print(\"Training History:\")\n",
    "    loss_fig = utils_plot_training_loss(loss_record_dict)\n",
    "    \n",
    "min_val = min(metric_record_dict['eer_val'][\"Valid\"])\n",
    "min_val_index=metric_record_dict['eer_val'][\"Valid\"].index(min_val)\n",
    "print(metric_record_dict['eer_val'][\"Valid\"])\n",
    "min_val_index\n",
    "\n",
    "optimal_lr_epoch_dict={}\n",
    "for i in range(min_val_index+1):\n",
    "    optimal_lr_epoch_dict[i] = lr_epoch_log_dict[i]\n",
    "    \n",
    "optimal_lr_epoch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07607dff-e682-4c75-9151-7ab6d063a71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4814207-6f7c-4a76-8596-dcb7b740d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCNN_1_5_conv_1_dense_arg_dict_default = {\n",
    "    \"input_dropout_streams\": [0.2],\n",
    "    \"filters_streams\": [[256, 32, 128, 32, 64]],\n",
    "    \"kernels_streams\": [[7, 3, 5, 3, 3]],\n",
    "    \"kernels_init_streams\": [[\"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\"]],\n",
    "    \"kernels_Max_Norm_constraint_streams\": [[3, 3, 3, 3, 3]],\n",
    "    \"conv_kernel_regularizer_streams\": [[\"l1\", \"l2\", \"l1\", \"l2\", \"l1\"]],\n",
    "    \"strides_streams\": [[1, 1, 1, 1, 1]],\n",
    "    \"paddings_streams\": [[\"causal\", \"causal\", \"causal\", \"causal\", \"causal\"]],\n",
    "    \"dropouts_streams\": [[0.5, 0.4, 0.3, 0.4, 0.5]],\n",
    "    \"activations_streams\": [['relu', 'relu', 'relu', 'relu', 'relu']],\n",
    "    \n",
    "    \"dense_layers\": [84],\n",
    "    \"dense_kernel_Max_Norm_constraints\":[3],\n",
    "    \"dense_kernel_regularizer\":[\"l1\"],\n",
    "    \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "    \"dense_dropouts\": [0.2], \n",
    "    \"dense_activations\": [\"sigmoid\"],\n",
    "\n",
    "    \n",
    "    \"loss_func_name\": \"k_contrastive_loss\",\n",
    "    \"contrastive_loss_margin\": 1.0,\n",
    "    \n",
    "    \"optimizer_name\": \"Adam\",\n",
    "    \"optimizer_lr\": 1e-2,\n",
    "    \"optimizer_decay\": None,\n",
    "    \n",
    "    \"batch_size\": 512,\n",
    "\n",
    "}\n",
    "\n",
    "SCNN_simple_1_5_conv_1_dense_arg_dict_default = {\n",
    "    \"input_dropout_streams\": [0.2],\n",
    "    \"filters_streams\": [[64, 16, 32, 8, 16]],\n",
    "    \"kernels_streams\": [[7, 3, 5, 3, 3]],\n",
    "    \"kernels_init_streams\": [[\"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\"]],\n",
    "    \"kernels_Max_Norm_constraint_streams\": [[3, 3, 3, 3, 3]],\n",
    "    \"conv_kernel_regularizer_streams\": [[\"l1\", \"l2\", \"l1\", \"l2\", \"l1\"]],\n",
    "    \"strides_streams\": [[1, 1, 1, 1, 1]],\n",
    "    \"paddings_streams\": [[\"causal\", \"causal\", \"causal\", \"causal\", \"causal\"]],\n",
    "    \"dropouts_streams\": [[0.1, 0.1, 0.1, 0.1, 0.1]],\n",
    "    \"activations_streams\": [['relu', 'relu', 'relu', 'relu', 'relu']],\n",
    "    \n",
    "    \"dense_layers\": [84],\n",
    "    \"dense_kernel_Max_Norm_constraints\":[3],\n",
    "    \"dense_kernel_regularizer\":[\"l1\"],\n",
    "    \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "    \"dense_dropouts\": [0.2], \n",
    "    \"dense_activations\": [\"sigmoid\"],\n",
    "\n",
    "    \n",
    "    \"loss_func_name\": \"k_contrastive_loss\",\n",
    "    \"contrastive_loss_margin\": 1.0,\n",
    "    \n",
    "    \"optimizer_name\": \"Adam\",\n",
    "    \"optimizer_lr\": 1e-2,\n",
    "    \"optimizer_decay\": None,\n",
    "    \n",
    "    \"batch_size\": 512,\n",
    "\n",
    "}\n",
    "\n",
    "SCNN_3_1_conv_1_dense_arg_dict_default = {\n",
    "    \"input_dropout_streams\": [.2, .2, .2], #5**3\n",
    "    \"filters_streams\": [[64], [64], [64]], #6**3\n",
    "    \"kernels_streams\": [[7], [5], [3]], #4*3\n",
    "    \"kernels_init_streams\": [[\"glorot_uniform\"], [\"glorot_uniform\"], #8**3\n",
    "                             [\"glorot_uniform\"]],\n",
    "    \"kernels_Max_Norm_constraint_streams\": [[3], [3], [3]], #3**3\n",
    "    \"conv_kernel_regularizer_streams\": [[\"l1\"],[\"l2\"], [\"l1\"]],\n",
    "    \"strides_streams\": [[1], [1], [1]], #4**3\n",
    "    \"paddings_streams\": [[\"same\"], [\"same\"], [\"same\"]], #2*3\n",
    "    \"dropouts_streams\": [[.2], [.2], [.2]], #5**3\n",
    "    \"activations_streams\": [['relu'], ['relu'], ['relu']], # 8**3\n",
    "    \n",
    "    \"dense_layers\": [84],\n",
    "    \"dense_kernel_Max_Norm_constraints\":[3],\n",
    "    \"dense_kernel_regularizer\":[\"l1\"],\n",
    "    \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "    \"dense_dropouts\": [0], \n",
    "    \"dense_activations\": [\"sigmoid\"],\n",
    "    \n",
    "    \"loss_func_name\": \"k_contrastive_loss\",\n",
    "    \"contrastive_loss_margin\": 1.0,\n",
    "\n",
    "    \"optimizer_name\": \"Adam\",\n",
    "    \"optimizer_lr\": 1e-2,\n",
    "    \"optimizer_decay\": None,\n",
    "    \n",
    "    \"batch_size\": 512,\n",
    "    \n",
    "}\n",
    "\n",
    "SCNN_4_1234_conv_1_dense_arg_dict_default = {\n",
    "    \"input_dropout_streams\": [.2, .2, .2, .2], #5**3\n",
    "    \"filters_streams\": [[16], [16, 32], [16, 32, 64], [16, 32, 64, 128]], #6**3\n",
    "    \"kernels_streams\": [[3], [5, 3], [7, 5, 3], [9, 7, 5, 3]], #4*3\n",
    "    \"kernels_init_streams\": [[\"glorot_uniform\"], [\"glorot_uniform\", \"glorot_uniform\"], #8**3\n",
    "                             [\"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\"],\n",
    "                            [\"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\"]],\n",
    "    \"kernels_Max_Norm_constraint_streams\": [[3], [3, 3], [3, 3, 3], [3, 3, 3, 3]], #3**3\n",
    "    \"conv_kernel_regularizer_streams\": [[\"l1\"], [\"l2\",\"l1\"], [\"l1\",\"l2\", \"l1\"], [\"l2\", \"l1\", \"l2\", \"l1\"]],\n",
    "    \"strides_streams\": [[1], [1, 1], [1, 1, 1], [1, 1, 1, 1]], #4**3\n",
    "    \"paddings_streams\": [[\"causal\"], [\"causal\", \"causal\"], [\"causal\", \"causal\", \"causal\"], [\"causal\", \"causal\", \"causal\", \"causal\"]], #2*3\n",
    "    \"dropouts_streams\": [[.2], [.2, .3], [.2, .3, .4], [.2, .3, .4, .5]], #5**3\n",
    "    \"activations_streams\": [['relu'], ['relu', 'relu'], ['relu', 'relu', 'relu'], ['relu', 'relu', 'relu', 'relu']], # 8**3\n",
    "    \n",
    "    \"dense_layers\": [84],\n",
    "    \"dense_kernel_Max_Norm_constraints\":[3],\n",
    "    \"dense_kernel_regularizer\":[\"l1\"],\n",
    "    \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "    \"dense_dropouts\": [0], \n",
    "    \"dense_activations\": [\"sigmoid\"],\n",
    "    \n",
    "    \"loss_func_name\": \"k_contrastive_loss\",\n",
    "    \"contrastive_loss_margin\": 1.0,\n",
    "\n",
    "    \"optimizer_name\": \"Adam\",\n",
    "    \"optimizer_lr\": 1e-2,\n",
    "    \"optimizer_decay\": None,\n",
    "    \n",
    "    \"batch_size\": 512,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "SCNN_3_123_conv_1_dense_arg_dict_default = {\n",
    "    \"input_dropout_streams\": [.2, .2, .2], #5**3\n",
    "    \"filters_streams\": [[64], [128, 64], [256, 128, 64]], #6**3\n",
    "    \"kernels_streams\": [[3], [5, 3], [7, 5, 3]], #4*3\n",
    "    \"kernels_init_streams\": [[\"glorot_uniform\"], [\"glorot_uniform\", \"glorot_uniform\"], #8**3\n",
    "                             [\"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\"]],\n",
    "    \"kernels_Max_Norm_constraint_streams\": [[3], [3, 3], [3, 3, 3]], #3**3\n",
    "    \"conv_kernel_regularizer_streams\": [[\"l1\"], [\"l2\",\"l1\"], [\"l1\",\"l2\", \"l1\"]],\n",
    "    \"strides_streams\": [[1], [1, 1], [1, 1, 1]], #4**3\n",
    "    \"paddings_streams\": [[\"same\"], [\"same\", \"same\"], [\"same\", \"same\", \"same\"]], #2*3\n",
    "    \"dropouts_streams\": [[.4], [.3, .4], [.2, .3, .4]], #5**3\n",
    "    \"activations_streams\": [['relu'], ['relu', 'relu'], ['relu', 'relu', 'relu']], # 8**3\n",
    "    \n",
    "    \"dense_layers\": [84],\n",
    "    \"dense_kernel_Max_Norm_constraints\":[3],\n",
    "    \"dense_kernel_regularizer\":[\"l1\"],\n",
    "    \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "    \"dense_dropouts\": [0], \n",
    "    \"dense_activations\": [\"sigmoid\"],\n",
    "    \n",
    "    \"loss_func_name\": \"k_contrastive_loss\",\n",
    "    \"contrastive_loss_margin\": 1.0,\n",
    "\n",
    "    \"optimizer_name\": \"Adam\",\n",
    "    \"optimizer_lr\": 1e-2,\n",
    "    \"optimizer_decay\": None,\n",
    "    \n",
    "    \"batch_size\": 512,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "SCNN_1_3_conv_1_dense_arg_dict_default = {\n",
    "    \"input_dropout_streams\": [0.2],\n",
    "    \"filters_streams\": [[256, 128, 64]],\n",
    "    \"kernels_streams\": [[7, 5, 3]],\n",
    "    \"kernels_init_streams\": [[\"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\"]],\n",
    "    \"kernels_Max_Norm_constraint_streams\": [[3, 3, 3]],\n",
    "    \"conv_kernel_regularizer_streams\": [[\"l1\",\"l2\", \"l1\"]],\n",
    "    \"strides_streams\": [[1, 1, 1]],\n",
    "    \"paddings_streams\": [[\"same\", \"same\", \"same\"]],\n",
    "    \"dropouts_streams\": [[0.3, 0.4, 0.5]],\n",
    "    \"activations_streams\": [['relu', 'relu', 'relu']],\n",
    "    \n",
    "    \"dense_layers\": [84],\n",
    "    \"dense_kernel_Max_Norm_constraints\":[3],\n",
    "    \"dense_kernel_regularizer\":[\"l1\"],\n",
    "    \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "    \"dense_dropouts\": [0.2], \n",
    "    \"dense_activations\": [\"sigmoid\"],\n",
    "\n",
    "    \n",
    "    \"loss_func_name\": \"k_contrastive_loss\",\n",
    "    \"contrastive_loss_margin\": 1.0,\n",
    "    \n",
    "    \"optimizer_name\": \"Adam\",\n",
    "    \"optimizer_lr\": 1e-2,\n",
    "    \"optimizer_decay\": None,\n",
    "    \n",
    "    \"batch_size\": 512,\n",
    "\n",
    "}\n",
    "\n",
    "SCNN_1_2_conv_1_dense_arg_dict_default = {\n",
    "    \"input_dropout_streams\": [0.2],\n",
    "    \"filters_streams\": [[64, 32]],\n",
    "    \"kernels_streams\": [[5, 3]],\n",
    "    \"kernels_init_streams\": [[\"glorot_uniform\", \"glorot_uniform\"]],\n",
    "    \"kernels_Max_Norm_constraint_streams\": [[3, 3]],\n",
    "    \"conv_kernel_regularizer_streams\": [[\"l1\",\"l2\"]],\n",
    "    \"strides_streams\": [[1, 1]],\n",
    "    \"paddings_streams\": [[\"same\", \"same\"]],\n",
    "    \"dropouts_streams\": [[0.2, 0.3]],\n",
    "    \"activations_streams\": [['relu', 'relu']],\n",
    "    \n",
    "    \"dense_layers\": [84],\n",
    "    \"dense_kernel_Max_Norm_constraints\": [3],\n",
    "    \"dense_kernel_regularizer\":[\"l1\"],\n",
    "    \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "    \"dense_dropouts\": [0.2], \n",
    "    \"dense_activations\": [\"sigmoid\"],\n",
    "    \n",
    "    \"loss_func_name\": \"k_contrastive_loss\",\n",
    "    \"contrastive_loss_margin\": 1.0,\n",
    "    \n",
    "    \"optimizer_name\": \"Adam\",\n",
    "    \"optimizer_lr\": 1e-2,\n",
    "    \"optimizer_decay\": None,\n",
    "    \n",
    "    \"batch_size\": 512,\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "SCNN_1_1_conv_1_dense_arg_dict_default = {\n",
    "    \"input_dropout_streams\": [0.2],\n",
    "    \"filters_streams\": [[64]],\n",
    "    \"kernels_streams\": [[3]],\n",
    "    \"kernels_init_streams\": [[\"glorot_uniform\"]],\n",
    "    \"kernels_Max_Norm_constraint_streams\": [[3]],\n",
    "    \"strides_streams\": [[1]],\n",
    "    \"conv_kernel_regularizer_streams\": [[\"l1\"]],\n",
    "    \"paddings_streams\": [[\"same\"]],\n",
    "    \"dropouts_streams\": [[0.2]],\n",
    "    \"activations_streams\": [['relu']],\n",
    "    \n",
    "    \"dense_layers\": [84],\n",
    "    \"dense_kernel_Max_Norm_constraints\": [3],\n",
    "    \"dense_kernel_regularizer\":[\"l1\"],\n",
    "    \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "    \"dense_dropouts\": [0.2], \n",
    "    \"dense_activations\": [\"sigmoid\"],\n",
    "    \n",
    "    \"loss_func_name\": \"k_contrastive_loss\",\n",
    "    \"contrastive_loss_margin\": 1.0,\n",
    "    \n",
    "    \"optimizer_name\": \"Adam\",\n",
    "    \"optimizer_lr\": 1e-2,\n",
    "    \"optimizer_decay\": None,\n",
    "    \n",
    "    \"batch_size\": 512,\n",
    "\n",
    "}\n",
    "\n",
    "SCNN_1_2_conv_2_dense_arg_dict_default = {\n",
    "    \"input_dropout_streams\": [0.2],\n",
    "    \"filters_streams\": [[128, 64]],\n",
    "    \"kernels_streams\": [[5, 3]],\n",
    "    \"kernels_init_streams\": [[\"glorot_uniform\", \"glorot_uniform\"]],\n",
    "    \"kernels_Max_Norm_constraint_streams\": [[3, 3]],\n",
    "    \"conv_kernel_regularizer_streams\": [[\"l1\",\"l2\"]],\n",
    "    \"strides_streams\": [[1, 1]],\n",
    "    \"paddings_streams\": [[\"same\", \"same\"]],\n",
    "    \"dropouts_streams\": [[0.2, 0.3]],\n",
    "    \"activations_streams\": [['relu', 'relu']],\n",
    "    \n",
    "    \"dense_layers\": [32, 84],\n",
    "    \"dense_kernel_Max_Norm_constraints\": [3, 3],\n",
    "    \"dense_kernel_regularizer\":[\"l1\", \"l2\"],\n",
    "    \"dense_kernel_inits\": [\"glorot_uniform\", \"glorot_uniform\"],\n",
    "    \"dense_dropouts\": [0.2, 0.2], \n",
    "    \"dense_activations\": ['relu', \"sigmoid\"],\n",
    "    \n",
    "    \"loss_func_name\": \"k_contrastive_loss\",\n",
    "    \"contrastive_loss_margin\": 1.0,\n",
    "    \n",
    "    \"optimizer_name\": \"Adam\",\n",
    "    \"optimizer_lr\": 1e-2,\n",
    "    \"optimizer_decay\": None,\n",
    "    \n",
    "    \"batch_size\": 512,\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# single_head_8layercnn_2layerdense_SCNN_arg_dict_default = {\n",
    "#     \"input_dropout_streams\": [0.2],\n",
    "#     \"filters_streams\": [[32, 32, 64, 64, 128, 128, 256, 256]],\n",
    "#     \"kernels_streams\": [[3, 3, 5, 5, 7, 7, 9, 9]],\n",
    "#     \"kernels_init_streams\": [[\"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\"]],\n",
    "#     \"kernels_Max_Norm_constraint_streams\": [[3, 3, 3, 3, 3, 3, 3, 3]],\n",
    "#     \"strides_streams\": [[15, 1, 1, 1, 1, 1, 1, 1]],\n",
    "#     \"paddings_streams\": [[\"same\", \"same\", \"same\", \"same\", \"same\", \"same\", \"same\", \"same\"]],\n",
    "#     \"dropouts_streams\": [[0.2, 0.2, 0.3, 0.3, 0.4, 0.4, 0.5, 0.5]],\n",
    "#     \"activations_streams\": [['swish', 'swish', 'swish', 'swish', 'swish', 'swish', 'swish', 'swish']],\n",
    "    \n",
    "#     \"dense_layers\": [128, 256],\n",
    "#     \"dense_kernel_Max_Norm_constraints\": [3, 3],\n",
    "#     \"dense_kernel_inits\": [\"glorot_uniform\", \"glorot_uniform\"],\n",
    "#     \"dense_dropouts\": [0.2, 0.2], \n",
    "#     \"dense_activations\": ['swish', \"sigmoid\"],\n",
    "    \n",
    "#     \"loss_func_name\": \"k_contrastive_loss\",\n",
    "#     \"contrastive_loss_margin\": 1.0,\n",
    "    \n",
    "#     \"optimizer_name\": \"Adam\",\n",
    "#     \"optimizer_lr\": 3e-4,\n",
    "#     \"optimizer_decay\": None,\n",
    "    \n",
    "#     \"batch_size\": 32,\n",
    "    \n",
    "    \n",
    "#     \"Training_samples\": str(np.unique(y_train, return_counts=True)),\n",
    "#     \"Validation_samples\": str(np.unique(y_valid, return_counts=True)),\n",
    "\n",
    "# }\n",
    "\n",
    "# arg_dict_default = {\n",
    "#     \"input_dropout_streams\": [0],\n",
    "#     \"filters_streams\": [[32, 32, 32]],\n",
    "#     \"kernels_streams\": [[3, 3, 3]],\n",
    "#     \"kernels_init_streams\": [[\"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\"]],\n",
    "#     \"kernels_constraint_streams\": [[MaxNorm(3), MaxNorm(3), MaxNorm(3)]],\n",
    "#     \"strides_streams\": [[1, 1, 1]],\n",
    "#     \"paddings_streams\": [[\"same\", \"same\", \"same\"]],\n",
    "#     \"dropouts_streams\": [[0.1, 0.1, 0]],\n",
    "#     \"activations_streams\": [['relu', 'relu', 'relu']],\n",
    "    \n",
    "#     \"dense_layers\": [128],\n",
    "#     \"dense_kernel_constraints\":[MaxNorm(3)],\n",
    "#     \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "#     \"dense_dropouts\": [0], \n",
    "#     \"dense_activations\": [\"sigmoid\"],\n",
    "    \n",
    "#     \"loss_func\": k_contrastive_loss,\n",
    "#     \"optimizer\": optimizer,\n",
    "# }\n",
    "\n",
    "# arg_dict_default = {\n",
    "#     \"input_dropout_streams\": [0],\n",
    "#     \"filters_streams\": [[32, 32]],\n",
    "#     \"kernels_streams\": [[3, 3]],\n",
    "#     \"kernels_init_streams\": [[\"glorot_uniform\", \"glorot_uniform\"]],\n",
    "#     \"kernels_constraint_streams\": [[MaxNorm(3), MaxNorm(3)]],\n",
    "#     \"strides_streams\": [[1, 1]],\n",
    "#     \"paddings_streams\": [[\"same\", \"same\"]],\n",
    "#     \"dropouts_streams\": [[0.1, 0]],\n",
    "#     \"activations_streams\": [['relu', 'relu']],\n",
    "    \n",
    "#     \"dense_layers\": [32],\n",
    "#     \"dense_kernel_constraints\":[MaxNorm(3)],\n",
    "#     \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "#     \"dense_dropouts\": [0], \n",
    "#     \"dense_activations\": [\"sigmoid\"],\n",
    "    \n",
    "#     \"loss_func\": k_contrastive_loss,\n",
    "#     \"optimizer\": optimizer,\n",
    "# }\n",
    "\n",
    "\n",
    "# arg_dict_default = {\n",
    "#     \"input_dropout_streams\": [0],\n",
    "#     \"filters_streams\": [[32]],\n",
    "#     \"kernels_streams\": [[3]],\n",
    "#     \"kernels_init_streams\": [[\"glorot_uniform\"]],\n",
    "#     \"kernels_constraint_streams\": [[MaxNorm(3)]],\n",
    "#     \"strides_streams\": [[1]],\n",
    "#     \"paddings_streams\": [[\"same\"]],\n",
    "#     \"dropouts_streams\": [[0.1]],\n",
    "#     \"activations_streams\": [['relu']],\n",
    "    \n",
    "#     \"dense_layers\": [84],\n",
    "#     \"dense_kernel_constraints\":[MaxNorm(3)],\n",
    "#     \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "#     \"dense_dropouts\": [0], \n",
    "#     \"dense_activations\": [\"sigmoid\"],\n",
    "    \n",
    "#     \"loss_func\": k_contrastive_loss,\n",
    "#     \"optimizer\": optimizer,\n",
    "# }\n",
    "\n",
    "# arg_dict_default = {\n",
    "#     \"input_dropout_streams\": [0, 0],\n",
    "#     \"filters_streams\": [[32, 32, 32], [32, 32, 32]],\n",
    "#     \"kernels_streams\": [[5, 5, 5], [3, 3, 3]],\n",
    "#     \"kernels_init_streams\": [[\"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\"], [\"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\"]],\n",
    "#     \"kernels_constraint_streams\": [[MaxNorm(3), MaxNorm(3), MaxNorm(3)], [MaxNorm(3), MaxNorm(3), MaxNorm(3)]],\n",
    "#     \"strides_streams\": [[1, 1, 1], [1, 1, 1]],\n",
    "#     \"paddings_streams\": [[\"same\", \"same\", \"same\"], [\"same\", \"same\", \"same\"]],\n",
    "#     \"dropouts_streams\": [[.1, .1, .1], [0.1, .1, .1]],\n",
    "#     \"activations_streams\": [['relu', 'relu', 'relu'], ['relu', 'relu', 'relu']],\n",
    "    \n",
    "#     \"dense_layers\": [32],\n",
    "#     \"dense_kernel_constraints\":[MaxNorm(3)],\n",
    "#     \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "#     \"dense_dropouts\": [0], \n",
    "#     \"dense_activations\": [\"sigmoid\"],\n",
    "    \n",
    "#     \"loss_func\": k_contrastive_loss,\n",
    "#     \"optimizer\": optimizer,\n",
    "# }\n",
    "\n",
    "# arg_dict_default = {\n",
    "#     \"input_dropout_streams\": [0, 0, 0],\n",
    "#     \"filters_streams\": [[32, 32, 32], [32, 32, 32], [32, 32, 32]],\n",
    "#     \"kernels_streams\": [[7, 7, 7], [5, 5, 5], [3, 3, 3]],\n",
    "#     \"kernels_init_streams\": [[\"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\"], [\"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\"], \n",
    "#                              [\"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\"]],\n",
    "#     \"kernels_constraint_streams\": [[MaxNorm(3), MaxNorm(3), MaxNorm(3)], [MaxNorm(3), MaxNorm(3), MaxNorm(3)], [MaxNorm(3), MaxNorm(3), MaxNorm(3)]],\n",
    "#     \"strides_streams\": [[1, 1, 1], [1, 1, 1], [1, 1, 1]],\n",
    "#     \"paddings_streams\": [[\"same\", \"same\", \"same\"], [\"same\", \"same\", \"same\"], [\"same\", \"same\", \"same\"]],\n",
    "#     \"dropouts_streams\": [[.1, .1, .1], [.1, .1, .1], [0.1, .1, .1]],\n",
    "#     \"activations_streams\": [['relu', 'relu', 'relu'], ['relu', 'relu', 'relu'], ['relu', 'relu', 'relu']],\n",
    "    \n",
    "#     \"dense_layers\": [32],\n",
    "#     \"dense_kernel_constraints\":[MaxNorm(3)],\n",
    "#     \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "#     \"dense_dropouts\": [0], \n",
    "#     \"dense_activations\": [\"sigmoid\"],\n",
    "    \n",
    "#     \"loss_func\": k_contrastive_loss,\n",
    "#     \"optimizer\": optimizer,\n",
    "# }\n",
    "\n",
    "# arg_dict_default = {\n",
    "#     \"input_dropout_streams\": [0, 0, 0],\n",
    "#     \"filters_streams\": [[32, 32], [32, 32], [32, 32]],\n",
    "#     \"kernels_streams\": [[7, 7], [5, 5], [3, 3]],\n",
    "#     \"kernels_init_streams\": [[\"glorot_uniform\", \"glorot_uniform\"], [\"glorot_uniform\", \"glorot_uniform\"], \n",
    "#                              [\"glorot_uniform\", \"glorot_uniform\"]],\n",
    "#     \"kernels_constraint_streams\": [[MaxNorm(3), MaxNorm(3)], [MaxNorm(3), MaxNorm(3)], [MaxNorm(3), MaxNorm(3)]],\n",
    "#     \"strides_streams\": [[1, 1], [1, 1], [1, 1]],\n",
    "#     \"paddings_streams\": [[\"same\", \"same\"], [\"same\", \"same\"], [\"same\", \"same\"]],\n",
    "#     \"dropouts_streams\": [[.1, .1], [.1, .1], [0.1, .1]],\n",
    "#     \"activations_streams\": [['relu', 'relu'], ['relu', 'relu'], ['relu', 'relu']],\n",
    "    \n",
    "#     \"dense_layers\": [32],\n",
    "#     \"dense_kernel_constraints\":[MaxNorm(3)],\n",
    "#     \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "#     \"dense_dropouts\": [0], \n",
    "#     \"dense_activations\": [\"sigmoid\"],\n",
    "    \n",
    "#     \"loss_func\": k_contrastive_loss,\n",
    "#     \"optimizer\": optimizer,\n",
    "# }\n",
    "\n",
    "# arg_dict_default = {\n",
    "#     \"input_dropout_streams\": [0, 0, 0],\n",
    "#     \"filters_streams\": [[32], [32], [32]],\n",
    "#     \"kernels_streams\": [[7], [5], [3]],\n",
    "#     \"kernels_init_streams\": [[\"glorot_uniform\"], [\"glorot_uniform\"], \n",
    "#                              [\"glorot_uniform\"]],\n",
    "#     \"kernels_constraint_streams\": [[MaxNorm(3)], [MaxNorm(3)], [MaxNorm(3)]],\n",
    "#     \"strides_streams\": [[1], [1], [1]],\n",
    "#     \"paddings_streams\": [[\"same\"], [\"same\"], [\"same\"]],\n",
    "#     \"dropouts_streams\": [[.1], [.1], [0.1]],\n",
    "#     \"activations_streams\": [['relu'], ['relu'], ['relu']],\n",
    "    \n",
    "#     \"dense_layers\": [84],\n",
    "#     \"dense_kernel_constraints\":[MaxNorm(3)],\n",
    "#     \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "#     \"dense_dropouts\": [0], \n",
    "#     \"dense_activations\": [\"sigmoid\"],\n",
    "    \n",
    "#     \"loss_func\": k_contrastive_loss,\n",
    "#     \"optimizer\": optimizer,\n",
    "# }\n",
    "\n",
    "# arg_dict = {\n",
    "#     \"input_dropout_streams\": [0, 0, 0],\n",
    "#     \"filters_streams\": [[32], [32], [32]],\n",
    "#     \"kernels_streams\": [[7], [5], [3]],\n",
    "#     \"kernels_init_streams\": [[\"glorot_uniform\"], [\"glorot_uniform\"], \n",
    "#                              [\"glorot_uniform\"]],\n",
    "#     \"kernels_constraint_streams\": [[\"MaxNorm(3)\"], [\"MaxNorm(3)\"], [\"MaxNorm(3)\"]],\n",
    "#     \"strides_streams\": [[1], [1], [1]],\n",
    "#     \"paddings_streams\": [[\"same\"], [\"same\"], [\"same\"]],\n",
    "#     \"dropouts_streams\": [[.1], [.1], [0.1]],\n",
    "#     \"activations_streams\": [['relu'], ['relu'], ['relu']],\n",
    "    \n",
    "#     \"dense_layers\": [84],\n",
    "#     \"dense_kernel_constraints\":[\"MaxNorm(3)\"],\n",
    "#     \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "#     \"dense_dropouts\": [0], \n",
    "#     \"dense_activations\": [\"sigmoid\"],\n",
    "    \n",
    "#     \"loss_func\": \"k_contrastive_loss\",\n",
    "#     \"optimizer\": \"Adam\",\n",
    "# }\n",
    "\n",
    "\n",
    "# arg_dict = {\n",
    "#     \"input_dropout_streams\": [.2, .2, .2], #5**3\n",
    "#     \"filters_streams\": [[32], [32], [32]], #6**3\n",
    "#     \"kernels_streams\": [[7], [5], [3]], #4*3\n",
    "#     \"kernels_init_streams\": [[\"glorot_uniform\"], [\"glorot_uniform\"], #8**3\n",
    "#                              [\"glorot_uniform\"]],\n",
    "#     \"kernels_constraint_streams\": [[\"MaxNorm(3)\"], [\"MaxNorm(3)\"], [\"MaxNorm(3)\"]], #3**3\n",
    "#     \"strides_streams\": [[1], [1], [1]], #4**3\n",
    "#     \"paddings_streams\": [[\"same\"], [\"same\"], [\"same\"]], #2*3\n",
    "#     \"dropouts_streams\": [[.1], [.1], [.1]], #5**3\n",
    "#     \"activations_streams\": [['relu'], ['relu'], ['relu']], # 8**3\n",
    "    \n",
    "#     \"dense_layers\": [84],\n",
    "#     \"dense_kernel_constraints\":[\"MaxNorm(3)\"],\n",
    "#     \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "#     \"dense_dropouts\": [0], \n",
    "#     \"dense_activations\": [\"sigmoid\"],\n",
    "    \n",
    "#     \"loss_func\": \"k_contrastive_loss\",\n",
    "#     \"optimizer\": \"Adam\",\n",
    "# }\n",
    "\n",
    "\n",
    "# arg_dict = {\n",
    "#     \"input_dropout_streams\": [.2, .2, .2], #5**3\n",
    "#     \"filters_streams\": [[32], [32], [32]], #6**3\n",
    "#     \"kernels_streams\": [[7], [5], [3]], #4*3\n",
    "#     \"kernels_init_streams\": [[\"glorot_uniform\"], [\"glorot_uniform\"], #8**3\n",
    "#                              [\"glorot_uniform\"]],\n",
    "#     \"kernels_constraint_streams\": [[\"MaxNorm(3)\"], [\"MaxNorm(3)\"], [\"MaxNorm(3)\"]], #3**3\n",
    "#     \"strides_streams\": [[1], [1], [1]], #4**3\n",
    "#     \"paddings_streams\": [[\"same\"], [\"same\"], [\"same\"]], #2*3\n",
    "#     \"dropouts_streams\": [[.1], [.1], [.1]], #5**3\n",
    "#     \"activations_streams\": [['relu'], ['relu'], ['relu']], # 8**3\n",
    "    \n",
    "#     \"dense_layers\": [84],\n",
    "#     \"dense_kernel_constraints\":[\"MaxNorm(3)\"],\n",
    "#     \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "#     \"dense_dropouts\": [0], \n",
    "#     \"dense_activations\": [\"sigmoid\"],\n",
    "    \n",
    "#     \"loss_func\": \"k_contrastive_loss\",\n",
    "#     \"optimizer\": \"Adam\",\n",
    "# }\n",
    "\n",
    "# arg_dict = {\n",
    "#     \"input_dropout_streams\": [.2, .2, .2], #5**3\n",
    "#     \"filters_streams\": [[32], [32], [32]], #6**3\n",
    "#     \"kernels_streams\": [[7], [5], [3]], #4*3\n",
    "#     \"kernels_init_streams\": [[\"glorot_uniform\"], [\"glorot_uniform\"], #8**3\n",
    "#                              [\"glorot_uniform\"]],\n",
    "#     \"kernels_constraint_streams\": [[\"MaxNorm(3)\"], [\"MaxNorm(3)\"], [\"MaxNorm(3)\"]], #3**3\n",
    "#     \"strides_streams\": [[1], [1], [1]], #4**3\n",
    "#     \"paddings_streams\": [[\"same\"], [\"same\"], [\"same\"]], #2*3\n",
    "#     \"dropouts_streams\": [[.1], [.1], [.1]], #5**3\n",
    "#     \"activations_streams\": [['relu'], ['relu'], ['relu']], # 8**3\n",
    "    \n",
    "#     \"dense_layers\": [84],\n",
    "#     \"dense_kernel_constraints\":[\"MaxNorm(3)\"],\n",
    "#     \"dense_kernel_inits\": [\"glorot_uniform\"],\n",
    "#     \"dense_dropouts\": [0], \n",
    "#     \"dense_activations\": [\"sigmoid\"],\n",
    "    \n",
    "#     \"loss_func\": \"k_contrastive_loss\",\n",
    "#     \"optimizer\": \"Adam\",\n",
    "# }\n",
    "\n",
    "# arg_dict_default = {\n",
    "#     \"input_dropout_streams\": [0, 0, 0],\n",
    "#     \"filters_streams\": [[32, 32, 32], [32, 32, 32], [32, 32, 32]],\n",
    "#     \"kernels_streams\": [[7, 7, 7], [5, 5, 5], [3, 3, 3]],\n",
    "#     \"kernels_init_streams\": [[\"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\"], [\"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\"], \n",
    "#                              [\"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\"]],\n",
    "#     \"kernels_constraint_streams\": [[MaxNorm(3), MaxNorm(3), MaxNorm(3)], [MaxNorm(3), MaxNorm(3), MaxNorm(3)], [MaxNorm(3), MaxNorm(3), MaxNorm(3)]],\n",
    "#     \"strides_streams\": [[1, 1, 1], [1, 1, 1], [1, 1, 1]],\n",
    "#     \"paddings_streams\": [[\"same\", \"same\", \"same\"], [\"same\", \"same\", \"same\"], [\"same\", \"same\", \"same\"]],\n",
    "#     \"dropouts_streams\": [[.1, .1, .1], [.1, .1, .1], [0.1, .1, .1]],\n",
    "#     \"activations_streams\": [['relu', 'relu', 'relu'], ['relu', 'relu', 'relu'], ['relu', 'relu', 'relu']],\n",
    "    \n",
    "#     \"dense_layers\": [],\n",
    "#     \"dense_kernel_constraints\":[],\n",
    "#     \"dense_kernel_inits\": [],\n",
    "#     \"dense_dropouts\": [], \n",
    "#     \"dense_activations\": [],\n",
    "    \n",
    "#     \"loss_func\": k_contrastive_loss,\n",
    "#     \"optimizer\": optimizer,\n",
    "# }\n",
    "\n",
    "# arg_dict_default = {\n",
    "#     \"input_dropout_streams\": [0.2, 0.2, 0.2],\n",
    "#     \"filters_streams\": [[32], [32], [32]],\n",
    "#     \"kernels_streams\": [[7], [5], [3]],\n",
    "#     \"kernels_init_streams\": [[\"glorot_uniform\"], [\"glorot_uniform\"], \n",
    "#                              [\"glorot_uniform\"]],\n",
    "#     \"kernels_constraint_streams\": [[MaxNorm(3)], [MaxNorm(3)], [MaxNorm(3)]],\n",
    "#     \"strides_streams\": [[1], [1], [1]],\n",
    "#     \"paddings_streams\": [[\"same\"], [\"same\"], [\"same\"]],\n",
    "#     \"dropouts_streams\": [[.1], [.1], [0.1]],\n",
    "#     \"activations_streams\": [['relu'], ['relu'], ['relu']],\n",
    "    \n",
    "#     \"dense_layers\": [84, 84],\n",
    "#     \"dense_kernel_constraints\":[MaxNorm(3), MaxNorm(3)],\n",
    "#     \"dense_kernel_inits\": [\"glorot_uniform\", \"glorot_uniform\"],\n",
    "#     \"dense_dropouts\": [0.1, 0.1], \n",
    "#     \"dense_activations\": ['relu', \"sigmoid\"],\n",
    "    \n",
    "#     \"loss_func\": k_contrastive_loss,\n",
    "#     \"optimizer\": optimizer,\n",
    "# }\n",
    "\n",
    "# arg_dict_default = {\n",
    "#     \"input_dropout_streams\": [0.2, 0.2, 0.2],\n",
    "#     \"filters_streams\": [[10, 20, 30], [10, 20, 30], [10, 20, 30]],\n",
    "#     \"kernels_streams\": [[8, 5, 3], [5, 5, 3], [3, 3, 3]],\n",
    "#     \"kernels_init_streams\": [[\"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\"], [\"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\"], \n",
    "#                              [\"glorot_uniform\", \"glorot_uniform\", \"glorot_uniform\"]],\n",
    "#     \"kernels_constraint_streams\": [[MaxNorm(3), MaxNorm(3), MaxNorm(3)], [MaxNorm(3), MaxNorm(3), MaxNorm(3)], [MaxNorm(3), MaxNorm(3), MaxNorm(3)]],\n",
    "#     \"strides_streams\": [[1, 1, 1], [1, 1, 1], [1, 1, 1]],\n",
    "#     \"paddings_streams\": [[\"same\", \"same\", \"same\"], [\"same\", \"same\", \"same\"], [\"same\", \"same\", \"same\"]],\n",
    "#     \"dropouts_streams\": [[.2, .3, .4], [.2, .3, .4], [0.2, .3, .4]],\n",
    "#     \"activations_streams\": [['tanh', 'tanh', 'tanh'], ['tanh', 'tanh', 'tanh'], ['tanh', 'tanh', 'tanh']],\n",
    "    \n",
    "#     \"dense_layers\": [32, 64],\n",
    "#     \"dense_kernel_constraints\":[MaxNorm(3), MaxNorm(3)],\n",
    "#     \"dense_kernel_inits\": [\"glorot_uniform\", \"glorot_uniform\"],\n",
    "#     \"dense_dropouts\": [0.4, 0.5], \n",
    "#     \"dense_activations\": ['tanh', \"sigmoid\"],\n",
    "    \n",
    "#     \"loss_func\": k_contrastive_loss,\n",
    "#     \"optimizer\": optimizer,\n",
    "# }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
