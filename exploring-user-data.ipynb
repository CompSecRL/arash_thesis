{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\n\"Exploratory analysis is performed to extract insights and comprehend the dataset.\nThis is the first step we need to complete as soon as we get the dataset before we get lost in the weeds.\nWhy do we do Exploratory Data Analysis (EDA)?\nThe goal is to learn more about the data set, which will help us make informed decisions throughout the project during data cleansing, function engineering, etc.\nEDA will also help us to understand the relationships between features, data quality, and data visualization.\nEDA is a very important first step in creating machine learning models, this step should not be ignored nor extended for a very long time. It should be quick and decisive.\"[-source](https://medium.com/mlearning-ai/exploratory-data-analysis-eda-89e9468bf75)","metadata":{}},{"cell_type":"markdown","source":"Dataset [link](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fdrive.google.com%2Fdrive%2Ffolders%2F1-6sd6E3cL8uWdNeO4xiXxLbTSYXSTiVr%3Fusp%3Dsharing&data=04%7C01%7Carash.gholami%40queensu.ca%7C92b180b5b67d475fb70e08d87b7f4305%7Cd61ecb3b38b142d582c4efb2838b925c%7C1%7C0%7C637395133804367020%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=1ieR3E8llMblC0QMBtx1%2FpHS71RQuCxzdNQLmH0HJVU%3D&reserved=0)\n\nExplanation: There are **49** users' data here. Naming is in the format of Username_ExperimentNumber. Please also note that it is unprocessed raw data, which includes a lot of noise etc. \n\nExperiment-1: **20** participants are involved in this task and the participants were asked to type **a story** from **a set** of short and simple stories from the American Literature https://americanliterature.com/100-great-short-stories for **four minutes**. The story was chosen randomly by the participants.\n\nExperiment-2: 20 participants are involved in this task and for this data set, all the participants were asked to type the **same text** https://en.wikipedia.org/wiki/The_Adventures_of_Tom_Sawyer for **four minutes**. For each participant, almost the same amount of data is collected as Experiment-1. This dataset is important to be able to measure the quality of the features. The collection of this dataset is quite realistic since in a real case the users could be also asked to type a given text during their CA sessions. \n\nExperiment-3: The participants are asked to type **10 different pangram sentences**. Pangrams sentences contain** all 26 letters** of the alphabet. This data set is used to test how the **letter distribution** of text affects the result.\n\nExperiment-4: **34** participants are involved in this task and the participants were instructed to imitate someone else's typing pattern by watching the prerecorded video of the other person. For these experiments, one of the participants was recorded on video while typing a short and simple sentence for **15 seconds** from a perspective that the hand motions, smartwatch, keyboard, and the screen could be seen. Although it was not required, the perspective allowed to infer what the victim was typing by watching. This dataset was primarily used to analyze the **attacking scenarios**. It is left to the participant to type the exact same text or imitate the hand movement of the victim.\n\n-----------------------\n\nI realized that you have mostly 4s and 10s as values on column A, but I also found 2s. What does column A represent? \nA represents the sensor type:\n \n**10 for the accelerometer\n4 for the gyroscope**\n2 for the magnetic field sensor\n\nYou can find more info about sensors and constant values that are used to define the sensor type in these links:\n- https://developer.android.com/guide/topics/sensors/sensors_overview\n- https://developer.android.com/reference/android/hardware/Sensor#TYPE_LINEAR_ACCELERATION\n\nNormally, I only collected and used accelerometer and gyroscope sensor data. Magnetometer data should not be in the dataset. If there is, that means it is an **outlier**.\n \nIs column B the time stamp, is it recorded at 100HZ?\n\nIn an android app, you can set the sampling rate \"SENSOR_DELAY_NORMAL, SENSOR_DELAY_UI, SENSOR_DELAY_GAME, or SENSOR_DELAY_FASTEST \" \n\nAs far as I remember I used **\"SENSOR_DELAY_FASTEST\" for all sensors**, but in practice, I had a lot of issues regarding this. They **don't give a constant sampling rate**. Every time I used they were giving different sampling rates. They were giving different sampling rates for two sessions of the same user, for different users, or for different sensors. Therefore, I would recommend using the one in the practice, which you can calculate using the timestamp and the sample count. \n\nOn the paper you said you have recorded both accelerometer and gyro data; but you seem to only have one of them at a time on columns C, D, and F. I am guessing 10 on column A represents accelerometer and 4 gyro, but I could be wrong. \nAlso, some rows have more columns, and start with 11 on column A. What do those represent?\nPlease see the explanation above. \n \n\nIn short, I would really appreciate it if you could let me know information I need to know for working with the data.\n\nAlso, as these sensors were giving me a lot of headaches because they are really not stable, I needed to perform a lot of preprocessing. I **cleaned the outliers** before further processing. I also applied some other filtering techniques such as **moving average** to obtain more smooth data. Moreover, I needed to **clean the part the users are not performing typing operation from the beginning and end**, which you can see if you plot the values, i.e., the **user stops meaning no value and then big deviations, where the user takes off the smartwatch**.","metadata":{}},{"cell_type":"markdown","source":"# Notes\n1. Outliers can be removed from our dataset after we examine or ask domain expertise if they are anomalies or not.","metadata":{}},{"cell_type":"markdown","source":"# Preparations","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n!pip install --upgrade pip\n!pip install python-docx\n\nfrom scipy import signal\nimport datetime\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.cm as cm\nimport seaborn as sns\nfrom scipy.stats import kurtosis\nfrom scipy.stats import skew\nfrom scipy.stats import iqr\nfrom scipy.stats import median_absolute_deviation\nfrom scipy.stats import mode\nfrom scipy.signal import find_peaks\nfrom scipy.signal import peak_widths\nfrom scipy.special import entr\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import auc\nimport time\nimport docx\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nprint(\"Setup Complete\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-12T16:04:57.135455Z","iopub.execute_input":"2022-04-12T16:04:57.135796Z","iopub.status.idle":"2022-04-12T16:05:23.829567Z","shell.execute_reply.started":"2022-04-12T16:04:57.135768Z","shell.execute_reply":"2022-04-12T16:05:23.828091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_names =['sensor_id', 'time_stamp', 'x', 'y', 'z']\ndata = pd.read_csv('../input/wearable-assisted-ca/user{}_1.csv'.format(20), error_bad_lines = False, header=None, dtype = str, usecols = range(len(column_names)))\ndata.columns = column_names\ndata = data[(data.sensor_id == '10') | (data.sensor_id =='4')]\ndata.sort_values(by=['time_stamp'], inplace=True)\ndata.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:14:49.984849Z","iopub.status.idle":"2022-04-12T13:14:49.985352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.isna().sum().sum())\ndata.dropna(inplace=True)\ndata.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:14:49.986936Z","iopub.status.idle":"2022-04-12T13:14:49.987701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"types_dict = {'sensor_id': 'int32', 'time_stamp': 'int64', 'x': 'float64', 'y': 'float64', 'z': 'float64'}\nfor col, col_type in types_dict.items():\n    data[col] = data[col].astype(col_type)\n# data['x'][0]\ndata['time_stamp'] = pd.to_datetime(data['time_stamp'], unit='ns')\n# data.set_index(\"time_stamp\")\ndata.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:14:49.989206Z","iopub.status.idle":"2022-04-12T13:14:49.989938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.to_datetime(data.tail(200)['time_stamp'], unit='ns')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:14:49.991667Z","iopub.status.idle":"2022-04-12T13:14:49.992423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:14:49.993744Z","iopub.status.idle":"2022-04-12T13:14:49.994533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def matchAccelGyroData(accel, gyro):\n    # Match the numbers by merge_asof to the higher length vector\n    accel_count = accel.count().time_stamp\n    gyro_count = gyro.count().time_stamp\n    column_names =['sensor_id', 'time_stamp', 'x', 'y', 'z']\n    if accel_count > gyro_count:\n        df = pd.merge_asof(accel, gyro, on=\"time_stamp\", direction='nearest')\n        df = df.sort_values(by=['time_stamp'])\n        df = df.dropna()\n        accel = df[[\"sensor_id_x\", \"time_stamp\", \"x_x\", \"y_x\", \"z_x\"]]\n        gyro = df[[\"sensor_id_y\", \"time_stamp\", \"x_y\", \"y_y\", \"z_y\"]]\n    else:\n        df = pd.merge_asof(gyro, accel, on=\"time_stamp\", direction='nearest')\n        df = df.sort_values(by=['time_stamp'])\n        df = df.dropna()\n        gyro = df[[\"sensor_id_x\", \"time_stamp\", \"x_x\", \"y_x\", \"z_x\"]]\n        accel = df[[\"sensor_id_y\", \"time_stamp\", \"x_y\", \"y_y\", \"z_y\"]]\n\n    accel.columns = column_names\n    gyro.columns = column_names\n\n\n    return {'accel': accel, 'gyro': gyro}","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:05:23.834025Z","iopub.execute_input":"2022-04-12T16:05:23.834431Z","iopub.status.idle":"2022-04-12T16:05:23.847775Z","shell.execute_reply.started":"2022-04-12T16:05:23.834398Z","shell.execute_reply":"2022-04-12T16:05:23.845093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getDataStats(user_id, exp_id, print_accel_gyro_array_size=1, print_na_df_array_size=1, begin_idx=0, end_idx=None, eliminate_extreme_deviations=None, \n                 match_accel_gyro_data=False):\n    '''\n    user_id: int, user id from the dataset in the form of user<id>_<exp_id>.csv\n    exp_id: int, exp id from the dataset in the form of user<id>_<exp_id>.csv\n    print_accel_gyro_array_size: bool, whether to include accelerometer and gyroscope array sizes found in the dataset.\n    print_na_df_array_size: bool, whether to include number of na values found in the raw data.\n    begin_idx: int, to cut off the raw data until the begin_idx and return the resulting array.\n    end_idx: int, to cut off the ending of raw data from the end_idx and return the resulting array.\n    eliminate_extreme_deviations: float, to manually suppress outliers above +eliminate_extreme_deviations, and -eliminate_extreme_deviations. \n    current value is None, previously it was 10.1\n    \n    '''\n    log_report = f\"Experiment: {exp_id}, User: {user_id}, begin_idx: {begin_idx}, end_idx: {end_idx}\"\n    \n    #load the data with these column names\n    column_names =['sensor_id', 'system_time', 'x', 'y', 'z']\n    \n    try:\n        data = pd.read_csv(f'../input/wearable-assisted-ca/user{user_id}_{exp_id}.csv', error_bad_lines = False, header=None, usecols = range(len(column_names)), \n                           dtype = str)\n    except:\n        print(f\"Could not open the csv file for User: {user_id}, Experiment: {exp_id}, the file probably does not exist.\")\n        return \n        \n    data.columns = column_names\n    \n    # extract linear accel and gyro data\n    # 10 is for linear_accel and 4 is for gyro data. \n    # Note: linear accel excludes gravity; i.e. acceleration = gravity + linear-acceleration\n    data = data[(data.sensor_id == '10') | (data.sensor_id =='4') & (data.system_time != '0.0') & (data.system_time != '2.04')\n                & (data.system_time != '2.10')]\n\n    # find how many NAN values in the data\n    num_NaN = data.isna().sum().sum()\n\n    # since only 7 NAN is a very small amount, drop them\n    df = data.dropna()\n    num_NaN_dropped = df.isna().sum().sum()\n    \n    # set the column types\n    types_dict = {'sensor_id': 'int32', 'system_time': 'int64', 'x': 'float64', 'y': 'float64', 'z': 'float64'}\n    for col, col_type in types_dict.items():\n        df[col] = df[col].astype(col_type)\n\n    \n    # convert nano seconds system time, to human readable timestamps, and change the index\n    df[\"time_stamp\"] = pd.to_datetime(df['system_time'], unit='ns')\n#     df = df.set_index(\"time_stamp\")\n#     print(df.head())\n    \n    if print_na_df_array_size:\n        log_report += f\", df count: {df.count().system_time}, NaN_count: {num_NaN}, NaN_dropped: {num_NaN_dropped}\"\n        \n    # get rid of begin and end noise\n    #sort df and filter\n    df.sort_values(by=['time_stamp'], inplace=True)\n    df = df[begin_idx:end_idx]\n    \n    # cleaning extreme outliers\n    if eliminate_extreme_deviations is not None:\n        \n        df = df[(df.x < eliminate_extreme_deviations) & ( -eliminate_extreme_deviations < df.x) \n                & (df.y < eliminate_extreme_deviations) & ( -eliminate_extreme_deviations < df.y) \n                & (df.z < eliminate_extreme_deviations) & ( -eliminate_extreme_deviations < df.z) ]\n        log_report += f', eliminate_extreme_deviations: {eliminate_extreme_deviations}'\n\n    # Extract Accelerometer values and sort\n    accel = df[df.sensor_id == 10]#.copy()\n    accel.sort_values(by=['time_stamp'], inplace=True)\n\n    # Extract gyro values and sort\n    gyro = df[df.sensor_id == 4]#.copy()\n    gyro.sort_values(by=['time_stamp'], inplace=True)\n    \n    if print_accel_gyro_array_size:\n        log_report += f\", accel_count: {accel.count().x}, gyro_count: {gyro.count().x}\"\n    \n    # Match accel, gyro count. The result should be of the same size as the bigger array\n    if match_accel_gyro_data:\n        result = matchAccelGyroData(accel, gyro)\n        accel, gyro = result['accel'], result['gyro']\n    \n    # Calculate Exponential Moving Average\n    accel['EMA_x_a'] = accel['x'].ewm(span=40,adjust=False).mean()\n    accel['EMA_y_a'] = accel['y'].ewm(span=40,adjust=False).mean()\n    accel['EMA_z_a'] = accel['z'].ewm(span=40,adjust=False).mean()\n\n    gyro['EMA_x_g'] = gyro['x'].ewm(span=40,adjust=False).mean()\n    gyro['EMA_y_g'] = gyro['y'].ewm(span=40,adjust=False).mean()\n    gyro['EMA_z_g'] = gyro['z'].ewm(span=40,adjust=False).mean()\n    \n    left = accel[[\"time_stamp\", \"EMA_x_a\", \"EMA_y_a\", \"EMA_z_a\"]]\n    right = gyro[[\"time_stamp\", \"EMA_x_g\", \"EMA_y_g\", \"EMA_z_g\"]].set_index('time_stamp')\n    df = left.join(right, on='time_stamp')\n    \n    # Testing if merge works the same way compared to what i did above\n#     print((left.merge(right, on=\"time_stamp\")==df).all())\n\n    session_duration = df[\"time_stamp\"].max() - df[\"time_stamp\"].min()\n    session_frequency = datetime.timedelta(seconds=len(df[\"time_stamp\"])) / session_duration\n    \n    accel_frequency = datetime.timedelta(seconds=len(accel[\"time_stamp\"])) / session_duration\n    gyro_frequency = datetime.timedelta(seconds=len(gyro[\"time_stamp\"])) / session_duration\n    \n    accel_duration = accel[\"time_stamp\"].max() - accel[\"time_stamp\"].min()\n    gyro_duration = gyro[\"time_stamp\"].max() - gyro[\"time_stamp\"].min()\n    \n\n\n    log_report += f\", session_duration: {session_duration}, session_frequency: {session_frequency}, accel_frequency: {accel_frequency}, gyro_frequency: {gyro_frequency}\\\n, accel_duration: {accel_duration}, gyro_duration: {gyro_duration}\"\n    \n    print(log_report)\n    \n    return {\"accel\":accel, \"gyro\": gyro, \"df\": df, \"user_idx\": user_id, \"exp_id\":exp_id, \"session_duration\": session_duration, \"accel_duration\": accel_duration, \n            \"gyro_duration\": gyro_duration, \"session_frequency\": session_frequency, \"accel_frequency\": accel_frequency, \"gyro_frequency\": gyro_frequency, \"num_NaN\": num_NaN, \"num_NaN_dropped\": num_NaN_dropped}\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:05:23.849318Z","iopub.execute_input":"2022-04-12T16:05:23.849758Z","iopub.status.idle":"2022-04-12T16:05:23.887339Z","shell.execute_reply.started":"2022-04-12T16:05:23.849728Z","shell.execute_reply":"2022-04-12T16:05:23.886143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#     Q1 = df[\"COLUMN_NAME\"].quantile(0.25)\n#     Q3 = df[\"COLUMN_NAME\"].quantile(0.75)\n#     IQR = Q3 - Q1\n    \n#     Lower_Fence = Q1 - (1.5 * IQR)\n#     Upper_Fence = Q3 + (1.5 * IQR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r = [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 28, 29, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49]\nprint(len(r))\n# data_exp1 = getDataStats1(27, begin_idx=0, end_idx=-1)\n# data_exp2 = getDataStats2(27, begin_idx=0, end_idx=-1)\ndata_exp1 = getDataStats(user_id=17, exp_id=2, print_accel_gyro_array_size=1, print_na_df_array_size=1, begin_idx=1, end_idx=None, eliminate_extreme_deviations=None, \n                 match_accel_gyro_data=False)\n# data_exp2 = getDataStats2(27, begin_idx=0, end_idx=-1)\ndata_exp1[\"gyro\"].set_index('time_stamp')\n# print(len(data_exp1['accel']))\n# gyro","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:18:55.59789Z","iopub.execute_input":"2022-04-12T13:18:55.598604Z","iopub.status.idle":"2022-04-12T13:18:55.998812Z","shell.execute_reply.started":"2022-04-12T13:18:55.59855Z","shell.execute_reply":"2022-04-12T13:18:55.997745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accel, gyro = data_exp1['accel'], data_exp1[\"gyro\"]\naccel.rename(columns={'x': 'x_a', 'y': 'y_a', 'z': 'z_a'}, inplace=True)\ngyro.rename(columns={'x': 'x_g', 'y': 'y_g', 'z': 'z_g'}, inplace=True)\ndata = [accel, gyro]\nplot_rows = [\n    [\"x_a\", \"y_a\", \"z_a\"],\n    [\"x_g\", \"y_g\", \"z_g\"],\n]\n\nmin_date_0 = data[0]['time_stamp'].min()\nmin_date_1 = data[1]['time_stamp'].min()\nbase = min_date_0 if min_date_0 < min_date_1 else min_date_1 \n\n\ndata[0]['delta_time'] = data[0]['time_stamp'] - base\ndata[1]['delta_time'] = data[1]['time_stamp'] - base\n\nget_seconds = lambda date_time: date_time.total_seconds()\ndata[0]['delta_time'] = data[0]['delta_time'].apply(get_seconds)\ndata[1]['delta_time'] = data[1]['delta_time'].apply(get_seconds)\n\ndata[0] = data[0].set_index('delta_time')\ndata[1] = data[1].set_index('delta_time')\n\n\n# data[0] = data[0].set_index('time_stamp')\n# data[1] = data[1].set_index('time_stamp')\nf, axes = plt.subplots(2, 1, dpi=180, figsize=(5.473, 2))\nfor i, row in enumerate(plot_rows):\n    print(i, row)\n    g = data[i][row].plot(linewidth=0.3, ax=axes[i], sharex=\"col\")\n#     g = sns.lineplot(x=\"time_stamp\", y=row, data=data[i], linewidth=0.3, ax=axes[i], sharex=\"col\")\n    g.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.0)\n    \ng.axes.set_xlabel('seconds')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:19:08.486114Z","iopub.execute_input":"2022-04-12T13:19:08.486843Z","iopub.status.idle":"2022-04-12T13:19:09.574736Z","shell.execute_reply.started":"2022-04-12T13:19:08.486783Z","shell.execute_reply":"2022-04-12T13:19:09.573532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[0].head()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:19:24.183605Z","iopub.execute_input":"2022-04-12T13:19:24.183972Z","iopub.status.idle":"2022-04-12T13:19:24.201347Z","shell.execute_reply.started":"2022-04-12T13:19:24.18394Z","shell.execute_reply":"2022-04-12T13:19:24.2003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sensor Data Distribution for User 18","metadata":{}},{"cell_type":"code","source":"data = [accel, gyro]\nplot_rows = [[\"x_a\", \"y_a\", \"z_a\"],\n            [\"x_g\", \"y_g\", \"z_g\"]]\nplot_rows = np.array(plot_rows)\nplot_cols = plot_rows.T\n\nf, axes = plt.subplots(3, 2, sharex=\"col\", sharey=\"col\", dpi=180, figsize=(6, 4))\nf.subplots_adjust(hspace=0.6, wspace=0.2)\ncmap = cm.get_cmap(\"tab10\")\n\nfor i, col in enumerate(plot_cols.ravel()):\n    plot_column = i % 2\n    plot_row = i // 2\n    print(plot_column, col)\n    print(plot_row, plot_column)\n    g = sns.distplot(\n        data[plot_column][col],\n        kde=False,\n        ax=axes[plot_row][plot_column],\n        color=cmap(plot_column),\n        hist_kws=dict(alpha=1),\n    )\n    g.set_title(f\"{col}\")\n    g.set_yscale(\"log\")\n    g.axes.set_xlabel(\"\")\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:19:31.917336Z","iopub.execute_input":"2022-04-12T13:19:31.918082Z","iopub.status.idle":"2022-04-12T13:19:33.897191Z","shell.execute_reply.started":"2022-04-12T13:19:31.918038Z","shell.execute_reply":"2022-04-12T13:19:33.896455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extractTextFromDocx(path):\n    try:\n        data = \"\"\n        doc = docx.Document(path)  # Creating word reader object.\n        fullText = []\n        for para in doc.paragraphs:\n            fullText.append(para.text)\n            data = '\\n'.join(fullText)\n\n    except IOError:\n        print(f'There was an error opening file: {path}')\n        return\n    except Exception:\n        print(f\"Could not find file: {path}\")\n    \n    return data\n\ndef numberOfWords(text):\n    return len(text.strip().split())\n\ndef numberOfChars(text):\n    return len(text)\n\ndef wordsPerMinute(number_of_words, session_duration):\n    wps = datetime.timedelta(seconds=number_of_words) / session_duration\n    return wps * 60 \n\ndef charsPerMinute(number_of_chars, session_duration):\n    return charsPerSecond(number_of_chars, session_duration) * 60\n\ndef charsPerSecond(number_of_chars, session_duration):\n    return datetime.timedelta(seconds=number_of_chars) / session_duration","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:05:23.888877Z","iopub.execute_input":"2022-04-12T16:05:23.893526Z","iopub.status.idle":"2022-04-12T16:05:23.907041Z","shell.execute_reply.started":"2022-04-12T16:05:23.893482Z","shell.execute_reply":"2022-04-12T16:05:23.90606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n\nr = range(1, 50)\n\nexp_user_words_typed_dict = {}\nexp_user_chars_typed_dict = {}\nexp_id = 1\nfor user_id in r:\n    user_text_data = extractTextFromDocx(f'../input/wearable-assisted-ca/user{user_id}_{exp_id}.docx')\n    \n    if user_text_data is not None:\n        exp_user_words_typed_dict[user_id] = numberOfWords(user_text_data)\n        exp_user_chars_typed_dict[user_id] = numberOfChars(user_text_data)\n\nexp_user_lst = []\nfor user_id in r:\n    print(f\"----------------------user_id: {user_id} --------------------\")\n    data_exp = getDataStats(user_id=user_id, exp_id=exp_id, print_accel_gyro_array_size=1, print_na_df_array_size=1, begin_idx=None, end_idx=None,\n                                 eliminate_extreme_deviations=15, match_accel_gyro_data=False) \n    \n    if data_exp is not None: \n        accel, gyro = data_exp['accel'], data_exp[\"gyro\"]\n        accel.rename(columns={'x': 'x_a', 'y': 'y_a', 'z': 'z_a'}, inplace=True)\n        gyro.rename(columns={'x': 'x_g', 'y': 'y_g', 'z': 'z_g'}, inplace=True)\n\n        words_per_minute = wordsPerMinute(exp_user_words_typed_dict[user_id], data_exp[\"session_duration\"])\n        chars_per_second = charsPerSecond(exp_user_chars_typed_dict[user_id], data_exp[\"session_duration\"])\n        \n        exp_user_lst.append([user_id, accel['x_a'], accel['y_a'], accel[\"z_a\"], gyro[\"x_g\"], gyro[\"y_g\"], gyro[\"z_g\"], data_exp[\"session_duration\"], \n                              data_exp[\"accel_duration\"], data_exp[\"gyro_duration\"], data_exp[\"session_frequency\"], data_exp[\"accel_frequency\"], \n                              data_exp[\"gyro_frequency\"], data_exp[\"num_NaN\"], data_exp[\"num_NaN_dropped\"], words_per_minute, chars_per_second])\n\nexp_user_arr = np.array(exp_user_lst)\ncolumns = ['user_id', 'x_a', \"y_a\", \"z_a\", \"x_g\", \"y_g\", \"z_g\", \"session_duration\", \"accel_duration\", \"gyro_duration\", \"session_frequency\", \n           \"accel_frequency\", \"gyro_frequency\", \"num_NaN\", \"num_NaN_dropped\", \"words_per_minute\", \"chars_per_second\"]\nexp_user_arr = np.array(exp_user_lst)\nexp_user_df = pd.DataFrame(exp_user_arr, columns=columns)\nexp_user_df","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:05:23.908766Z","iopub.execute_input":"2022-04-12T16:05:23.909084Z","iopub.status.idle":"2022-04-12T16:05:46.002322Z","shell.execute_reply.started":"2022-04-12T16:05:23.909039Z","shell.execute_reply":"2022-04-12T16:05:46.001299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_of_interest = ['user_id', \"session_duration\", \"accel_duration\", \"gyro_duration\",\n           \"accel_frequency\", \"gyro_frequency\", \"num_NaN\", \"num_NaN_dropped\", \"words_per_minute\", \"chars_per_second\"]\nexp_user_df[columns_of_interest]","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:05:46.440257Z","iopub.execute_input":"2022-04-12T16:05:46.440618Z","iopub.status.idle":"2022-04-12T16:05:46.486972Z","shell.execute_reply.started":"2022-04-12T16:05:46.440585Z","shell.execute_reply":"2022-04-12T16:05:46.486091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfList_exp1[i][['EMA_x_a']].plot(figsize=(30, 6))\ndfList_exp1[i][['EMA_y_a']].plot(figsize=(30, 6))\ndfList_exp1[i][['EMA_z_a']].plot(figsize=(30, 6))\ndfList_exp1[i][['EMA_x_g']].plot(figsize=(30, 6))\ndfList_exp1[i][['EMA_y_g']].plot(figsize=(30, 6))\ndfList_exp1[i][['EMA_z_g']].plot(figsize=(30, 6))\n\ndfList_exp2[i][['EMA_x_a']].plot(figsize=(30, 6))\ndfList_exp2[i][['EMA_y_a']].plot(figsize=(30, 6))\ndfList_exp2[i][['EMA_z_a']].plot(figsize=(30, 6))\ndfList_exp2[i][['EMA_x_g']].plot(figsize=(30, 6))\ndfList_exp2[i][['EMA_y_g']].plot(figsize=(30, 6))\ndfList_exp2[i][['EMA_z_g']].plot(figsize=(30, 6))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp1_user_df.iloc[0]['x_a']","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:35:29.331338Z","iopub.execute_input":"2021-10-20T14:35:29.331799Z","iopub.status.idle":"2021-10-20T14:35:29.342314Z","shell.execute_reply.started":"2021-10-20T14:35:29.331727Z","shell.execute_reply":"2021-10-20T14:35:29.341455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp1_user_df.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:35:29.343448Z","iopub.execute_input":"2021-10-20T14:35:29.343996Z","iopub.status.idle":"2021-10-20T14:35:29.354059Z","shell.execute_reply.started":"2021-10-20T14:35:29.343957Z","shell.execute_reply":"2021-10-20T14:35:29.35302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_cols = [\"words_per_minute\", \"chars_per_second\", \"session_frequency\"]\ndf = exp1_user_df[feature_cols]\ntypes_dict = {'words_per_minute': 'float64', 'chars_per_second': 'float64', 'session_frequency': 'float64'}\nfor col, col_type in types_dict.items():\n    df[col] = df[col].astype(col_type)\n\ncorr = df[feature_cols].corr()","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:35:29.355478Z","iopub.execute_input":"2021-10-20T14:35:29.355829Z","iopub.status.idle":"2021-10-20T14:35:29.373997Z","shell.execute_reply.started":"2021-10-20T14:35:29.355796Z","shell.execute_reply":"2021-10-20T14:35:29.372927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(corr)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:35:29.376239Z","iopub.execute_input":"2021-10-20T14:35:29.376738Z","iopub.status.idle":"2021-10-20T14:35:29.613556Z","shell.execute_reply.started":"2021-10-20T14:35:29.376706Z","shell.execute_reply":"2021-10-20T14:35:29.612698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp1_user_df[\"chars_per_second\"].plot()","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:35:29.615087Z","iopub.execute_input":"2021-10-20T14:35:29.615388Z","iopub.status.idle":"2021-10-20T14:35:29.768409Z","shell.execute_reply.started":"2021-10-20T14:35:29.615357Z","shell.execute_reply":"2021-10-20T14:35:29.767477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp1_user_df[\"chars_per_second\"].hist()","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:35:29.770283Z","iopub.execute_input":"2021-10-20T14:35:29.771007Z","iopub.status.idle":"2021-10-20T14:35:29.950531Z","shell.execute_reply.started":"2021-10-20T14:35:29.770959Z","shell.execute_reply":"2021-10-20T14:35:29.949268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp1_user_df[\"words_per_minute\"].plot()","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:35:29.952106Z","iopub.execute_input":"2021-10-20T14:35:29.952527Z","iopub.status.idle":"2021-10-20T14:35:30.108951Z","shell.execute_reply.started":"2021-10-20T14:35:29.95249Z","shell.execute_reply":"2021-10-20T14:35:30.107874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp1_user_df[\"words_per_minute\"].hist()","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:35:30.110165Z","iopub.execute_input":"2021-10-20T14:35:30.11047Z","iopub.status.idle":"2021-10-20T14:35:30.290203Z","shell.execute_reply.started":"2021-10-20T14:35:30.110441Z","shell.execute_reply":"2021-10-20T14:35:30.289045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(exp1_user_df[\"session_duration\"] / np.timedelta64(1, 'm')).hist()","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:35:30.291796Z","iopub.execute_input":"2021-10-20T14:35:30.292217Z","iopub.status.idle":"2021-10-20T14:35:30.46141Z","shell.execute_reply.started":"2021-10-20T14:35:30.292181Z","shell.execute_reply":"2021-10-20T14:35:30.46056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"session_duration_minutes =  exp1_user_df[\"session_duration\"] / np.timedelta64(1, 'm')\nmax_y = len(exp1_user_df)/2\ntext_dist_from_mean_line = 0.1\nnew_line_dist = 2\nMAGENTA=(1,0,1,1)\n\nprint(f\"Histogramm of session duration, for all {len(exp1_user_df)} exp1 sessions in minutes\")\nmean_duration = session_duration_minutes.mean()\nfig = plt.figure(dpi=180, figsize=(5.473, 2))\ng = sns.distplot(\n    session_duration_minutes, kde=False, hist_kws=dict(alpha=1, lw=0.5)\n)\ng.set_xlabel(\"Session Duration in Minutes\", fontsize=6)\nplt.plot(\n    [mean_duration, mean_duration], [0, max_y], linestyle=\"dashed\", color=MAGENTA, lw=0.8\n)\nplt.text(\n    mean_duration + text_dist_from_mean_line,\n    max_y - new_line_dist,\n    \"mean\",\n    fontsize=6,\n    color=MAGENTA,\n    horizontalalignment=\"left\",\n)\nplt.text(\n    mean_duration + text_dist_from_mean_line,\n    max_y - 2 * new_line_dist,\n    f\"({mean_duration:.1f} min)\",\n    fontsize=5,\n    color=MAGENTA,\n    horizontalalignment=\"left\",\n)\nplt.xticks(fontsize=6)\nplt.yticks(fontsize=6)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:35:30.462848Z","iopub.execute_input":"2021-10-20T14:35:30.463152Z","iopub.status.idle":"2021-10-20T14:35:30.881285Z","shell.execute_reply.started":"2021-10-20T14:35:30.46312Z","shell.execute_reply":"2021-10-20T14:35:30.880327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection","metadata":{}},{"cell_type":"markdown","source":"Correlation Matrix","metadata":{}},{"cell_type":"code","source":"def spectral_energy(x):\n    '''\n    spectral_energy according to Parseval's theorem\n    '''\n    return (1/len(x)) * np.sum(np.abs(np.fft.rfft(x))**2)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:35:44.084801Z","iopub.execute_input":"2021-10-20T14:35:44.085151Z","iopub.status.idle":"2021-10-20T14:35:44.091291Z","shell.execute_reply.started":"2021-10-20T14:35:44.085121Z","shell.execute_reply":"2021-10-20T14:35:44.090125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def signal_to_encoding(signal_df):\n    dic = {}\n    \n#     print(\"mean calculation started\")\n    dic['mean_x_a'] = np.mean(signal_df['x_a'])\n    dic['mean_y_a'] = np.mean(signal_df['y_a'])\n    dic['mean_z_a'] = np.mean(signal_df['z_a'])\n    dic['mean_x_g'] = np.mean(signal_df['x_g'])\n    dic['mean_y_g'] = np.mean(signal_df['y_g'])\n    dic['mean_z_g'] = np.mean(signal_df['z_g'])\n#     print(\"mean calculation ended\")\n    \n#     print(\"median calculation started\")\n    dic['median_x_a'] = np.median(signal_df['x_a'])\n    dic['median_y_a'] = np.median(signal_df['y_a'])\n    dic['median_z_a'] = np.median(signal_df['z_a'])\n    dic['median_x_g'] = np.median(signal_df['x_g'])\n    dic['median_y_g'] = np.median(signal_df['y_g'])\n    dic['median_z_g'] = np.median(signal_df['z_g'])\n#     print(\"median calculation ended\")\n    \n#     print(\"var calculation started\")\n    dic['var_x_a'] = np.var(signal_df['x_a'])\n    dic['var_y_a'] = np.var(signal_df['y_a'])\n    dic['var_z_a'] = np.var(signal_df['z_a'])\n    dic['var_x_g'] = np.var(signal_df['x_g'])\n    dic['var_y_g'] = np.var(signal_df['y_g'])\n    dic['var_z_g'] = np.var(signal_df['z_g'])\n#     print(\"var calculation ended\")\n    \n#     print(\"avg absolute difference of peaks calculation started\")\n    peaks_x_a, _ = find_peaks(signal_df['x_a'])\n    peaks_y_a, _ = find_peaks(signal_df['y_a'])\n    peaks_z_a, _ = find_peaks(signal_df['z_a'])\n    peaks_x_g, _ = find_peaks(signal_df['x_g'])\n    peaks_y_g, _ = find_peaks(signal_df['y_g'])\n    peaks_z_g, _ = find_peaks(signal_df['z_g'])\n    \n#     print(type(peak_widths(peaks_x_a, signal_df['EMA_x_a'], rel_height=0.5)[0]))\n    dic['aadp_x_a'] = np.mean(peak_widths(signal_df['x_a'], peaks_x_a, rel_height=0.5)[0])\n    dic['aadp_y_a'] = np.mean(peak_widths(signal_df['y_a'], peaks_y_a, rel_height=0.5)[0])\n    dic['aadp_z_a'] = np.mean(peak_widths(signal_df['z_a'], peaks_z_a, rel_height=0.5)[0])\n    dic['aadp_x_g'] = np.mean(peak_widths(signal_df['x_g'], peaks_x_g, rel_height=0.5)[0])\n    dic['aadp_y_g'] = np.mean(peak_widths(signal_df['y_g'], peaks_y_g, rel_height=0.5)[0])\n    dic['aadp_z_g'] = np.mean(peak_widths(signal_df['z_g'], peaks_z_g, rel_height=0.5)[0])\n#     print(\"avg absolute difference of peaks calculation ended\")\n    \n#     print(\"range calculation started\")\n    dic['ptp_x_a'] = np.ptp(signal_df['x_a'])\n    dic['ptp_y_a'] = np.ptp(signal_df['y_a'])\n    dic['ptp_z_a'] = np.ptp(signal_df['z_a'])\n    dic['ptp_x_g'] = np.ptp(signal_df['x_g'])\n    dic['ptp_y_g'] = np.ptp(signal_df['y_g'])\n    dic['ptp_z_g'] = np.ptp(signal_df['z_g'])\n#     print(\"range calculation ended\")\n    \n#     print(\"mode calculation started\")\n    dic['mode_x_a'] = mode(signal_df['x_a'])[0][0]\n    dic['mode_y_a'] = mode(signal_df['y_a'])[0][0]\n    dic['mode_z_a'] = mode(signal_df['z_a'])[0][0]\n    dic['mode_x_g'] = mode(signal_df['x_g'])[0][0]\n    dic['mode_y_g'] = mode(signal_df['y_g'])[0][0]\n    dic['mode_z_g'] = mode(signal_df['z_g'])[0][0]\n#     print(\"mode calculation ended\")\n    \n#     print(\"cov calculation started\")\n    dic['cov_x_a'] = np.cov(signal_df['x_a']) * 1\n    dic['cov_y_a'] = np.cov(signal_df['y_a']) * 1\n    dic['cov_z_a'] = np.cov(signal_df['z_a']) * 1\n    dic['cov_x_g'] = np.cov(signal_df['x_g']) * 1\n    dic['cov_y_g'] = np.cov(signal_df['y_g']) * 1\n    dic['cov_z_g'] = np.cov(signal_df['z_g']) * 1\n#     print(\"cov calculation ended\")\n    \n#     print(\"mean absolute deviation calculation started\")\n    dic['mad_x_a'] = median_absolute_deviation(signal_df['x_a'])\n    dic['mad_y_a'] = median_absolute_deviation(signal_df['y_a'])\n    dic['mad_z_a'] = median_absolute_deviation(signal_df['z_a'])\n    dic['mad_x_g'] = median_absolute_deviation(signal_df['x_g'])\n    dic['mad_y_g'] = median_absolute_deviation(signal_df['y_g'])\n    dic['mad_z_g'] = median_absolute_deviation(signal_df['z_g'])\n#     print(\"mean absolute deviation calculation ended\")\n    \n#     print(\"inter-quartile range calculation started\")\n    dic['iqr_x_a'] = iqr(signal_df['x_a'])\n    dic['iqr_y_a'] = iqr(signal_df['y_a'])\n    dic['iqr_z_a'] = iqr(signal_df['z_a'])\n    dic['iqr_x_g'] = iqr(signal_df['x_g'])\n    dic['iqr_y_g'] = iqr(signal_df['y_g'])\n    dic['iqr_z_g'] = iqr(signal_df['z_g'])\n#     print(\"inter-quartile range calculation ended\")\n    \n#     print(\"correlation calculation started\")\n    dic['correlate_xy_a'] = np.correlate(signal_df['x_a'], signal_df['y_a'])[0]\n    dic['correlate_yz_a'] = np.correlate(signal_df['y_a'], signal_df['z_a'])[0]\n    dic['correlate_xz_a'] = np.correlate(signal_df['x_a'], signal_df['z_a'])[0]\n    dic['correlate_xy_g'] = np.correlate(signal_df['x_g'], signal_df['y_g'])[0]\n    dic['correlate_yz_g'] = np.correlate(signal_df['y_g'], signal_df['z_g'])[0]\n    dic['correlate_xz_g'] = np.correlate(signal_df['x_g'], signal_df['z_g'])[0]\n#     print(\"correlation calculation ended\")\n    \n#     print(\"skew calculation started\")\n    dic['skew_x_a'] = skew(signal_df['x_a'])\n    dic['skew_y_a'] = skew(signal_df['y_a'])\n    dic['skew_z_a'] = skew(signal_df['z_a'])\n    dic['skew_x_g'] = skew(signal_df['x_g'])\n    dic['skew_y_g'] = skew(signal_df['y_g'])\n    dic['skew_z_g'] = skew(signal_df['z_g'])\n#     print(\"skew calculation ended\")\n    \n#     print(\"kurtosis calculation started\")\n    dic['kurtosis_x_a'] = kurtosis(signal_df['x_a'])\n    dic['kurtosis_y_a'] = kurtosis(signal_df['y_a'])\n    dic['kurtosis_z_a'] = kurtosis(signal_df['z_a'])\n    dic['kurtosis_x_g'] = kurtosis(signal_df['x_g'])\n    dic['kurtosis_y_g'] = kurtosis(signal_df['y_g'])\n    dic['kurtosis_z_g'] = kurtosis(signal_df['z_g'])\n#     print(\"kurtosis calculation ended\")\n    \n    \n#     print(\"spectral energy calculation started\")\n    dic['spectral_energy_x_a'] = spectral_energy(signal_df['x_a'])\n    dic['spectral_energy_y_a'] = spectral_energy(signal_df['y_a'])\n    dic['spectral_energy_z_a'] = spectral_energy(signal_df['z_a'])\n    dic['spectral_energy_x_g'] = spectral_energy(signal_df['x_g'])\n    dic['spectral_energy_y_g'] = spectral_energy(signal_df['y_g'])\n    dic['spectral_energy_z_g'] = spectral_energy(signal_df['z_g'])\n#     print(\"spectral energy calculation ended\")\n\n\n#     print(\"spectral entropy calculation started\")\n#     method = 'fft'\n#     normalize = False\n# #     print(signal_df['EMA_x_a'])\n# #     print(signal_df['EMA_x_a'].shape)\n#     axis = -1\n#     dic['spectral_entropy_x_a'] = ant.spectral_entropy(signal_df['EMA_x_a'], sf=len(signal_df['EMA_x_a']), method=method, normalize=normalize, axis=axis)\n#     dic['spectral_entropy_y_a'] = ant.spectral_entropy(signal_df['EMA_y_a'], sf=len(signal_df['EMA_y_a']), method=method, normalize=normalize, axis=axis)\n#     dic['spectral_entropy_z_a'] = ant.spectral_entropy(signal_df['EMA_z_a'], sf=len(signal_df['EMA_z_a']), method=method, normalize=normalize, axis=axis)\n#     dic['spectral_entropy_x_g'] = ant.spectral_entropy(signal_df['EMA_x_g'], sf=len(signal_df['EMA_x_g']), method=method, normalize=normalize, axis=axis)\n#     dic['spectral_entropy_y_g'] = ant.spectral_entropy(signal_df['EMA_y_g'], sf=len(signal_df['EMA_y_g']), method=method, normalize=normalize, axis=axis)\n#     dic['spectral_entropy_z_g'] = ant.spectral_entropy(signal_df['EMA_z_g'], sf=len(signal_df['EMA_z_g']), method=method, normalize=normalize, axis=axis)\n    \n#     print(dic['spectral_entropy_x_a'],\n#               dic['spectral_entropy_y_a'],\n#               dic['spectral_entropy_z_a'],\n#               dic['spectral_entropy_x_g'],\n#               dic['spectral_entropy_y_g'],\n#               dic['spectral_entropy_z_g'])\n#     print(\"spectral entropy calculation ended\")\n\n\n#     print(\"entropy calculation started\")\n    \n#     cols = signal_df[[\"EMA_x_a\", \"EMA_y_a\", \"EMA_z_a\", \"EMA_x_g\", \"EMA_y_g\", \"EMA_z_g\"]]\n#     cols = normalize(cols, norm='l2', axis = 0)\n#     print(cols.sum(axis = 0))\n#     cols = StandardScaler().fit_transform(cols)\n#     p = cols/cols.sum(axis=0)\n#     print(p.sum(axis=0))\n#     print(p.shape)\n#     print(cols.sum(axis=0))\n#     entropy = entr(p).sum(axis=0)\n#     print(entropy.shape)\n#     dic['entropy_x_a'] = entropy[0]\n#     dic['entropy_y_a'] = entropy[1]\n#     dic['entropy_z_a'] = entropy[2]\n#     dic['entropy_x_g'] = entropy[3]\n#     dic['entropy_y_g'] = entropy[4]\n#     dic['entropy_z_g'] = entropy[5]\n#     print(\"entropy calculation ended\")\n    \n#     vector = [dic['mean_x_a'], \n#               dic['mean_y_a'],\n#               dic['mean_z_a'],\n#               dic['mean_x_g'],\n#               dic['mean_y_g'],\n#               dic['mean_z_g'],\n              \n#               dic['median_x_a'],\n#               dic['median_y_a'],\n#               dic['median_z_a'],\n#               dic['median_x_g'],\n#               dic['median_y_g'],\n#               dic['median_z_g'],\n              \n#               dic['var_x_a'],\n#               dic['var_y_a'],\n#               dic['var_z_a'],\n#               dic['var_x_g'],\n#               dic['var_y_g'],\n#               dic['var_z_g'],\n              \n#               dic['aadp_x_a'],\n#               dic['aadp_y_a'],\n#               dic['aadp_z_a'],\n#               dic['aadp_x_g'],\n#               dic['aadp_y_g'],\n#               dic['aadp_z_g'],\n              \n#               dic['ptp_x_a'],\n#               dic['ptp_y_a'],\n#               dic['ptp_z_a'],\n#               dic['ptp_x_g'],\n#               dic['ptp_y_g'],\n#               dic['ptp_z_g'],\n              \n#               dic['mode_x_a'],\n#               dic['mode_y_a'],\n#               dic['mode_z_a'],\n#               dic['mode_x_g'],\n#               dic['mode_y_g'],\n#               dic['mode_z_g'],\n              \n#               dic['cov_x_a'],\n#               dic['cov_y_a'],\n#               dic['cov_z_a'],\n#               dic['cov_x_g'],\n#               dic['cov_y_g'],\n#               dic['cov_z_g'],\n              \n#               dic['mad_x_a'],\n#               dic['mad_y_a'],\n#               dic['mad_z_a'],\n#               dic['mad_x_g'],\n#               dic['mad_y_g'],\n#               dic['mad_z_g'],\n              \n#               dic['iqr_x_a'],\n#               dic['iqr_y_a'],\n#               dic['iqr_z_a'],\n#               dic['iqr_x_g'],\n#               dic['iqr_y_g'],\n#               dic['iqr_z_g'],\n              \n#               dic['correlate_xy_a'],\n#               dic['correlate_yz_a'],\n#               dic['correlate_xz_a'],\n#               dic['correlate_xy_g'],\n#               dic['correlate_yz_g'],\n#               dic['correlate_xz_g'],\n              \n#               dic['skew_x_a'],\n#               dic['skew_y_a'],\n#               dic['skew_z_a'],\n#               dic['skew_x_g'],\n#               dic['skew_y_g'],\n#               dic['skew_z_g'],\n              \n#               dic['kurtosis_x_a'],\n#               dic['kurtosis_y_a'],\n#               dic['kurtosis_z_a'],\n#               dic['kurtosis_x_g'],\n#               dic['kurtosis_y_g'],\n#               dic['kurtosis_z_g'],\n              \n#               dic['spectral_energy_x_a'],\n#               dic['spectral_energy_y_a'],\n#               dic['spectral_energy_z_a'],\n#               dic['spectral_energy_x_g'],\n#               dic['spectral_energy_y_g'],\n#               dic['spectral_energy_z_g'],\n              \n#               dic['spectral_entropy_x_a'],\n#               dic['spectral_entropy_y_a'],\n#               dic['spectral_entropy_z_a'],\n#               dic['spectral_entropy_x_g'],\n#               dic['spectral_entropy_y_g'],\n#               dic['spectral_entropy_z_g']\n#              ]\n    \n#     types_dict = {\n#               'mean_x_a': 'float64', \n#               'mean_y_a': 'float64',\n#               'mean_z_a': 'float64',\n#               'mean_x_g': 'float64',\n#               'mean_y_g': 'float64',\n#               'mean_z_g': 'float64',\n              \n#               'median_x_a': 'float64',\n#               'median_y_a': 'float64',\n#               'median_z_a': 'float64',\n#               'median_x_g': 'float64',\n#               'median_y_g': 'float64',\n#               'median_z_g': 'float64',\n              \n#               'var_x_a': 'float64',\n#               'var_y_a': 'float64',\n#               'var_z_a': 'float64',\n#               'var_x_g': 'float64',\n#               'var_y_g': 'float64',\n#               'var_z_g': 'float64',\n              \n#               'aadp_x_a': 'float64',\n#               'aadp_y_a': 'float64',\n#               'aadp_z_a': 'float64',\n#               'aadp_x_g': 'float64',\n#               'aadp_y_g': 'float64',\n#               'aadp_z_g': 'float64',\n              \n#               'ptp_x_a': 'float64',\n#               'ptp_y_a': 'float64',\n#               'ptp_z_a': 'float64',\n#               'ptp_x_g': 'float64',\n#               'ptp_y_g': 'float64',\n#               'ptp_z_g': 'float64',\n              \n#               'mode_x_a': 'float64',\n#               'mode_y_a': 'float64',\n#               'mode_z_a': 'float64',\n#               'mode_x_g': 'float64',\n#               'mode_y_g': 'float64',\n#               'mode_z_g': 'float64',\n              \n#               'cov_x_a': 'float64',\n#               'cov_y_a': 'float64',\n#               'cov_z_a': 'float64',\n#               'cov_x_g': 'float64',\n#               'cov_y_g': 'float64',\n#               'cov_z_g': 'float64',\n              \n#               'mad_x_a': 'float64',\n#               'mad_y_a': 'float64',\n#               'mad_z_a': 'float64',\n#               'mad_x_g': 'float64',\n#               'mad_y_g': 'float64',\n#               'mad_z_g': 'float64',\n              \n#               'iqr_x_a': 'float64',\n#               'iqr_y_a': 'float64',\n#               'iqr_z_a': 'float64',\n#               'iqr_x_g': 'float64',\n#               'iqr_y_g': 'float64',\n#               'iqr_z_g': 'float64',\n              \n#               'correlate_xy_a': 'float64',\n#               'correlate_yz_a': 'float64',\n#               'correlate_xz_a': 'float64',\n#               'correlate_xy_g': 'float64',\n#               'correlate_yz_g': 'float64',\n#               'correlate_xz_g': 'float64',\n              \n#               'skew_x_a': 'float64',\n#               'skew_y_a': 'float64',\n#               'skew_z_a': 'float64',\n#               'skew_x_g': 'float64',\n#               'skew_y_g': 'float64',\n#               'skew_z_g': 'float64',\n              \n#               'kurtosis_x_a': 'float64',\n#               'kurtosis_y_a': 'float64',\n#               'kurtosis_z_a': 'float64',\n#               'kurtosis_x_g': 'float64',\n#               'kurtosis_y_g': 'float64',\n#               'kurtosis_z_g': 'float64',\n              \n#               'spectral_energy_x_a': 'float64',\n#               'spectral_energy_y_a': 'float64',\n#               'spectral_energy_z_a': 'float64',\n#               'spectral_energy_x_g': 'float64',\n#               'spectral_energy_y_g': 'float64',\n#               'spectral_energy_z_g': 'float64'\n#             }\n\n    \n    return dic\n\n\ndef rolling_window(a, window, stride):\n    shape = a.shape[:-1] + (int((a.shape[-1] - window)/stride + 1), window)\n    strides = (stride*a.strides[-1],) + (a.strides[-1],)\n    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n\n\n\ndef getIndices2(sampleSize=1000, step=1000, numSamplePoints=24000):\n    indices = np.arange(0, numSamplePoints, 1)\n    indices = rolling_window(indices, sampleSize, step)\n    \n    return indices\n\n\ndef getEncodingArray(df, windows):\n    encoding_lst = []\n    for i in range(len(windows)):\n        # replaced loc with iloc per documentation\n#         a.append(signal_to_encoding(df.loc[windows[i], :])[1])\n        encoding_dict = signal_to_encoding(df.iloc[windows[i], :])\n        encoding_lst.append(list(encoding_dict.values()))\n        \n    return np.array(encoding_lst, dtype=np.float64), list(encoding_dict.keys())\n\n\ndef MultipleSeriesToDf(df_of_series):\n    \n    x_a = df_of_series[\"x_a\"].to_frame()\n    y_a = df_of_series[\"y_a\"].to_frame()\n    z_a = df_of_series[\"z_a\"].to_frame()\n    x_g = df_of_series[\"x_g\"].to_frame()\n    y_g = df_of_series[\"y_g\"].to_frame()\n    z_g = df_of_series[\"z_g\"].to_frame()\n    \n    return pd.concat([x_a, y_a, z_a, x_g, y_g, z_g], axis=1)\n    \ndef PlotCorrelatoinMatrices(users_df, window_size = 250, step = 1000, numSamplePoints= 18000):\n    \n    windows = getIndices2(sampleSize=window_size, step=step, numSamplePoints= numSamplePoints)\n    \n    print(len(users_df))\n    total_corr = 0.0\n    for user_idx in range(len(users_df)):\n        \n        raw_df = MultipleSeriesToDf(users_df.iloc[user_idx])\n        encoding_array, column_names = getEncodingArray(raw_df, windows)\n\n\n        encoding_df = pd.DataFrame(encoding_array, columns = column_names, dtype=np.float64)\n        total_corr += encoding_df.corr()\n        \n#         plt.subplots(figsize=(20,15))\n\n    mean_corr = total_corr/len(users_df)\n    print(f\"mean_corr: {mean_corr.sum().sum()}, windows shape: {windows.shape}, windows last rows: {windows[-5:]}\" )\n\n    windows = getIndices2(sampleSize=window_size, step=step, numSamplePoints= numSamplePoints*len(users_df))\n#     column_names = [\"x_a\", \"y_a\", \"z_a\", \"x_g\", \"y_g\", \"z_g\"]\n    cum_raw_df_lst = []\n    for user_idx in range(len(users_df)):\n        raw_df = MultipleSeriesToDf(users_df.iloc[user_idx])[:numSamplePoints]\n        cum_raw_df_lst.append(raw_df)\n        \n    cum_raw_df = pd.concat(cum_raw_df_lst, axis=0).reset_index(drop=True)\n\n    cum_encoding_array, column_names = getEncodingArray(cum_raw_df, windows)\n    cum_encoding_df = pd.DataFrame(cum_encoding_array, columns = column_names, dtype=np.float64)\n    cum_corr = cum_encoding_df.corr()\n    \n#     print(windows[70:73])\n    print(f\"mean_corr: {cum_corr.sum().sum()}, windows shape: {windows.shape}, windows last rows: {windows[-5:]}, cum_encoding_df shape: {cum_encoding_df.shape}\" )\n#     sns.heatmap(corr)\n\n    total_corr = 0.0\n    for i in range(len(users_df)):\n        df = cum_encoding_df[i*numSamplePoints: (i+1) * numSamplePoints]\n        \n        total_corr += df.corr()\n        \n    print(f\"method 3 corr: {total_corr/len(users_df)}\")\n        \n    return cum_corr, mean_corr, cum_encoding_df\n    \n#     for col, col_type in types_dict.items():\n#         df[col] = df[col].astype(col_type)\n#     f, axes = plt.subplots(3, 2, sharex=\"col\", sharey=\"col\", dpi=180, figsize=(6, 4))\n#     f.subplots_adjust(hspace=0.6, wspace=0.2)\n#     cmap = cm.get_cmap(\"tab10\")\n    \n#     for m in range(len(dfList)):\n        \n        \n#         encoding_array = getEncodingArray(dfList[m], windows)\n\n#         for i, col in enumerate(plot_cols.ravel()):\n#             plot_column = i % 2\n#             plot_row = i // 2\n#             print(plot_column, col)\n#             print(plot_row, plot_column)\n#             g = sns.distplot(\n#                 data[plot_column][col],\n#                 kde=False,\n#                 ax=axes[plot_row][plot_column],\n#                 color=cmap(plot_column),\n#                 hist_kws=dict(alpha=1),\n#             )\n#             g.set_title(f\"{col}\")\n#             g.set_yscale(\"log\")\n#             g.axes.set_xlabel(\"\")\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:35:45.443145Z","iopub.execute_input":"2021-10-20T14:35:45.443847Z","iopub.status.idle":"2021-10-20T14:35:45.52428Z","shell.execute_reply.started":"2021-10-20T14:35:45.443794Z","shell.execute_reply":"2021-10-20T14:35:45.52322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"window_size = 250\nstep = 250\nnumSamplePoints= 18000\nwindows = getIndices2(sampleSize=window_size, step=step, numSamplePoints= numSamplePoints)\nwindows.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-19T23:13:42.390582Z","iopub.execute_input":"2021-10-19T23:13:42.390979Z","iopub.status.idle":"2021-10-19T23:13:42.398683Z","shell.execute_reply.started":"2021-10-19T23:13:42.390934Z","shell.execute_reply":"2021-10-19T23:13:42.397714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"72*250","metadata":{"execution":{"iopub.status.busy":"2021-10-19T21:25:11.539207Z","iopub.execute_input":"2021-10-19T21:25:11.539789Z","iopub.status.idle":"2021-10-19T21:25:11.544929Z","shell.execute_reply.started":"2021-10-19T21:25:11.539755Z","shell.execute_reply":"2021-10-19T21:25:11.544134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"window_size = 250\nstep = 250\nnumSamplePoints= 18000\ncc, mc, cum_encoding_df= PlotCorrelatoinMatrices(exp1_user_df, window_size = window_size, step = step, numSamplePoints= numSamplePoints)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T15:01:44.935319Z","iopub.execute_input":"2021-10-20T15:01:44.935702Z","iopub.status.idle":"2021-10-20T15:03:07.475421Z","shell.execute_reply.started":"2021-10-20T15:01:44.935669Z","shell.execute_reply":"2021-10-20T15:03:07.474491Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,15))\ntitle='mean corr for all samples of exp 1'\nprint(title)\nsns.heatmap(mc, cmap=\"PiYG\", vmin=-1, vmax=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T15:03:07.477971Z","iopub.execute_input":"2021-10-20T15:03:07.478307Z","iopub.status.idle":"2021-10-20T15:03:09.666176Z","shell.execute_reply.started":"2021-10-20T15:03:07.478274Z","shell.execute_reply":"2021-10-20T15:03:09.665081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,15))\ntitle ='concatenated corr for all samples of exp 1'\nprint(title)\nsns.heatmap(cc, cmap=\"PiYG\", vmin=-1, vmax=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T15:03:09.667562Z","iopub.execute_input":"2021-10-20T15:03:09.667903Z","iopub.status.idle":"2021-10-20T15:03:11.655488Z","shell.execute_reply.started":"2021-10-20T15:03:09.667871Z","shell.execute_reply":"2021-10-20T15:03:11.654279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"window_size = 250\nstep = 250\nnumSamplePoints= 18000\nexclude_lst = [9, 10, 11, 12, 13, 14 , 15, 16, 17, 18, 20, 23, 24, 25, 30, 31, 32, 34]\ngood_samples_df = exp1_user_df[~exp1_user_df.user_id.isin(exclude_lst)].reset_index(drop=True)\ncc, mc, cum_encoding_df= PlotCorrelatoinMatrices(good_samples_df, window_size = window_size, step = step, numSamplePoints= numSamplePoints)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T15:03:11.657228Z","iopub.execute_input":"2021-10-20T15:03:11.657619Z","iopub.status.idle":"2021-10-20T15:04:04.814845Z","shell.execute_reply.started":"2021-10-20T15:03:11.65758Z","shell.execute_reply":"2021-10-20T15:04:04.813652Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,15))\ntitle='mean corr for good samples of exp 1'\nprint(title)\nsns.heatmap(mc, cmap=\"PiYG\", vmin=-1, vmax=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T15:04:04.816838Z","iopub.execute_input":"2021-10-20T15:04:04.817133Z","iopub.status.idle":"2021-10-20T15:04:06.795359Z","shell.execute_reply.started":"2021-10-20T15:04:04.817104Z","shell.execute_reply":"2021-10-20T15:04:06.794288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,15))\ntitle ='concatenated corr for good samples of exp 1'\nprint(title)\nsns.heatmap(cc, cmap=\"PiYG\", vmin=-1, vmax=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T15:04:06.797035Z","iopub.execute_input":"2021-10-20T15:04:06.797342Z","iopub.status.idle":"2021-10-20T15:04:08.779175Z","shell.execute_reply.started":"2021-10-20T15:04:06.797311Z","shell.execute_reply":"2021-10-20T15:04:08.777963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_corr = 0.0\nwin_idx = numSamplePoints//window_size\nprint(win_idx)\nfor i in range(len(exp1_user_df)):\n    df = cum_encoding_df[i*win_idx: (i+1) * win_idx]\n\n    total_corr += df.corr()\n\n(total_corr/48).sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T21:40:43.691796Z","iopub.execute_input":"2021-10-19T21:40:43.692216Z","iopub.status.idle":"2021-10-19T21:40:43.791116Z","shell.execute_reply.started":"2021-10-19T21:40:43.692178Z","shell.execute_reply":"2021-10-19T21:40:43.789682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"windows = getIndices2(sampleSize=window_size, step=step, numSamplePoints= numSamplePoints)\nraw_df = MultipleSeriesToDf(exp1_user_df.iloc[47])\nencoding_array, column_names = getEncodingArray(raw_df, windows)\n\nencoding_df = pd.DataFrame(encoding_array, columns = column_names, dtype=np.float64)\nencoding_df.sum().sum()\n# encoding_df.corr()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T21:33:31.122814Z","iopub.execute_input":"2021-10-19T21:33:31.123468Z","iopub.status.idle":"2021-10-19T21:33:32.007375Z","shell.execute_reply.started":"2021-10-19T21:33:31.123427Z","shell.execute_reply":"2021-10-19T21:33:32.006392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cum_encoding_df.corr().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T21:41:45.774995Z","iopub.execute_input":"2021-10-19T21:41:45.775455Z","iopub.status.idle":"2021-10-19T21:41:45.831062Z","shell.execute_reply.started":"2021-10-19T21:41:45.775421Z","shell.execute_reply":"2021-10-19T21:41:45.829876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = cum_encoding_df.head(72)","metadata":{"execution":{"iopub.status.busy":"2021-10-19T21:45:45.350495Z","iopub.execute_input":"2021-10-19T21:45:45.351095Z","iopub.status.idle":"2021-10-19T21:45:45.3558Z","shell.execute_reply.started":"2021-10-19T21:45:45.351061Z","shell.execute_reply":"2021-10-19T21:45:45.354714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b = cum_encoding_df[1*72:2*72]","metadata":{"execution":{"iopub.status.busy":"2021-10-19T21:45:45.868893Z","iopub.execute_input":"2021-10-19T21:45:45.869273Z","iopub.status.idle":"2021-10-19T21:45:45.874234Z","shell.execute_reply.started":"2021-10-19T21:45:45.869241Z","shell.execute_reply":"2021-10-19T21:45:45.873448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"((a.corr() + b.corr())/2).sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T21:45:46.430924Z","iopub.execute_input":"2021-10-19T21:45:46.431278Z","iopub.status.idle":"2021-10-19T21:45:46.44215Z","shell.execute_reply.started":"2021-10-19T21:45:46.431246Z","shell.execute_reply":"2021-10-19T21:45:46.441495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cum_encoding_df[:72*2].corr().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T21:45:47.735428Z","iopub.execute_input":"2021-10-19T21:45:47.736061Z","iopub.status.idle":"2021-10-19T21:45:47.745826Z","shell.execute_reply.started":"2021-10-19T21:45:47.736026Z","shell.execute_reply":"2021-10-19T21:45:47.744803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp1_user_df[0:1]","metadata":{"execution":{"iopub.status.busy":"2021-10-19T21:06:06.007699Z","iopub.execute_input":"2021-10-19T21:06:06.008447Z","iopub.status.idle":"2021-10-19T21:06:06.046956Z","shell.execute_reply.started":"2021-10-19T21:06:06.008396Z","shell.execute_reply":"2021-10-19T21:06:06.045906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = exp1_user_df.iloc[0]\nx_a = df[\"x_a\"].to_frame()\ny_a = df[\"y_a\"].to_frame()\nz_a = df[\"z_a\"].to_frame()\nx_g = df[\"x_g\"].to_frame()\ny_g = df[\"y_g\"].to_frame()\nz_g = df[\"z_g\"].to_frame()\ndf = pd.concat([x_a, y_a, z_a, x_g, y_g, z_g], axis=1)\ndic = signal_to_encoding(df.iloc[windows[0], :])\n(list(dic.values()) == c.iloc[0]).all()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T19:46:12.942316Z","iopub.execute_input":"2021-10-18T19:46:12.942713Z","iopub.status.idle":"2021-10-18T19:46:12.971787Z","shell.execute_reply.started":"2021-10-18T19:46:12.942679Z","shell.execute_reply":"2021-10-18T19:46:12.970528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.array(list(dic.values())).sum()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T19:38:33.158139Z","iopub.execute_input":"2021-10-18T19:38:33.158615Z","iopub.status.idle":"2021-10-18T19:38:33.166548Z","shell.execute_reply.started":"2021-10-18T19:38:33.158584Z","shell.execute_reply":"2021-10-18T19:38:33.165298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_df[[\"x_a\", \"y_a\", \"z_a\"]]","metadata":{"execution":{"iopub.status.busy":"2021-10-19T23:32:58.633474Z","iopub.execute_input":"2021-10-19T23:32:58.634294Z","iopub.status.idle":"2021-10-19T23:32:58.650231Z","shell.execute_reply.started":"2021-10-19T23:32:58.634234Z","shell.execute_reply":"2021-10-19T23:32:58.649099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp1_user_df.iloc[45]['chars_per_second']*5*2","metadata":{"execution":{"iopub.status.busy":"2021-10-20T01:19:35.933199Z","iopub.execute_input":"2021-10-20T01:19:35.933583Z","iopub.status.idle":"2021-10-20T01:19:35.941073Z","shell.execute_reply.started":"2021-10-20T01:19:35.933551Z","shell.execute_reply":"2021-10-20T01:19:35.94026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df","metadata":{"execution":{"iopub.status.busy":"2021-10-20T02:30:40.431555Z","iopub.execute_input":"2021-10-20T02:30:40.43196Z","iopub.status.idle":"2021-10-20T02:30:40.448589Z","shell.execute_reply.started":"2021-10-20T02:30:40.431924Z","shell.execute_reply":"2021-10-20T02:30:40.447723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"window_size=250\nstep=50\nnumSamplePoints=18000\nwindows = getIndices2(sampleSize=window_size, step=step, numSamplePoints= numSamplePoints)\n\nr = [44, 45, 46, 47]\nuser_min_max_dict = {}\nfor user_idx in r:\n    \n    new_df = pd.DataFrame([], columns = ['Mag_a'], dtype=np.float64)    \n    raw_df = MultipleSeriesToDf(exp1_user_df.iloc[user_idx])\n    new_df[\"Mag_a\"] = np.sqrt(raw_df[\"x_a\"]**2 + raw_df[\"y_a\"]**2 + raw_df[\"z_a\"]**2)\n    new_df[\"Mag_g\"] = np.sqrt(raw_df[\"x_g\"]**2 + raw_df[\"y_g\"]**2 + raw_df[\"z_g\"]**2)\n\n\n    num_local_min_lst = []\n    num_local_max_lst = []\n\n    for i in range(len(windows)):\n        mag_g_window = new_df.iloc[windows[i], :][\"Mag_g\"].to_numpy()\n        minimums = signal.argrelextrema(mag_g_window, np.less)\n        maximums = signal.argrelextrema(mag_g_window, np.greater)\n        num_local_min_lst.append(len(minimums[0]))\n        num_local_max_lst.append(len(maximums[0]))\n\n    user_min_max_dict[user_idx] = np.array(num_local_min_lst), np.array(num_local_max_lst)\n\n\ndf = pd.DataFrame([min_arr for min_arr, max_arr in user_min_max_dict.values()]).T\ndf.columns = list(user_min_max_dict.keys())","metadata":{"execution":{"iopub.status.busy":"2021-10-20T03:12:23.610454Z","iopub.execute_input":"2021-10-20T03:12:23.610899Z","iopub.status.idle":"2021-10-20T03:12:24.255775Z","shell.execute_reply.started":"2021-10-20T03:12:23.610859Z","shell.execute_reply":"2021-10-20T03:12:24.254612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.plot(figsize=(24, 12))","metadata":{"execution":{"iopub.status.busy":"2021-10-20T03:12:24.258586Z","iopub.execute_input":"2021-10-20T03:12:24.259096Z","iopub.status.idle":"2021-10-20T03:12:24.56195Z","shell.execute_reply.started":"2021-10-20T03:12:24.259044Z","shell.execute_reply":"2021-10-20T03:12:24.560584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame([num_local_min_lst, num_local_max_lst], )\ndf.T.plot()","metadata":{"execution":{"iopub.status.busy":"2021-10-20T02:42:09.627791Z","iopub.execute_input":"2021-10-20T02:42:09.628399Z","iopub.status.idle":"2021-10-20T02:42:09.808368Z","shell.execute_reply.started":"2021-10-20T02:42:09.628351Z","shell.execute_reply":"2021-10-20T02:42:09.807232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r = [1,2,3]\n\nuser_id =44\nnew_df = pd.DataFrame([], columns = ['Mag_a'], dtype=np.float64)    \nraw_df = MultipleSeriesToDf(exp1_user_df.iloc[user_id])\nnew_df[\"Mag_a\"] = np.sqrt(raw_df[\"x_a\"]**2 + raw_df[\"y_a\"]**2 + raw_df[\"z_a\"]**2)\nnew_df[\"Mag_g\"] = np.sqrt(raw_df[\"x_g\"]**2 + raw_df[\"y_g\"]**2 + raw_df[\"z_g\"]**2)\nnew_df\nprint((np.linalg.norm(raw_df[[\"x_a\", \"y_a\", \"z_a\"]], axis = 1) == new_df[\"Mag_a\"].to_numpy().flatten()).all())\nprint((np.linalg.norm(raw_df[[\"x_g\", \"y_g\", \"z_g\"]], axis = 1) == new_df[\"Mag_g\"].to_numpy().flatten()).all())\nnew_df.plot(figsize=(24,12))","metadata":{"execution":{"iopub.status.busy":"2021-10-20T03:13:31.519208Z","iopub.execute_input":"2021-10-20T03:13:31.519627Z","iopub.status.idle":"2021-10-20T03:13:32.020206Z","shell.execute_reply.started":"2021-10-20T03:13:31.519588Z","shell.execute_reply":"2021-10-20T03:13:32.019191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df[begin:end]['Mag_g'].hist(bins=100, figsize=(24,12))","metadata":{"execution":{"iopub.status.busy":"2021-10-20T03:17:26.426723Z","iopub.execute_input":"2021-10-20T03:17:26.427604Z","iopub.status.idle":"2021-10-20T03:17:27.040362Z","shell.execute_reply.started":"2021-10-20T03:17:26.427535Z","shell.execute_reply":"2021-10-20T03:17:27.039255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq = 100\nmins_per_stroke = 2\nbegin, end = 2000, 2500\nchunk = new_df[begin:end]\nchunk.plot(figsize=(24,12))\nmag_g_arr = chunk[\"Mag_g\"].to_numpy()\nminimums = signal.argrelextrema(mag_g_arr, np.less)\nmaximums = signal.argrelextrema(mag_g_arr, np.greater)\nprint(f\"local min: {minimums}, local max: {maximums}\")\nprint(f\"num local min: {len(minimums[0])}, num local max: {len(maximums[0])}\")\nprint(f\"char per sec: {exp1_user_df.iloc[user_id]['chars_per_second']}, estimated mins: {exp1_user_df.iloc[user_id]['chars_per_second']*((end-begin)/freq)*mins_per_stroke}\")\nprint((new_df[begin:end]['Mag_g']<0.045).sum())","metadata":{"execution":{"iopub.status.busy":"2021-10-20T03:15:41.862911Z","iopub.execute_input":"2021-10-20T03:15:41.863344Z","iopub.status.idle":"2021-10-20T03:15:42.129343Z","shell.execute_reply.started":"2021-10-20T03:15:41.863302Z","shell.execute_reply":"2021-10-20T03:15:42.1283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.pi","metadata":{"execution":{"iopub.status.busy":"2021-10-20T01:15:26.7462Z","iopub.execute_input":"2021-10-20T01:15:26.746676Z","iopub.status.idle":"2021-10-20T01:15:26.753539Z","shell.execute_reply.started":"2021-10-20T01:15:26.746638Z","shell.execute_reply":"2021-10-20T01:15:26.752412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = df['x_g'].to_frame()\ndf2 = df[\"y_g\"].to_frame()\ndf_all = pd.concat([df1, df2 ], axis=1)\n(df_all['x_g'] == df[\"x_g\"]).all()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T19:04:54.252818Z","iopub.execute_input":"2021-10-18T19:04:54.253236Z","iopub.status.idle":"2021-10-18T19:04:54.267176Z","shell.execute_reply.started":"2021-10-18T19:04:54.253202Z","shell.execute_reply":"2021-10-18T19:04:54.265659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PlotCorrelatoinMatrices(exp1_user_df, window_size = 250, step = 1000, numSamplePoints= 18000)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T23:14:51.257833Z","iopub.execute_input":"2021-10-17T23:14:51.258201Z","iopub.status.idle":"2021-10-17T23:14:51.30691Z","shell.execute_reply.started":"2021-10-17T23:14:51.258171Z","shell.execute_reply":"2021-10-17T23:14:51.304994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_text_data = extractTextFromDocx(f'../input/wearable-assisted-ca/user{49}_{1}.docx')\n    \nprint(numberOfWords(user_text_data))\nprint(numberOfChars(user_text_data))\nprint((115/254.278653184)*60)\nprint((595/254.278653184))","metadata":{"execution":{"iopub.status.busy":"2021-10-13T17:39:27.537167Z","iopub.execute_input":"2021-10-13T17:39:27.537918Z","iopub.status.idle":"2021-10-13T17:39:27.559499Z","shell.execute_reply.started":"2021-10-13T17:39:27.537822Z","shell.execute_reply":"2021-10-13T17:39:27.557877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(exp1_user_df[\"x_a\"])","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:27:44.672802Z","iopub.execute_input":"2021-10-13T13:27:44.673182Z","iopub.status.idle":"2021-10-13T13:27:44.680742Z","shell.execute_reply.started":"2021-10-13T13:27:44.673147Z","shell.execute_reply":"2021-10-13T13:27:44.679539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(flatten_series_of_series(exp1_user_df[\"x_a\"]))","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:27:44.682633Z","iopub.execute_input":"2021-10-13T13:27:44.683099Z","iopub.status.idle":"2021-10-13T13:27:44.780752Z","shell.execute_reply.started":"2021-10-13T13:27:44.683053Z","shell.execute_reply":"2021-10-13T13:27:44.779606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Overall Sensor Data Distribution","metadata":{}},{"cell_type":"code","source":"def flatten_series_of_series(s):\n    arr = s.values.flatten()\n    lst = []\n    for s in arr:\n        lst.append(s.to_numpy())\n    \n    return np.concatenate(lst)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T22:39:48.72532Z","iopub.execute_input":"2021-10-17T22:39:48.725864Z","iopub.status.idle":"2021-10-17T22:39:48.731932Z","shell.execute_reply.started":"2021-10-17T22:39:48.725821Z","shell.execute_reply":"2021-10-17T22:39:48.73075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp1_user_df\nplot_rows = [[\"x_a\", \"y_a\", \"z_a\"],\n            [\"x_g\", \"y_g\", \"z_g\"]]\nplot_rows = np.array(plot_rows)\nplot_cols = plot_rows.T\n\nf, axes = plt.subplots(3, 2, sharex=\"col\", sharey=\"col\", dpi=180, figsize=(6, 4))\nf.subplots_adjust(hspace=0.6, wspace=0.2)\ncmap = cm.get_cmap(\"tab10\")\n\nfor i, col in enumerate(plot_cols.ravel()):\n    plot_column = i % 2\n    plot_row = i // 2\n    print(plot_column, col)\n    print(plot_row, plot_column)\n\n    g = sns.distplot(\n        flatten_series_of_series(exp1_user_df[col]),\n        kde=False,\n        ax=axes[plot_row][plot_column],\n        color=cmap(plot_column),\n        hist_kws=dict(alpha=1),\n    )\n    g.set_title(f\"{col}\")\n    g.set_yscale(\"log\")\n    g.axes.set_xlabel(\"\")","metadata":{"execution":{"iopub.status.busy":"2021-10-17T22:39:49.588039Z","iopub.execute_input":"2021-10-17T22:39:49.588423Z","iopub.status.idle":"2021-10-17T22:39:52.773719Z","shell.execute_reply.started":"2021-10-17T22:39:49.588382Z","shell.execute_reply":"2021-10-17T22:39:52.772392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_cols = [\"x_a\", \"y_a\", \"z_a\", \"x_g\", \"y_g\", \"z_g\"]\nfor i, col in enumerate(plot_cols):\n    print(i, col)\n    plt.figure(i, figsize=(20, 30))\n    g = sns.boxplot(\n        y=\"user_id\",\n        x=col,\n        data=exp1_user_df,\n        orient=\"h\",\n        fliersize=2,\n        color=\"tab:blue\",\n        saturation=1,\n    )\n    g.set_title(f'Distribution of \"{col}\" per Subject')\n    g.axes.set_xlabel(\"\")\n    g.axes.xaxis.grid(True)\n    g.axes.yaxis.grid(True)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T22:39:52.776156Z","iopub.execute_input":"2021-10-17T22:39:52.776624Z","iopub.status.idle":"2021-10-17T22:40:00.608714Z","shell.execute_reply.started":"2021-10-17T22:39:52.776578Z","shell.execute_reply":"2021-10-17T22:40:00.607685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Partial plot for use in thesis\nsubjects = [\n    1,\n    8,\n    10\n]\n# df_temp = hmog.all[hmog.all[\"subject\"].isin(subjects)].copy(deep=True)\n# df_temp[\"Subject\"] = df_temp[\"subject\"].cat.remove_unused_categories()\n# df_temp = df_temp.rename(columns={\"mag_z\": \"$Mag_z$\"})\n\n# plt.figure(dpi=180, figsize=(5.473, 2))\n# g = sns.boxplot(\n#     y=\"Subject\",\n#     x=\"$Mag_z$\",\n#     data=df_temp,\n#     orient=\"h\",\n#     **utils_boxplot_style\n# )\n# g.axes.xaxis.grid(True)\n# g.axes.yaxis.grid(True)\n\nplot_cols = [\"x_a\", \"y_a\", \"z_a\", \"x_g\", \"y_g\", \"z_g\"]\nfor i, col in enumerate(plot_cols):\n    print(i, col)\n    plt.figure(i, dpi=180, figsize=(20, 1.5))\n    g = sns.boxplot(\n        y=\"user_id\",\n        x=col,\n        data=exp1_user_df[exp1_user_df['user_id'].isin(subjects)],\n        orient=\"h\",\n        fliersize=2,\n        color=\"tab:blue\",\n        saturation=1,\n    )\n    g.set_title(f'Distribution of \"{col}\" per Subject')\n    g.axes.set_xlabel(\"\")\n    g.axes.xaxis.grid(True)\n    g.axes.yaxis.grid(True)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T22:40:00.610867Z","iopub.execute_input":"2021-10-17T22:40:00.611228Z","iopub.status.idle":"2021-10-17T22:40:02.316381Z","shell.execute_reply.started":"2021-10-17T22:40:00.611192Z","shell.execute_reply":"2021-10-17T22:40:02.315278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_rows = [\n    [\"EMA_x_a\", \"EMA_y_a\", \"EMA_z_a\"],\n    [\"EMA_x_g\", \"EMA_y_g\", \"EMA_z_g\"],\n]\n\nf, axes = plt.subplots(3, 1, dpi=180, figsize=(5.473, 2))\nfor i, row in enumerate(plot_rows):\n    g = df_single_session[row].plot(linewidth=0.3, ax=axes[i], sharex=\"col\")\n    g.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.0)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T00:16:26.3721Z","iopub.execute_input":"2021-10-13T00:16:26.372537Z","iopub.status.idle":"2021-10-13T00:16:26.708279Z","shell.execute_reply.started":"2021-10-13T00:16:26.372504Z","shell.execute_reply":"2021-10-13T00:16:26.707201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_exp1 = data_exp1['df']\ndf_exp2 = data_exp2['df']","metadata":{"execution":{"iopub.status.busy":"2021-10-13T00:13:10.496359Z","iopub.status.idle":"2021-10-13T00:13:10.496908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"begin, end = 0, -1\ng = -9.806\n\ndata_exp_dict = {}\nfor user_idx in range(len(r)):\n    \n    # load data \n    data_exp1 = getDataStats1(r[user_idx], begin_idx=begin, end_idx=end)\n    data_exp2 = getDataStats2(r[user_idx], begin_idx=begin, end_idx=end)\n\n    accel = data_exp1['accel']\n    # Calculate length of accel vector in g-force\n    accel['raw_length'] = np.sqrt(accel.x**2 + accel.y**2 + accel.z**2)/g\n    accel['smooth_length'] = np.sqrt(accel.EMA_x_a**2 + accel.EMA_y_a**2 + accel.EMA_z_a**2)/g\n    \n    accel = data_exp2['accel']\n    # Calculate length of accel vector in g-force\n    accel['raw_length'] = np.sqrt(accel.x**2 + accel.y**2 + accel.z**2)/g\n    accel['smooth_length'] = np.sqrt(accel.EMA_x_a**2 + accel.EMA_y_a**2 + accel.EMA_z_a**2)/g\n    \n    data_exp_dict[r[user_idx]] = (data_exp1['accel'], data_exp2['accel'])\n\n    \nfig, axs =plt.subplots(len(r), 2, figsize=(30, len(r) *  8))\n\nfig.suptitle('g-force through time, left raw, right smooth')\n\nfor user_idx in range(len(r)):\n    \n    accel_exp1 = data_exp_dict[r[user_idx]][0]\n    \n    sns.lineplot(x=accel_exp1[\"time_stamp\"], y=accel_exp1['raw_length'], label = 'raw_length', ax=axs[user_idx, 0])\n    sns.lineplot(x=accel_exp1[\"time_stamp\"], y=accel_exp1['smooth_length'], label = 'smooth_length', ax=axs[user_idx, 0])\n\n    \n    accel_exp2 = data_exp_dict[r[user_idx]][1]\n    \n    sns.lineplot(x=accel_exp2[\"time_stamp\"], y=accel_exp2['raw_length'], label = 'raw_length', ax=axs[user_idx, 1])\n    sns.lineplot(x=accel_exp2[\"time_stamp\"], y=accel_exp2['smooth_length'], label = 'smooth_length', ax=axs[user_idx, 1])\n\n\n    axs[user_idx, 0].set_title(f'user: {r[user_idx]}, exp: {1}')\n    axs[user_idx, 1].set_title(f'user: {r[user_idx]}, exp: {2}')\n\n\nfor ax in axs.flat:\n    ax.set(xlabel='timestamp', ylabel='g-force')\n\n# # Hide x labels and tick labels for top plots and y ticks for right plots.\n# for ax in axs.flat:\n#     ax.label_outer()","metadata":{"execution":{"iopub.status.busy":"2021-10-08T14:06:14.464339Z","iopub.status.idle":"2021-10-08T14:06:14.465059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_exp1","metadata":{"execution":{"iopub.status.busy":"2021-10-08T14:06:14.466577Z","iopub.status.idle":"2021-10-08T14:06:14.467261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"begin, end = 0, -1\n\naccel_graph = data_exp1['accel'][begin:end]\n\nfig, ax =plt.subplots(1,2, figsize=(30,8))\n\nax[0].set_title('accel x vs smooth x')\nsns.lineplot(x=accel_graph[\"time_stamp\"], y=accel_graph['x'], label = 'x', ax = ax[0])\nsns.lineplot(x=accel_graph[\"time_stamp\"], y=accel_graph['EMA_x_a'], label = 'EMA_x_a', ax = ax[0])\nax[0].set_ylabel('accel x')\n\n\ngyro_graph = data_exp1[\"gyro\"][begin:end]\nax[1].set_title('gyro x vs smooth x')\n\nsns.lineplot(x=gyro_graph[\"time_stamp\"], y=gyro_graph['x'], label = 'x', ax = ax[1])\nsns.lineplot(x=gyro_graph[\"time_stamp\"], y=gyro_graph['EMA_x_g'], label = 'EMA_x_g', ax = ax[1])\nax[1].set_ylabel('gyro x')\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T04:19:31.570681Z","iopub.execute_input":"2021-09-30T04:19:31.571051Z","iopub.status.idle":"2021-09-30T04:19:38.233761Z","shell.execute_reply.started":"2021-09-30T04:19:31.571019Z","shell.execute_reply":"2021-09-30T04:19:38.232894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#exp 2\nbegin, end = 0, -1\n\naccel_graph = data_exp2['accel'][begin:end]\n\nfig, ax =plt.subplots(1,2, figsize=(30,8))\n\nax[0].set_title('accel x vs smooth x')\nsns.lineplot(x=accel_graph[\"time_stamp\"], y=accel_graph['x'], label = 'x', ax = ax[0])\nsns.lineplot(x=accel_graph[\"time_stamp\"], y=accel_graph['EMA_x_a'], label = 'EMA_x_a', ax = ax[0])\nax[0].set_ylabel('accel x')\n\n\ngyro_graph = data_exp2['gyro'][begin:end]\nax[1].set_title('gyro x vs smooth x')\n\nsns.lineplot(x=gyro_graph[\"time_stamp\"], y=gyro_graph['x'], label = 'x', ax = ax[1])\nsns.lineplot(x=gyro_graph[\"time_stamp\"], y=gyro_graph['EMA_x_g'], label = 'EMA_x_g', ax = ax[1])\nax[1].set_ylabel('gyro x')\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T04:19:51.498398Z","iopub.execute_input":"2021-09-30T04:19:51.499034Z","iopub.status.idle":"2021-09-30T04:19:58.59125Z","shell.execute_reply.started":"2021-09-30T04:19:51.498986Z","shell.execute_reply":"2021-09-30T04:19:58.590165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"begin, end = 0, -1\n\naccel_graph = data_exp1['accel'][begin:end]\n\nfig, ax =plt.subplots(1,2, figsize=(30,8))\n\nax[0].set_title('accel y vs smooth y')\nsns.lineplot(x=accel_graph[\"time_stamp\"], y=accel_graph['y'], label = 'y', ax = ax[0])\nsns.lineplot(x=accel_graph[\"time_stamp\"], y=accel_graph['EMA_y_a'], label = 'EMA_y_a', ax = ax[0])\nax[0].set_ylabel('accel y')\n\n\ngyro_graph = data_exp1[\"gyro\"][begin:end]\n\nax[1].set_title('gyro y vs smooth y')\n\nsns.lineplot(x=gyro_graph[\"time_stamp\"], y=gyro_graph['y'], label = 'y', ax = ax[1])\nsns.lineplot(x=gyro_graph[\"time_stamp\"], y=gyro_graph['EMA_y_g'], label = 'EMA_y_g', ax = ax[1])\nax[1].set_ylabel('gyro y')\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T19:57:23.838579Z","iopub.execute_input":"2021-09-24T19:57:23.839011Z","iopub.status.idle":"2021-09-24T19:57:30.665919Z","shell.execute_reply.started":"2021-09-24T19:57:23.838975Z","shell.execute_reply":"2021-09-24T19:57:30.665089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"begin, end = 0, -1\n\naccel_graph = data_exp2['accel'][begin:end]\n\nfig, ax =plt.subplots(1,2, figsize=(30,8))\n\nax[0].set_title('accel y vs smooth y')\nsns.lineplot(x=accel_graph[\"time_stamp\"], y=accel_graph['y'], label = 'y', ax = ax[0])\nsns.lineplot(x=accel_graph[\"time_stamp\"], y=accel_graph['EMA_y_a'], label = 'EMA_y_a', ax = ax[0])\nax[0].set_ylabel('accel y')\n\n\ngyro_graph = data_exp2[\"gyro\"][begin:end]\n\nax[1].set_title('gyro y vs smooth y')\n\nsns.lineplot(x=gyro_graph[\"time_stamp\"], y=gyro_graph['y'], label = 'y', ax = ax[1])\nsns.lineplot(x=gyro_graph[\"time_stamp\"], y=gyro_graph['EMA_y_g'], label = 'EMA_y_g', ax = ax[1])\nax[1].set_ylabel('gyro y')\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T19:57:30.667022Z","iopub.execute_input":"2021-09-24T19:57:30.667443Z","iopub.status.idle":"2021-09-24T19:57:37.882396Z","shell.execute_reply.started":"2021-09-24T19:57:30.667412Z","shell.execute_reply":"2021-09-24T19:57:37.88143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"begin, end = 0, -1\n# begin, end = begin + 500, end + 500\n\naccel_graph = data_exp1['accel'][begin:end]\n\n\nfig, ax =plt.subplots(1,2, figsize=(30,8))\n\nax[0].set_title('accel z vs smooth z')\nsns.lineplot(x=accel_graph[\"time_stamp\"], y=accel_graph['z'], label = 'z', ax = ax[0])\nsns.lineplot(x=accel_graph[\"time_stamp\"], y=accel_graph['EMA_z_a'], label = 'EMA_z_a', ax = ax[0])\nax[0].set_ylabel('accel z')\n\n\ngyro_graph = data_exp1[\"gyro\"][begin:end]\n\nax[1].set_title('gyro z vs smooth z')\n\nsns.lineplot(x=gyro_graph[\"time_stamp\"], y=gyro_graph['z'], label = 'z', ax = ax[1])\nsns.lineplot(x=gyro_graph[\"time_stamp\"], y=gyro_graph['EMA_z_g'], label = 'EMA_z_g', ax = ax[1])\nax[1].set_ylabel('gyro z')\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T19:57:37.885169Z","iopub.execute_input":"2021-09-24T19:57:37.885487Z","iopub.status.idle":"2021-09-24T19:57:44.509625Z","shell.execute_reply.started":"2021-09-24T19:57:37.885455Z","shell.execute_reply":"2021-09-24T19:57:44.508793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"begin, end = 0, -1\n# begin, end = begin + 500, end + 500\n\naccel_graph = data_exp2['accel'][begin:end]\n\nfig, ax =plt.subplots(1,2, figsize=(30,8))\n\nax[0].set_title('accel z vs smooth z')\nsns.lineplot(x=accel_graph[\"time_stamp\"], y=accel_graph['z'], label = 'z', ax = ax[0])\nsns.lineplot(x=accel_graph[\"time_stamp\"], y=accel_graph['EMA_z_a'], label = 'EMA_z_a', ax = ax[0])\nax[0].set_ylabel('accel z')\n\n\ngyro_graph = data_exp2[\"gyro\"][begin:end]\n\nax[1].set_title('gyro z vs smooth z')\n\nsns.lineplot(x=gyro_graph[\"time_stamp\"], y=gyro_graph['z'], label = 'z', ax = ax[1])\nsns.lineplot(x=gyro_graph[\"time_stamp\"], y=gyro_graph['EMA_z_g'], label = 'EMA_z_g', ax = ax[1])\nax[1].set_ylabel('gyro z')\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T19:57:44.511505Z","iopub.execute_input":"2021-09-24T19:57:44.511826Z","iopub.status.idle":"2021-09-24T19:57:51.606497Z","shell.execute_reply.started":"2021-09-24T19:57:44.511796Z","shell.execute_reply":"2021-09-24T19:57:51.605401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = '''It was the design of Angelo Ricci and Joe Czanek and Manuel Silva to call on the Terrible Old Man.  This old man dwells all alone in a very ancient house on Water Street near the sea, and is reputed to be both exceedingly rich and exceedingly feeble; which forms a situation very attractive to men of the profession of Messrs. Ricii, Czanek, and Silva, for that profession was nothing less dignified than robbery.\nThe inhabitants of Kingsport say and think many things about the Terrible Old Man which generally keep him safe from the attention of gentlemen like Mr. Ricci and his colleagues, despite the almost certain fact that he hides a fortune of indefinite magnitude somewhere about his musty and venerable abode.  He is, in truth, a very strange person, believed to have been a captain of East India clipper ships in his day; so old that no one can remember when was young and so tacitum that few know his real name.  Among the gnarled trees in the front yard of his aged and neglected plae he maintains a strange collection of large stones, oddly grouped and painted so that they resemble the idols in some obscure Eastern temple.  This collection frightens away most of the small boys who love to taunt the Terrible Old Man about his long white hair and beard, or to break the small-paned windows of his dwelling with wicked missles; but there are other things which frighten the older and more curious folk who sometimes steal up to the house to peer in through the dusty panes.  These folks say that on atable in a bare room on the ground floor are many peculiar bottles, in each a small piece of lead suspended pendulum-wise from a string.  And they say that the Terrible Old Man talks to these bottles,'''","metadata":{"execution":{"iopub.status.busy":"2021-09-30T04:17:00.459894Z","iopub.execute_input":"2021-09-30T04:17:00.46036Z","iopub.status.idle":"2021-09-30T04:17:00.46793Z","shell.execute_reply.started":"2021-09-30T04:17:00.46032Z","shell.execute_reply":"2021-09-30T04:17:00.466716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(s)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T04:17:05.114428Z","iopub.execute_input":"2021-09-30T04:17:05.115028Z","iopub.status.idle":"2021-09-30T04:17:05.12085Z","shell.execute_reply.started":"2021-09-30T04:17:05.114991Z","shell.execute_reply":"2021-09-30T04:17:05.120034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"begin, end = -500, -1\ng = -9.806\n\naccel = data_exp1['accel']\n# Calculate length of accel vector\naccel['raw_length'] = np.sqrt(accel.x**2 + accel.y**2 + accel.z**2)/g\naccel['smooth_length'] = np.sqrt(accel.EMA_x_a**2 + accel.EMA_y_a**2 + accel.EMA_z_a**2)/g\naccel.head(10)\naccel.describe()\nprint(accel[begin:end][accel['raw_length'] > .5].count())\naccel_graph = accel[begin:end]\n\n\nfig, ax =plt.subplots(1,2, figsize=(30,8))\n\nax[0].set_title('Length of accelerometer vector through time')\nsns.lineplot(x=accel_graph[\"time_stamp\"], y=accel_graph['raw_length'], label = 'raw_length', ax=ax[0])\nsns.lineplot(x=accel_graph[\"time_stamp\"], y=accel_graph['smooth_length'], label = 'smooth_length', ax=ax[0])\n# sns.lineplot(x=accel_graph[\"time_stamp\"], y=accel_graph['EMA_raw_length'], label = 'EMA_raw_length', ax=ax[0])\nax[0].set_ylabel('g-force')\n\n\ngyro = data_exp1['gyro']\n# Calculate length of gyro vector\ngyro['raw_length'] = np.sqrt(gyro.x**2 + gyro.y**2 + gyro.z**2)\ngyro['smooth_length'] = np.sqrt(gyro.EMA_x_g**2 + gyro.EMA_y_g**2 + gyro.EMA_z_g**2)\ngyro.head(10)\ngyro.describe()\n\ngyro_graph = gyro[begin:end]\nax[1].set_title('Length of gyro vector through time')\n\nsns.lineplot(x=gyro_graph[\"time_stamp\"], y=gyro_graph['raw_length'], label = 'raw_length', ax=ax[1])\nsns.lineplot(x=gyro_graph[\"time_stamp\"], y=gyro_graph['smooth_length'], label = 'smooth_length', ax = ax[1])\n# sns.lineplot(x=gyro_graph[\"time_stamp\"], y=gyro_graph['EMA_raw_length'], label = 'EMA_raw_length')\nax[1].set_ylabel('vector length')\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T16:46:30.858268Z","iopub.execute_input":"2021-10-01T16:46:30.858674Z","iopub.status.idle":"2021-10-01T16:46:31.53207Z","shell.execute_reply.started":"2021-10-01T16:46:30.858642Z","shell.execute_reply":"2021-10-01T16:46:31.531059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_exp1","metadata":{"execution":{"iopub.status.busy":"2021-10-01T16:46:48.838937Z","iopub.execute_input":"2021-10-01T16:46:48.839385Z","iopub.status.idle":"2021-10-01T16:46:48.867827Z","shell.execute_reply.started":"2021-10-01T16:46:48.839345Z","shell.execute_reply":"2021-10-01T16:46:48.866036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"begin, end = 0, -1\ng = -9.806\n\naccel = data_exp2['accel']\n# Calculate length of accel vector\naccel['raw_length'] = np.sqrt(accel.x**2 + accel.y**2 + accel.z**2)/g\naccel['smooth_length'] = np.sqrt(accel.EMA_x_a**2 + accel.EMA_y_a**2 + accel.EMA_z_a**2)/g\naccel.head(10)\naccel.describe()\n\naccel_graph = accel[begin:end]\n\n\nfig, ax =plt.subplots(1,2, figsize=(30,8))\n\nax[0].set_title('Length of accelerometer vector through time')\nsns.lineplot(x=accel_graph[\"time_stamp\"], y=accel_graph['raw_length'], label = 'raw_length', ax=ax[0])\nsns.lineplot(x=accel_graph[\"time_stamp\"], y=accel_graph['smooth_length'], label = 'smooth_length', ax=ax[0])\n# sns.lineplot(x=accel_graph[\"time_stamp\"], y=accel_graph['EMA_raw_length'], label = 'EMA_raw_length', ax=ax[0])\nax[0].set_ylabel('vector length')\n\n\ngyro = data_exp2['gyro']\n# Calculate length of gyro vector\ngyro['raw_length'] = np.sqrt(gyro.x**2 + gyro.y**2 + gyro.z**2)\ngyro['smooth_length'] = np.sqrt(gyro.EMA_x_g**2 + gyro.EMA_y_g**2 + gyro.EMA_z_g**2)\ngyro.head(10)\ngyro.describe()\n\ngyro_graph = gyro[begin:end]\nax[1].set_title('Length of gyro vector through time')\n\nsns.lineplot(x=gyro_graph[\"time_stamp\"], y=gyro_graph['raw_length'], label = 'raw_length', ax=ax[1])\nsns.lineplot(x=gyro_graph[\"time_stamp\"], y=gyro_graph['smooth_length'], label = 'smooth_length', ax = ax[1])\n# sns.lineplot(x=gyro_graph[\"time_stamp\"], y=gyro_graph['EMA_raw_length'], label = 'EMA_raw_length')\nax[1].set_ylabel('vector length')\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T04:20:56.194328Z","iopub.execute_input":"2021-09-30T04:20:56.194718Z","iopub.status.idle":"2021-09-30T04:21:03.617824Z","shell.execute_reply.started":"2021-09-30T04:20:56.194683Z","shell.execute_reply":"2021-09-30T04:21:03.616458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}