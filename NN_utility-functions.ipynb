{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "3ae86968-b273-4857-a4fd-537b178d854b",
    "_uuid": "bcec77b5-0738-4d15-8aad-f6b59b3ac023",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "EER: 0.333, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.400, Threshold: 0.200 <-- Worse case\n",
      "EER: 0.167, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.333, Threshold: 1.000 <-- Worse case\n",
      "--------------------\u001b[32mUtility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mPreprocessing utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade pip\n",
    "\n",
    "TEST_MODE = 0 # Testing macro\n",
    "\n",
    "import os\n",
    "import numpy as np # linear algebraf\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle as sklearn_shuffle\n",
    "\n",
    "from tensorflow.keras import layers, regularizers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Input,\n",
    "    Conv1D,\n",
    "    MaxPooling1D,\n",
    "    AveragePooling1D,\n",
    "    Flatten,\n",
    "    Lambda,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    "    GlobalAveragePooling1D,\n",
    "    Activation,\n",
    ")\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Ftrl, Nadam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import concatenate as keras_concat\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from functools import partial as partial_func\n",
    "\n",
    "# Define custom MinPooling1D layer\n",
    "class MinPooling1D(layers.Layer):\n",
    "    def __init__(self, pool_size, strides=None, padding='valid', **kwargs):\n",
    "        super(MinPooling1D, self).__init__(**kwargs)\n",
    "        self.pool_size = pool_size\n",
    "        self.strides = strides if strides is not None else pool_size\n",
    "        self.padding = padding\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Change max pooling to min pooling by multiplying input by -1\n",
    "        # Then, apply max pooling, and finally, multiply by -1 again\n",
    "        return -1 * tf.nn.max_pool1d(-1 * inputs,\n",
    "                                     ksize=[1, self.pool_size, 1],\n",
    "                                     strides=[1, self.strides, 1],\n",
    "                                     padding=self.padding.upper())\n",
    "\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import time\n",
    "import json\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "%run ./Preprocessing_utility-functions.ipynb\n",
    "%run ./SEED-CONSTANTS.ipynb\n",
    "\n",
    "print(f\"\\x1b[32mSEED: {SEED}\\x1b[0m\")\n",
    "# Global utitlity functions are in separate notebook\n",
    "if TEST_MODE: print(\"setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.10\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def k_euclidean_dist(t):\n",
    "    x = t[0]\n",
    "    y = t[1]    \n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=-1, keepdims=True))\n",
    "\n",
    "def k_manhattan_dist(t):\n",
    "    x = t[0]\n",
    "    y = t[1]\n",
    "    return K.sum(K.abs(x - y), axis=-1, keepdims=True)\n",
    "\n",
    "def k_minkowski_dist_p5(t):\n",
    "    x = t[0]\n",
    "    y = t[1]\n",
    "    return K.pow(K.sum(K.pow(K.abs(x - y), 5), axis=-1, keepdims=True), 1./5)\n",
    "\n",
    "\n",
    "def k_contrastive_loss(y_true, dist, margin):\n",
    "    \"\"\"Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "    return K.mean(y_true * K.square(dist) + (1 - y_true) * K.square(K.maximum(margin - dist, 0)))\n",
    "\n",
    "def contrastive_loss_test(y_true, dist, margin=1):\n",
    "    \"\"\"Test function above using implementation with numpy instead tensors.\"\"\"\n",
    "    return y_true * np.square(dist) + (1 - y_true) * np.square(np.max(margin - dist, 0))\n",
    "\n",
    "if TEST_MODE:\n",
    "    print(\"Positive: class=1, distance=0,     loss:\", contrastive_loss_test(1, 0))\n",
    "    print(\"Positive: class=1, distance=0.01,  loss:\", contrastive_loss_test(1, 0.01))\n",
    "    print(\"Positive: class=1, distance=0.3,   loss:\", contrastive_loss_test(1, 0.3))\n",
    "    print(\"Positive: class=1, distance=0.5,   loss:\", contrastive_loss_test(1, 0.5))\n",
    "    print(\"Positive: class=1, distance=1,     loss:\", contrastive_loss_test(1, 1))\n",
    "\n",
    "    print(\"Negative: class=0, distance=0,     loss:\", contrastive_loss_test(0, 0))\n",
    "    print(\"Negative: class=0, distance=0.01,  loss:\", contrastive_loss_test(0, 0.01))\n",
    "    print(\"Negative: class=0, distance=0.3,   loss:\", contrastive_loss_test(0, 0.3))\n",
    "    print(\"Negative: class=0, distance=0.5,   loss:\", contrastive_loss_test(0, 0.5))\n",
    "    print(\"Negative: class=0, distance=1,     loss:\", contrastive_loss_test(0, 1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Positive: class=1, distance=0,     loss:\", k_contrastive_loss(tf.constant(1, dtype=tf.float32), tf.constant(0, dtype=tf.float32), 1))\n",
    "    print(\"Positive: class=1, distance=0.01,  loss:\", k_contrastive_loss(tf.constant(1, dtype=tf.float32), tf.constant(0.01, dtype=tf.float32), 1))\n",
    "    print(\"Positive: class=1, distance=0.3,   loss:\", k_contrastive_loss(tf.constant(1, dtype=tf.float32), tf.constant(0.3, dtype=tf.float32), 1))\n",
    "    print(\"Positive: class=1, distance=0.5,   loss:\", k_contrastive_loss(tf.constant(1, dtype=tf.float32), tf.constant(0.5, dtype=tf.float32), 1))\n",
    "    print(\"Positive: class=1, distance=1,     loss:\", k_contrastive_loss(tf.constant(1, dtype=tf.float32), tf.constant(1, dtype=tf.float32), 1))\n",
    "\n",
    "    print(\"Negative: class=0, distance=0,     loss:\", k_contrastive_loss(tf.constant(0, dtype=tf.float32), tf.constant(0, dtype=tf.float32), 1))\n",
    "    print(\"Negative: class=0, distance=0.01,  loss:\", k_contrastive_loss(tf.constant(0, dtype=tf.float32), tf.constant(0.01, dtype=tf.float32), 1))\n",
    "    print(\"Negative: class=0, distance=0.3,   loss:\", k_contrastive_loss(tf.constant(0, dtype=tf.float32), tf.constant(0.3, dtype=tf.float32), 1))\n",
    "    print(\"Negative: class=0, distance=0.5,   loss:\", k_contrastive_loss(tf.constant(0, dtype=tf.float32), tf.constant(0.5, dtype=tf.float32), 1))\n",
    "    print(\"Negative: class=0, distance=1,     loss:\", k_contrastive_loss(tf.constant(0, dtype=tf.float32), tf.constant(1, dtype=tf.float32), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_optimizer(name, lr=None, decay=None):\n",
    "    if name == \"SGD\":\n",
    "        lr = lr if lr != None else 0.01\n",
    "        decay = decay if decay != None else 0\n",
    "        optimizer = SGD(learning_rate=lr, decay=decay)\n",
    "    elif name == \"Adam\":\n",
    "        lr = lr if lr != None else 0.001\n",
    "        decay = decay if decay != None else 0\n",
    "        optimizer = Adam(learning_rate=lr, decay=decay)\n",
    "    elif name == \"RMSprop\":\n",
    "        lr = lr if lr != None else 0.001\n",
    "        optimizer = RMSprop(learning_rate=lr)\n",
    "    elif name == \"Adadelta\":\n",
    "        lr = lr if lr != None else 0.001\n",
    "        optimizer = Adadelta(learning_rate=lr)\n",
    "    elif name == \"Adagrad\":\n",
    "        lr = lr if lr != None else 0.001\n",
    "        optimizer = Adagrad(learning_rate=lr)\n",
    "    elif name == \"Adamax\":\n",
    "        lr = lr if lr != None else 0.001\n",
    "        optimizer = Adamax(learning_rate=lr)\n",
    "    elif name == \"Ftrl\":\n",
    "        lr = lr if lr != None else 0.001\n",
    "        optimizer = Ftrl(learning_rate=lr)\n",
    "    elif name == \"Nadam\":\n",
    "        lr = lr if lr != None else 0.001\n",
    "        optimizer = Nadam(learning_rate=lr)\n",
    "    else:\n",
    "        print(name)\n",
    "        raise BaseException(\"Error: Not a valid model name: 1d or 2d.\")\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def get_loss_func(arg_dict):\n",
    "    name = arg_dict[\"loss_func_name\"]\n",
    "    if name == \"k_contrastive_loss\":\n",
    "        loss_func = lambda y_true, dist: k_contrastive_loss(y_true, dist, arg_dict[\"contrastive_loss_margin\"])\n",
    "    else:\n",
    "        print(name)\n",
    "        raise BaseException(\"Error: Not a valid loss func name\")\n",
    "    return loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_windows_NN(dfList_exp1, dfList_exp2, window_size, step_width, user_idx_set, scaler, num_sample_points_per_exp, \n",
    "                       min_max_scaler=None, EMA_per_win_span=None, SMA_per_win_winsize=None, Butter_per_win_argdict=None, \n",
    "                       verbose=0):\n",
    "\n",
    "    # preparing data\n",
    "    exp1_df_user_set_dict, exp2_df_user_set_dict = {user_key: dfList_exp1[user_key] for user_key in user_idx_set}, {user_key: dfList_exp2[user_key] for user_key in user_idx_set}\n",
    "    print(f\"len(exp1_df_user_set_dict): {len(exp1_df_user_set_dict)}\")\n",
    "    print(f\"len(exp2_df_user_set_dict): {len(exp2_df_user_set_dict)}\")\n",
    "    \n",
    "    # do not scale exp1 on its own, but instead use exp2 scaler from next steps to scale exp1\n",
    "    exp1_data_prep_dict = MakeRawXExpDict(users_exp_df_dict=exp1_df_user_set_dict, window_size=window_size, step=step_width, numSamplePoints=num_sample_points_per_exp, \n",
    "                                          scaler=None, min_max_scaler=None, exp_num=1, EMA_per_win_span=EMA_per_win_span, \n",
    "                                          SMA_per_win_winsize=SMA_per_win_winsize, Butter_per_win_argdict=Butter_per_win_argdict, \n",
    "                                          verbose=verbose)\n",
    "    \n",
    "    \n",
    "    exp2_data_prep_dict = MakeRawXExpDict(users_exp_df_dict=exp2_df_user_set_dict, window_size=window_size, step=step_width, numSamplePoints=num_sample_points_per_exp, \n",
    "                                          scaler=scaler, min_max_scaler=min_max_scaler, exp_num=2, EMA_per_win_span=EMA_per_win_span, \n",
    "                                          SMA_per_win_winsize=SMA_per_win_winsize, Butter_per_win_argdict=Butter_per_win_argdict, \n",
    "                                          verbose=verbose)\n",
    "\n",
    "    fitted_scaler_exp2_dict = exp2_data_prep_dict[\"fitted_scaler_exp_dic\"]\n",
    "    X_exp1_dict, X_exp2_dict = exp1_data_prep_dict[\"raw_X_exp_dict\"], exp2_data_prep_dict[\"raw_X_exp_dict\"]\n",
    "    \n",
    "    \n",
    "    if verbose: print(f\"Done getting raw wondows for window_size: {window_size}\")\n",
    "    \n",
    "    if min_max_scaler!=None:\n",
    "        \n",
    "        fitted_min_max_scaler_exp2_dict = exp2_data_prep_dict[\"fitted_min_max_scaler_dic\"]\n",
    "        \n",
    "        return X_exp1_dict, X_exp2_dict, fitted_scaler_exp2_dict, fitted_min_max_scaler_exp2_dict\n",
    "\n",
    "\n",
    "    return X_exp1_dict, X_exp2_dict, fitted_scaler_exp2_dict\n",
    "\n",
    "    \n",
    "def get_raw_windows_user_47_NN(dfList_exp1_user_47, dfList_exp2_user_47, window_size, step_width, scaler, num_sample_points_per_exp, \n",
    "                               min_max_scaler=None, EMA_per_win_span=None, SMA_per_win_winsize=None, Butter_per_win_argdict=None, \n",
    "                               verbose=0):\n",
    "\n",
    "\n",
    "    exp1_df_dict_user_47, exp2_df_dict_user_47 = {47: dfList_exp1_user_47[0]}, {47: dfList_exp2_user_47[0]}\n",
    "    \n",
    "    \n",
    "    # Loading exp1 data:\n",
    "    # 47) accel_count: 22777, gyro_count: 22226\n",
    "    exp1_data_prep_dict_user_47 = MakeRawXExpDict(users_exp_df_dict=exp1_df_dict_user_47, window_size=window_size, step=step_width, \n",
    "                                                  numSamplePoints=num_sample_points_per_exp, scaler=None, min_max_scaler=None, exp_num=1, #+4000\n",
    "                                                  EMA_per_win_span=EMA_per_win_span, SMA_per_win_winsize=SMA_per_win_winsize, \n",
    "                                                  Butter_per_win_argdict=Butter_per_win_argdict, verbose=verbose)\n",
    "    \n",
    "    # Loading exp2 data:\n",
    "    # 47) accel_count: 17718, gyro_count: 18353\n",
    "    exp2_data_prep_dict_user_47 = MakeRawXExpDict(users_exp_df_dict=exp2_df_dict_user_47, window_size=window_size, step=step_width, \n",
    "                                                  numSamplePoints=18000, scaler=scaler, min_max_scaler=min_max_scaler, exp_num=2, # has to be manually chosen\n",
    "                                                  EMA_per_win_span=EMA_per_win_span, SMA_per_win_winsize=SMA_per_win_winsize, \n",
    "                                                  Butter_per_win_argdict=Butter_per_win_argdict, verbose=verbose)\n",
    "    \n",
    "    \n",
    "    fitted_scaler_exp2_dict_user_47 = exp2_data_prep_dict_user_47[\"fitted_scaler_exp_dic\"]\n",
    "    X_exp1_dict_user_47, X_exp2_dict_user_47 = exp1_data_prep_dict_user_47[\"raw_X_exp_dict\"], exp2_data_prep_dict_user_47[\"raw_X_exp_dict\"]\n",
    "\n",
    "    \n",
    "    # ---- adjusting the len_exp2_user_47\n",
    "    total_samples_per_user=len(X_exp1_dict_user_47[47])\n",
    "    print(f\"len(X_exp1_dict_user_47[47]): {len(X_exp1_dict_user_47[47])}\")\n",
    "    \n",
    "    len_exp2_user_47 = len(X_exp2_dict_user_47[47])\n",
    "    print(f\"len_exp2_user_47: {len_exp2_user_47}\")\n",
    "    np.random.seed(SEED+len_exp2_user_47)\n",
    "    X_exp2_dict_user_47[47] = X_exp2_dict_user_47[47] + [X_exp2_dict_user_47[47][idx] for idx in np.random.choice(len_exp2_user_47,\n",
    "                                                                                                      size=total_samples_per_user-len_exp2_user_47,\n",
    "                                                                                                      replace=False)]\n",
    "    # ----\n",
    "    \n",
    "    if verbose: print(f\"Done getting raw wondows for window_size: {window_size}\")\n",
    "    \n",
    "    if min_max_scaler!=None:\n",
    "        \n",
    "        fitted_min_max_scaler_exp2_dict_user_47 = exp2_data_prep_dict_user_47[\"fitted_min_max_scaler_dic\"]\n",
    "        \n",
    "        return X_exp1_dict_user_47, X_exp2_dict_user_47, fitted_scaler_exp2_dict_user_47, fitted_min_max_scaler_exp2_dict_user_47\n",
    "    \n",
    "        \n",
    "    return X_exp1_dict_user_47, X_exp2_dict_user_47, fitted_scaler_exp2_dict_user_47\n",
    "\n",
    "\n",
    "\n",
    "def append_user_47_to_data_NN(X_exp1_dict, X_exp2_dict, fitted_scaler_exp2_dict, all_user_set, X_exp1_dict_user_47, X_exp2_dict_user_47, \n",
    "                              fitted_scaler_exp2_dict_user_47, fitted_min_max_scaler_exp2_dict=None, fitted_min_max_scaler_exp2_dict_user_47=None, \n",
    "                              verbose=0):\n",
    "    user_47_idx = len(all_user_set)\n",
    "    \n",
    "    if user_47_idx not in all_user_set:\n",
    "        \n",
    "        if isinstance(all_user_set, list):\n",
    "            all_user_set.append(user_47_idx)\n",
    "            \n",
    "        elif isinstance(all_user_set, set):\n",
    "            all_user_set.add(user_47_idx)\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"the collection of all users is neither a list, nor a set\")\n",
    "            \n",
    "        \n",
    "    user_47_key = user_47_idx\n",
    "    X_exp1_dict[user_47_key] = X_exp1_dict_user_47[47]\n",
    "    X_exp2_dict[user_47_key] = X_exp2_dict_user_47[47]\n",
    "    fitted_scaler_exp2_dict[user_47_key] = fitted_scaler_exp2_dict_user_47[47]\n",
    "    \n",
    "    if fitted_min_max_scaler_exp2_dict!=None and fitted_min_max_scaler_exp2_dict_user_47!=None:\n",
    "        fitted_min_max_scaler_exp2_dict[user_47_key] = fitted_min_max_scaler_exp2_dict_user_47[47]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"user_47_idx: {user_47_idx}\")\n",
    "        print(f\"np.unique(all_user_set): {np.unique(all_user_set)}\")\n",
    "\n",
    "        print(f\"X_exp1_dict.keys(): {X_exp1_dict.keys()}\")\n",
    "        print(f\"X_exp2_dict.keys(): {X_exp2_dict.keys()}\")\n",
    "        print(f\"fitted_scaler_exp2_dict.keys(): {fitted_scaler_exp2_dict.keys()}\")\n",
    "        \n",
    "    if fitted_min_max_scaler_exp2_dict!=None:\n",
    "        \n",
    "        return  X_exp1_dict, X_exp2_dict, fitted_scaler_exp2_dict, fitted_min_max_scaler_exp2_dict\n",
    "    \n",
    "    return X_exp1_dict, X_exp2_dict, fitted_scaler_exp2_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Siamese Model with FCN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install keras-self-attention\n",
    "\n",
    "# from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "class SelfAttention(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.WQ = layers.Dense(units)\n",
    "        self.WK = layers.Dense(units)\n",
    "        self.WV = layers.Dense(units)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Q = self.WQ(inputs)\n",
    "        K = self.WK(inputs)\n",
    "        V = self.WV(inputs)\n",
    "        \n",
    "        attention_scores = tf.matmul(Q, K, transpose_b=True)\n",
    "        attention_scores = attention_scores / tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
    "        \n",
    "        return tf.matmul(attention_weights, V)\n",
    "\n",
    "class ConvolutionalAttention(layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(ConvolutionalAttention, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        query_with_time_axis = tf.expand_dims(inputs, 1)\n",
    "        scores = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(inputs)))\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(scores, axis=1)\n",
    "        context_vector = attention_weights * inputs\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector\n",
    "    \n",
    "    \n",
    "class TemporalAttention(layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(TemporalAttention, self).__init__(**kwargs)\n",
    "        self.W1 = Dense(units, activation='tanh')\n",
    "        self.W2 = Dense(1, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.W1(inputs)\n",
    "        x = self.W2(x)\n",
    "        return x * inputs\n",
    "    \n",
    "# class TemporalAttentionLayer(Layer):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super(TemporalAttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.W = self.add_weight(shape=(input_shape[-1], 1),\n",
    "#                                  initializer='normal', trainable=True)\n",
    "#         super(TemporalAttentionLayer, self).build(input_shape)\n",
    "\n",
    "#     def call(self, inputs, **kwargs):\n",
    "#         attention_weights = K.softmax(K.dot(inputs, self.W), axis=-2)\n",
    "#         weighted_output = K.sum(attention_weights * inputs, axis=-2)\n",
    "#         return weighted_output\n",
    "\n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         return input_shape[0], input_shape[-1]\n",
    "\n",
    "    \n",
    "\n",
    "def create_multi_headed_model_fcn(input_shape, arg_dict, do_plot=True):\n",
    "    '''\n",
    "    input_dropout_streams = arg_dict[\"input_dropout_streams\"]\n",
    "    filters_streams = arg_dict[\"filters_streams\"]\n",
    "    kernels_streams = arg_dict[\"kernels_streams\"]\n",
    "    kernels_init_streams = arg_dict[\"kernels_init_streams\"] \n",
    "    kernels_constraint_streams = arg_dict[\"kernels_constraint_streams\"] \n",
    "    strides_streams = arg_dict[\"strides_streams\"] \n",
    "    paddings_streams = arg_dict[\"paddings_streams\"] \n",
    "    activations_streams = arg_dict[\"activations_streams\"]\n",
    "    dropouts_streams = arg_dict[\"dropouts_streams\"] \n",
    "    \n",
    "    dense_layers = arg_dict[\"dense_layers\"] \n",
    "    dense_kernel_constraints = arg_dict[\"dense_kernel_constraints\"]\n",
    "    dense_kernel_inits = arg_dict[\"dense_kernel_inits\"] \n",
    "    dense_dropouts = arg_dict[\"dense_dropouts\"] \n",
    "    dense_activations = arg_dict[\"dense_activations\"] \n",
    "    \n",
    "    loss_func = arg_dict[\"loss_func\"] \n",
    "    optimizer = arg_dict[\"optimizer\"] \n",
    "    \n",
    "    # Define the tensors for the two input images\n",
    "    left_inputs = Input(input_shape, name=\"left_inputs\")\n",
    "    right_inputs = Input(input_shape, name=\"right_inputs\")\n",
    "    \n",
    "    '''\n",
    "    # regularizers_dict={ \"l1\": regularizers.L1(0.01), \"l2\": regularizers.L2(0.01), \"l1_l2\": regularizers.L1L2(0.01, 0.01)}\n",
    "    regularizers_dict={ \"l1\": regularizers.L1, \"l2\": regularizers.L2, \"l1_l2\": regularizers.L1L2}\n",
    "\n",
    "    \n",
    "    # conv\n",
    "    input_dropout_streams = arg_dict[\"input_dropout_streams\"]\n",
    "    filters_streams = arg_dict[\"filters_streams\"]\n",
    "    kernels_streams = arg_dict[\"kernels_streams\"]\n",
    "    kernels_init_streams = arg_dict[\"kernels_init_streams\"] \n",
    "    kernels_Max_Norm_constraint_streams = arg_dict[\"kernels_Max_Norm_constraint_streams\"] \n",
    "    strides_streams = arg_dict[\"strides_streams\"] \n",
    "    paddings_streams = arg_dict[\"paddings_streams\"] \n",
    "    activations_streams = arg_dict[\"activations_streams\"]\n",
    "    dropouts_streams = arg_dict[\"dropouts_streams\"]\n",
    "    conv_kernel_regularizer_streams = arg_dict[\"conv_kernel_regularizer_streams\"]\n",
    "    # arguments must be in tupples\n",
    "    conv_kernel_regularizer_strength_streams = arg_dict[\"conv_kernel_regularizer_strength_streams\"]\n",
    "    \n",
    "    # dense\n",
    "    dense_layers = arg_dict[\"dense_layers\"] \n",
    "    dense_kernel_Max_Norm_constraints = arg_dict[\"dense_kernel_Max_Norm_constraints\"]\n",
    "    dense_kernel_inits = arg_dict[\"dense_kernel_inits\"] \n",
    "    dense_dropouts = arg_dict[\"dense_dropouts\"] \n",
    "    dense_activations = arg_dict[\"dense_activations\"] \n",
    "    dense_kernel_regularizer = arg_dict[\"dense_kernel_regularizer\"]\n",
    "    dense_kernel_regularizer_strength = arg_dict[\"dense_kernel_regularizer_strength\"]\n",
    "    \n",
    "    # loss_func = get_loss_func(arg_dict[\"loss_func_name\"], arg_dict[\"loss_func_args\"]) improve in future\n",
    "    loss_func = get_loss_func(arg_dict)\n",
    "    optimizer = get_optimizer(arg_dict[\"optimizer_name\"] , lr=arg_dict[\"optimizer_lr\"], decay=arg_dict[\"optimizer_decay\"])\n",
    "    \n",
    "    # Define the tensors for the two input images\n",
    "    left_inputs = Input(input_shape, name=\"left_inputs\")\n",
    "    right_inputs = Input(input_shape, name=\"right_inputs\")\n",
    "    \n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "    inputs_dict = {}\n",
    "    for head_idx in range(len(filters_streams)):\n",
    "        inputs_dict[head_idx] = Input(input_shape, name=f\"inputs_{head_idx}\")\n",
    "\n",
    "    \n",
    "    flat_output_dict = {}\n",
    "    for head in inputs_dict:\n",
    "        x= inputs_dict[head]\n",
    "\n",
    "        x = Dropout(input_dropout_streams[head], seed=SEED+head, name=f\"stream_{head}_input_drop\")(x)\n",
    "        for idx in range(len(filters_streams[head])):\n",
    "            \n",
    "            kernel_regularizer = regularizers_dict[conv_kernel_regularizer_streams[head][idx]](*conv_kernel_regularizer_strength_streams[head][idx])\n",
    "            # kernel_regularizer = regularizers_dict[conv_kernel_regularizer_streams[head][idx]]\n",
    "\n",
    "            x = Conv1D(filters=filters_streams[head][idx], kernel_size=kernels_streams[head][idx], kernel_initializer=kernels_init_streams[head][idx], \n",
    "                       kernel_constraint=MaxNorm(kernels_Max_Norm_constraint_streams[head][idx]), strides=strides_streams[head][idx], activation=None, \n",
    "                       padding=paddings_streams[head][idx], kernel_regularizer=kernel_regularizer, name=f\"stream_{head}_conv_{idx+1}\")(x)\n",
    "            \n",
    "\n",
    "            \n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation(activations_streams[head][idx])(x)\n",
    "            \n",
    "            # x = SeqSelfAttention()(x)\n",
    "\n",
    "            # x = TemporalAttention(filters_streams[head][idx])(x)\n",
    "            # x = ConvolutionalAttention(units=filters_streams[head][idx])(x)\n",
    "            # x = SelfAttention(filters_streams[head][idx])(x)\n",
    "            \n",
    "            # x = MaxPooling1D(pool_size=2, name=f\"stream_{head}_maxpool_{idx+1}\")(x)\n",
    "            # min_pool = MinPooling1D(pool_size=2, name=f\"stream_{head}_minpool_{idx+1}\")(x)\n",
    "            # x = keras_concat([max_pool, min_pool])\n",
    "            # x=max_pool\n",
    "            # min_pool = MinPooling1D(pool_size=2, name=f\"stream_{head}_minpool_{idx+1}\")(x)\n",
    "            \n",
    "            \n",
    "            # x = MaxPooling1D(pool_size=2, name=f\"stream_{head}_maxpool_{idx+1}\")(x)\n",
    "            x = AveragePooling1D(pool_size=2, name=f\"stream_{head}_avgpool_{idx+1}\")(x)\n",
    "            # x = keras_concat([avg_pool, max_pool])\n",
    "                             \n",
    "            x = Dropout(dropouts_streams[head][idx], seed=SEED+head+idx+1, name=f\"stream_{head}_drop_{idx+1}\")(x)\n",
    "\n",
    "            # # Add a MaxPooling layer here\n",
    "            # x = MaxPooling1D(pool_size=2, name=f\"stream_{head}_maxpool_{idx+1}\")(x)\n",
    "            # x = Flatten()(x)\n",
    "            \n",
    "\n",
    "            \n",
    "            # x = SelfAttention(32)(x)\n",
    "            # x = ConvolutionalAttention(units=32)(x)\n",
    "            # x = TemporalAttention(32)(x)\n",
    "            \n",
    "            \n",
    "            \n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        \n",
    "        flat_output_dict[head] = x\n",
    "    \n",
    "    # merge\n",
    "    # print(f\"flat_output_dict: {flat_output_dict}\")\n",
    "    if len(flat_output_dict)>1:\n",
    "        x = keras_concat([flat_output_dict[head_idx] for head_idx in range(len(flat_output_dict))])\n",
    "    \n",
    "    for dense_idx in range(len(dense_layers)):\n",
    "        \n",
    "        kernel_regularizer=regularizers_dict[dense_kernel_regularizer[dense_idx]](*dense_kernel_regularizer_strength[dense_idx])\n",
    "        # kernel_regularizer=regularizers_dict[dense_kernel_regularizer[dense_idx]]\n",
    "\n",
    "        \n",
    "        x = Dropout(dense_dropouts[dense_idx], name=f\"dense_{dense_idx+1}_dropout\")(x)\n",
    "        x = Dense(dense_layers[dense_idx], kernel_constraint=MaxNorm(dense_kernel_Max_Norm_constraints[dense_idx]), kernel_initializer=dense_kernel_inits[dense_idx], \n",
    "                  kernel_regularizer=kernel_regularizer, name=f\"dense_{dense_idx+1}\")(x) # excluded activation=dense_activations[dense_idx]\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(dense_activations[dense_idx], name=f\"dense_activation_{dense_idx+1}\" )(x)\n",
    "            \n",
    "    # added later\n",
    "    # x = tf.nn.l2_normalize(x, axis=-1)\n",
    "    \n",
    "    # Basemodel instance\n",
    "    basemodel = Model(inputs=[inputs_dict[idx] for idx in range(len(inputs_dict))], outputs=x, name=\"basemodel\")\n",
    "\n",
    "    # using same instance of \"basemodel\" to share weights between left/right networks\n",
    "    encoded_l = basemodel([left_inputs for _ in range(len(inputs_dict))])\n",
    "    encoded_r = basemodel([right_inputs for _ in range(len(inputs_dict))])\n",
    "\n",
    "    # Add a customized layer to compute the distance between the encodings\n",
    "    distance_layer = Lambda(k_euclidean_dist, name=\"distance\")([encoded_l, encoded_r])\n",
    "    # distance_layer = Lambda(k_manhattan_dist, name=\"distance\")([encoded_l, encoded_r])\n",
    "    # distance_layer = Lambda(k_minkowski_dist_p5, name=\"distance\")([encoded_l, encoded_r])\n",
    "\n",
    "    \n",
    "#     #------new code_____\n",
    "#     # Calculate the pairwise distances\n",
    "#     pairwise_distances = tf.norm(encoded_l - encoded_r, axis=-1)\n",
    "\n",
    "#     # Select the hardest positive and negative pairs\n",
    "#     hardest_positive = tf.reduce_max(pairwise_distances, axis=1)\n",
    "#     hardest_negative = tf.reduce_min(pairwise_distances, axis=1)\n",
    "\n",
    "#     # Calculate contrastive loss\n",
    "#     margin = 1.0  # Contrastive loss margin\n",
    "#     contrastive_loss = tf.maximum(0.0, hardest_positive - hardest_negative + margin)\n",
    "#     #------new code______\n",
    "\n",
    "    # Combine into one net\n",
    "    siamese_net = Model(inputs=[left_inputs, right_inputs], outputs=distance_layer)\n",
    "    if do_plot: \n",
    "        plot_model(siamese_net, show_shapes=True, to_file='multichannel.png')\n",
    "        plot_model(basemodel, show_shapes=True, to_file='basemodel.png')\n",
    "        \n",
    "        \n",
    "        \n",
    "    siamese_net.compile(loss=loss_func, optimizer=optimizer)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net\n",
    "\n",
    "\n",
    "\n",
    "def get_create_model_func(name, window_size, feature_cols):\n",
    "    print(f\"Using Model variant {name}...\")\n",
    "    if name == \"1d\":\n",
    "        return lambda filters, kernels, strides, dropouts, activations, dense, loss_func, optimizer: build_model_1d((window_size, len(feature_cols)),\n",
    "                                                                                                               filters, kernels, strides, \n",
    "                                                                                                                    dropouts, activations, dense, \n",
    "                                                                                                                    loss_func, optimizer)\n",
    "    elif name == \"2d\":\n",
    "        return lambda filters, kernels, strides, dropouts, activations, dense, loss_func, optimizer: build_model_2d((window_size, len(feature_cols)),\n",
    "                                                                                                               filters, kernels, strides, \n",
    "                                                                                                                    dropouts, activations, dense, \n",
    "                                                                                                                    loss_func, optimizer)\n",
    "    elif name == \"fcn\":\n",
    "        return lambda filters, kernels, strides, dropouts, activations, dense, loss_func, optimizer: create_model_fcn((window_size, len(feature_cols)),\n",
    "                                                                                                               filters, kernels, strides, \n",
    "                                                                                                                      dropouts, activations, dense, \n",
    "                                                                                                                      loss_func, optimizer)\n",
    "    elif name == \"multi_head_fcn\":\n",
    "         return lambda arg_dict: create_multi_headed_model_fcn((window_size, len(feature_cols)), arg_dict)\n",
    "        \n",
    "    else:\n",
    "        raise BaseException(\"Error: Not a valid model name: {1d, 2d, fcn}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_index_pairs(diagonal_size):\n",
    "    '''\n",
    "    return an array of indices for negative/positive pairs. It is assumed that every \n",
    "    index of pair corresponds to different sessions.\n",
    "    '''\n",
    "    indices = list(range(diagonal_size))\n",
    "    mesh = np.array(np.meshgrid(indices, indices))\n",
    "    index_pairs = mesh.T.reshape(-1, 2)\n",
    "    \n",
    "    return index_pairs\n",
    "\n",
    "\n",
    "def generate_pairs(X_exp2_dic, X_exp1_dic, user_id_list, fitted_raw_scaler_dict, num_pair_limit = 99999999):\n",
    "    '''\n",
    "    return positive and negative pairs\n",
    "    '''\n",
    "    num_users = len(user_id_list)\n",
    "    # print(num_users)\n",
    "#     X_neg[np.random.choice(X_neg.shape[0], size=X_pos.shape[0], replace=False), :]\n",
    "    negative_pairs_dic = {}\n",
    "    positive_pairs_dic = {}\n",
    "    \n",
    "    total_samples_per_user = len(X_exp2_dic[list(X_exp2_dic.keys())[0]])\n",
    "    # print(total_samples_per_user)\n",
    "    positive_pairs_indices = generate_index_pairs(total_samples_per_user)\n",
    "    negative_pairs_indices = generate_index_pairs(total_samples_per_user)\n",
    "    \n",
    "    # negative_pairs_indices = negative_pairs_indices[np.random.choice(negative_pairs_indices.shape[0], size=positive_pairs_indices.shape[0], replace=False)]\n",
    "    # negative_pairs_indices = np.sort(negative_pairs_indices)\n",
    "    # print(negative_pairs_indices)\n",
    "    positive_left = positive_pairs_indices[:, 0]\n",
    "    positive_right = positive_pairs_indices[:, 1]\n",
    "    \n",
    "    negative_left = negative_pairs_indices[:, 0]\n",
    "    negative_right = negative_pairs_indices[:, 1]\n",
    "    # fitted_scaler_SNN_exp2_train_dic\n",
    "    # print(positive_pairs_indices.shape)\n",
    "    # print(negative_pairs_indices.shape)\n",
    "    \n",
    "    total_num_pairs = (positive_pairs_indices.shape[0]+negative_pairs_indices.shape[0]) * num_users \n",
    "    if num_pair_limit < total_num_pairs: \n",
    "        limit_ratio = num_pair_limit/total_num_pairs\n",
    "    else:\n",
    "        limit_ratio=1\n",
    "    \n",
    "    print(limit_ratio)\n",
    "    \n",
    "    # neg_fraction =  ((total_samples_per_user-1) / (total_samples_per_user * (num_users-1)))  # works accurate\n",
    "    neg_fraction =  1 / (num_users-1)  # works accurate\n",
    "    # neg_fraction =  (total_samples_per_user-1) / (total_samples_per_user * (num_users)) # works accurate\n",
    "    for owner_key in tqdm(user_id_list):\n",
    "        # debugging was: X_exp2_dic is not an array, but needs to be an array if we want integer indexing. This can be changed in utility_functions; however, need to modify other code. \n",
    "        # print(type(positive_left))\n",
    "        # print(len(X_exp2_dic[owner_key]))\n",
    "        # print(np.array(X_exp2_dic[owner_key])[positive_left])\n",
    "        np.random.seed(SEED + owner_key)\n",
    "        # transform_user_windows(X_exp_unknown_df_dict[unknown_user], fitted_raw_scaler_dict[owner_key])\n",
    "        u1_array = np.array(X_exp2_dic[owner_key])\n",
    "        u2_array = np.array(transform_user_windows(X_exp1_dic[owner_key], fitted_raw_scaler_dict[owner_key]))\n",
    "        \n",
    "        # print(f\"u1_array.shape: {u1_array.shape}\")\n",
    "        # print(f\"u2_array.shape: {u2_array.shape}\")\n",
    "        \n",
    "        # print(total_samples_per_user)\n",
    "        # print(positive_right.shape)\n",
    "        # print(negative_right.shape)\n",
    "        \n",
    "        # quick fix no longer needed as this is done in get_raw_windows_user_47()\n",
    "        # quick fix for user 47\n",
    "#         if owner_key == 29:\n",
    "            \n",
    "#             u1_array = np.concatenate([u1_array, u1_array[np.random.choice(u1_array.shape[0], size=total_samples_per_user-u1_array.shape[0], replace=False)]], axis=0)#55-47\n",
    "#             u2_array = np.concatenate([u2_array, u2_array[np.random.choice(u2_array.shape[0], size=total_samples_per_user-u2_array.shape[0], replace=False)]], axis=0)\n",
    "                \n",
    "        # print(u1_array.shape)\n",
    "        # print(u2_array.shape)\n",
    "        # try:\n",
    "        # print(f\"owner_key: {owner_key}, u1_array.shape: {u1_array.shape}, u2_array.shape: {u2_array.shape}\")\n",
    "        positive_pairs_dic[owner_key] = np.array(list(zip(u1_array[positive_left], u2_array[positive_right])))\n",
    "        # except:\n",
    "        #     print(owner_key)\n",
    "        # print(f\"positive_pairs per user: {positive_pairs_dic[owner_key].shape[0]}\")\n",
    "        new_pos_size = int(np.round(limit_ratio * positive_pairs_dic[owner_key].shape[0]))\n",
    "        positive_pairs_dic[owner_key] = positive_pairs_dic[owner_key][np.random.choice(\n",
    "                    positive_pairs_dic[owner_key].shape[0], size=new_pos_size, replace=False), :]\n",
    "        \n",
    "\n",
    "        for u2_key in user_id_list:\n",
    "            # 47 55\n",
    "            np.random.seed(SEED + u2_key)\n",
    "            # should i eliminate less informative pairs like (w_n, w_m) vs (w_m, w_n)?\n",
    "            if u2_key != owner_key:\n",
    "                u2_array = np.array(transform_user_windows(X_exp1_dic[u2_key], fitted_raw_scaler_dict[owner_key]))\n",
    "                \n",
    "                # quick fix no longer needed as this is done in get_raw_windows_user_47()\n",
    "                # quick fix for user 47\n",
    "                # if u2_key == 29:\n",
    "                #     u2_array = np.concatenate([u2_array, u2_array[np.random.choice(u2_array.shape[0], size=total_samples_per_user-u2_array.shape[0], replace=False)]], axis=0)#55-47,\n",
    "                \n",
    "                \n",
    "                # print(u1_array.shape)\n",
    "                # print(u2_array.shape)\n",
    "                # print(f\"u2_key {u2_key}, u1_array.shape: {u1_array.shape}, u2_array.shape: {u2_array.shape}\")\n",
    "                negative_pairs_dic[(owner_key, u2_key)] = np.array(list(zip(u1_array[negative_left], u2_array[negative_right])))\n",
    "                # print(f\"neg_pairs per user: {negative_pairs_dic[(owner_key, u2_key)].shape[0]}\")\n",
    "                \n",
    "                new_neg_size = int(np.round(negative_pairs_dic[(owner_key, u2_key)].shape[0] * neg_fraction * limit_ratio))\n",
    "                negative_pairs_dic[(owner_key, u2_key)] = negative_pairs_dic[(owner_key, u2_key)][np.random.choice(\n",
    "                    negative_pairs_dic[(owner_key, u2_key)].shape[0], size=new_neg_size, replace=False), :]\n",
    "                \n",
    "    # print(new_pos_size)\n",
    "    # print(new_neg_size)\n",
    "    return {\"positive_pairs_dic\": positive_pairs_dic, \"negative_pairs_dic\": negative_pairs_dic}\n",
    "\n",
    "\n",
    "\n",
    "# def simple_moving_average_np(arr, window_size):\n",
    "#     weights = np.repeat(1.0, window_size) / window_size\n",
    "#     sma = np.convolve(arr, weights, mode='same')\n",
    "#     return sma\n",
    "\n",
    "# def exponential_moving_average_3d(arr, window_size):\n",
    "#     num_windows, win_size, num_axes = arr.shape\n",
    "#     weights = np.exp(np.linspace(-1., 0., window_size))\n",
    "#     weights /= weights.sum()\n",
    "#     ema = np.zeros_like(arr)\n",
    "\n",
    "#     print(arr.shape)\n",
    "#     for i in range(num_axes):\n",
    "#         axis_data = arr[:, :, i]\n",
    "#         ema[:, :, i] = np.convolve(axis_data, weights[::-1], mode='same')\n",
    "\n",
    "#     return ema\n",
    "\n",
    "def generate_pairs_train(X_exp2_dic, X_exp1_dic, user_id_list, fitted_raw_Robust_scaler_dict, num_pair_limit = 99999999):\n",
    "    '''\n",
    "    return positive and negative pairs\n",
    "    '''\n",
    "    num_users = len(user_id_list)\n",
    "    # print(num_users)\n",
    "#     X_neg[np.random.choice(X_neg.shape[0], size=X_pos.shape[0], replace=False), :]\n",
    "    negative_pairs_dic = {}\n",
    "    positive_pairs_dic = {}\n",
    "    \n",
    "    total_samples_per_user = len(X_exp2_dic[list(X_exp2_dic.keys())[0]])\n",
    "    \n",
    "    # sign flipping\n",
    "    total_samples_per_user*=2\n",
    "    \n",
    "    # print(total_samples_per_user)\n",
    "    positive_pairs_indices = generate_index_pairs(total_samples_per_user)\n",
    "    negative_pairs_indices = generate_index_pairs(total_samples_per_user)\n",
    "    \n",
    "    # negative_pairs_indices = negative_pairs_indices[np.random.choice(negative_pairs_indices.shape[0], size=positive_pairs_indices.shape[0], replace=False)]\n",
    "    # negative_pairs_indices = np.sort(negative_pairs_indices)\n",
    "    # print(negative_pairs_indices)\n",
    "    positive_left = positive_pairs_indices[:, 0]\n",
    "    positive_right = positive_pairs_indices[:, 1]\n",
    "    print(total_samples_per_user)\n",
    "    negative_left = negative_pairs_indices[:, 0]\n",
    "    negative_right = negative_pairs_indices[:, 1]\n",
    "    # fitted_scaler_SNN_exp2_train_dic\n",
    "    # print(positive_pairs_indices.shape)\n",
    "    # print(negative_pairs_indices.shape)\n",
    "    \n",
    "    total_num_pairs = (positive_pairs_indices.shape[0]+negative_pairs_indices.shape[0]) * num_users \n",
    "    if num_pair_limit < total_num_pairs: \n",
    "        limit_ratio = num_pair_limit/total_num_pairs\n",
    "    else:\n",
    "        limit_ratio=1\n",
    "    \n",
    "    print(limit_ratio)\n",
    "    \n",
    "\n",
    "    neg_fraction =  1 / (num_users-1)  # works accurate\n",
    "\n",
    "    for owner_key in tqdm(user_id_list):\n",
    "\n",
    "        np.random.seed(SEED + owner_key + 2)\n",
    "\n",
    "        u1_array = np.array(X_exp2_dic[owner_key])\n",
    "        \n",
    "        #-------- new code --------\n",
    "        flipped_data = u1_array.copy()\n",
    "        flipped_data[:, :, 0:2] *= -1\n",
    "        flipped_data[:, :, 4:6] *= -1\n",
    "\n",
    "        # ema=exponential_moving_average_3d(u1_array, 25)\n",
    "        # u1_array = np.concatenate((u1_array, ema), axis=0)\n",
    "        # Stack the original data and flipped data along a new axis\n",
    "        u1_array = np.concatenate((u1_array, flipped_data), axis=0)\n",
    "\n",
    "        #-------- new code --------\n",
    "        \n",
    "\n",
    "        # print(np.amin(np.reshape(u1_array, (-1, 6)), axis=0))\n",
    "        # print(np.amax(np.reshape(u1_array, (-1, 6)), axis=0))\n",
    "        #-------- new code --------\n",
    "        # Reshape the data to 2D array with shape (number of windows * size of window, 6)\n",
    "        reshaped_data = np.reshape(u1_array, (-1, 6))\n",
    "        \n",
    "        # print(f\"reshaped_data.shape: {reshaped_data.shape}\")\n",
    "        \n",
    "        # fit to owner data\n",
    "        # for tanh\n",
    "        owner_minMax_scaler = MinMaxScaler(feature_range=(-1.0, 1.0), clip=False)\n",
    "        \n",
    "        # for relu\n",
    "        # owner_minMax_scaler = MinMaxScaler(feature_range=(0, 2.0))\n",
    "        \n",
    "        owner_minMax_scaler = owner_minMax_scaler.fit(reshaped_data)\n",
    "        \n",
    "\n",
    "        scaled_data = owner_minMax_scaler.transform(reshaped_data)\n",
    "        \n",
    "        # Reshape the scaled data back to the original shape\n",
    "        scaled_data = np.reshape(scaled_data, (u1_array.shape[0], u1_array.shape[1], 6))\n",
    "        \n",
    "        u1_array = scaled_data\n",
    "        #-------- new code --------\n",
    "        \n",
    "        \n",
    "        # apply robust on owner unknown data\n",
    "        u2_array = np.array(transform_user_windows(X_exp1_dic[owner_key], fitted_raw_Robust_scaler_dict[owner_key]))\n",
    "        \n",
    "        #-------- new code --------\n",
    "        flipped_data = u2_array.copy()\n",
    "        flipped_data[:, :, 0:2] *= -1\n",
    "        flipped_data[:, :, 4:6] *= -1\n",
    "\n",
    "        \n",
    "        # ema=exponential_moving_average_3d(u2_array, 25)\n",
    "        # u2_array = np.concatenate((u2_array, ema), axis=0)\n",
    "        # Stack the original data and flipped data along a new axis\n",
    "        u2_array = np.concatenate((u2_array, flipped_data), axis=0)\n",
    "        #-------- new code --------\n",
    "        \n",
    "        #-------- new code --------\n",
    "        \n",
    "        # normalize owner unknown data\n",
    "        # propper way\n",
    "        # u2_array = np.array(transform_user_windows(u2_array, fitted_raw_MinMax_scaler_dict[owner_key]))\n",
    "        \n",
    "        # Reshape the data to 2D array with shape (number of windows * size of window, 6)\n",
    "        reshaped_data = np.reshape(u2_array, (-1, 6))\n",
    "        \n",
    "        # print(f\"reshaped_data.shape: {reshaped_data.shape}\") \n",
    "\n",
    "        scaled_data = owner_minMax_scaler.transform(reshaped_data)\n",
    "        \n",
    "        # Reshape the scaled data back to the original shape\n",
    "        scaled_data = np.reshape(scaled_data, (u2_array.shape[0], u2_array.shape[1], 6))\n",
    "        \n",
    "        u2_array = scaled_data\n",
    "        #-------- new code --------\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        # print(f\"u1_array.shape: {u1_array.shape}\")\n",
    "        # print(f\"u2_array.shape: {u2_array.shape}\")\n",
    "        \n",
    "        \n",
    "        np.random.shuffle(u1_array)\n",
    "        np.random.shuffle(u2_array)\n",
    "        positive_pairs_dic[owner_key] = np.array(list(zip(u1_array[positive_left], u2_array[positive_right])))\n",
    "\n",
    "        \n",
    "        new_pos_size = int(np.round(limit_ratio * positive_pairs_dic[owner_key].shape[0]))\n",
    "        positive_pairs_dic[owner_key] = positive_pairs_dic[owner_key][np.random.choice(\n",
    "                    positive_pairs_dic[owner_key].shape[0], size=new_pos_size, replace=False), :]\n",
    "        \n",
    "\n",
    "        for u2_key in user_id_list:\n",
    "\n",
    "            np.random.seed(SEED + (owner_key+2)*(u2_key+ 2))\n",
    "\n",
    "            if u2_key != owner_key:\n",
    "                \n",
    "                # apply robust scaler on adversary unknown data\n",
    "                u2_array = np.array(transform_user_windows(X_exp1_dic[u2_key], fitted_raw_Robust_scaler_dict[owner_key]))\n",
    "                \n",
    "                #-------- new code --------\n",
    "                flipped_data = u2_array.copy()\n",
    "                flipped_data[:, :, 0:2] *= -1\n",
    "                flipped_data[:, :, 4:6] *= -1\n",
    "\n",
    "                # ema=exponential_moving_average_3d(u2_array, 25)\n",
    "                # u2_array = np.concatenate((u2_array, ema), axis=0)\n",
    "                \n",
    "                # Stack the original data and flipped data along a new axis\n",
    "                u2_array = np.concatenate((u2_array, flipped_data), axis=0)\n",
    "                #-------- new code --------\n",
    "                \n",
    "                #-------- new code --------\n",
    "\n",
    "                # apply min max on adversary unknown data\n",
    "                # u2_array = np.array(transform_user_windows(u2_array, fitted_raw_MinMax_scaler_dict[owner_key]))\n",
    "\n",
    "                # Reshape the data to 2D array with shape (number of windows * size of window, 6)\n",
    "                reshaped_data = np.reshape(u2_array, (-1, 6))\n",
    "\n",
    "                # print(f\"reshaped_data.shape: {reshaped_data.shape}\") \n",
    "\n",
    "                scaled_data = owner_minMax_scaler.transform(reshaped_data)\n",
    "\n",
    "                # Reshape the scaled data back to the original shape\n",
    "                scaled_data = np.reshape(scaled_data, (u2_array.shape[0], u2_array.shape[1], 6))\n",
    "\n",
    "                u2_array = scaled_data\n",
    "                #-------- new code --------\n",
    "        \n",
    "                \n",
    "                np.random.shuffle(u1_array)\n",
    "                np.random.shuffle(u2_array)\n",
    "                negative_pairs_dic[(owner_key, u2_key)] = np.array(list(zip(u1_array[negative_left], u2_array[negative_right])))\n",
    "\n",
    "                \n",
    "                new_neg_size = int(np.round(negative_pairs_dic[(owner_key, u2_key)].shape[0] * neg_fraction * limit_ratio))\n",
    "                negative_pairs_dic[(owner_key, u2_key)] = negative_pairs_dic[(owner_key, u2_key)][np.random.choice(\n",
    "                    negative_pairs_dic[(owner_key, u2_key)].shape[0], size=new_neg_size, replace=False), :]\n",
    "                \n",
    "\n",
    "    return {\"positive_pairs_dic\": positive_pairs_dic, \"negative_pairs_dic\": negative_pairs_dic}\n",
    "\n",
    "\n",
    "def generate_pairs_valid_test(X_exp2_dic, X_exp1_dic, user_id_list, fitted_raw_Robust_scaler_dict, num_pair_limit = 99999999):\n",
    "    '''\n",
    "    return positive and negative pairs\n",
    "    '''\n",
    "    num_users = len(user_id_list)\n",
    "    # print(num_users)\n",
    "#     X_neg[np.random.choice(X_neg.shape[0], size=X_pos.shape[0], replace=False), :]\n",
    "    negative_pairs_dic = {}\n",
    "    positive_pairs_dic = {}\n",
    "    \n",
    "    total_samples_per_user = len(X_exp2_dic[list(X_exp2_dic.keys())[0]])\n",
    "    \n",
    "    # NO sign flipping\n",
    "    # total_samples_per_user*=2\n",
    "    \n",
    "    # print(total_samples_per_user)\n",
    "    positive_pairs_indices = generate_index_pairs(total_samples_per_user)\n",
    "    negative_pairs_indices = generate_index_pairs(total_samples_per_user)\n",
    "    \n",
    "    # negative_pairs_indices = negative_pairs_indices[np.random.choice(negative_pairs_indices.shape[0], size=positive_pairs_indices.shape[0], replace=False)]\n",
    "    # negative_pairs_indices = np.sort(negative_pairs_indices)\n",
    "    # print(negative_pairs_indices)\n",
    "    positive_left = positive_pairs_indices[:, 0]\n",
    "    positive_right = positive_pairs_indices[:, 1]\n",
    "    print(total_samples_per_user)\n",
    "    negative_left = negative_pairs_indices[:, 0]\n",
    "    negative_right = negative_pairs_indices[:, 1]\n",
    "    # fitted_scaler_SNN_exp2_train_dic\n",
    "    # print(positive_pairs_indices.shape)\n",
    "    # print(negative_pairs_indices.shape)\n",
    "    \n",
    "    total_num_pairs = (positive_pairs_indices.shape[0]+negative_pairs_indices.shape[0]) * num_users \n",
    "    if num_pair_limit < total_num_pairs: \n",
    "        limit_ratio = num_pair_limit/total_num_pairs\n",
    "    else:\n",
    "        limit_ratio=1\n",
    "    \n",
    "    print(limit_ratio)\n",
    "    \n",
    "\n",
    "    neg_fraction =  1 / (num_users-1)  # works accurate\n",
    "\n",
    "    for owner_key in tqdm(user_id_list):\n",
    "\n",
    "        np.random.seed(SEED + owner_key + 2)\n",
    "\n",
    "        u1_array = np.array(X_exp2_dic[owner_key])\n",
    "        \n",
    " \n",
    "        \n",
    "\n",
    "        # print(np.amin(np.reshape(u1_array, (-1, 6)), axis=0))\n",
    "        # print(np.amax(np.reshape(u1_array, (-1, 6)), axis=0))\n",
    "        #-------- new code --------\n",
    "        # Reshape the data to 2D array with shape (number of windows * size of window, 6)\n",
    "        reshaped_data = np.reshape(u1_array, (-1, 6))\n",
    "        \n",
    "        # print(f\"reshaped_data.shape: {reshaped_data.shape}\")\n",
    "        \n",
    "        # fit to owner data\n",
    "        # for tanh\n",
    "        owner_minMax_scaler = MinMaxScaler(feature_range=(-1.0, 1.0), clip=False)\n",
    "        \n",
    "        # for relu\n",
    "        # owner_minMax_scaler = MinMaxScaler(feature_range=(0, 2.0))\n",
    "        \n",
    "        owner_minMax_scaler = owner_minMax_scaler.fit(reshaped_data)\n",
    "        \n",
    "\n",
    "        scaled_data = owner_minMax_scaler.transform(reshaped_data)\n",
    "        \n",
    "        # Reshape the scaled data back to the original shape\n",
    "        scaled_data = np.reshape(scaled_data, (u1_array.shape[0], u1_array.shape[1], u1_array.shape[2]))\n",
    "        \n",
    "        u1_array = scaled_data\n",
    "        #-------- new code --------\n",
    "        \n",
    "        \n",
    "        # apply robust on owner unknown data\n",
    "        u2_array = np.array(transform_user_windows(X_exp1_dic[owner_key], fitted_raw_Robust_scaler_dict[owner_key]))\n",
    "        \n",
    "        \n",
    "        #-------- new code --------\n",
    "        \n",
    "        # normalize owner unknown data\n",
    "        # propper way\n",
    "        # u2_array = np.array(transform_user_windows(u2_array, fitted_raw_MinMax_scaler_dict[owner_key]))\n",
    "        \n",
    "        # Reshape the data to 2D array with shape (number of windows * size of window, 6)\n",
    "        reshaped_data = np.reshape(u2_array, (-1, 6))\n",
    "        \n",
    "        # print(f\"reshaped_data.shape: {reshaped_data.shape}\") \n",
    "\n",
    "        scaled_data = owner_minMax_scaler.transform(reshaped_data)\n",
    "        \n",
    "        # Reshape the scaled data back to the original shape\n",
    "        scaled_data = np.reshape(scaled_data, (u2_array.shape[0], u2_array.shape[1], u2_array.shape[2]))\n",
    "        \n",
    "        u2_array = scaled_data\n",
    "        #-------- new code --------\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        # print(f\"u1_array.shape: {u1_array.shape}\")\n",
    "        # print(f\"u2_array.shape: {u2_array.shape}\")\n",
    "        \n",
    "        \n",
    "        np.random.shuffle(u1_array)\n",
    "        np.random.shuffle(u2_array)\n",
    "        positive_pairs_dic[owner_key] = np.array(list(zip(u1_array[positive_left], u2_array[positive_right])))\n",
    "\n",
    "        \n",
    "        new_pos_size = int(np.round(limit_ratio * positive_pairs_dic[owner_key].shape[0]))\n",
    "        positive_pairs_dic[owner_key] = positive_pairs_dic[owner_key][np.random.choice(\n",
    "                    positive_pairs_dic[owner_key].shape[0], size=new_pos_size, replace=False), :]\n",
    "        \n",
    "\n",
    "        for u2_key in user_id_list:\n",
    "\n",
    "            np.random.seed(SEED + (owner_key+2)*(u2_key+ 2))\n",
    "\n",
    "            if u2_key != owner_key:\n",
    "                \n",
    "                # apply robust scaler on adversary unknown data\n",
    "                u2_array = np.array(transform_user_windows(X_exp1_dic[u2_key], fitted_raw_Robust_scaler_dict[owner_key]))\n",
    "                \n",
    "                #-------- new code --------\n",
    "\n",
    "                # apply min max on adversary unknown data\n",
    "                # u2_array = np.array(transform_user_windows(u2_array, fitted_raw_MinMax_scaler_dict[owner_key]))\n",
    "\n",
    "                # Reshape the data to 2D array with shape (number of windows * size of window, 6)\n",
    "                reshaped_data = np.reshape(u2_array, (-1, 6))\n",
    "\n",
    "                # print(f\"reshaped_data.shape: {reshaped_data.shape}\") \n",
    "\n",
    "                scaled_data = owner_minMax_scaler.transform(reshaped_data)\n",
    "\n",
    "                # Reshape the scaled data back to the original shape\n",
    "                scaled_data = np.reshape(scaled_data, (u2_array.shape[0], u2_array.shape[1], u2_array.shape[2]))\n",
    "\n",
    "                u2_array = scaled_data\n",
    "                #-------- new code --------\n",
    "        \n",
    "                \n",
    "                np.random.shuffle(u1_array)\n",
    "                np.random.shuffle(u2_array)\n",
    "                negative_pairs_dic[(owner_key, u2_key)] = np.array(list(zip(u1_array[negative_left], u2_array[negative_right])))\n",
    "\n",
    "                \n",
    "                new_neg_size = int(np.round(negative_pairs_dic[(owner_key, u2_key)].shape[0] * neg_fraction * limit_ratio))\n",
    "                negative_pairs_dic[(owner_key, u2_key)] = negative_pairs_dic[(owner_key, u2_key)][np.random.choice(\n",
    "                    negative_pairs_dic[(owner_key, u2_key)].shape[0], size=new_neg_size, replace=False), :]\n",
    "                \n",
    "\n",
    "    return {\"positive_pairs_dic\": positive_pairs_dic, \"negative_pairs_dic\": negative_pairs_dic}\n",
    "# def transform_user_windows2(X_exp, fitted_scaler, columns):\n",
    "    \n",
    "    \n",
    "#         transformed_X_exp = []\n",
    "\n",
    "#     #     print(X_exp[0].shape)\n",
    "#         for window in X_exp:\n",
    "#             df=pd.DataFrame(window, columns=columns)\n",
    "#             scaled_array = fitted_scaler.transform(df)\n",
    "#             scaled_window_df = pd.DataFrame(data=scaled_array, columns = df.columns, dtype=scaled_array.dtype)\n",
    "#             transformed_X_exp.append(scaled_window_df)\n",
    "\n",
    "#         return transformed_X_exp\n",
    "    \n",
    "def generate_pairs3(X_exp2_dic, X_exp1_dic, user_id_list, fitted_raw_Robust_scaler_dict, fitted_raw_MinMax_scaler_dict=None, num_pair_limit = 99999999):\n",
    "    '''\n",
    "    return positive and negative pairs\n",
    "    '''\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    num_users = len(user_id_list)\n",
    "    # print(num_users)\n",
    "#     X_neg[np.random.choice(X_neg.shape[0], size=X_pos.shape[0], replace=False), :]\n",
    "    negative_pairs_dic = {}\n",
    "    positive_pairs_dic = {}\n",
    "    \n",
    "    total_samples_per_user = len(X_exp2_dic[list(X_exp2_dic.keys())[0]])\n",
    "    # sign flipping\n",
    "    total_samples_per_user*=2\n",
    "    # print(total_samples_per_user)\n",
    "    positive_pairs_indices = generate_index_pairs(total_samples_per_user)\n",
    "    negative_pairs_indices = generate_index_pairs(total_samples_per_user)\n",
    "    \n",
    "    # negative_pairs_indices = negative_pairs_indices[np.random.choice(negative_pairs_indices.shape[0], size=positive_pairs_indices.shape[0], replace=False)]\n",
    "    # negative_pairs_indices = np.sort(negative_pairs_indices)\n",
    "    # print(negative_pairs_indices)\n",
    "    positive_left = positive_pairs_indices[:, 0]\n",
    "    positive_right = positive_pairs_indices[:, 1]\n",
    "    print(total_samples_per_user)\n",
    "    negative_left = negative_pairs_indices[:, 0]\n",
    "    negative_right = negative_pairs_indices[:, 1]\n",
    "    # fitted_scaler_SNN_exp2_train_dic\n",
    "    # print(positive_pairs_indices.shape)\n",
    "    # print(negative_pairs_indices.shape)\n",
    "    \n",
    "    total_num_pairs = (positive_pairs_indices.shape[0]+negative_pairs_indices.shape[0]) * num_users \n",
    "    if num_pair_limit < total_num_pairs: \n",
    "        limit_ratio = num_pair_limit/total_num_pairs\n",
    "    else:\n",
    "        limit_ratio=1\n",
    "    \n",
    "    print(limit_ratio)\n",
    "    \n",
    "\n",
    "    neg_fraction =  1 / (num_users-1)  # works accurate\n",
    "\n",
    "    for owner_key in tqdm(user_id_list):\n",
    "\n",
    "        np.random.seed(SEED + owner_key)\n",
    "\n",
    "        \n",
    "        columns=X_exp2_dic[owner_key][0].columns\n",
    "        u1_array = np.array(X_exp2_dic[owner_key])\n",
    "        \n",
    "        #-------- new code --------\n",
    "        flipped_data = u1_array.copy()\n",
    "        flipped_data[:, :, 0:2] *= -1\n",
    "        flipped_data[:, :, 4:6] *= -1\n",
    "\n",
    "        # ema=exponential_moving_average_3d(u1_array, 25)\n",
    "        # u1_array = np.concatenate((u1_array, ema), axis=0)\n",
    "        # Stack the original data and flipped data along a new axis\n",
    "        u1_array = np.concatenate([u1_array, flipped_data], axis=0)\n",
    "\n",
    "        #-------- new code --------\n",
    "        \n",
    "\n",
    "        print(np.amin(np.reshape(u1_array, (-1, 6)), axis=0))\n",
    "        print(np.amax(np.reshape(u1_array, (-1, 6)), axis=0))\n",
    "\n",
    "        \n",
    "        u2_array = np.array(X_exp1_dic[owner_key])\n",
    "        #-------- new code --------\n",
    "        flipped_data = u2_array.copy()\n",
    "        flipped_data[:, :, 0:2] *= -1\n",
    "        flipped_data[:, :, 4:6] *= -1\n",
    "\n",
    "        \n",
    "        # ema=exponential_moving_average_3d(u2_array, 25)\n",
    "        # u2_array = np.concatenate((u2_array, ema), axis=0)\n",
    "        # Stack the original data and flipped data along a new axis\n",
    "\n",
    "        # apply robust on owner unknown data\n",
    "        u2_array = np.array(transform_user_windows2(u2_array, fitted_raw_Robust_scaler_dict[owner_key], columns))\n",
    "        u2_array = np.array(transform_user_windows2(u2_array, fitted_raw_MinMax_scaler_dict[owner_key], columns))\n",
    "        \n",
    "        flipped_data = np.array(transform_user_windows2(flipped_data, fitted_raw_Robust_scaler_dict[owner_key], columns))\n",
    "        flipped_data = np.array(transform_user_windows2(flipped_data, fitted_raw_MinMax_scaler_dict[owner_key], columns))\n",
    "        \n",
    "        u2_array = np.concatenate([u2_array, flipped_data], axis=0)\n",
    "        #-------- new code --------\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        print(np.amin(np.reshape(u2_array, (-1, 6)), axis=0))\n",
    "        print(np.amax(np.reshape(u2_array, (-1, 6)), axis=0))\n",
    "        \n",
    "        \n",
    "\n",
    "        positive_pairs_dic[owner_key] = np.array(list(zip(u1_array[positive_left], u2_array[positive_right])))\n",
    "\n",
    "        \n",
    "        new_pos_size = int(np.round(limit_ratio * positive_pairs_dic[owner_key].shape[0]))\n",
    "        positive_pairs_dic[owner_key] = positive_pairs_dic[owner_key][np.random.choice(\n",
    "                    positive_pairs_dic[owner_key].shape[0], size=new_pos_size, replace=False), :]\n",
    "        \n",
    "\n",
    "        for u2_key in user_id_list:\n",
    "\n",
    "            np.random.seed(SEED + u2_key)\n",
    "\n",
    "            if u2_key != owner_key:\n",
    "                \n",
    "                u2_array = np.array(X_exp1_dic[u2_key])\n",
    "                #-------- new code --------\n",
    "                flipped_data = u2_array.copy()\n",
    "                flipped_data[:, :, 0:2] *= -1\n",
    "                flipped_data[:, :, 4:6] *= -1\n",
    "\n",
    "                # ema=exponential_moving_average_3d(u2_array, 25)\n",
    "                # u2_array = np.concatenate((u2_array, ema), axis=0)\n",
    "                \n",
    "                # Stack the original data and flipped data along a new axis\n",
    "\n",
    "                # apply robust scaler on adversary unknown data\n",
    "                u2_array = np.array(transform_user_windows2(u2_array, fitted_raw_Robust_scaler_dict[owner_key], columns))\n",
    "                u2_array = np.array(transform_user_windows2(u2_array, fitted_raw_MinMax_scaler_dict[owner_key], columns))\n",
    "                \n",
    "                flipped_data = np.array(transform_user_windows2(flipped_data, fitted_raw_Robust_scaler_dict[owner_key], columns))\n",
    "                flipped_data = np.array(transform_user_windows2(flipped_data, fitted_raw_MinMax_scaler_dict[owner_key], columns))\n",
    "                \n",
    "                u2_array = np.concatenate([u2_array, flipped_data], axis=0)\n",
    "                #-------- new code --------\n",
    "                \n",
    "\n",
    "                \n",
    "                \n",
    "\n",
    "                negative_pairs_dic[(owner_key, u2_key)] = np.array(list(zip(u1_array[negative_left], u2_array[negative_right])))\n",
    "\n",
    "                \n",
    "                new_neg_size = int(np.round(negative_pairs_dic[(owner_key, u2_key)].shape[0] * neg_fraction * limit_ratio))\n",
    "                negative_pairs_dic[(owner_key, u2_key)] = negative_pairs_dic[(owner_key, u2_key)][np.random.choice(\n",
    "                    negative_pairs_dic[(owner_key, u2_key)].shape[0], size=new_neg_size, replace=False), :]\n",
    "                \n",
    "\n",
    "    return {\"positive_pairs_dic\": positive_pairs_dic, \"negative_pairs_dic\": negative_pairs_dic}\n",
    "\n",
    "\n",
    "def generate_pairs4(X_exp2_dic, X_exp1_dic, user_id_list, fitted_raw_Robust_scaler_dict, fitted_raw_MinMax_scaler_dict=None, num_pair_limit = 99999999):\n",
    "    '''\n",
    "    return positive and negative pairs\n",
    "    '''\n",
    "    num_users = len(user_id_list)\n",
    "    # print(num_users)\n",
    "#     X_neg[np.random.choice(X_neg.shape[0], size=X_pos.shape[0], replace=False), :]\n",
    "    negative_pairs_dic = {}\n",
    "    positive_pairs_dic = {}\n",
    "    \n",
    "    total_samples_per_user = len(X_exp2_dic[list(X_exp2_dic.keys())[0]])\n",
    "    # sign flipping\n",
    "    total_samples_per_user*=2\n",
    "    # print(total_samples_per_user)\n",
    "    positive_pairs_indices = generate_index_pairs(total_samples_per_user)\n",
    "    negative_pairs_indices = generate_index_pairs(total_samples_per_user)\n",
    "    \n",
    "    # negative_pairs_indices = negative_pairs_indices[np.random.choice(negative_pairs_indices.shape[0], size=positive_pairs_indices.shape[0], replace=False)]\n",
    "    # negative_pairs_indices = np.sort(negative_pairs_indices)\n",
    "    # print(negative_pairs_indices)\n",
    "    positive_left = positive_pairs_indices[:, 0]\n",
    "    positive_right = positive_pairs_indices[:, 1]\n",
    "    print(total_samples_per_user)\n",
    "    negative_left = negative_pairs_indices[:, 0]\n",
    "    negative_right = negative_pairs_indices[:, 1]\n",
    "    # fitted_scaler_SNN_exp2_train_dic\n",
    "    # print(positive_pairs_indices.shape)\n",
    "    # print(negative_pairs_indices.shape)\n",
    "    \n",
    "    total_num_pairs = (positive_pairs_indices.shape[0]+negative_pairs_indices.shape[0]) * num_users \n",
    "    if num_pair_limit < total_num_pairs: \n",
    "        limit_ratio = num_pair_limit/total_num_pairs\n",
    "    else:\n",
    "        limit_ratio=1\n",
    "    \n",
    "    print(limit_ratio)\n",
    "    \n",
    "\n",
    "    neg_fraction =  1 / (num_users-1)  # works accurate\n",
    "\n",
    "    for owner_key in tqdm(user_id_list):\n",
    "\n",
    "        np.random.seed(SEED + owner_key)\n",
    "\n",
    "        u1_array = np.array(X_exp2_dic[owner_key])\n",
    "        \n",
    "        #-------- new code --------\n",
    "        flipped_data = u1_array.copy()\n",
    "        flipped_data[:, :, 0:2] *= -1\n",
    "        flipped_data[:, :, 4:6] *= -1\n",
    "\n",
    "        # ema=exponential_moving_average_3d(u1_array, 25)\n",
    "        # u1_array = np.concatenate((u1_array, ema), axis=0)\n",
    "        # Stack the original data and flipped data along a new axis\n",
    "        u1_array = np.concatenate((u1_array, flipped_data), axis=0)\n",
    "\n",
    "        #-------- new code --------\n",
    "        \n",
    "\n",
    "        # print(np.amin(np.reshape(u1_array, (-1, 6)), axis=0))\n",
    "        # print(np.amax(np.reshape(u1_array, (-1, 6)), axis=0))\n",
    "        #-------- new code --------\n",
    "        # Reshape the data to 2D array with shape (number of windows * size of window, 6)\n",
    "        reshaped_data = np.reshape(u1_array, (-1, 6))\n",
    "        \n",
    "        # print(f\"reshaped_data.shape: {reshaped_data.shape}\")\n",
    "        \n",
    "        # fit to owner data\n",
    "        # for tanh\n",
    "        owner_minMax_scaler = MinMaxScaler(feature_range=(-1.0, 1.0), clip=False)\n",
    "        \n",
    "        # for relu\n",
    "        # owner_minMax_scaler = MinMaxScaler(feature_range=(0, 2.0))\n",
    "        \n",
    "        owner_minMax_scaler = owner_minMax_scaler.fit(reshaped_data)\n",
    "        \n",
    "\n",
    "        scaled_data = owner_minMax_scaler.transform(reshaped_data)\n",
    "        \n",
    "        # Reshape the scaled data back to the original shape\n",
    "        scaled_data = np.reshape(scaled_data, (u1_array.shape[0], u1_array.shape[1], 6))\n",
    "        \n",
    "        u1_array = scaled_data\n",
    "        #-------- new code --------\n",
    "        \n",
    "        \n",
    "        # apply robust on owner unknown data\n",
    "        u2_array = np.array(transform_user_windows(X_exp1_dic[owner_key], fitted_raw_Robust_scaler_dict[owner_key]))\n",
    "        \n",
    "        #-------- new code --------\n",
    "        flipped_data = u2_array.copy()\n",
    "        flipped_data[:, :, 0:2] *= -1\n",
    "        flipped_data[:, :, 4:6] *= -1\n",
    "\n",
    "        \n",
    "        # ema=exponential_moving_average_3d(u2_array, 25)\n",
    "        # u2_array = np.concatenate((u2_array, ema), axis=0)\n",
    "        # Stack the original data and flipped data along a new axis\n",
    "        u2_array = np.concatenate((u2_array, flipped_data), axis=0)\n",
    "        #-------- new code --------\n",
    "        \n",
    "        #-------- new code --------\n",
    "        \n",
    "        # normalize owner unknown data\n",
    "        # propper way\n",
    "        # u2_array = np.array(transform_user_windows(u2_array, fitted_raw_MinMax_scaler_dict[owner_key]))\n",
    "        \n",
    "        # Reshape the data to 2D array with shape (number of windows * size of window, 6)\n",
    "        reshaped_data = np.reshape(u2_array, (-1, 6))\n",
    "        \n",
    "        # print(f\"reshaped_data.shape: {reshaped_data.shape}\") \n",
    "\n",
    "        scaled_data = owner_minMax_scaler.transform(reshaped_data)\n",
    "        \n",
    "        # Reshape the scaled data back to the original shape\n",
    "        scaled_data = np.reshape(scaled_data, (u2_array.shape[0], u2_array.shape[1], 6))\n",
    "        \n",
    "        u2_array = scaled_data\n",
    "        #-------- new code --------\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        # print(f\"u1_array.shape: {u1_array.shape}\")\n",
    "        # print(f\"u2_array.shape: {u2_array.shape}\")\n",
    "        \n",
    "        \n",
    "        # np.random.shuffle()\n",
    "        # np.random.shuffle()\n",
    "        positive_pairs_dic[owner_key] = np.array(list(zip(u1_array[positive_left], u2_array[positive_right])))\n",
    "\n",
    "        \n",
    "        new_pos_size = int(np.round(limit_ratio * positive_pairs_dic[owner_key].shape[0]))\n",
    "        positive_pairs_dic[owner_key] = positive_pairs_dic[owner_key][np.random.choice(\n",
    "                    positive_pairs_dic[owner_key].shape[0], size=new_pos_size, replace=False), :]\n",
    "        \n",
    "\n",
    "        for u2_key in user_id_list:\n",
    "\n",
    "            np.random.seed(SEED + u2_key)\n",
    "\n",
    "            if u2_key != owner_key:\n",
    "                \n",
    "                # apply robust scaler on adversary unknown data\n",
    "                u2_array = np.array(transform_user_windows(X_exp1_dic[u2_key], fitted_raw_Robust_scaler_dict[owner_key]))\n",
    "                \n",
    "                #-------- new code --------\n",
    "                flipped_data = u2_array.copy()\n",
    "                flipped_data[:, :, 0:2] *= -1\n",
    "                flipped_data[:, :, 4:6] *= -1\n",
    "\n",
    "                # ema=exponential_moving_average_3d(u2_array, 25)\n",
    "                # u2_array = np.concatenate((u2_array, ema), axis=0)\n",
    "                \n",
    "                # Stack the original data and flipped data along a new axis\n",
    "                u2_array = np.concatenate((u2_array, flipped_data), axis=0)\n",
    "                #-------- new code --------\n",
    "                \n",
    "                #-------- new code --------\n",
    "\n",
    "                # apply min max on adversary unknown data\n",
    "                # u2_array = np.array(transform_user_windows(u2_array, fitted_raw_MinMax_scaler_dict[owner_key]))\n",
    "\n",
    "                # Reshape the data to 2D array with shape (number of windows * size of window, 6)\n",
    "                reshaped_data = np.reshape(u2_array, (-1, 6))\n",
    "\n",
    "                # print(f\"reshaped_data.shape: {reshaped_data.shape}\") \n",
    "\n",
    "                scaled_data = owner_minMax_scaler.transform(reshaped_data)\n",
    "\n",
    "                # Reshape the scaled data back to the original shape\n",
    "                scaled_data = np.reshape(scaled_data, (u2_array.shape[0], u2_array.shape[1], 6))\n",
    "\n",
    "                u2_array = scaled_data\n",
    "                #-------- new code --------\n",
    "        \n",
    "                \n",
    "\n",
    "                negative_pairs_dic[(owner_key, u2_key)] = np.array(list(zip(u1_array[negative_left], u2_array[negative_right])))\n",
    "\n",
    "                \n",
    "                new_neg_size = int(np.round(negative_pairs_dic[(owner_key, u2_key)].shape[0] * neg_fraction * limit_ratio))\n",
    "                negative_pairs_dic[(owner_key, u2_key)] = negative_pairs_dic[(owner_key, u2_key)][np.random.choice(\n",
    "                    negative_pairs_dic[(owner_key, u2_key)].shape[0], size=new_neg_size, replace=False), :]\n",
    "                \n",
    "\n",
    "    return {\"positive_pairs_dic\": positive_pairs_dic, \"negative_pairs_dic\": negative_pairs_dic}\n",
    "\n",
    "\n",
    "def get_pos_array(X_dic):\n",
    "    \n",
    "    return np.concatenate([X_dic[key] for key in X_dic])\n",
    "\n",
    "def get_neg_array(X_dic):\n",
    "    \n",
    "    return np.concatenate([X_dic[key] for key in X_dic])\n",
    "\n",
    "# sklearn.utils.shuffle(*arrays, random_state=None, n_samples=None)\n",
    "def prep_X_y_pair(X_exp2_dic, X_exp1_dic, user_id_list, fitted_raw_scaler_dict, num_pair_limit = 50000):\n",
    "    \n",
    "    X_dic = generate_pairs(X_exp2_dic, X_exp1_dic, user_id_list, fitted_raw_scaler_dict=fitted_raw_scaler_dict, num_pair_limit=num_pair_limit)\n",
    "    # print(X_dic['negative_pairs_dic'].keys())\n",
    "    pos_X, neg_X = get_pos_array(X_dic['positive_pairs_dic']), get_neg_array(X_dic['negative_pairs_dic'])\n",
    "    \n",
    "    pos_y = np.repeat(1., pos_X.shape[0]).reshape((pos_X.shape[0], 1))\n",
    "    neg_y = np.repeat(0., neg_X.shape[0]).reshape((neg_X.shape[0], 1))\n",
    "    \n",
    "    pos_X_left = pos_X[:, 0, :, :]\n",
    "    pos_X_right = pos_X[:, 1, :, :]\n",
    "    \n",
    "    neg_X_left = neg_X[:, 0, :, :]\n",
    "    neg_X_right = neg_X[:, 1, :, :]\n",
    "    \n",
    "    X_left = np.concatenate([pos_X_left, neg_X_left]).astype(\"float32\")\n",
    "    X_right = np.concatenate([pos_X_right, neg_X_right]).astype(\"float32\")\n",
    "    y = np.concatenate([pos_y, neg_y]).astype(\"float32\")\n",
    "\n",
    "    np.random.seed(SEED)\n",
    "    X_left, X_right, y = sklearn_shuffle(X_left, X_right, y, random_state=SEED)\n",
    "    \n",
    "    X = [X_left, X_right]\n",
    "    \n",
    "    return {\"X\": X, \"y\": y, \"X_dic\": X_dic}\n",
    "\n",
    "def prep_X_y_pair_robust_minmax(X_exp2_dic, X_exp1_dic, user_id_list, fitted_raw_Robust_scaler_dict, is_train, num_pair_limit = 50000):\n",
    "    \n",
    "    if is_train:\n",
    "        X_dic = generate_pairs_train(X_exp2_dic, X_exp1_dic, user_id_list, \n",
    "                                     fitted_raw_Robust_scaler_dict=fitted_raw_Robust_scaler_dict, \n",
    "                                     num_pair_limit=num_pair_limit)\n",
    "        \n",
    "    else:\n",
    "        X_dic = generate_pairs_valid_test(X_exp2_dic, X_exp1_dic, user_id_list, \n",
    "                                          fitted_raw_Robust_scaler_dict=fitted_raw_Robust_scaler_dict, \n",
    "                                          num_pair_limit=num_pair_limit)\n",
    "    \n",
    "    \n",
    "    # print(X_dic['negative_pairs_dic'].keys())\n",
    "    pos_X, neg_X = get_pos_array(X_dic['positive_pairs_dic']), get_neg_array(X_dic['negative_pairs_dic'])\n",
    "    \n",
    "    pos_y = np.repeat(1., pos_X.shape[0]).reshape((pos_X.shape[0], 1))\n",
    "    neg_y = np.repeat(0., neg_X.shape[0]).reshape((neg_X.shape[0], 1))\n",
    "    \n",
    "    pos_X_left = pos_X[:, 0, :, :]\n",
    "    pos_X_right = pos_X[:, 1, :, :]\n",
    "    \n",
    "    neg_X_left = neg_X[:, 0, :, :]\n",
    "    neg_X_right = neg_X[:, 1, :, :]\n",
    "    \n",
    "    X_left = np.concatenate([pos_X_left, neg_X_left]).astype(\"float32\")\n",
    "    X_right = np.concatenate([pos_X_right, neg_X_right]).astype(\"float32\")\n",
    "    y = np.concatenate([pos_y, neg_y]).astype(\"float32\")\n",
    "\n",
    "    np.random.seed(SEED)\n",
    "    X_left, X_right, y = sklearn_shuffle(X_left, X_right, y, random_state=SEED)\n",
    "    \n",
    "    X = [X_left, X_right]\n",
    "    \n",
    "    return {\"X\": X, \"y\": y, \"X_dic\": X_dic}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# coppied and modified to get optimizer: https://github.com/keras-team/keras/blob/v2.11.0/keras/callbacks.py#L2905-L3042\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "class MyReduceLROnPlateau(Callback):\n",
    "    \"\"\"Reduce learning rate when a metric has stopped improving.\n",
    "    Models often benefit from reducing the learning rate by a factor\n",
    "    of 2-10 once learning stagnates. This callback monitors a\n",
    "    quantity and if no improvement is seen for a 'patience' number\n",
    "    of epochs, the learning rate is reduced.\n",
    "    Example:\n",
    "    ```python\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                  patience=5, min_lr=0.001)\n",
    "    model.fit(X_train, Y_train, callbacks=[reduce_lr])\n",
    "    ```\n",
    "    Args:\n",
    "        monitor: quantity to be monitored.\n",
    "        factor: factor by which the learning rate will be reduced.\n",
    "          `new_lr = lr * factor`.\n",
    "        patience: number of epochs with no improvement after which learning rate\n",
    "          will be reduced.\n",
    "        verbose: int. 0: quiet, 1: update messages.\n",
    "        mode: one of `{'auto', 'min', 'max'}`. In `'min'` mode,\n",
    "          the learning rate will be reduced when the\n",
    "          quantity monitored has stopped decreasing; in `'max'` mode it will be\n",
    "          reduced when the quantity monitored has stopped increasing; in\n",
    "          `'auto'` mode, the direction is automatically inferred from the name\n",
    "          of the monitored quantity.\n",
    "        min_delta: threshold for measuring the new optimum, to only focus on\n",
    "          significant changes.\n",
    "        cooldown: number of epochs to wait before resuming normal operation\n",
    "          after lr has been reduced.\n",
    "        min_lr: lower bound on the learning rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.1,\n",
    "        patience=10,\n",
    "        verbose=0,\n",
    "        mode=\"auto\",\n",
    "        min_delta=1e-4,\n",
    "        cooldown=0,\n",
    "        min_lr=0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.monitor = monitor\n",
    "        if factor >= 1.0:\n",
    "            raise ValueError(\n",
    "                \"ReduceLROnPlateau does not support \"\n",
    "                f\"a factor >= 1.0. Got {factor}\"\n",
    "            )\n",
    "        if \"epsilon\" in kwargs:\n",
    "            min_delta = kwargs.pop(\"epsilon\")\n",
    "            logging.warning(\n",
    "                \"`epsilon` argument is deprecated and \"\n",
    "                \"will be removed, use `min_delta` instead.\"\n",
    "            )\n",
    "        self.factor = factor\n",
    "        self.min_lr = min_lr\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.cooldown = cooldown\n",
    "        self.cooldown_counter = 0  # Cooldown counter.\n",
    "        self.wait = 0\n",
    "        self.best = 0\n",
    "        self.mode = mode\n",
    "        self.monitor_op = None\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Resets wait counter and cooldown counter.\"\"\"\n",
    "        if self.mode not in [\"auto\", \"min\", \"max\"]:\n",
    "            logging.warning(\n",
    "                \"Learning rate reduction mode %s is unknown, \"\n",
    "                \"fallback to auto mode.\",\n",
    "                self.mode,\n",
    "            )\n",
    "            self.mode = \"auto\"\n",
    "        if self.mode == \"min\" or (\n",
    "            self.mode == \"auto\" and \"acc\" not in self.monitor\n",
    "        ):\n",
    "            self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)\n",
    "            self.best = np.Inf\n",
    "        else:\n",
    "            self.monitor_op = lambda a, b: np.greater(a, b + self.min_delta)\n",
    "            self.best = -np.Inf\n",
    "        self.cooldown_counter = 0\n",
    "        self.wait = 0\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self._reset()\n",
    "\n",
    "    def on_epoch_end(self, epoch, model, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs[\"lr\"] = K.get_value(model.optimizer.lr)\n",
    "        # print(logs)\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            logging.warning(\n",
    "                \"Learning rate reduction is conditioned on metric `%s` \"\n",
    "                \"which is not available. Available metrics are: %s\",\n",
    "                self.monitor,\n",
    "                \",\".join(list(logs.keys())),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            if self.in_cooldown():\n",
    "                self.cooldown_counter -= 1\n",
    "                self.wait = 0\n",
    "\n",
    "            if self.monitor_op(current, self.best):\n",
    "                self.best = current\n",
    "                self.wait = 0\n",
    "            elif not self.in_cooldown():\n",
    "                self.wait += 1\n",
    "                if self.wait >= self.patience:\n",
    "                    old_lr = K.get_value(model.optimizer.lr)\n",
    "                    if old_lr > np.float32(self.min_lr):\n",
    "                        new_lr = old_lr * self.factor\n",
    "                        new_lr = max(new_lr, self.min_lr)\n",
    "                        K.set_value(model.optimizer.lr, new_lr)\n",
    "                        if self.verbose > 0:\n",
    "                            print(\n",
    "                                f\"\\nEpoch {epoch +1}: \"\n",
    "                                \"ReduceLROnPlateau reducing \"\n",
    "                                f\"learning rate to {new_lr}.\"\n",
    "                            )\n",
    "                        self.cooldown_counter = self.cooldown\n",
    "                        self.wait = 0\n",
    "\n",
    "    def in_cooldown(self):\n",
    "        return self.cooldown_counter > 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MetricsCallback(Callback):\n",
    "    \"\"\"\n",
    "    Custom Keras Callback function.\n",
    "    \n",
    "    Used to predict and plot distances for positive and negative pairs\n",
    "    after each n-th epoch, along with some 'classification' metrics. \n",
    "    'Classification' here means to ability to distinguish between positive \n",
    "    and negative pairs using a threshold for the distance.\n",
    "    \n",
    "    Arguments:\n",
    "        payload {tuple}           -- Datasets used for evaluation: (X_valid, y_valid, X_train, y_train)\n",
    "        epoch_evaluate_freq {int} -- Frequency for evaluation. After every n-th epoch, \n",
    "                                     the results are evaluated and printed\n",
    "        save_plots {boolean}      -- Do you want to save plots as PDF? Path is configured via global\n",
    "                                     parameter REPORT_PATH.\n",
    "    \"\"\"\n",
    "    def __init__(self, payload, epoch_evaluate_freq=1, loss_record_dict={}, metric_record_dict={}, save_plots=False, \n",
    "                 plot_pca=False, print_interm_epochs=True, early_stoping=False, ReduceLROnPlateau_args=None, optimal_lr_epoch_dict=None, lr_epoch_log_dict={}, verbose=0):\n",
    "        # super(MetricsCallback, self).__init__()\n",
    "        \n",
    "        self.verbose=verbose\n",
    "        self.lr_epoch_log_dict=lr_epoch_log_dict\n",
    "        \n",
    "        self.optimal_lr_epoch_dict=optimal_lr_epoch_dict\n",
    "            \n",
    "        \n",
    "        self.ReduceLROnPlateau_args=ReduceLROnPlateau_args\n",
    "        self.valid_metrics={\"roc_val\", \"eer_val\", \"thres\", \"acc_val\", \"f1_val\"}\n",
    "        if self.ReduceLROnPlateau_args != None:\n",
    "            ReduceLROnPlateau_mode={\"val_loss\": \"min\", \"roc_val\": \"max\", \"eer_val\": \"min\", \"thres\": \"min\", \"acc_val\": \"max\", \"f1_val\": \"max\"}\n",
    "            self.reduce_lr = MyReduceLROnPlateau(monitor=ReduceLROnPlateau_args['mointored_metric'], factor=ReduceLROnPlateau_args[\"factor\"], \n",
    "                                               patience=ReduceLROnPlateau_args[\"patience\"], verbose=ReduceLROnPlateau_args[\"verbose\"], \n",
    "                                               min_lr=ReduceLROnPlateau_args[\"min_lr\"], mode=ReduceLROnPlateau_mode[ReduceLROnPlateau_args['mointored_metric']])\n",
    "\n",
    "        self.X_valid, self.y_valid, self.X_train, self.y_train = payload\n",
    "        self.save_plots = save_plots\n",
    "        self.epoch_evaluate_freq = epoch_evaluate_freq\n",
    "        self.loss_record_dict = loss_record_dict\n",
    "        self.metric_record_dict = metric_record_dict\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "        self.plot_pca = plot_pca\n",
    "        self.print_interm_epochs = print_interm_epochs\n",
    "        self.early_stoping = early_stoping\n",
    "        \n",
    "        self.metric_record_dict['roc_val'] = {\"Train\": [], \"Valid\": []}\n",
    "        self.metric_record_dict['eer_val'] = {\"Train\": [], \"Valid\": []}\n",
    "        self.metric_record_dict['thres'] = {\"Train\": [], \"Valid\": []}\n",
    "        self.metric_record_dict['acc_val'] = {\"Train\": [], \"Valid\": []}\n",
    "        self.metric_record_dict['f1_val'] = {\"Train\": [], \"Valid\": []}\n",
    "        \n",
    "        # Do we have train and valid set?\n",
    "        self.sets = []\n",
    "        if self.X_train:\n",
    "            self.sets.append([self.X_train, self.y_train, \"Train\"])\n",
    "        if self.X_valid:\n",
    "            self.sets.append([self.X_valid, self.y_valid, \"Valid\"])\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "\n",
    "        print(32 * \"=\" + f\"[ Initial State ]\" + 32 * \"=\", end=\"\")\n",
    "        for X, y, desc in self.sets:\n",
    "            self.evaluate(X, y, logs, desc, -1)\n",
    "            \n",
    "        \n",
    "\n",
    "        if self.plot_pca:\n",
    "            deep_feature_model = Model(\n",
    "                inputs=self.model.layers[0].get_input_at(0),  # get_layer(\"left_inputs\").input,\n",
    "                outputs=self.model.get_layer(\"basemodel\").get_output_at(0),\n",
    "                )\n",
    "            deep_feature_model.summary()\n",
    "            deep_features_test = None\n",
    "            for X, y, subj in samples_test:  \n",
    "                \n",
    "                # if 2d, maybe need to import P\n",
    "                # if P.model_variant == \"2d\":\n",
    "                #     X = X.reshape((*X.shape, 1))\n",
    "                pred = deep_feature_model.predict(X)\n",
    "                df_features = pd.DataFrame(pred)\n",
    "                df_features[\"subject\"] = subj\n",
    "                deep_features_test = pd.concat([deep_features_test, df_features])\n",
    "            \n",
    "            plot_pca(deep_features_test)\n",
    "            \n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        print(32 * \"=\" + f\"[ Final State ]\" + 32 * \"=\", end=\"\")\n",
    "        for X, y, desc in self.sets:\n",
    "            self.evaluate(X, y, logs, desc, -1)\n",
    "            \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        \n",
    "        optimizer = self.model.optimizer\n",
    "        if self.optimal_lr_epoch_dict != None:\n",
    "\n",
    "            K.set_value(optimizer.lr, self.optimal_lr_epoch_dict[epoch])\n",
    "            if self.verbose:\n",
    "                print(f\"Learning rate changed to {self.optimal_lr_epoch_dict[epoch]} for epoch {epoch}\")\n",
    "            \n",
    "        else:\n",
    "            self.lr_epoch_log_dict[epoch]=K.get_value(optimizer.lr)\n",
    "            if self.verbose:\n",
    "                print(f\"Logged learning rate of {self.lr_epoch_log_dict[epoch]} for epoch {epoch}\") \n",
    "            \n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if self.print_interm_epochs:\n",
    "            print(32 * \"=\" + f\"[   Epoch {epoch}   ]\" + 32 * \"=\", end=\"\")\n",
    "            if epoch % self.epoch_evaluate_freq == 0:  # Evaluate only every n-th epoch\n",
    "                for X, y, desc in self.sets:\n",
    "                    self.evaluate(X, y, logs, desc, epoch)\n",
    "            else:\n",
    "                print(f\"\\n{ ', '.join([k + ': ' + f'{v:.3f}' for k,v in logs.items()]) }\")\n",
    "            \n",
    "\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "    \n",
    "        \n",
    "        if 'val_loss' in logs:\n",
    "            if self.early_stoping and len(self.loss_record_dict['val_loss'])>1 and self.loss_record_dict['val_loss'][-1] < logs['val_loss']:\n",
    "                self.model.stop_training = True\n",
    "                \n",
    "            else:\n",
    "                self.loss_record_dict['val_loss'].append(logs['val_loss'])\n",
    "\n",
    "                self.loss_record_dict['loss'].append(logs['loss'])\n",
    "\n",
    "                for X, y, desc in self.sets:\n",
    "                    self.evaluate(X, y, logs, desc, epoch, print_metric=False, save_metric_record_dict=True)\n",
    "            \n",
    "            # print(logs)\n",
    "            if self.ReduceLROnPlateau_args != None:\n",
    "                if self.ReduceLROnPlateau_args['mointored_metric'] == \"val_loss\":\n",
    "                    self.reduce_lr.on_epoch_end(epoch, self.model, logs)\n",
    "\n",
    "                elif self.ReduceLROnPlateau_args['mointored_metric'] in self.valid_metrics:\n",
    "                    mointored_metric_dict={self.ReduceLROnPlateau_args['mointored_metric']: self.metric_record_dict[self.ReduceLROnPlateau_args['mointored_metric']][\"Valid\"][-1]}\n",
    "                    self.reduce_lr.on_epoch_end(epoch, self.model, mointored_metric_dict)\n",
    "\n",
    "                else:\n",
    "                    raise Exception(\"invalid mointored metric\")\n",
    "            \n",
    "                    \n",
    "        else:\n",
    "            self.loss_record_dict['loss'].append(logs['loss'])\n",
    "\n",
    "            for X, y, desc in self.sets:\n",
    "                self.evaluate(X, y, logs, desc, epoch, print_metric=False, save_metric_record_dict=True)\n",
    "                \n",
    "        \n",
    "\n",
    "            \n",
    "    def evaluate(self, X, y, logs, desc, epoch, print_metric=True, save_metric_record_dict=False):\n",
    "        # Predict\n",
    "        y_score = self.model.predict(X)\n",
    "        y_score_neg = y_score * -1  # lower distance means closer to positive class\n",
    "\n",
    "        # Calc Metrics\n",
    "        roc_val = metrics.roc_auc_score(y, y_score_neg)\n",
    "        eer_val, thres = utils_eer(y, y_score_neg, True)\n",
    "        y_pred = np.where(y_score_neg > thres, 1, 0)\n",
    "        acc_val = metrics.accuracy_score(y, y_pred)\n",
    "        f1_val = metrics.f1_score(y, y_pred)\n",
    "        \n",
    "        if print_metric:\n",
    "            print(\n",
    "                f\"\\n{desc.upper()}: roc_auc: {roc_val:.4f}, \"\n",
    "                + f\"eer: {eer_val:.4f}, thres: {thres*-1:.4f} => \"\n",
    "                + f\"acc: {acc_val:.4f}, f1: {f1_val:.4f}\\n\"\n",
    "                + f\"{ ', '.join([k + ': ' + f'{v:.3f}' for k,v in logs.items()]) }\"\n",
    "            )\n",
    "\n",
    "        if save_metric_record_dict:\n",
    "            self.metric_record_dict['roc_val'][desc].append(roc_val)\n",
    "            self.metric_record_dict['eer_val'][desc].append(eer_val)\n",
    "            self.metric_record_dict['thres'][desc].append(float(thres))\n",
    "            self.metric_record_dict['acc_val'][desc].append(acc_val)\n",
    "            self.metric_record_dict['f1_val'][desc].append(f1_val)\n",
    "        \n",
    "        # Plot distances\n",
    "        mask = np.where(y == 1, True, False)\n",
    "        dist_positive = y_score[mask]\n",
    "        dist_negative = y_score[~mask]\n",
    "        \n",
    "        \n",
    "        if self.plot_pca:\n",
    "            # Extract one of the child networks\n",
    "            deep_feature_model = Model(\n",
    "                inputs=self.model.layers[0].get_input_at(0),  # get_layer(\"left_inputs\").input,\n",
    "                outputs=self.model.get_layer(\"basemodel\").get_output_at(0),\n",
    "            )\n",
    "            deep_feature_model.summary()\n",
    "            deep_features_test = None\n",
    "            for X, y, subj in samples_test:  \n",
    "                # if 2d, maybe need to import P\n",
    "                # if P.model_variant == \"2d\":\n",
    "                #     X = X.reshape((*X.shape, 1))\n",
    "                pred = deep_feature_model.predict(X)\n",
    "                df_features = pd.DataFrame(pred)\n",
    "                df_features[\"subject\"] = subj\n",
    "                deep_features_test = pd.concat([deep_features_test, df_features])\n",
    "\n",
    "            plot_pca(deep_features_test)\n",
    "        \n",
    "        #plt = utils_plot_distance_hist(\n",
    "        #    dist_positive, dist_negative, thres * -1, desc=desc, fig_size=(12, 2), margin=P.margin\n",
    "        #)\n",
    "\n",
    "#         if self.save_plots:\n",
    "#             utils_save_plot(\n",
    "#                 plt,\n",
    "#                 REPORT_PATH\n",
    "#                 / f\"buech2019-siamese-{P.name.lower()}-epoch-{epoch+1}-{desc.lower()}.pdf\",\n",
    "#             )\n",
    "\n",
    "        # plt.show()\n",
    "    \n",
    "def tune_stream(data_dict, arg_dict, stream_key, pool_dict, pool_key, create_model_func, tuning_metric, tuning_metric_val_dict, verbose=0):\n",
    "    '''\n",
    "    return the best value found after one round of hyper paramteer tuning of a stream\n",
    "    \n",
    "    arg_dict is the dict of arguments needed for training\n",
    "    stream key is the dict key that specifies the stream from the arg_dict we want to tune\n",
    "    pool dict is the dict containing possible values for all hyperparameters of arg_dict\n",
    "    pool_key is a key for pool dict that specifies the stream key value type so we can choose appropriate \n",
    "    values to tune\n",
    "    create_model_func is the function that that takes an arg_dic tand turnes it into an architectuer.\n",
    "    '''\n",
    "    best_params_lst=[]\n",
    "    for stream_idx in range(len(arg_dict[stream_key])):\n",
    "        for idx in range(len(arg_dict[stream_key][stream_idx])):\n",
    "            result=find_param_ranking(data_dict, arg_dict=arg_dict, dict_key=stream_key, \n",
    "                                      pool_dict=pool_dict, pool_key=pool_key, \n",
    "                                      create_model_func=create_model_func, \n",
    "                                      tuning_metric=tuning_metric, idx=idx, \n",
    "                                      stream_idx=stream_idx, verbose=verbose)\n",
    "            arg_dict = result[\"improved_arg_dict\"]\n",
    "            best_params_lst.append(result[\"best_params\"])\n",
    "    \n",
    "    tuning_metric_val_dict[stream_key]=best_params_lst\n",
    "    return arg_dict, tuning_metric_val_dict\n",
    "\n",
    "def tune_param(data_dict, arg_dict, param_key, pool_dict, pool_key, create_model_func, tuning_metric, tuning_metric_val_dict, verbose=0):\n",
    "    \"\"\"\n",
    "    return the best value found after one round of hyper parameter tuning of a param\n",
    "    \n",
    "    arg_dict is the dict of arguments needed for training\n",
    "    param_key is the dict key that specifies the param from the arg_dict we want to tune\n",
    "    pool dict is the dict containing possible values for all hyperparameters of arg_dict\n",
    "    pool_key is a key for pool dict that specifies the param_key value type so we can choose appropriate \n",
    "    values to tune\n",
    "    create_model_func is the function that that takes an arg_dic tand turnes it into an architectuer.\n",
    "    \"\"\"\n",
    "    best_params_lst=[]\n",
    "    for idx in range(len(arg_dict[param_key])):\n",
    "        result=find_param_ranking(data_dict, arg_dict=arg_dict, dict_key=param_key, \n",
    "                                  pool_dict=pool_dict, pool_key=pool_key, \n",
    "                                  create_model_func=create_model_func, \n",
    "                                  tuning_metric=tuning_metric, idx=idx, \n",
    "                                  stream_idx=None, verbose=verbose)\n",
    "        \n",
    "        arg_dict = result[\"improved_arg_dict\"]\n",
    "        best_params_lst.append(result[\"best_params\"])\n",
    "        \n",
    "    tuning_metric_val_dict[param_key]=best_params_lst\n",
    "    return arg_dict, tuning_metric_val_dict\n",
    "\n",
    "def tune_dense_activations(data_dict, arg_dict, pool_dict, pool_key, create_model_func, tuning_metric, tuning_metric_val_dict, verbose=0):\n",
    "    \"\"\"\n",
    "    return the best value found after one round of hyper parameter tuning of a param\n",
    "    \n",
    "    arg_dict is the dict of arguments needed for training\n",
    "    pool dict is the dict containing possible values for all hyperparameters of arg_dict\n",
    "    pool_key is a key for pool dict that specifies the dense_activations value type so we can choose appropriate \n",
    "    values to tune\n",
    "    create_model_func is the function that that takes an arg_dic tand turnes it into an architectuer.\n",
    "    \"\"\"\n",
    "    best_params_lst=[]\n",
    "    for idx in range(len(arg_dict[\"dense_activations\"]) - 1): # do not tune the final sigmoid\n",
    "        result=find_param_ranking(data_dict, arg_dict=arg_dict, dict_key=\"dense_activations\", \n",
    "                                  pool_dict=pool_dict, pool_key=pool_key, \n",
    "                                  create_model_func=create_model_func, \n",
    "                                  tuning_metric=tuning_metric, idx=idx, \n",
    "                                  stream_idx=None, verbose=verbose)\n",
    "        \n",
    "        arg_dict = result[\"improved_arg_dict\"]\n",
    "        best_params_lst.append(result[\"best_params\"])\n",
    "        \n",
    "    tuning_metric_val_dict['dense_activations']=best_params_lst\n",
    "    return arg_dict, tuning_metric_val_dict\n",
    "\n",
    "def tune_training_params(data_dict, arg_dict, param_key, pool_dict, pool_key, create_model_func, tuning_metric, tuning_metric_val_dict, verbose=0):\n",
    "    \"\"\"\n",
    "    return the best value found after one round of hyper parameter tuning of a training param\n",
    "    training params are as follows: \n",
    "    batch_size, loss function, loss function args, optimizer function, optimizer function args\n",
    "    \n",
    "    arg_dict is the dict of arguments needed for training\n",
    "    param_key is the dict key that specifies the param from the arg_dict we want to tune\n",
    "    pool dict is the dict containing possible values for all hyperparameters of arg_dict\n",
    "    pool_key is a key for pool dict that specifies the param_key value type so we can choose appropriate \n",
    "    values to tune\n",
    "    create_model_func is the function that that takes an arg_dic tand turnes it into an architectuer.\n",
    "    \"\"\"\n",
    "    best_params_lst=[]\n",
    "    result=find_param_ranking(data_dict, arg_dict=arg_dict, dict_key=param_key, \n",
    "                              pool_dict=pool_dict, pool_key=pool_key, \n",
    "                              create_model_func=create_model_func, \n",
    "                              tuning_metric=tuning_metric, idx=None, \n",
    "                              stream_idx=None, verbose=verbose)\n",
    "\n",
    "    arg_dict = result[\"improved_arg_dict\"]\n",
    "    best_params_lst.append(result[\"best_params\"])\n",
    "    \n",
    "    tuning_metric_val_dict[param_key]=best_params_lst\n",
    "    return arg_dict, tuning_metric_val_dict\n",
    "\n",
    "def find_param_ranking(data_dict, arg_dict, dict_key, pool_dict, pool_key, create_model_func, tuning_metric, idx=None, stream_idx=None, verbose=0, early_stoping=False):\n",
    "\n",
    "    X_valid, y_valid, X_train, y_train = data_dict[\"X_valid\"], data_dict[\"y_valid\"], data_dict[\"X_train\"], data_dict[\"y_train\"]\n",
    "    param_metric_performance_dict = {}\n",
    "    \n",
    "    pool=pool_dict[pool_key]\n",
    "    for pool_param in pool:\n",
    "        # keep the initial state fixed\n",
    "        tf.random.set_seed(SEED)\n",
    "        \n",
    "        new_arg_dict = arg_dict.copy()\n",
    "        if stream_idx != None:\n",
    "            new_arg_dict[dict_key][stream_idx][idx] = pool_param\n",
    "        elif idx != None:\n",
    "            new_arg_dict[dict_key][idx] = pool_param\n",
    "        else:\n",
    "            new_arg_dict[dict_key] = pool_param\n",
    "            \n",
    "        # P.margin = new_arg_dict[\"contrastive_loss_margin\"] obsolete\n",
    "        print(new_arg_dict)\n",
    "        loss_record_dict = {'loss': [], 'val_loss': []}\n",
    "        metric_record_dict = {}\n",
    "        model = create_model_func(new_arg_dict)\n",
    "        \n",
    "        ReduceLROnPlateau_err_mode={}\n",
    "        ReduceLROnPlateau_args={'mointored_metric': tuning_metric, \"factor\": 0.2, \"patience\": 2, \"verbose\": 1, \"min_lr\": 1e-6}\n",
    "\n",
    "        # Train\n",
    "        history = model.fit(\n",
    "            x=X_train,\n",
    "            y=y_train,\n",
    "            batch_size=new_arg_dict[\"batch_size\"],\n",
    "            epochs=30,\n",
    "            verbose=verbose,\n",
    "            validation_data=(X_valid, y_valid),\n",
    "            callbacks=[MetricsCallback((X_valid, y_valid, X_train, y_train), epoch_evaluate_freq=5, \n",
    "                                       loss_record_dict=loss_record_dict, metric_record_dict=metric_record_dict, \n",
    "                                       save_plots=True, print_interm_epochs=False, early_stoping=early_stoping,\n",
    "                                       ReduceLROnPlateau_args=ReduceLROnPlateau_args)],\n",
    "        )\n",
    "        print(loss_record_dict)\n",
    "        print(\"Training History:\")\n",
    "        # loss_fig = utils_plot_training_loss(loss_record_dict)\n",
    "        if early_stoping:\n",
    "            param_metric_performance_dict[pool_param]={\n",
    "                'val_loss': loss_record_dict['val_loss'][-1], \n",
    "                'roc_val': metric_record_dict['roc_val']['Valid'][-1], \n",
    "                'eer_val': metric_record_dict['eer_val']['Valid'][-1], \n",
    "                'thres': metric_record_dict['thres']['Valid'][-1], \n",
    "                'acc_val': metric_record_dict['acc_val']['Valid'][-1], \n",
    "                'f1_val': metric_record_dict['f1_val']['Valid'][-1],\n",
    "                }\n",
    "            \n",
    "        else: \n",
    "            param_metric_performance_dict[pool_param]={\n",
    "                'val_loss': min(loss_record_dict['val_loss']), \n",
    "                'roc_val': max(metric_record_dict['roc_val']['Valid']), \n",
    "                'eer_val': min(metric_record_dict['eer_val']['Valid']), \n",
    "                'thres': min(metric_record_dict['thres']['Valid']), \n",
    "                'acc_val': max(metric_record_dict['acc_val']['Valid']), \n",
    "                'f1_val': max(metric_record_dict['f1_val']['Valid']),\n",
    "                }\n",
    "    \n",
    "    print(param_metric_performance_dict)\n",
    "    best_params = get_best_parameters(param_metric_performance_dict)\n",
    "    print((\"{}\\n\"*len(best_params)).format(*best_params.items()))\n",
    "    improved_arg_dict = arg_dict.copy()\n",
    "    \n",
    "    if stream_idx != None:\n",
    "        improved_arg_dict[dict_key][stream_idx][idx] = best_params['best_'+tuning_metric][\"param\"]\n",
    "    elif idx != None:\n",
    "        improved_arg_dict[dict_key][idx] = best_params['best_'+tuning_metric][\"param\"]\n",
    "    else:\n",
    "        improved_arg_dict[dict_key] = best_params['best_'+tuning_metric][\"param\"]\n",
    "    \n",
    "    return {\"improved_arg_dict\": improved_arg_dict, \"best_params\": best_params} #, param_ranking # not sure if returning this is useful for now\n",
    "\n",
    "# def log_metrics_tuning(arg_dict, best_params_dict):\n",
    "#     \"\"\"\n",
    "#     log best params through tuning in arg_dict\n",
    "#     \"\"\"\n",
    "#     best_params = {\n",
    "#     'best_val_loss': {\"param\": best_val_loss[0], \"value\": best_val_loss[1]['val_loss']}, \n",
    "#     'best_roc_val': {\"param\": best_roc_val[0], \"value\": best_roc_val[1]['roc_val']},\n",
    "#     'best_eer_val': {\"param\": best_eer_val[0], \"value\": best_eer_val[1]['eer_val']},\n",
    "#     'best_thres': {\"param\": best_thres[0], \"value\": best_thres[1]['thres']},\n",
    "#     'best_acc_val': {\"param\": best_acc_val[0], \"value\": best_acc_val[1]['acc_val']},\n",
    "#     'best_f1_val': {\"param\": best_f1_val[0], \"value\": best_f1_val[1]['f1_val']},\n",
    "#     }\n",
    "def get_best_parameters(param_metric_performance_dict):\n",
    "    '''\n",
    "    calculate and return best valiation parameters given the param_metric_dict.\n",
    "    param_metric_dict is the dicitonary containing the parameters with their correspongin performance\n",
    "    '''\n",
    "    lst = list(param_metric_performance_dict.items())\n",
    "    # sort all in descending order, first element is max and last element is min\n",
    "    # best_val_loss = sorted(lst, key = lambda x: x[1]['val_loss'], reverse=True)[-1] # min\n",
    "    # best_roc_val = sorted(lst, key = lambda x: x[1]['roc_val'], reverse=True)[0] # max\n",
    "    # best_eer_val = sorted(lst, key = lambda x: x[1]['eer_val'], reverse=True)[-1] # min\n",
    "    # best_thres = sorted(lst, key = lambda x: x[1]['thres'], reverse=True)[-1] # min\n",
    "    # best_acc_val = sorted(lst, key = lambda x: x[1]['acc_val'], reverse=True)[0] # max\n",
    "    # best_f1_val = sorted(lst, key = lambda x: x[1]['f1_val'], reverse=True)[0] # max\n",
    "    \n",
    "    best_val_loss = min(lst, key = lambda x: x[1]['val_loss']) # min\n",
    "    best_roc_val = max(lst, key = lambda x: x[1]['roc_val']) # max\n",
    "    best_eer_val = min(lst, key = lambda x: x[1]['eer_val']) # min\n",
    "    best_thres = min(lst, key = lambda x: x[1]['thres']) # min\n",
    "    best_acc_val = max(lst, key = lambda x: x[1]['acc_val']) # max\n",
    "    best_f1_val = max(lst, key = lambda x: x[1]['f1_val']) # max\n",
    "    \n",
    "    best_params = {\n",
    "        'best_val_loss': {\"param\": best_val_loss[0], \"value\": best_val_loss[1]['val_loss']}, \n",
    "        'best_roc_val': {\"param\": best_roc_val[0], \"value\": best_roc_val[1]['roc_val']},\n",
    "        'best_eer_val': {\"param\": best_eer_val[0], \"value\": best_eer_val[1]['eer_val']},\n",
    "        'best_thres': {\"param\": best_thres[0], \"value\": best_thres[1]['thres']},\n",
    "        'best_acc_val': {\"param\": best_acc_val[0], \"value\": best_acc_val[1]['acc_val']},\n",
    "        'best_f1_val': {\"param\": best_f1_val[0], \"value\": best_f1_val[1]['f1_val']},\n",
    "            }\n",
    "    \n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_module = 0\n",
    "if test_module:\n",
    "    # param_metric_performance_dict={}\n",
    "    param_metric_performance_dict={3: {'val_loss': 1, \n",
    "                                          'roc_val': 2, \n",
    "                                          'eer_val': 3, \n",
    "                                          'thres': 4, \n",
    "                                          'acc_val': 5, \n",
    "                                          'f1_val': 6,\n",
    "                                         },\n",
    "                                  4: {'val_loss': 6, \n",
    "                                          'roc_val': 5, \n",
    "                                          'eer_val': 4, \n",
    "                                          'thres': 3, \n",
    "                                          'acc_val': 2, \n",
    "                                          'f1_val': 1,\n",
    "                                         }\n",
    "                                  }\n",
    "\n",
    "    best_params = get_best_parameters(param_metric_performance_dict)\n",
    "    assert best_params['best_val_loss']['value'] == param_metric_performance_dict[3]['val_loss']\n",
    "    assert best_params['best_roc_val']['value'] == param_metric_performance_dict[4]['roc_val']\n",
    "    assert best_params['best_eer_val']['value'] == param_metric_performance_dict[3]['eer_val']\n",
    "    assert best_params['best_thres']['value'] == param_metric_performance_dict[4]['thres']\n",
    "    assert best_params['best_acc_val']['value'] == param_metric_performance_dict[3]['acc_val']\n",
    "    assert best_params['best_f1_val']['value'] == param_metric_performance_dict[3]['f1_val']\n",
    "\n",
    "    assert best_params['best_val_loss']['param'] == 3\n",
    "    assert best_params['best_roc_val']['param'] == 4\n",
    "    assert best_params['best_eer_val']['param'] == 3\n",
    "    assert best_params['best_thres']['param'] == 4\n",
    "    assert best_params['best_acc_val']['param'] == 3\n",
    "    assert best_params['best_f1_val']['param'] == 3\n",
    "    print(\"no err\")\n",
    "    # # rank_f = lambda \n",
    "    # # use lambda functions to sort and store\n",
    "    # lst = list(param_metric_performance_dict.items())\n",
    "    # print(max(lst, key = lambda x: x[1]['thres']))\n",
    "    # param_metric_performance_dict.items()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def MakeDeepXExpDic(X_exp1_train_dic, X_exp2_train_dic, fitted_scaler_ocsvm_exp2_train_dic, X_exp1_test_dic, X_exp2_test_dic, fitted_scaler_ocsvm_exp2_test_dic, deep_feature_model):\n",
    "#     X_exp_train_dic = MakeDeepXExpDicOwner(X_exp2_train_dic, deep_feature_model, scaler_clip=False, scaler_type=\"MinMaxScaler\")\n",
    "#     X_exp_train_dic = MakeDeepXExpDicUnknown(X_exp1_train_dic, deep_feature_model, X_exp_train_dic, fitted_raw_scaler_dict=fitted_scaler_ocsvm_exp2_train_dic)\n",
    "#     X_exp_test_dic = MakeDeepXExpDicOwner(X_exp2_test_dic, deep_feature_model, scaler_clip=False, scaler_type=\"MinMaxScaler\")\n",
    "#     X_exp_test_dic = MakeDeepXExpDicUnknown(X_exp1_test_dic, deep_feature_model, X_exp_test_dic, fitted_raw_scaler_dict=fitted_scaler_ocsvm_exp2_test_dic)\n",
    "    \n",
    "#     return X_exp_train_dic, X_exp_test_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "# change this name later\n",
    "def extract_learnt_features(X_exp1_dic, X_exp2_dic, fitted_scaler_classifier_exp2_dic, deep_feature_model, scaler_clip, \n",
    "                            fitted_min_max_scaler_exp2_dict=None, \n",
    "                            verbose=1):\n",
    "    \n",
    "    deep_feature_extractor = partial_func(ExtractDeepFeatures, deep_feature_model=deep_feature_model)\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "\n",
    "    X_exp_dic = MakeDeepXExpDicOwner(X_exp_reg_df_dict=X_exp2_dic, scaler_clip=scaler_clip, scaler_type=\"MinMaxScaler\", feature_extractor=deep_feature_extractor, \n",
    "                                     feature_extractor_transformer=transform_user_deep_windows, \n",
    "                                     verbose=0)\n",
    "    stop = timeit.default_timer()\n",
    "    print('MakeSCNNExpDicOwner Time: ', stop - start)\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "\n",
    "    X_exp_dic = MakeDeepXExpDicUnknown(X_exp_unknown_df_dict=X_exp1_dic, X_exp_dic=X_exp_dic, fitted_raw_scaler_dict=fitted_scaler_classifier_exp2_dic, \n",
    "                                       feature_extractor=deep_feature_extractor, feature_extractor_transformer=transform_user_deep_windows,\n",
    "                                       fitted_min_max_scaler_exp2_dict=fitted_min_max_scaler_exp2_dict)\n",
    "    stop = timeit.default_timer()\n",
    "    print('MakeSCNNXExpDicUnknown Time: ', stop - start)\n",
    "    \n",
    "    if verbose: print(\"Done extracting features\")\n",
    "    return X_exp_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0092c407-5812-43bd-a3a9-f78bfadff3b7",
    "_uuid": "35621051-b220-428d-b505-85ef106f8ce6"
   },
   "source": [
    "# Split Dataset for Valid/Test  \n",
    "In two splits: one used during hyperparameter optimization, and one used during testing.\n",
    "\n",
    "The split is done along the subjects: All sessions of a single subject will either be in the validation split or in the testing split, never in both.\n",
    "\n",
    "They did a 30 60 split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "92d8caee-6b8e-45c7-b272-010046daec00",
    "_uuid": "0c7969f6-43b9-4d95-9253-76d5d040d30a"
   },
   "source": [
    "# Reshaping Raw Features.\n",
    "We have our own function of windows for this. Do this for both training and testing.\n",
    "\n",
    "# Extracting time and frequency based features.\n",
    "Again, we have a function for this. Do this for both training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "11bbc563-4576-4588-a73c-44936653c13b",
    "_uuid": "b9984fbd-a13f-4d5d-ad57-8b0c41aafe1d"
   },
   "source": [
    "# Hyperparameter Optimization \n",
    "\n",
    "I do not find any reaqsonable explaination how to use a cross-validation as we are talking about anomaly detection.\n",
    "\n",
    "I am using the experiment 1 data as train, and experiment 2 data as validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d09c240f-d000-4840-880c-260436115209",
    "_uuid": "aca85e4e-ed2f-4c54-bf0f-2f938ac45a07"
   },
   "source": [
    "# Using SVM in a real-world Scenario with multiple genuine users and intruders\n",
    "Source: https://datascience.stackexchange.com/questions/23623/what-is-the-best-way-to-classify-data-not-belonging-to-set-of-classes\n",
    "\n",
    "Stage 1: \n",
    "    Use one-class SVM to assign those images that do not belong to the set of predefined classes as the 9-th class.\n",
    "\n",
    "Stage 2:\n",
    "    For those images that passes through your filter, let the multi-class SVM assign them to one of the 8 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79b10ce2-42b2-4156-a1df-633bf52c05e6",
    "_uuid": "bf0780fa-46ed-4d13-90ee-3640fe7cc4aa"
   },
   "source": [
    "Loading data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is needed as parallelization does not work well with TensorFlow\n",
    "def MakeDeepXExpDicOwner(X_exp_reg_df_dict, scaler_clip, scaler_type, feature_extractor, feature_extractor_transformer, \n",
    "                         verbose=0):\n",
    "    '''k\n",
    "    ???\n",
    "    return \n",
    "    X_exp_dic\n",
    "    dfLists are of the same size.\n",
    "    '''\n",
    "\n",
    "    X_exp_dic = {}\n",
    "    for owner in X_exp_reg_df_dict.keys():\n",
    "\n",
    "        if verbose: print(f\"raw exp2 samples of owner: {owner} are scaled already so we {feature_extractor.__name__}\")\n",
    "        \n",
    "        X_exp_dic[owner] = {\"profile_windows\": feature_extractor(X_exp_reg_df_dict[owner]), \"unknown_users_dict\": {}, \"fitted_feature_scaler\": None}\n",
    "        \n",
    "#         #------- DEBUG--------\n",
    "#         if owner == 2: \n",
    "#             X_exp_dic['debug']= X_exp_dic[owner], X_exp_reg_df_dict[owner]\n",
    "#             return X_exp_dic\n",
    "        \n",
    "#         #------- DEBUG--------\n",
    "        \n",
    "        # do minmax scaling here\n",
    "        if verbose: print(f\"fit_transform {feature_extractor.__name__} features using scaler of type: {scaler_type}, for profile_windows of owner: {owner}\")\n",
    "        if verbose: print(f\"owner: {owner}, exp_num: {2}, scaler: {scaler_type}, scaler_clip: {scaler_clip}\")\n",
    "        \n",
    "        scaler = get_new_scaler_dict[scaler_type]\n",
    "        scaler = scaler(clip=scaler_clip).fit(X_exp_dic[owner][\"profile_windows\"]) #feature_range=(-1.0, 1.0),  explore this later\n",
    "        X_exp_dic[owner][\"fitted_feature_scaler\"] = scaler\n",
    "\n",
    "        X_exp_dic[owner][\"profile_windows\"] = feature_extractor_transformer(X_exp_dic[owner][\"profile_windows\"], X_exp_dic[owner][\"fitted_feature_scaler\"])\n",
    "        \n",
    "        \n",
    "    return X_exp_dic\n",
    "    \n",
    "    \n",
    "def MakeDeepXExpDicUnknown(X_exp_unknown_df_dict, X_exp_dic, fitted_raw_scaler_dict, feature_extractor, feature_extractor_transformer, \n",
    "                           fitted_min_max_scaler_exp2_dict=None, \n",
    "                           verbose=0):\n",
    "    '''k\n",
    "    ???\n",
    "    return \n",
    "    X_exp_dic\n",
    "    dfLists are of the same size.\n",
    "    '''\n",
    "\n",
    "    for owner in X_exp_unknown_df_dict.keys():\n",
    "        if fitted_raw_scaler_dict:\n",
    "            if verbose: print(f\"scaling exp1 unknown windows for when user: {owner}, is owner\")\n",
    "            for unknown_user in X_exp_unknown_df_dict.keys():\n",
    "                \n",
    "                # scale the unkown window using the scaler/s fitted on raw registeration data\n",
    "                ###------ newly added\n",
    "                if fitted_raw_scaler_dict[owner]!=None:\n",
    "        \n",
    "                    if verbose: print(f\"first scaling exp1 unknown windows of user: {unknown_user} for when user: {owner}, is owner\")\n",
    "                    # scale the unknown df dict using the scaler fitted on raw registeration data of owner\n",
    "                    X_exp_unknown_df_unknown_user = transform_user_windows(X_exp_unknown_df_dict[unknown_user], fitted_raw_scaler_dict[owner])\n",
    "                    \n",
    "                    if fitted_min_max_scaler_exp2_dict!=None and owner in fitted_min_max_scaler_exp2_dict:\n",
    "\n",
    "                        if verbose: print(f\"min_max scaling exp1 unknown windows of user: {unknown_user} for when user: {owner}, is owner\")\n",
    "                        # scale the unknown df dict using the scaler fitted on raw registeration data of owner\n",
    "                        X_exp_unknown_df_unknown_user = transform_user_windows(X_exp_unknown_df_unknown_user, fitted_min_max_scaler_exp2_dict[owner])\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "                    # if scaler is not available\n",
    "                    X_exp_unknown_df_unknown_user = X_exp_unknown_df_dict[unknown_user]\n",
    "                    \n",
    "                ####------\n",
    "                \n",
    "                # extract the unknown features\n",
    "                X_exp_dic[owner][\"unknown_users_dict\"][unknown_user] = feature_extractor(X_exp_unknown_df_unknown_user)\n",
    "                \n",
    "                # get the scaler fitted on the registeration features\n",
    "                fitted_feature_scaler = X_exp_dic[owner][\"fitted_feature_scaler\"]\n",
    "                \n",
    "                # scale the unkown features using the registeration feature scaler\n",
    "                X_exp_dic[owner][\"unknown_users_dict\"][unknown_user] = feature_extractor_transformer(X_exp_dic[owner][\"unknown_users_dict\"][unknown_user], fitted_feature_scaler)\n",
    "            \n",
    "    return X_exp_dic\n",
    "\n",
    "def transform_user_deep_windows(X_exp, fitted_scaler):\n",
    "    \n",
    "    \n",
    "    transformed_X_exp = []\n",
    "    \n",
    "#     print(X_exp[0].shape)\n",
    "    for window in X_exp:\n",
    "        if len(window.shape) == 1:\n",
    "            window = window.reshape(1, -1)\n",
    "        scaled_array = fitted_scaler.transform(window)\n",
    "        transformed_X_exp.append(scaled_array.reshape(-1))\n",
    "        \n",
    "    return np.array(transformed_X_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #doesnt work with paralleilization\n",
    "# def MakeDeepXExpDicOwner(X_exp_reg_df_dict, deep_feature_model, scaler_clip=True, scaler_type=\"MinMaxScaler\"):\n",
    "#     '''k\n",
    "#     ???\n",
    "#     return \n",
    "#     X_exp_dic\n",
    "#     dfLists are of the same size.\n",
    "#     '''\n",
    "    \n",
    "#     extract_deep_features = partial_func(ExtractDeepFeatures, deep_feature_model=deep_feature_model)\n",
    "#     # extract_deep_features = lambda X_exp : ExtractDeepFeatures(X_exp, deep_feature_model)\n",
    "#     return MakeXExpDicOwner(X_exp_reg_df_dict, scaler_clip=scaler_clip, scaler_type=\"MinMaxScaler\",\n",
    "#                             feature_extractor=extract_deep_features, feature_extractor_transformer=transform_user_deep_windows)\n",
    "    \n",
    "\n",
    "# #doesnt work with paralleilization\n",
    "# def MakeDeepXExpDicUnknown(X_exp_unknown_df_dict, deep_feature_model, X_exp_dic, fitted_raw_scaler_dict):\n",
    "#     '''k\n",
    "#     ???\n",
    "#     return \n",
    "#     X_exp_dic\n",
    "#     dfLists are of the same size.\n",
    "#     '''  \n",
    "    \n",
    "#     extract_deep_features = partial_func(ExtractDeepFeatures, deep_feature_model=deep_feature_model)\n",
    "#     # extract_deep_features = lambda X_exp : ExtractDeepFeatures(X_exp, deep_feature_model)\n",
    "#     return MakeXExpDicUnknown(X_exp_unknown_df_dict, X_exp_dic, fitted_raw_scaler_dict, \n",
    "#                               feature_extractor=extract_deep_features, feature_extractor_transformer=transform_user_deep_windows)\n",
    "\n",
    "\n",
    "\n",
    "def ExtractDeepFeatures(X_exp, deep_feature_model):\n",
    "        \n",
    "    # Specify CPU as the device for execution\n",
    "    with tf.device('/CPU:0'):\n",
    "    # Your inference code here\n",
    "    # if X_exp and deep_feature_model:\n",
    "        return deep_feature_model.predict(np.array(X_exp))\n",
    "    \n",
    "    raise Exception(\"Failed to use the cpu for inference\")\n",
    "    \n",
    "#     elif deep_feature_model:\n",
    "#         return lambda X_exp : deep_feature_model.predict(np.array(X_exp))\n",
    "    \n",
    "#     else:\n",
    "#         raise Exception(f'arguments: (X_exp, deep_feature_model) are ({X_exp}, {deep_feature_model})')\n",
    "\n",
    "\n",
    "\n",
    "def extract_deep_feature_extactor(model):\n",
    "    \"\"\"\n",
    "    extracts and returns the a subnetwork of a two branch siamese network.\n",
    "    input: siamese model containing sub networks\n",
    "    \"\"\"\n",
    "    \n",
    "    return Model(\n",
    "                inputs=model.layers[0].get_input_at(0),\n",
    "                outputs=model.get_layer(\"basemodel\").get_output_at(0))\n",
    "\n",
    "\n",
    "def custom_save_model(arg_dict, win_size, loss_record_dict, metric_record_dict, deep_feature_model=None, test_res_fig_dic = None, other_dict=None, custom_prefix_dir_name=\"\"):\n",
    "    \"\"\"\n",
    "    save the deep learning feature extractor model, along with dictionary of arguments as a json,\n",
    "    best epoch found, a dictionary containing the accurcy and EER figures, and the window size, and\n",
    "    a dicgionary of validation and training loss values over time that can be later plotted.\n",
    "    inputs: \n",
    "    deep_feature_model: tf model\n",
    "    arg_dict: serializable dictionary\n",
    "    test_res_fig_dic: dict containing keys {\"acc\", \"eer\"}\n",
    "    win_size: int\n",
    "    loss_record_dict\n",
    "    \"\"\"\n",
    "    \n",
    "    dir_name = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    if deep_feature_model:\n",
    "        save_dir = f\"siamese_cnn_results/{win_size}/\"+ custom_prefix_dir_name + \"best_model_\" + dir_name\n",
    "        os.mkdir(save_dir)\n",
    "        deep_feature_model.save(save_dir)\n",
    "    else:\n",
    "        save_dir = f\"siamese_cnn_results/{win_size}/\"+ custom_prefix_dir_name + dir_name\n",
    "        os.mkdir(save_dir)\n",
    "    \n",
    "    \n",
    "    if test_res_fig_dic:\n",
    "        test_res_fig_dic[\"acc\"].savefig(f'{save_dir}/accuracy.svg', bbox_inches='tight')\n",
    "        test_res_fig_dic[\"eer\"].savefig(f'{save_dir}/eer.svg', bbox_inches='tight')\n",
    "    \n",
    "    with open(f\"{save_dir}/arg_dict.json\", 'w') as file:\n",
    "        arg_dict_json = json.dumps(arg_dict)\n",
    "        file.write(arg_dict_json)\n",
    "        \n",
    "    with open(f\"{save_dir}/loss_record.json\", 'w') as file:\n",
    "        loss_record_json = json.dumps(loss_record_dict)\n",
    "        file.write(loss_record_json)\n",
    "        \n",
    "    with open(f\"{save_dir}/metric_record.json\", 'w') as file:\n",
    "        metric_record_json = json.dumps(metric_record_dict)\n",
    "        file.write(metric_record_json)\n",
    "    \n",
    "    if other_dict != None:\n",
    "        with open(f\"{save_dir}/other_dict.json\", 'w') as file:\n",
    "            other_dict_json = json.dumps(other_dict)\n",
    "            file.write(other_dict_json)\n",
    "\n",
    "        \n",
    "    fig_dict = utils_plot_validation_metric(metric_record_dict)\n",
    "    for metric in fig_dict:\n",
    "        fig = fig_dict[metric]\n",
    "        fig.savefig(f'{save_dir}/{metric}_epoch.svg', bbox_inches='tight')\n",
    "    \n",
    "    print(f\"saved model at {save_dir}\")\n",
    "    \n",
    "    \n",
    "def utils_plot_validation_metric(metric_record_dict):\n",
    "    \"\"\"Plot Train/Valid metric during Epochs.\"\"\"\n",
    "    \n",
    "    fig_dict = {}\n",
    "    for metric in metric_record_dict:\n",
    "        fig = plt.figure(figsize=(5.473, 2.7), dpi=180)\n",
    "        plt.plot(metric_record_dict[metric]['Train'], label=\"train\", color=\"tab:blue\")\n",
    "        plt.plot(metric_record_dict[metric]['Valid'], label=\"valid\", color=MAGENTA)\n",
    "        plt.ylabel(metric)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        fig.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        fig.tight_layout()\n",
    "        \n",
    "        fig_dict[metric] = fig\n",
    "    \n",
    "    return fig_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils_plot_distance_hist() For CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_plot_distance_hist(dist_positive, dist_negative, thres, desc, fig_size=(12, 4), margin=None):\n",
    "    \"\"\"Plot histogramm of Euclidean Distances for Positive & Negative Pairs.\"\"\"\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Plot Distributions\n",
    "    plt.figure(figsize=fig_size, dpi=180)\n",
    "    bins = np.linspace(\n",
    "        min(dist_positive.min(), dist_negative.min()),\n",
    "        max(dist_positive.max(), dist_negative.max()),\n",
    "        num=21,\n",
    "    )\n",
    "    g1 = sns.distplot(\n",
    "        dist_positive,\n",
    "        label=\"positive pairs\",\n",
    "        bins=bins,\n",
    "        axlabel=False,\n",
    "        hist_kws=dict(edgecolor=\"k\", lw=0.5),\n",
    "        kde_kws=dict(linewidth=0.8),\n",
    "        color=\"tab:blue\",\n",
    "    )\n",
    "    g2 = sns.distplot(\n",
    "        dist_negative,\n",
    "        label=\"negative pairs\",\n",
    "        bins=bins,\n",
    "        hist_kws=dict(edgecolor=\"k\", lw=0.5),\n",
    "        kde_kws=dict(linewidth=0.8),\n",
    "        color=\"tab:gray\",\n",
    "    )\n",
    "\n",
    "    # Plot vertical lines\n",
    "    if thres > 0:\n",
    "        max_y = max(g1.get_ylim()[1], g2.get_ylim()[1])\n",
    "        plt.axvline(x=thres, color=MAGENTA, linestyle=\"--\", lw=0.8, alpha=0.7)\n",
    "        plt.text(\n",
    "            x=thres + 0.001,\n",
    "            y=max_y * 0.65,\n",
    "            s=f\"EER Threshold\\n({thres:.2f})\",\n",
    "            color=MAGENTA,\n",
    "            weight=\"bold\",\n",
    "            fontsize=5,\n",
    "            alpha=1\n",
    "        )\n",
    "        if margin:\n",
    "            plt.axvline(x=margin, color=MAGENTA, linestyle=\"--\", lw=0.8, alpha=0.7)\n",
    "            plt.text(\n",
    "                x=margin + 0.001,\n",
    "                y=max_y * 0.15,\n",
    "                s=f\"Margin\\n({margin})\",\n",
    "                color=MAGENTA,\n",
    "                weight=\"bold\",\n",
    "                fontsize=5,\n",
    "                alpha=1\n",
    "            )\n",
    "\n",
    "    # Legend\n",
    "    plt.legend(\n",
    "        loc=\"upper right\",\n",
    "        title=f\"{desc} Distances\",\n",
    "        title_fontsize=5,\n",
    "        fontsize=6,\n",
    "    )\n",
    "\n",
    "    warnings.filterwarnings(\"default\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TEST_MODE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mTEST_MODE\u001b[49m:\n\u001b[1;32m      2\u001b[0m     dist_pos \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.8\u001b[39m])\n\u001b[1;32m      3\u001b[0m     dist_neg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TEST_MODE' is not defined"
     ]
    }
   ],
   "source": [
    "if TEST_MODE:\n",
    "    dist_pos = np.array([0.0, 0.1, 0.1, 0.2, 0.2, 0.3, 0.3, 0.3, 0.3, 0.3, 0.5, 0.8])\n",
    "    dist_neg = np.array([0.4, 0.5, 0.6, 0.6, 0.6, 0.7, 0.7, 0.7, 0.7, 0.8, 1, 1])\n",
    "    utils_plot_distance_hist(\n",
    "        dist_pos, dist_neg, thres=0.4, desc=\"Pair\", fig_size=(12, 4), margin=0.8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pca_dataframe(Raw_X_exp_test_dic, session_id=None):\n",
    "    \n",
    "    columns=['X', 'subject', 'session', 'win_idx']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for subject_id in Raw_X_exp_test_dic:\n",
    "        win_idx = 0\n",
    "        for subject_window in Raw_X_exp_test_dic[subject_id]:\n",
    "            new_row = pd.DataFrame([[subject_window.to_numpy(), subject_id, session_id, win_idx]], columns=columns)\n",
    "            df = pd.concat([df, new_row])\n",
    "            win_idx +=1\n",
    "        \n",
    "            \n",
    "    return df\n",
    "   \n",
    "def prep_X_y_single(df):\n",
    "    X = np.stack(list(df[\"X\"].values))\n",
    "    y = df[\"label\"].values\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def transform_to_sample_by_subject(df):\n",
    "    sample_by_subject = []\n",
    "    df[\"label\"] = 1\n",
    "    for subj in df[\"subject\"].unique():\n",
    "        df_subj = df[df[\"subject\"] == subj]\n",
    "        X_sub, y_sub = prep_X_y_single(df_subj)\n",
    "        sample_by_subject.append((X_sub, y_sub, subj))\n",
    "    return sample_by_subject\n",
    "\n",
    "def plot_pca(df):\n",
    "    # PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    deep_transformed = pca.fit_transform(df.drop(columns=[\"subject\"]).values)\n",
    "\n",
    "    # Create df with data needed for chart only\n",
    "    df_viz = df.copy()\n",
    "    df_viz[\"PCA0\"] = deep_transformed[:, 0]\n",
    "    df_viz[\"PCA1\"] = deep_transformed[:, 1]\n",
    "    df_viz.drop(\n",
    "        columns=[c for c in df_viz.columns if c not in [\"PCA0\", \"PCA1\", \"subject\"]]\n",
    "    )\n",
    "\n",
    "    # Generate color index for every subject\n",
    "    df_viz[\"Subject\"] = pd.Categorical(df_viz[\"subject\"])\n",
    "    df_viz[\"colors\"] = df_viz[\"Subject\"].cat.codes\n",
    "\n",
    "    if len(df_viz[\"Subject\"].unique()) <= 10:\n",
    "        pal = sns.color_palette(\"tab10\")\n",
    "    else:\n",
    "        pal = sns.color_palette(\"tab20\")\n",
    "        \n",
    "    # Actual plot\n",
    "    fig = plt.figure(figsize=(10 / 1.5, 10 / 2), dpi=180)\n",
    "    sns.scatterplot(\n",
    "        x=\"PCA0\",\n",
    "        y=\"PCA1\",\n",
    "        data=df_viz,\n",
    "        hue=\"Subject\",\n",
    "        legend=\"full\",\n",
    "        # palette=pal,\n",
    "        s=5,\n",
    "        linewidth=0,\n",
    "        alpha=0.6,\n",
    "    )\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0, fontsize=5)\n",
    "    fig.tight_layout()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_plot_training_loss(history):\n",
    "    \"\"\"Plot Train/Valid Loss during Epochs.\"\"\"\n",
    "    fig = plt.figure(figsize=(5.473, 2.7), dpi=180)\n",
    "    plt.plot(history[\"loss\"], label=\"train\", color=\"tab:blue\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"valid\", color=MAGENTA)\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    fig.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    fig.tight_layout()\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    HistoryDummy = type(\"History\", (object,), {})\n",
    "    history = HistoryDummy()\n",
    "    history.history = {}\n",
    "    history.history[\"loss\"] = [0.6, 0.4, 0.3, 0.2, 0.21, 0.15]\n",
    "    history.history[\"val_loss\"] = [0.9, 0.7, 0.5, 0.4, 0.35, 0.3]\n",
    "    utils_plot_training_loss(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "92b22de7-7efb-4cce-aa17-a941d9bc262d",
    "_uuid": "7900b26e-3f79-4af8-b0dd-3c66f8193ad6"
   },
   "source": [
    "**use the following to write tests for distro functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\u001b[32mNeural Networks utility functions imported\u001b[0m--------------------\n"
     ]
    }
   ],
   "source": [
    "print(20*'-' + \"\\x1b[32mNeural Networks utility functions imported\\x1b[0m\" + 20*'-')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
