{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "EXP_PATH_NAME=\"WACA-kNN\"\n",
    "joblib.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "EER: 0.333, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.400, Threshold: 0.200 <-- Worse case\n",
      "EER: 0.167, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.333, Threshold: 1.000 <-- Worse case\n",
      "--------------------\u001b[32mUtility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mPreprocessing utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mNeural Networks utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "EER: 0.333, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.400, Threshold: 0.200 <-- Worse case\n",
      "EER: 0.167, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.333, Threshold: 1.000 <-- Worse case\n",
      "--------------------\u001b[32mUtility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mPreprocessing utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mWACA utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mClassification utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "Numpy Seed was set to: 567\n",
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade pip\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import dataclasses\n",
    "from dataclasses import asdict\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import random\n",
    "import time\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "%run ./Classification_utility-functions.ipynb\n",
    "%run ./SEED-CONSTANTS.ipynb\n",
    "\n",
    "np.random.seed(SEED)\n",
    "print(f\"Numpy Seed was set to: {SEED}\")\n",
    "\n",
    "print(\"Setup Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__dir__()\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class ExperimentParameters:\n",
    "    \"\"\"Contains all relevant parameters to run an experiment.\"\"\"\n",
    "\n",
    "    name: str  # Name of Parameter set. Used as identifier for charts etc.\n",
    "    frequency: int\n",
    "    max_subjects: int\n",
    "    max_test_subjects: int\n",
    "        \n",
    "    user_ids: list\n",
    "    num_sample_points_per_exp: int\n",
    "    exp_begin_cutoff_idx: int\n",
    "    exp_end_cutoff_idx: int\n",
    "        \n",
    "    \n",
    "    seconds_per_subject_train: float\n",
    "    seconds_per_subject_test: float\n",
    "    window_size: int  # After resampling\n",
    "    ocsvm_step_width: int  # After resampling\n",
    "    scaler: str  # StandardScaler, MinMaxScaler, Normalizer, MaxAbsScaler, RobustScaler, PowerTransformer\n",
    "    scaler_scope: str  # {\"subject\", \"session\"}\n",
    "    scaler_global: bool  # fit transform scale on all data (True) or fit on training only (False)\n",
    "    ocsvm_kernel: str # ocsvm kernel\n",
    "    ocsvm_nu: float  # Best value found in random search, used for final model\n",
    "    ocsvm_gamma: float  # Best value found in random search, used for final model\n",
    "    feature_cols: list  # Columns used as features\n",
    "    exclude_subjects: list  # Don't load data from those users\n",
    "        \n",
    "    # Calculated values\n",
    "    def __post_init__(self):\n",
    "        # HDF key of table:\n",
    "        self.table_name = f\"sensors_{self.frequency}hz\"\n",
    "\n",
    "        \n",
    "\n",
    "# INSTANCES\n",
    "# ===========================================================\n",
    "\n",
    "# NAIVE_APPROACH\n",
    "# -----------------------------------------------------------\n",
    "NAIVE_MINMAX_OCSVM = ExperimentParameters(\n",
    "    name=\"NAIVE-MINMAX_OCSVM\",\n",
    "    frequency=100,\n",
    "    max_subjects=29,\n",
    "    max_test_subjects=10,\n",
    "    user_ids = [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 28, 29, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49],\n",
    "    num_sample_points_per_exp=21000,\n",
    "    exp_begin_cutoff_idx=500,\n",
    "    exp_end_cutoff_idx=-500,\n",
    "    seconds_per_subject_train=210,\n",
    "    seconds_per_subject_test=210,    \n",
    "    window_size=250,\n",
    "    ocsvm_step_width=250,\n",
    "    scaler=\"minmax\",\n",
    "    scaler_scope=\"subject\",\n",
    "    scaler_global=True,\n",
    "    ocsvm_kernel=\"rbf\",\n",
    "    ocsvm_nu=None,\n",
    "    ocsvm_gamma=None,\n",
    "    feature_cols=[\n",
    "        \"x_a\",\n",
    "        \"y_a\",\n",
    "        \"z_a\",\n",
    "        \"x_g\",\n",
    "        \"y_g\",\n",
    "        \"z_g\",\n",
    "    ],\n",
    "    exclude_subjects=[],\n",
    ")\n",
    "\n",
    "# VALID_APPROACH\n",
    "# -----------------------------------------------------------\n",
    "VALID_MINMAX_OCSVM = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-MINMAX-OCSVM\",\n",
    "    scaler_global=False,\n",
    "    ocsvm_nu=0.165,\n",
    "    ocsvm_gamma=0.039,\n",
    ")\n",
    "\n",
    "# NAIVE_ROBUST_APPROACH\n",
    "# -----------------------------------------------------------\n",
    "NAIVE_ROBUST_OCSVM = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"NAIVE-ROBUST-OCSVM\",\n",
    "    scaler=\"robust\",\n",
    "    scaler_global=True,\n",
    "    ocsvm_nu=0.153,\n",
    "    ocsvm_gamma=0.091,  # below median, selected by chart\n",
    ")\n",
    "\n",
    "# ROBUST_APPROACH (VALID)\n",
    "# -----------------------------------------------------------\n",
    "VALID_ROBUST_OCSVM = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    ocsvm_nu=0.037,\n",
    "    ocsvm_gamma= 0.001,\n",
    ")\n",
    "# NORMALIZER_APPROACH (VALID)\n",
    "# -----------------------------------------------------------\n",
    "VALID_NORMALIZER_OCSVM = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-NORMALIZER-OCSVM\",\n",
    "    scaler=\"Normalizer\",\n",
    "    scaler_global=False,\n",
    "    ocsvm_nu=0.074,\n",
    "    ocsvm_gamma= 0.029,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "P = VALID_ROBUST_OCSVM\n",
    "# P = VALID_NORMALIZER_OCSVM\n",
    "N_NEIGHBORS_PARAMS = np.arange(1,20) \n",
    "P.p=2\n",
    "# n_neighbors_params = np.arange(1, 50, 5) \n",
    "\n",
    "P.scaler_clip=False\n",
    "P.is_NN=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>VALID-ROBUST-OCSVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frequency</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_subjects</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_test_subjects</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_ids</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_sample_points_per_exp</th>\n",
       "      <td>21000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp_begin_cutoff_idx</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp_end_cutoff_idx</th>\n",
       "      <td>-500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seconds_per_subject_train</th>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seconds_per_subject_test</th>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>window_size</th>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocsvm_step_width</th>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler</th>\n",
       "      <td>RobustScaler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler_scope</th>\n",
       "      <td>subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler_global</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocsvm_kernel</th>\n",
       "      <td>rbf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocsvm_nu</th>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocsvm_gamma</th>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_cols</th>\n",
       "      <td>[x_a, y_a, z_a, x_g, y_g, z_g]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exclude_subjects</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       Value\n",
       "name                                                      VALID-ROBUST-OCSVM\n",
       "frequency                                                                100\n",
       "max_subjects                                                              29\n",
       "max_test_subjects                                                         10\n",
       "user_ids                   [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 2...\n",
       "num_sample_points_per_exp                                              21000\n",
       "exp_begin_cutoff_idx                                                     500\n",
       "exp_end_cutoff_idx                                                      -500\n",
       "seconds_per_subject_train                                                210\n",
       "seconds_per_subject_test                                                 210\n",
       "window_size                                                              250\n",
       "ocsvm_step_width                                                         250\n",
       "scaler                                                          RobustScaler\n",
       "scaler_scope                                                         subject\n",
       "scaler_global                                                          False\n",
       "ocsvm_kernel                                                             rbf\n",
       "ocsvm_nu                                                               0.037\n",
       "ocsvm_gamma                                                            0.001\n",
       "feature_cols                                  [x_a, y_a, z_a, x_g, y_g, z_g]\n",
       "exclude_subjects                                                          []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils_ppp(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_scorer(utils_eer, greater_is_better=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils_eer_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading exp1 data:\n",
      "1) accel_count: 28388, gyro_count: 31997\n",
      "2) accel_count: 26010, gyro_count: 28954\n",
      "3) accel_count: 28227, gyro_count: 31814\n",
      "4) accel_count: 24860, gyro_count: 26105\n",
      "5) accel_count: 24270, gyro_count: 24347\n",
      "6) accel_count: 25012, gyro_count: 25060\n",
      "7) accel_count: 25301, gyro_count: 25382\n",
      "8) accel_count: 21975, gyro_count: 21658\n",
      "19) accel_count: 24110, gyro_count: 25050\n",
      "21) accel_count: 24326, gyro_count: 23809\n",
      "22) accel_count: 29123, gyro_count: 28724\n",
      "26) accel_count: 23148, gyro_count: 24291\n",
      "27) accel_count: 24299, gyro_count: 23589\n",
      "28) accel_count: 23807, gyro_count: 24523\n",
      "29) accel_count: 24030, gyro_count: 23457\n",
      "35) accel_count: 24388, gyro_count: 23673\n",
      "36) accel_count: 24228, gyro_count: 24208\n",
      "37) accel_count: 31945, gyro_count: 31816\n",
      "38) accel_count: 22135, gyro_count: 22327\n",
      "39) accel_count: 23573, gyro_count: 23459\n",
      "40) accel_count: 23057, gyro_count: 24296\n",
      "41) accel_count: 24102, gyro_count: 23681\n",
      "42) accel_count: 24074, gyro_count: 24328\n",
      "43) accel_count: 22631, gyro_count: 23835\n",
      "44) accel_count: 24473, gyro_count: 23749\n",
      "45) accel_count: 23974, gyro_count: 23229\n",
      "46) accel_count: 23614, gyro_count: 23827\n",
      "48) accel_count: 22828, gyro_count: 23904\n",
      "49) accel_count: 24183, gyro_count: 24633\n",
      "Loading exp2 data:\n",
      "1) accel_count: 24049, gyro_count: 26943\n",
      "2) accel_count: 24468, gyro_count: 27667\n",
      "3) accel_count: 24611, gyro_count: 27000\n",
      "4) accel_count: 24972, gyro_count: 26798\n",
      "5) accel_count: 23573, gyro_count: 23372\n",
      "6) accel_count: 23800, gyro_count: 23890\n",
      "7) accel_count: 23347, gyro_count: 24145\n",
      "8) accel_count: 22947, gyro_count: 22660\n",
      "19) accel_count: 26156, gyro_count: 25815\n",
      "21) accel_count: 23566, gyro_count: 24408\n",
      "22) accel_count: 23844, gyro_count: 24589\n",
      "26) accel_count: 23179, gyro_count: 23925\n",
      "27) accel_count: 25109, gyro_count: 25820\n",
      "28) accel_count: 23133, gyro_count: 24028\n",
      "29) accel_count: 23180, gyro_count: 24314\n",
      "35) accel_count: 23299, gyro_count: 23854\n",
      "36) accel_count: 25497, gyro_count: 25059\n",
      "37) accel_count: 25994, gyro_count: 25232\n",
      "38) accel_count: 21164, gyro_count: 21182\n",
      "39) accel_count: 24214, gyro_count: 23585\n",
      "40) accel_count: 23944, gyro_count: 23170\n",
      "41) accel_count: 23193, gyro_count: 24111\n",
      "42) accel_count: 26505, gyro_count: 25697\n",
      "43) accel_count: 22690, gyro_count: 23981\n",
      "44) accel_count: 23002, gyro_count: 23829\n",
      "45) accel_count: 23978, gyro_count: 23350\n",
      "46) accel_count: 21128, gyro_count: 21848\n",
      "48) accel_count: 27996, gyro_count: 27205\n",
      "49) accel_count: 23061, gyro_count: 24129\n"
     ]
    }
   ],
   "source": [
    "#include 47 later\n",
    "# user_ids = [9]\n",
    "df_exps_dict = load_data_frames(P.user_ids, P.exp_begin_cutoff_idx, P.exp_end_cutoff_idx, P.num_sample_points_per_exp)\n",
    "raw_dfList_exp1, raw_dfList_exp2 = df_exps_dict['dfList_exp1'], df_exps_dict['dfList_exp2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set: [7, 24, 8, 11, 13, 19, 28, 21, 26, 3, 20, 22, 6, 25, 16, 1, 17, 27, 5]\n",
      "test_set: [0, 12, 14, 9, 18, 23, 2, 15, 10, 4]\n",
      "train_set: [7, 24, 8, 11, 13, 19, 28, 21, 26, 3, 20, 22, 6, 25, 16, 1, 17, 27, 5]\n",
      "test_set: [0, 12, 14, 9, 18, 23, 2, 15, 10, 4]\n"
     ]
    }
   ],
   "source": [
    "randomized_data_idx = list(range(len(P.user_ids)))\n",
    "random.Random(SEED).shuffle(randomized_data_idx)\n",
    "split_idx = 2 * (len(randomized_data_idx)//3) + 1\n",
    "train_set = randomized_data_idx[: split_idx]\n",
    "test_set = randomized_data_idx[split_idx: ]\n",
    "# train_set = randomized_data_idx\n",
    "print(f\"train_set: {train_set}\\ntest_set: {test_set}\")\n",
    "# train_set = test_set\n",
    "# test_set = train_set\n",
    "print(f\"train_set: {train_set}\\ntest_set: {test_set}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading exp1 data:\n",
      "47) accel_count: 22777, gyro_count: 22226\n",
      "Loading exp2 data:\n",
      "47) accel_count: 17718, gyro_count: 18353\n"
     ]
    }
   ],
   "source": [
    "num_sample_points_per_exp_user_47 = 18000\n",
    "df_exps_dict_user_47 = load_data_frames([47], P.exp_begin_cutoff_idx, P.exp_end_cutoff_idx, num_sample_points_per_exp_user_47)\n",
    "dfList_exp1_user_47, dfList_exp2_user_47 = df_exps_dict_user_47['dfList_exp1'], df_exps_dict_user_47['dfList_exp2']\n",
    "\n",
    "raw_dfList_exp1_user_47 = dfList_exp1_user_47\n",
    "raw_dfList_exp2_user_47 = dfList_exp2_user_47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set: [7, 24, 8, 11, 13, 19, 28, 21, 26, 3, 20, 22, 6, 25, 16, 1, 17, 27, 5]\n"
     ]
    }
   ],
   "source": [
    "print(f\"train_set: {train_set}\")\n",
    "# print(f\"X_exp1_train_dic: {X_exp1_train_dic.keys()}\")\n",
    "# print(f\"X_exp2_train_dic: {X_exp2_train_dic.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_set: [0, 12, 14, 9, 18, 23, 2, 15, 10, 4]\n"
     ]
    }
   ],
   "source": [
    "print(f\"test_set: {test_set}\")\n",
    "# print(f\"X_exp1_test_dic: {X_exp1_test_dic.keys()}\")\n",
    "# print(f\"X_exp2_test_dic: {X_exp2_test_dic.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_experiment_params(exp_config=P)\n",
    "# P.smoothing = None\n",
    "# P.p=1. #UNCOMMENT WHEN YOU RESET IT FOR THE REST OF EXPS\n",
    "\n",
    "\n",
    "# preprocessing_method=None\n",
    "# time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "# train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/Manhattan-dist_{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "# test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/Manhattan-dist_{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "# with open(train_file_name, \"w\") as f:\n",
    "#     f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "#     f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "# with open(test_file_name, \"w\") as f:\n",
    "#     f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "#     f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# EER_df_train_dict={}\n",
    "# EER_df_test_dict={}\n",
    "\n",
    "\n",
    "# dfList_dict={\n",
    "#             \"dfList_exp1\": raw_dfList_exp1,\n",
    "#             \"dfList_exp2\": raw_dfList_exp2,\n",
    "#             \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "#             \"dfList_exp2_user_47\": raw_dfList_exp2_user_47\n",
    "# }\n",
    "\n",
    "# test_dict_key=DASH_MACRO_NUM\n",
    "# EER_df_train_dict[test_dict_key] = calculate_EER_different_window_sizes_optimize_num_neighbors(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "#                                                                                                 extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "#                                                                                                 n_neighbors_params=N_NEIGHBORS_PARAMS)\n",
    "\n",
    "# with open(train_file_name, \"a\") as f:\n",
    "#     f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "#     f.write(f\"\\ntest_dict_key: {test_dict_key}\\n\")\n",
    "#     f.write(EER_df_train_dict[test_dict_key].to_string())\n",
    "\n",
    "        \n",
    "# mean_EER_train_dict={}\n",
    "# for key in EER_df_train_dict:\n",
    "#     mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "# #-------\n",
    "# train_lst = list(mean_EER_train_dict.items())\n",
    "# train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "# with open(train_file_name, \"a\") as f:\n",
    "#     f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "#     f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "#     for i in range(len(train_lst)):\n",
    "#         f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "# min_key=train_lst[0][0]\n",
    "# EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "#                                                                                    extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "#                                                                                    best_param_df=EER_df_train_dict[min_key])\n",
    "# with open(test_file_name, \"a\") as f:\n",
    "#     f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "#     f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "#     f.write(EER_df_test_dict[min_key].to_string())\n",
    "# #-------\n",
    "# #-------\n",
    "# key_column= [\"cut_off_freq\"]\n",
    "# EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "# eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "# EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "# eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "# #-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WINDOW_SIZE_LST=[2000]\n",
    "P.p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. No Smoothing\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "P.smoothing = None\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=None\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "\n",
    "# train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/tmp_train.txt\"\n",
    "# test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/tmp_test.txt\"\n",
    "\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": raw_dfList_exp1,\n",
    "            \"dfList_exp2\": raw_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": raw_dfList_exp2_user_47\n",
    "}\n",
    "\n",
    "\n",
    "# #-----CV_FOLD-------\n",
    "# for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "#     train_set, test_set = THREE_FOLD_CV[cv_fold_idx]\n",
    "#     print(f\"train_set: {train_set}\")\n",
    "#     print(f\"test_set: {test_set}\")\n",
    "    \n",
    "    \n",
    "#     test_dict_key=DASH_MACRO_NUM\n",
    "#     EER_df_train_dict[test_dict_key] = calculate_EER_different_window_sizes_optimize_num_neighbors(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "#                                                                                                    extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "#                                                                                                    n_neighbors_params=N_NEIGHBORS_PARAMS)\n",
    "\n",
    "#     with open(train_file_name, \"a\") as f:\n",
    "#         f.write(\"\\n\" + \"-\"*22 + f\"Training results for cv_fold_idx: {cv_fold_idx}\" + \"-\"*22 + \"\\n\")  \n",
    "#         f.write(f\"\\nTest_dict_key: {test_dict_key}\\n\")\n",
    "#         f.write(EER_df_train_dict[test_dict_key].to_string())\n",
    "\n",
    "\n",
    "\n",
    "#     min_key=test_dict_key\n",
    "#     EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "#                                                                           extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "#                                                                           best_param_df=EER_df_train_dict[min_key])\n",
    "#     with open(test_file_name, \"a\") as f:\n",
    "#         f.write(\"\\n\" + \"-\"*22 + f\"Testing results for cv_fold_idx: {cv_fold_idx}\" + \"-\"*22 + \"\\n\")\n",
    "#         f.write(f\"\\nTest_dict_key: {test_dict_key}\\n\")\n",
    "#         f.write(EER_df_test_dict[min_key].to_string())\n",
    "#     #-------\n",
    "#     #-------\n",
    "#     key_column= [\"cut_off_freq\"]\n",
    "#     EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "#     eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "#     EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df_cv{cv_fold_idx}.json')\n",
    "#     eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df_cv{cv_fold_idx}.json')\n",
    "#     #-------\n",
    "    \n",
    "#---------\n",
    "key_column= [\"cut_off_freq\"]\n",
    "min_key=DASH_MACRO_NUM\n",
    "#-----CV_FOLD-------\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_kNN(\\\n",
    "                        cv_fold_idx=cv_fold_idx, \n",
    "                        cv_sets=THREE_FOLD_CV, \n",
    "                        dfList_dict=dfList_dict, \n",
    "                        window_size_lst=WINDOW_SIZE_LST, \n",
    "                        exp_config=P, \n",
    "                        extract_features_dict=EXTRACT_WACA_features_DICT, \n",
    "                        overlap=OVERLAP, \n",
    "                        n_neighbors_params=N_NEIGHBORS_PARAMS, \n",
    "                        train_file_name=train_file_name, \n",
    "                        test_file_name=test_file_name, \n",
    "                        preprocessing_params=min_key, \n",
    "                        key_column=key_column,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Butterworth frequency Cut-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Naive Approach\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "P.smoothing = \"Butterworth\"\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "old_test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/old_{time_of_execution}_{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "\n",
    "\n",
    "EER_df_test_dict={}\n",
    "    \n",
    "old_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "old_test_hyperparameters_df = pd.read_json(old_test_hyperparameters_file_name)\n",
    "old_test_hyperparameters_df.to_json(f'{old_test_file_name[:-4]}_raw_df.json')\n",
    "\n",
    "P.cut_off_freq=old_test_hyperparameters_df[\"cut_off_freq\"][0]\n",
    "min_key=P.cut_off_freq\n",
    "print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "\n",
    "ffted_dfList_exp1 = get_ffted_dfList(raw_dfList_exp1, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "ffted_dfList_exp1_user_47 = get_ffted_dfList(raw_dfList_exp1_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": ffted_dfList_exp1,\n",
    "            \"dfList_exp2\": ffted_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": ffted_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": ffted_dfList_exp2_user_47\n",
    "}\n",
    "\n",
    "#---------\n",
    "key_column= [\"cut_off_freq\"]\n",
    "#-----CV_FOLD-------\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_kNN(\\\n",
    "                        cv_fold_idx=cv_fold_idx, \n",
    "                        cv_sets=THREE_FOLD_CV, \n",
    "                        dfList_dict=dfList_dict, \n",
    "                        window_size_lst=WINDOW_SIZE_LST, \n",
    "                        exp_config=P, \n",
    "                        extract_features_dict=EXTRACT_WACA_features_DICT, \n",
    "                        overlap=OVERLAP, \n",
    "                        n_neighbors_params=N_NEIGHBORS_PARAMS, \n",
    "                        train_file_name=train_file_name, \n",
    "                        test_file_name=test_file_name, \n",
    "                        preprocessing_params=min_key, \n",
    "                        key_column=key_column,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butterworth\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for cut_off_freq in tqdm(CUT_OFF_FREQ_RANGE):\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "\n",
    "    \n",
    "    ffted_dfList_exp1 = get_ffted_dfList(raw_dfList_exp1, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    \n",
    "    ffted_dfList_exp1_user_47 = get_ffted_dfList(raw_dfList_exp1_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": ffted_dfList_exp1,\n",
    "                \"dfList_exp2\": ffted_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": ffted_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": ffted_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.cut_off_freq] = calculate_EER_different_window_sizes_optimize_num_neighbors(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                                    extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                                    n_neighbors_params=N_NEIGHBORS_PARAMS)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq: {P.cut_off_freq}\\n\")\n",
    "        f.write(EER_df_train_dict[P.cut_off_freq].to_string())\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                      extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                      best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "P.smoothing = \"Butterworth\"\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "\n",
    "time_of_execution = time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "old_test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/old_{time_of_execution}_{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "\n",
    "\n",
    "\n",
    "P.Butter_per_win_argdict={\n",
    "    \"filter_order\": P.filter_order,\n",
    "    \"cut_off_freq\": None,\n",
    "    \"sampling_freq\": P.sampling_freq,\n",
    "    \"filtfilt\": P.filtfilt,\n",
    "    }\n",
    "\n",
    "\n",
    "EER_df_test_dict={}\n",
    "\n",
    "    \n",
    "old_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "old_test_hyperparameters_df = pd.read_json(old_test_hyperparameters_file_name)\n",
    "old_test_hyperparameters_df.to_json(f'{old_test_file_name[:-4]}_raw_df.json')\n",
    "\n",
    "P.cut_off_freq=old_test_hyperparameters_df[\"cut_off_freq\"][0]\n",
    "\n",
    "P.Butter_per_win_argdict[\"cut_off_freq\"]=P.cut_off_freq\n",
    "min_key=P.cut_off_freq\n",
    "print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "\n",
    "\n",
    "ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": raw_dfList_exp1,\n",
    "            \"dfList_exp2\": ffted_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": ffted_dfList_exp2_user_47\n",
    "}\n",
    "\n",
    "key_column= [\"cut_off_freq\"]\n",
    "#-----CV_FOLD-------\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_kNN(\\\n",
    "                        cv_fold_idx=cv_fold_idx, \n",
    "                        cv_sets=THREE_FOLD_CV, \n",
    "                        dfList_dict=dfList_dict, \n",
    "                        window_size_lst=WINDOW_SIZE_LST, \n",
    "                        exp_config=P, \n",
    "                        extract_features_dict=EXTRACT_WACA_features_DICT, \n",
    "                        overlap=OVERLAP, \n",
    "                        n_neighbors_params=N_NEIGHBORS_PARAMS, \n",
    "                        train_file_name=train_file_name, \n",
    "                        test_file_name=test_file_name, \n",
    "                        preprocessing_params=min_key, \n",
    "                        key_column=key_column,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butterworth\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P.Butter_per_win_argdict={\n",
    "    \"filter_order\": P.filter_order,\n",
    "    \"cut_off_freq\": None,\n",
    "    \"sampling_freq\": P.sampling_freq,\n",
    "    \"filtfilt\": P.filtfilt,\n",
    "    }\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for cut_off_freq in tqdm(CUT_OFF_FREQ_RANGE):\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.Butter_per_win_argdict[\"cut_off_freq\"]=cut_off_freq\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "\n",
    "    \n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    \n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": ffted_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": ffted_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.cut_off_freq] = calculate_EER_different_window_sizes_optimize_num_neighbors(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                                    extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                                    n_neighbors_params=N_NEIGHBORS_PARAMS)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq: {P.cut_off_freq}\\n\")\n",
    "        f.write(EER_df_train_dict[P.cut_off_freq].to_string())\n",
    "\n",
    "\n",
    "\n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                      extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                      best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Butterworth frequency Cut-off + EMA span\n",
    "## 2.1 Naive Approach\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_2)\n",
    "P.smoothing = \"Butter+EMA\"\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "old_test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/old_{time_of_execution}_{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_test_dict={}\n",
    "\n",
    "    \n",
    "old_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "old_test_hyperparameters_df = pd.read_json(old_test_hyperparameters_file_name)\n",
    "old_test_hyperparameters_df.to_json(f'{old_test_file_name[:-4]}_raw_df.json')\n",
    "\n",
    "P.cut_off_freq=old_test_hyperparameters_df[\"cut_off_freq\"][0]\n",
    "P.span=old_test_hyperparameters_df[\"EMA_span\"][0]\n",
    "\n",
    "min_key= P.cut_off_freq, P.span\n",
    "print(f\"cut_off_freq: {P.cut_off_freq}, EMA span: {P.span}\")\n",
    "\n",
    "\n",
    "ffted_dfList_exp1 = get_ffted_dfList(raw_dfList_exp1, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "EMAed_dfList_exp1 = get_EMAed_dfList(ffted_dfList_exp1, span=P.span)\n",
    "EMAed_dfList_exp2 = get_EMAed_dfList(ffted_dfList_exp2, span=P.span)\n",
    "\n",
    "ffted_dfList_exp1_user_47 = get_ffted_dfList(raw_dfList_exp1_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "EMAed_dfList_exp1_user_47 = get_EMAed_dfList(ffted_dfList_exp1_user_47, span=P.span)\n",
    "EMAed_dfList_exp2_user_47 = get_EMAed_dfList(ffted_dfList_exp2_user_47, span=P.span)\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": EMAed_dfList_exp1,\n",
    "            \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": EMAed_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "}\n",
    "\n",
    "\n",
    "key_column= [\"cut_off_freq\", \"EMA_span\"]\n",
    "#-----CV_FOLD-------\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_kNN(\\\n",
    "                        cv_fold_idx=cv_fold_idx, \n",
    "                        cv_sets=THREE_FOLD_CV, \n",
    "                        dfList_dict=dfList_dict, \n",
    "                        window_size_lst=WINDOW_SIZE_LST, \n",
    "                        exp_config=P, \n",
    "                        extract_features_dict=EXTRACT_WACA_features_DICT, \n",
    "                        overlap=OVERLAP, \n",
    "                        n_neighbors_params=N_NEIGHBORS_PARAMS, \n",
    "                        train_file_name=train_file_name, \n",
    "                        test_file_name=test_file_name, \n",
    "                        preprocessing_params=min_key, \n",
    "                        key_column=key_column,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_2)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butter+EMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "    \n",
    "indices = list(range(1, 50))\n",
    "mesh = np.array(np.meshgrid(indices, indices))\n",
    "index_pairs = mesh.T.reshape(-1, 2)\n",
    "\n",
    "print(f\"total cut_off_span_pairs: {index_pairs.shape}, choice_num: {CHOICE_NUM_PAIRS}\")\n",
    "cut_off_span_pairs = index_pairs[np.random.choice(index_pairs.shape[0], size=CHOICE_NUM_PAIRS, replace=False), :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for key_pair in tqdm(cut_off_span_pairs):\n",
    "    \n",
    "    key_pair = tuple(key_pair)\n",
    "    cut_off_freq, span = key_pair[0], key_pair[1]\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.span=span\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "    print(f\"span: {P.span}\")\n",
    "\n",
    "    \n",
    "    ffted_dfList_exp1 = get_ffted_dfList(raw_dfList_exp1, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    EMAed_dfList_exp1 = get_EMAed_dfList(ffted_dfList_exp1, span=P.span)\n",
    "    EMAed_dfList_exp2 = get_EMAed_dfList(ffted_dfList_exp2, span=P.span)\n",
    "    \n",
    "    ffted_dfList_exp1_user_47 = get_ffted_dfList(raw_dfList_exp1_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    EMAed_dfList_exp1_user_47 = get_EMAed_dfList(ffted_dfList_exp1_user_47, span=P.span)\n",
    "    EMAed_dfList_exp2_user_47 = get_EMAed_dfList(ffted_dfList_exp2_user_47, span=P.span)\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": EMAed_dfList_exp1,\n",
    "                \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": EMAed_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[key_pair] = calculate_EER_different_window_sizes_optimize_num_neighbors(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                                                                  extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                                                                  n_neighbors_params=N_NEIGHBORS_PARAMS)\n",
    "        \n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq, span: {key_pair}\\n\")\n",
    "        f.write(EER_df_train_dict[key_pair].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                      extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                      best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\", \"EMA_span\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_2)\n",
    "P.smoothing = \"Butter+EMA\"\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "old_test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/old_{time_of_execution}_{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "\n",
    "\n",
    "\n",
    "P.Butter_per_win_argdict={\n",
    "    \"filter_order\": P.filter_order,\n",
    "    \"cut_off_freq\": None,\n",
    "    \"sampling_freq\": P.sampling_freq,\n",
    "    \"filtfilt\": P.filtfilt,\n",
    "    }\n",
    "\n",
    "EER_df_test_dict={}\n",
    "\n",
    "\n",
    "old_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "old_test_hyperparameters_df = pd.read_json(old_test_hyperparameters_file_name)\n",
    "old_test_hyperparameters_df.to_json(f'{old_test_file_name[:-4]}_raw_df.json')\n",
    "\n",
    "P.cut_off_freq=old_test_hyperparameters_df[\"cut_off_freq\"][0]\n",
    "P.span=old_test_hyperparameters_df[\"EMA_span\"][0]\n",
    "\n",
    "P.Butter_per_win_argdict[\"cut_off_freq\"]=P.cut_off_freq\n",
    "P.EMA_per_win_span=P.span\n",
    "\n",
    "min_key= P.cut_off_freq, P.span\n",
    "print(f\"cut_off_freq: {P.cut_off_freq}, EMA span: {P.span}\")\n",
    "\n",
    "\n",
    "ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "EMAed_dfList_exp2 = get_EMAed_dfList(ffted_dfList_exp2, span=P.span)\n",
    "\n",
    "ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "EMAed_dfList_exp2_user_47 = get_EMAed_dfList(ffted_dfList_exp2_user_47, span=P.span)\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": raw_dfList_exp1,\n",
    "            \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "}\n",
    "\n",
    "\n",
    "key_column= [\"cut_off_freq\", \"EMA_span\"]\n",
    "#-----CV_FOLD-------\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_kNN(\\\n",
    "                        cv_fold_idx=cv_fold_idx, \n",
    "                        cv_sets=THREE_FOLD_CV, \n",
    "                        dfList_dict=dfList_dict, \n",
    "                        window_size_lst=WINDOW_SIZE_LST, \n",
    "                        exp_config=P, \n",
    "                        extract_features_dict=EXTRACT_WACA_features_DICT, \n",
    "                        overlap=OVERLAP, \n",
    "                        n_neighbors_params=N_NEIGHBORS_PARAMS, \n",
    "                        train_file_name=train_file_name, \n",
    "                        test_file_name=test_file_name, \n",
    "                        preprocessing_params=min_key, \n",
    "                        key_column=key_column,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_2)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butter+EMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "    \n",
    "indices = list(range(1, 50))\n",
    "mesh = np.array(np.meshgrid(indices, indices))\n",
    "index_pairs = mesh.T.reshape(-1, 2)\n",
    "\n",
    "print(f\"total cut_off_span_pairs: {index_pairs.shape}, choice_num: {CHOICE_NUM_PAIRS}\")\n",
    "cut_off_span_pairs = index_pairs[np.random.choice(index_pairs.shape[0], size=CHOICE_NUM_PAIRS, replace=False), :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P.Butter_per_win_argdict={\n",
    "    \"filter_order\": P.filter_order,\n",
    "    \"cut_off_freq\": None,\n",
    "    \"sampling_freq\": P.sampling_freq,\n",
    "    \"filtfilt\": P.filtfilt,\n",
    "    }\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for key_pair in tqdm(cut_off_span_pairs):\n",
    "    key_pair = tuple(key_pair)\n",
    "    cut_off_freq, span = key_pair[0], key_pair[1]\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.Butter_per_win_argdict[\"cut_off_freq\"]=cut_off_freq\n",
    "    \n",
    "    \n",
    "    P.span=span\n",
    "    P.EMA_per_win_span=span\n",
    "\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "    print(f\"span: {P.span}\")\n",
    "\n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    EMAed_dfList_exp2 = get_EMAed_dfList(ffted_dfList_exp2, span=P.span)\n",
    "    \n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    EMAed_dfList_exp2_user_47 = get_EMAed_dfList(ffted_dfList_exp2_user_47, span=P.span)\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[key_pair] = calculate_EER_different_window_sizes_optimize_num_neighbors(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                              extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                              n_neighbors_params=N_NEIGHBORS_PARAMS)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq, span: {key_pair}\\n\")\n",
    "        f.write(EER_df_train_dict[key_pair].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                      extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                      best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\", \"EMA_span\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. EMA span\n",
    "## 3.1 Naive Approach\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "P.smoothing = \"EMA\"\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "old_test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/old_{time_of_execution}_{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "\n",
    "\n",
    "\n",
    "EER_df_test_dict={}\n",
    "    \n",
    "old_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "old_test_hyperparameters_df = pd.read_json(old_test_hyperparameters_file_name)\n",
    "old_test_hyperparameters_df.to_json(f'{old_test_file_name[:-4]}_raw_df.json')\n",
    "\n",
    "P.span=old_test_hyperparameters_df[\"EMA_span\"][0]\n",
    "min_key=P.span\n",
    "print(f\"EMA span: {P.span}\")\n",
    "\n",
    "\n",
    "EMAed_dfList_exp1 = get_EMAed_dfList(raw_dfList_exp1, span=P.span)\n",
    "EMAed_dfList_exp2 = get_EMAed_dfList(raw_dfList_exp2, span=P.span)\n",
    "\n",
    "EMAed_dfList_exp1_user_47 = get_EMAed_dfList(raw_dfList_exp1_user_47, span=P.span)\n",
    "EMAed_dfList_exp2_user_47 = get_EMAed_dfList(raw_dfList_exp2_user_47, span=P.span)\n",
    "\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": EMAed_dfList_exp1,\n",
    "            \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": EMAed_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "key_column= [\"EMA_span\"]\n",
    "#-----CV_FOLD-------\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_kNN(\\\n",
    "                        cv_fold_idx=cv_fold_idx, \n",
    "                        cv_sets=THREE_FOLD_CV, \n",
    "                        dfList_dict=dfList_dict, \n",
    "                        window_size_lst=WINDOW_SIZE_LST, \n",
    "                        exp_config=P, \n",
    "                        extract_features_dict=EXTRACT_WACA_features_DICT, \n",
    "                        overlap=OVERLAP, \n",
    "                        n_neighbors_params=N_NEIGHBORS_PARAMS, \n",
    "                        train_file_name=train_file_name, \n",
    "                        test_file_name=test_file_name, \n",
    "                        preprocessing_params=min_key, \n",
    "                        key_column=key_column,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"EMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for span in tqdm(EMA_SPAN_RANGE):\n",
    "    P.span=span\n",
    "    print(f\"EMA span: {P.span}\")\n",
    "\n",
    "    \n",
    "    EMAed_dfList_exp1 = get_EMAed_dfList(raw_dfList_exp1, span=P.span)\n",
    "    EMAed_dfList_exp2 = get_EMAed_dfList(raw_dfList_exp2, span=P.span)\n",
    "    \n",
    "    EMAed_dfList_exp1_user_47 = get_EMAed_dfList(raw_dfList_exp1_user_47, span=P.span)\n",
    "    EMAed_dfList_exp2_user_47 = get_EMAed_dfList(raw_dfList_exp2_user_47, span=P.span)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": EMAed_dfList_exp1,\n",
    "                \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": EMAed_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.span] = calculate_EER_different_window_sizes_optimize_num_neighbors(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                            extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                            n_neighbors_params=N_NEIGHBORS_PARAMS)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\EMA span: {P.span}\\n\")\n",
    "        f.write(EER_df_train_dict[P.span].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                      extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                      best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"EMA_span\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "P.smoothing = \"EMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "old_test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/old_{time_of_execution}_{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "    \n",
    "\n",
    "EER_df_test_dict={}\n",
    "    \n",
    "old_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "old_test_hyperparameters_df = pd.read_json(old_test_hyperparameters_file_name)\n",
    "old_test_hyperparameters_df.to_json(f'{old_test_file_name[:-4]}_raw_df.json')\n",
    "\n",
    "P.span=old_test_hyperparameters_df[\"EMA_span\"][0]\n",
    "P.EMA_per_win_span=P.span\n",
    "min_key=P.span\n",
    "print(f\"EMA span: {P.span}\")\n",
    "\n",
    "\n",
    "EMAed_dfList_exp2 = get_EMAed_dfList(raw_dfList_exp2, span=P.span)\n",
    "\n",
    "EMAed_dfList_exp2_user_47 = get_EMAed_dfList(raw_dfList_exp2_user_47, span=P.span)\n",
    "\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": raw_dfList_exp1,\n",
    "            \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "key_column= [\"EMA_span\"]\n",
    "#-----CV_FOLD-------\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_kNN(\\\n",
    "                        cv_fold_idx=cv_fold_idx, \n",
    "                        cv_sets=THREE_FOLD_CV, \n",
    "                        dfList_dict=dfList_dict, \n",
    "                        window_size_lst=WINDOW_SIZE_LST, \n",
    "                        exp_config=P, \n",
    "                        extract_features_dict=EXTRACT_WACA_features_DICT, \n",
    "                        overlap=OVERLAP, \n",
    "                        n_neighbors_params=N_NEIGHBORS_PARAMS, \n",
    "                        train_file_name=train_file_name, \n",
    "                        test_file_name=test_file_name, \n",
    "                        preprocessing_params=min_key, \n",
    "                        key_column=key_column,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"EMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for span in tqdm(EMA_SPAN_RANGE):\n",
    "    P.span=span\n",
    "    print(f\"EMA span: {P.span}\")\n",
    "\n",
    "    P.EMA_per_win_span=P.span\n",
    "\n",
    "    EMAed_dfList_exp2 = get_EMAed_dfList(raw_dfList_exp2, span=P.span)\n",
    "    \n",
    "    EMAed_dfList_exp2_user_47 = get_EMAed_dfList(raw_dfList_exp2_user_47, span=P.span)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.span] = calculate_EER_different_window_sizes_optimize_num_neighbors(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                            extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                            n_neighbors_params=N_NEIGHBORS_PARAMS)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\EMA span: {P.span}\\n\")\n",
    "        f.write(EER_df_train_dict[P.span].to_string())\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                      extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                      best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"EMA_span\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SMA winsize\n",
    "## 4.1 Naive Approach\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "P.smoothing = \"SMA\"\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "old_test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/old_{time_of_execution}_{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "\n",
    "\n",
    "EER_df_test_dict={}\n",
    "\n",
    "old_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "old_test_hyperparameters_df = pd.read_json(old_test_hyperparameters_file_name)\n",
    "old_test_hyperparameters_df.to_json(f'{old_test_file_name[:-4]}_raw_df.json')\n",
    "\n",
    "P.winsize=old_test_hyperparameters_df[\"SMA_winsize\"][0]\n",
    "min_key=P.winsize\n",
    "print(f\"SMA winsize: {P.winsize}\")\n",
    "\n",
    "\n",
    "SMAed_dfList_exp1 = get_SMAed_dfList(raw_dfList_exp1, winsize=P.winsize)\n",
    "SMAed_dfList_exp2 = get_SMAed_dfList(raw_dfList_exp2, winsize=P.winsize)\n",
    "\n",
    "SMAed_dfList_exp1_user_47 = get_SMAed_dfList(raw_dfList_exp1_user_47, winsize=P.winsize)\n",
    "SMAed_dfList_exp2_user_47 = get_SMAed_dfList(raw_dfList_exp2_user_47, winsize=P.winsize)\n",
    "\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": SMAed_dfList_exp1,\n",
    "            \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": SMAed_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "}\n",
    "\n",
    "\n",
    "key_column= [\"SMA_winsize\"]\n",
    "#-----CV_FOLD-------\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_kNN(\\\n",
    "                        cv_fold_idx=cv_fold_idx, \n",
    "                        cv_sets=THREE_FOLD_CV, \n",
    "                        dfList_dict=dfList_dict, \n",
    "                        window_size_lst=WINDOW_SIZE_LST, \n",
    "                        exp_config=P, \n",
    "                        extract_features_dict=EXTRACT_WACA_features_DICT, \n",
    "                        overlap=OVERLAP, \n",
    "                        n_neighbors_params=N_NEIGHBORS_PARAMS, \n",
    "                        train_file_name=train_file_name, \n",
    "                        test_file_name=test_file_name, \n",
    "                        preprocessing_params=min_key, \n",
    "                        key_column=key_column,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"SMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for winsize in tqdm(SMA_WINSIZE_RANGE):\n",
    "    P.winsize=winsize\n",
    "    print(f\"SMA winsize: {P.winsize}\")\n",
    "\n",
    "\n",
    "    SMAed_dfList_exp1 = get_SMAed_dfList(raw_dfList_exp1, winsize=P.winsize)\n",
    "    SMAed_dfList_exp2 = get_SMAed_dfList(raw_dfList_exp2, winsize=P.winsize)\n",
    "    \n",
    "    SMAed_dfList_exp1_user_47 = get_SMAed_dfList(raw_dfList_exp1_user_47, winsize=P.winsize)\n",
    "    SMAed_dfList_exp2_user_47 = get_SMAed_dfList(raw_dfList_exp2_user_47, winsize=P.winsize)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": SMAed_dfList_exp1,\n",
    "                \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": SMAed_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.winsize] = calculate_EER_different_window_sizes_optimize_num_neighbors(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                               extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                               n_neighbors_params=N_NEIGHBORS_PARAMS)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\SMA winsize: {P.winsize}\\n\")\n",
    "        f.write(EER_df_train_dict[P.winsize].to_string())\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                      extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                      best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"SMA_winsize\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "P.smoothing = \"SMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "old_test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/old_{time_of_execution}_{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "\n",
    "\n",
    "EER_df_test_dict={}\n",
    "\n",
    "old_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "old_test_hyperparameters_df = pd.read_json(old_test_hyperparameters_file_name)\n",
    "old_test_hyperparameters_df.to_json(f'{old_test_file_name[:-4]}_raw_df.json')\n",
    "\n",
    "P.winsize=old_test_hyperparameters_df[\"SMA_winsize\"][0]\n",
    "P.SMA_per_win_winsize=P.winsize\n",
    "min_key=P.winsize\n",
    "print(f\"SMA winsize: {P.winsize}\")\n",
    "\n",
    "\n",
    "SMAed_dfList_exp2 = get_SMAed_dfList(raw_dfList_exp2, winsize=P.winsize)\n",
    "\n",
    "SMAed_dfList_exp2_user_47 = get_SMAed_dfList(raw_dfList_exp2_user_47, winsize=P.winsize)\n",
    "\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": raw_dfList_exp1,\n",
    "            \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "}\n",
    "\n",
    "\n",
    "key_column= [\"SMA_winsize\"]\n",
    "#-----CV_FOLD-------\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_kNN(\\\n",
    "                        cv_fold_idx=cv_fold_idx, \n",
    "                        cv_sets=THREE_FOLD_CV, \n",
    "                        dfList_dict=dfList_dict, \n",
    "                        window_size_lst=WINDOW_SIZE_LST, \n",
    "                        exp_config=P, \n",
    "                        extract_features_dict=EXTRACT_WACA_features_DICT, \n",
    "                        overlap=OVERLAP, \n",
    "                        n_neighbors_params=N_NEIGHBORS_PARAMS, \n",
    "                        train_file_name=train_file_name, \n",
    "                        test_file_name=test_file_name, \n",
    "                        preprocessing_params=min_key, \n",
    "                        key_column=key_column,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"SMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for winsize in tqdm(SMA_WINSIZE_RANGE):\n",
    "    P.winsize=winsize\n",
    "    P.SMA_per_win_winsize=P.winsize\n",
    "\n",
    "    print(f\"SMA winsize: {P.winsize}\")\n",
    "\n",
    "\n",
    "    SMAed_dfList_exp2 = get_SMAed_dfList(raw_dfList_exp2, winsize=P.winsize)\n",
    "    \n",
    "    SMAed_dfList_exp2_user_47 = get_SMAed_dfList(raw_dfList_exp2_user_47, winsize=P.winsize)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.winsize] = calculate_EER_different_window_sizes_optimize_num_neighbors(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                               extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                               n_neighbors_params=N_NEIGHBORS_PARAMS)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\SMA winsize: {P.winsize}\\n\")\n",
    "        f.write(EER_df_train_dict[P.winsize].to_string())\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                      extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                      best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"SMA_winsize\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Butterworth frequency Cut-off + SMA winsize\n",
    "## 5.1 Naive Approach\n",
    "### Optimizing and Testin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_5)\n",
    "P.smoothing = \"Butter+SMA\"\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "old_test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/old_{time_of_execution}_{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "\n",
    "\n",
    "EER_df_test_dict={}\n",
    "\n",
    "    \n",
    "old_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "old_test_hyperparameters_df = pd.read_json(old_test_hyperparameters_file_name)\n",
    "old_test_hyperparameters_df.to_json(f'{old_test_file_name[:-4]}_raw_df.json')\n",
    "\n",
    "P.winsize=old_test_hyperparameters_df[\"SMA_winsize\"][0]\n",
    "P.cut_off_freq=old_test_hyperparameters_df[\"cut_off_freq\"][0]\n",
    "\n",
    "min_key= P.cut_off_freq, P.winsize\n",
    "print(f\"cut_off_freq: {P.cut_off_freq}, winsize: {P.winsize}\")\n",
    "\n",
    "\n",
    "ffted_dfList_exp1 = get_ffted_dfList(raw_dfList_exp1, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "SMAed_dfList_exp1 = get_SMAed_dfList(ffted_dfList_exp1, winsize=P.winsize)\n",
    "SMAed_dfList_exp2 = get_SMAed_dfList(ffted_dfList_exp2, winsize=P.winsize)\n",
    "\n",
    "ffted_dfList_exp1_user_47 = get_ffted_dfList(raw_dfList_exp1_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "SMAed_dfList_exp1_user_47 = get_SMAed_dfList(ffted_dfList_exp1_user_47, winsize=P.winsize)\n",
    "SMAed_dfList_exp2_user_47 = get_SMAed_dfList(ffted_dfList_exp2_user_47, winsize=P.winsize)\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": SMAed_dfList_exp1,\n",
    "            \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": SMAed_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "}\n",
    "\n",
    "\n",
    "key_column= [\"cut_off_freq\", \"SMA_winsize\"]\n",
    "#-----CV_FOLD-------\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_kNN(\\\n",
    "                        cv_fold_idx=cv_fold_idx, \n",
    "                        cv_sets=THREE_FOLD_CV, \n",
    "                        dfList_dict=dfList_dict, \n",
    "                        window_size_lst=WINDOW_SIZE_LST, \n",
    "                        exp_config=P, \n",
    "                        extract_features_dict=EXTRACT_WACA_features_DICT, \n",
    "                        overlap=OVERLAP, \n",
    "                        n_neighbors_params=N_NEIGHBORS_PARAMS, \n",
    "                        train_file_name=train_file_name, \n",
    "                        test_file_name=test_file_name, \n",
    "                        preprocessing_params=min_key, \n",
    "                        key_column=key_column,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_5)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butter+SMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "    \n",
    "indices = list(range(1, 50))\n",
    "mesh = np.array(np.meshgrid(indices, indices))\n",
    "index_pairs = mesh.T.reshape(-1, 2)\n",
    "\n",
    "print(f\"total cut_off_winsize_pairs: {index_pairs.shape}, choice_num: {CHOICE_NUM_PAIRS}\")\n",
    "cut_off_winsize_pairs = index_pairs[np.random.choice(index_pairs.shape[0], size=CHOICE_NUM_PAIRS, replace=False), :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for key_pair in tqdm(cut_off_winsize_pairs):\n",
    "    \n",
    "    key_pair = tuple(key_pair)\n",
    "    cut_off_freq, winsize = key_pair[0], key_pair[1]\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.winsize=winsize\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "    print(f\"winsize: {P.winsize}\")\n",
    "\n",
    "    \n",
    "    ffted_dfList_exp1 = get_ffted_dfList(raw_dfList_exp1, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    SMAed_dfList_exp1 = get_SMAed_dfList(ffted_dfList_exp1, winsize=P.winsize)\n",
    "    SMAed_dfList_exp2 = get_SMAed_dfList(ffted_dfList_exp2, winsize=P.winsize)\n",
    "    \n",
    "    ffted_dfList_exp1_user_47 = get_ffted_dfList(raw_dfList_exp1_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    SMAed_dfList_exp1_user_47 = get_SMAed_dfList(ffted_dfList_exp1_user_47, winsize=P.winsize)\n",
    "    SMAed_dfList_exp2_user_47 = get_SMAed_dfList(ffted_dfList_exp2_user_47, winsize=P.winsize)\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": SMAed_dfList_exp1,\n",
    "                \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": SMAed_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[key_pair] = calculate_EER_different_window_sizes_optimize_num_neighbors(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                              extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                              n_neighbors_params=N_NEIGHBORS_PARAMS)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq, winsize: {key_pair}\\n\")\n",
    "        f.write(EER_df_train_dict[key_pair].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                      extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                      best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\", \"SMA_winsize\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_5)\n",
    "P.smoothing = \"Butter+SMA\"\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "old_test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/old_{time_of_execution}_{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "\n",
    "\n",
    "P.Butter_per_win_argdict={\n",
    "    \"filter_order\": P.filter_order,\n",
    "    \"cut_off_freq\": None,\n",
    "    \"sampling_freq\": P.sampling_freq,\n",
    "    \"filtfilt\": P.filtfilt,\n",
    "    }\n",
    "\n",
    "\n",
    "EER_df_test_dict={}\n",
    "\n",
    "    \n",
    "old_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "old_test_hyperparameters_df = pd.read_json(old_test_hyperparameters_file_name)\n",
    "old_test_hyperparameters_df.to_json(f'{old_test_file_name[:-4]}_raw_df.json')\n",
    "\n",
    "P.winsize=old_test_hyperparameters_df[\"SMA_winsize\"][0]\n",
    "P.cut_off_freq=old_test_hyperparameters_df[\"cut_off_freq\"][0]\n",
    "\n",
    "P.Butter_per_win_argdict[\"cut_off_freq\"]=P.cut_off_freq\n",
    "P.SMA_per_win_winsize=P.winsize\n",
    "\n",
    "min_key= P.cut_off_freq, P.winsize\n",
    "print(f\"cut_off_freq: {P.cut_off_freq}, winsize: {P.winsize}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "SMAed_dfList_exp2 = get_SMAed_dfList(ffted_dfList_exp2, winsize=P.winsize)\n",
    "\n",
    "ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "SMAed_dfList_exp2_user_47 = get_SMAed_dfList(ffted_dfList_exp2_user_47, winsize=P.winsize)\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": raw_dfList_exp1,\n",
    "            \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "}\n",
    "\n",
    "\n",
    "key_column= [\"cut_off_freq\", \"SMA_winsize\"]\n",
    "#-----CV_FOLD-------\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_kNN(\\\n",
    "                        cv_fold_idx=cv_fold_idx, \n",
    "                        cv_sets=THREE_FOLD_CV, \n",
    "                        dfList_dict=dfList_dict, \n",
    "                        window_size_lst=WINDOW_SIZE_LST, \n",
    "                        exp_config=P, \n",
    "                        extract_features_dict=EXTRACT_WACA_features_DICT, \n",
    "                        overlap=OVERLAP, \n",
    "                        n_neighbors_params=N_NEIGHBORS_PARAMS, \n",
    "                        train_file_name=train_file_name, \n",
    "                        test_file_name=test_file_name, \n",
    "                        preprocessing_params=min_key, \n",
    "                        key_column=key_column,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.smoothing\n",
    "preprocessing_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_5)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butter+SMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "    \n",
    "indices = list(range(1, 50))\n",
    "mesh = np.array(np.meshgrid(indices, indices))\n",
    "index_pairs = mesh.T.reshape(-1, 2)\n",
    "\n",
    "print(f\"total cut_off_winsize_pairs: {index_pairs.shape}, choice_num: {CHOICE_NUM_PAIRS}\")\n",
    "cut_off_winsize_pairs = index_pairs[np.random.choice(index_pairs.shape[0], size=CHOICE_NUM_PAIRS, replace=False), :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P.Butter_per_win_argdict={\n",
    "    \"filter_order\": P.filter_order,\n",
    "    \"cut_off_freq\": None,\n",
    "    \"sampling_freq\": P.sampling_freq,\n",
    "    \"filtfilt\": P.filtfilt,\n",
    "    }\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for key_pair in tqdm(cut_off_winsize_pairs):\n",
    "    \n",
    "    key_pair = tuple(key_pair)\n",
    "    cut_off_freq, winsize = key_pair[0], key_pair[1]\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.Butter_per_win_argdict[\"cut_off_freq\"]=P.cut_off_freq\n",
    "    \n",
    "    P.winsize=winsize\n",
    "    P.SMA_per_win_winsize=P.winsize\n",
    "    \n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "    print(f\"winsize: {P.winsize}\")\n",
    "\n",
    "    \n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    SMAed_dfList_exp2 = get_SMAed_dfList(ffted_dfList_exp2, winsize=P.winsize)\n",
    "    \n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    SMAed_dfList_exp2_user_47 = get_SMAed_dfList(ffted_dfList_exp2_user_47, winsize=P.winsize)\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[key_pair] = calculate_EER_different_window_sizes_optimize_num_neighbors(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                              extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                              n_neighbors_params=N_NEIGHBORS_PARAMS)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq, winsize: {key_pair}\\n\")\n",
    "        f.write(EER_df_train_dict[key_pair].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                      extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                      best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\", \"SMA_winsize\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. The effect of Varying Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reseting experiment params successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap: 0.01\n",
      "train_set: {0, 1, 2, 3, 4, 5, 6, 9, 10, 12, 14, 15, 16, 18, 19, 22, 23, 24, 25, 28}\n",
      "test_set: {7, 8, 11, 13, 17, 20, 21, 26, 27, 29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 125\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  8.529141063801944\n",
      "MakeWACAXExpDicUnknown Time:  57.842547392472625\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|         | 1/9 [02:23<19:05, 143.20s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  4.104598788544536\n",
      "MakeWACAXExpDicUnknown Time:  44.01284077949822\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|       | 2/9 [04:39<16:13, 139.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 500\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  3.1239006482064724\n",
      "MakeWACAXExpDicUnknown Time:  29.61167077999562\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|      | 3/9 [05:23<09:35, 95.84s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 750\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  2.9025398483499885\n",
      "MakeWACAXExpDicUnknown Time:  20.479793081991374\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 44%|     | 4/9 [05:58<05:57, 71.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1000\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  2.589440457522869\n",
      "MakeWACAXExpDicUnknown Time:  16.079033163376153\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 56%|    | 5/9 [06:26<03:44, 56.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  2.5765244904905558\n",
      "MakeWACAXExpDicUnknown Time:  15.51045602466911\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|   | 6/9 [06:58<02:23, 47.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1500\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  3.000904256477952\n",
      "MakeWACAXExpDicUnknown Time:  16.347674359567463\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|  | 7/9 [07:31<01:26, 43.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1750\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  3.1800068616867065\n",
      "MakeWACAXExpDicUnknown Time:  14.07335225213319\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 89%| | 8/9 [08:03<00:39, 39.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 2000\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  1.524534814991057\n",
      "MakeWACAXExpDicUnknown Time:  7.583844147622585\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|| 9/9 [08:24<00:00, 56.00s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 125\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 170\n",
      "len_exp2_user_47: 146\n",
      "MakeWACAXExpDicOwner Time:  7.727369498461485\n",
      "MakeWACAXExpDicUnknown Time:  53.58146076556295\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|         | 1/9 [01:07<08:57, 67.21s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 85\n",
      "len_exp2_user_47: 72\n",
      "MakeWACAXExpDicOwner Time:  3.986661206930876\n",
      "MakeWACAXExpDicUnknown Time:  24.7096864329651\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|       | 2/9 [01:40<05:32, 47.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 500\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 42\n",
      "len_exp2_user_47: 36\n",
      "MakeWACAXExpDicOwner Time:  2.526345049031079\n",
      "MakeWACAXExpDicUnknown Time:  15.35869756899774\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|      | 3/9 [01:59<03:26, 34.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 750\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 28\n",
      "len_exp2_user_47: 24\n",
      "MakeWACAXExpDicOwner Time:  1.5791514674201608\n",
      "MakeWACAXExpDicUnknown Time:  7.735887188464403\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 44%|     | 4/9 [02:09<02:03, 24.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1000\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 21\n",
      "len_exp2_user_47: 18\n",
      "MakeWACAXExpDicOwner Time:  1.1907092239707708\n",
      "MakeWACAXExpDicUnknown Time:  5.696951903402805\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 56%|    | 5/9 [02:17<01:13, 18.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 16\n",
      "len_exp2_user_47: 14\n",
      "MakeWACAXExpDicOwner Time:  0.8351087812334299\n",
      "MakeWACAXExpDicUnknown Time:  5.049308926798403\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|   | 6/9 [02:23<00:43, 14.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1500\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 14\n",
      "len_exp2_user_47: 12\n",
      "MakeWACAXExpDicOwner Time:  0.9827857287600636\n",
      "MakeWACAXExpDicUnknown Time:  4.0595796490088105\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|  | 7/9 [02:28<00:22, 11.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1750\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 12\n",
      "len_exp2_user_47: 10\n",
      "MakeWACAXExpDicOwner Time:  0.6299756839871407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 89%| | 8/9 [02:32<00:09,  9.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MakeWACAXExpDicUnknown Time:  3.0040828743949533\n",
      "Done extracting features\n",
      "window_size: 2000\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 10\n",
      "len_exp2_user_47: 9\n",
      "MakeWACAXExpDicOwner Time:  0.4312313152477145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|| 9/9 [02:36<00:00, 17.35s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MakeWACAXExpDicUnknown Time:  2.536687278188765\n",
      "Done extracting features\n",
      "train_set: {0, 2, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 20, 21, 23, 26, 27, 29}\n",
      "test_set: {1, 3, 5, 6, 16, 19, 22, 24, 25, 28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 125\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 170\n",
      "len_exp2_user_47: 146\n",
      "MakeWACAXExpDicOwner Time:  13.964055831544101\n",
      "MakeWACAXExpDicUnknown Time:  125.42391323298216\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|         | 1/9 [04:40<37:26, 280.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 85\n",
      "len_exp2_user_47: 72\n",
      "MakeWACAXExpDicOwner Time:  8.592084778472781\n",
      "MakeWACAXExpDicUnknown Time:  61.31079043354839\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|       | 2/9 [07:54<26:45, 229.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 500\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 42\n",
      "len_exp2_user_47: 36\n",
      "MakeWACAXExpDicOwner Time:  2.3949658358469605\n",
      "MakeWACAXExpDicUnknown Time:  24.81192760542035\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|      | 3/9 [08:31<14:11, 141.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 750\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 28\n",
      "len_exp2_user_47: 24\n",
      "MakeWACAXExpDicOwner Time:  2.3614584440365434\n",
      "MakeWACAXExpDicUnknown Time:  16.555331209674478\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 44%|     | 4/9 [09:00<08:05, 97.09s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1000\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 21\n",
      "len_exp2_user_47: 18\n",
      "MakeWACAXExpDicOwner Time:  2.106020174920559\n",
      "MakeWACAXExpDicUnknown Time:  13.087629702873528\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 56%|    | 5/9 [09:24<04:43, 70.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 16\n",
      "len_exp2_user_47: 14\n",
      "MakeWACAXExpDicOwner Time:  2.1149986535310745\n",
      "MakeWACAXExpDicUnknown Time:  11.702674079686403\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|   | 6/9 [09:47<02:43, 54.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1500\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 14\n",
      "len_exp2_user_47: 12\n",
      "MakeWACAXExpDicOwner Time:  2.8999367421492934\n",
      "MakeWACAXExpDicUnknown Time:  14.8975644512102\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|  | 7/9 [10:17<01:33, 46.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1750\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 12\n",
      "len_exp2_user_47: 10\n",
      "MakeWACAXExpDicOwner Time:  2.3820012593641877\n",
      "MakeWACAXExpDicUnknown Time:  12.303803419694304\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 89%| | 8/9 [10:42<00:39, 39.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 2000\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 10\n",
      "len_exp2_user_47: 9\n",
      "MakeWACAXExpDicOwner Time:  1.8222108287736773\n",
      "MakeWACAXExpDicUnknown Time:  10.550780978053808\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|| 9/9 [11:06<00:00, 74.08s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 125\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  6.684238580986857\n",
      "MakeWACAXExpDicUnknown Time:  41.584088671952486\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|         | 1/9 [00:53<07:07, 53.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  3.9444988081231713\n",
      "MakeWACAXExpDicUnknown Time:  19.584825783967972\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|       | 2/9 [01:19<04:22, 37.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 500\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  1.1426727278158069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|      | 3/9 [01:30<02:32, 25.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MakeWACAXExpDicUnknown Time:  9.295451014302671\n",
      "Done extracting features\n",
      "window_size: 750\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  0.7563139069825411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 44%|     | 4/9 [01:36<01:29, 17.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MakeWACAXExpDicUnknown Time:  5.1443654438480735\n",
      "Done extracting features\n",
      "window_size: 1000\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  0.8350281175225973\n",
      "MakeWACAXExpDicUnknown Time:  5.285982830449939\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 56%|    | 5/9 [01:43<00:54, 13.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  0.8127550231292844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|   | 6/9 [01:48<00:32, 10.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MakeWACAXExpDicUnknown Time:  4.165617844089866\n",
      "Done extracting features\n",
      "window_size: 1500\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  0.6206342205405235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|  | 7/9 [01:53<00:17,  8.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MakeWACAXExpDicUnknown Time:  4.038254698738456\n",
      "Done extracting features\n",
      "window_size: 1750\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  0.4406109619885683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 89%| | 8/9 [01:56<00:07,  7.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MakeWACAXExpDicUnknown Time:  2.094666037708521\n",
      "Done extracting features\n",
      "window_size: 2000\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  0.33545260690152645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|| 9/9 [01:58<00:00, 13.22s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MakeWACAXExpDicUnknown Time:  1.6946416683495045\n",
      "Done extracting features\n",
      "train_set: {1, 3, 5, 6, 7, 8, 11, 13, 16, 17, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29}\n",
      "test_set: {0, 2, 4, 9, 10, 12, 14, 15, 18, 23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 125\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 170\n",
      "len_exp2_user_47: 146\n",
      "MakeWACAXExpDicOwner Time:  6.665308497846127\n",
      "MakeWACAXExpDicUnknown Time:  74.66903078276664\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|         | 1/9 [03:09<25:18, 189.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 85\n",
      "len_exp2_user_47: 72\n",
      "MakeWACAXExpDicOwner Time:  5.825105699710548\n",
      "MakeWACAXExpDicUnknown Time:  61.63219138979912\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|       | 2/9 [06:03<21:03, 180.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 500\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 42\n",
      "len_exp2_user_47: 36\n",
      "MakeWACAXExpDicOwner Time:  3.3274094499647617\n",
      "MakeWACAXExpDicUnknown Time:  26.67732273414731\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|      | 3/9 [06:45<11:43, 117.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 750\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 28\n",
      "len_exp2_user_47: 24\n",
      "MakeWACAXExpDicOwner Time:  2.713779370300472\n",
      "MakeWACAXExpDicUnknown Time:  20.56868146918714\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 44%|     | 4/9 [07:21<07:04, 84.93s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1000\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 21\n",
      "len_exp2_user_47: 18\n",
      "MakeWACAXExpDicOwner Time:  3.284764466807246\n",
      "MakeWACAXExpDicUnknown Time:  12.495940098538995\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 56%|    | 5/9 [07:47<04:14, 63.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 16\n",
      "len_exp2_user_47: 14\n",
      "MakeWACAXExpDicOwner Time:  2.295332412235439\n",
      "MakeWACAXExpDicUnknown Time:  12.284452006220818\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|   | 6/9 [08:14<02:34, 51.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1500\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 14\n",
      "len_exp2_user_47: 12\n",
      "MakeWACAXExpDicOwner Time:  2.269710618071258\n",
      "MakeWACAXExpDicUnknown Time:  8.419622788205743\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|  | 7/9 [08:31<01:20, 40.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1750\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 12\n",
      "len_exp2_user_47: 10\n",
      "MakeWACAXExpDicOwner Time:  1.1117931958287954\n",
      "MakeWACAXExpDicUnknown Time:  8.458165244199336\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 89%| | 8/9 [08:49<00:32, 32.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 2000\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 10\n",
      "len_exp2_user_47: 9\n",
      "MakeWACAXExpDicOwner Time:  1.5426009558141232\n",
      "MakeWACAXExpDicUnknown Time:  8.066852522082627\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|| 9/9 [09:08<00:00, 60.95s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 125\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  7.814072878099978\n",
      "MakeWACAXExpDicUnknown Time:  43.495653483085334\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|         | 1/9 [00:58<07:49, 58.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  2.8536337167024612\n",
      "MakeWACAXExpDicUnknown Time:  19.56613949779421\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|       | 2/9 [01:27<04:46, 40.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 500\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  2.291360864415765\n",
      "MakeWACAXExpDicUnknown Time:  11.59154006652534\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|      | 3/9 [01:41<02:54, 29.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 750\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  1.678167698904872\n",
      "MakeWACAXExpDicUnknown Time:  9.702310654334724\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 44%|     | 4/9 [01:54<01:51, 22.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1000\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  1.7305980706587434\n",
      "MakeWACAXExpDicUnknown Time:  6.557984737679362\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 56%|    | 5/9 [02:02<01:09, 17.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  1.033795777708292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|   | 6/9 [02:08<00:40, 13.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MakeWACAXExpDicUnknown Time:  4.596187447197735\n",
      "Done extracting features\n",
      "window_size: 1500\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  0.5034039858728647\n",
      "MakeWACAXExpDicUnknown Time:  4.295810552313924\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|  | 7/9 [02:14<00:21, 10.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1750\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  1.008501029573381\n",
      "MakeWACAXExpDicUnknown Time:  4.010229877196252\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 89%| | 8/9 [02:19<00:09,  9.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 2000\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  0.7144563039764762\n",
      "MakeWACAXExpDicUnknown Time:  2.9413887467235327\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|| 9/9 [02:23<00:00, 15.98s/it]\u001b[A\n",
      "  4%|         | 1/25 [35:38<14:15:21, 2138.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap: 0.05\n",
      "train_set: {0, 1, 2, 3, 4, 5, 6, 9, 10, 12, 14, 15, 16, 18, 19, 22, 23, 24, 25, 28}\n",
      "test_set: {7, 8, 11, 13, 17, 20, 21, 26, 27, 29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 125\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  17.40810028463602\n",
      "MakeWACAXExpDicUnknown Time:  110.20090768299997\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|         | 1/9 [04:49<38:33, 289.21s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  9.691141751594841\n",
      "MakeWACAXExpDicUnknown Time:  55.02368037123233\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|       | 2/9 [07:53<26:32, 227.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 500\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  4.2208108846098185\n",
      "MakeWACAXExpDicUnknown Time:  25.02827851381153\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|      | 3/9 [08:31<14:06, 141.08s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 750\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  1.8943397961556911\n",
      "MakeWACAXExpDicUnknown Time:  16.990974615328014\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 44%|     | 4/9 [08:59<08:01, 96.24s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1000\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  1.5095852194353938\n",
      "MakeWACAXExpDicUnknown Time:  8.815456349402666\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 56%|    | 5/9 [09:17<04:31, 67.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  1.3175738770514727\n",
      "MakeWACAXExpDicUnknown Time:  7.21937223803252\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|   | 6/9 [09:32<02:30, 50.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1500\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  1.2123791249468923\n",
      "MakeWACAXExpDicUnknown Time:  6.053384591825306\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|  | 7/9 [09:47<01:17, 38.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1750\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  1.6751910913735628\n",
      "MakeWACAXExpDicUnknown Time:  10.177837694063783\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 89%| | 8/9 [10:09<00:33, 33.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 2000\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  1.2822447065263987\n",
      "MakeWACAXExpDicUnknown Time:  7.635876663029194\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|| 9/9 [10:27<00:00, 69.67s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 125\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 177\n",
      "len_exp2_user_47: 152\n",
      "MakeWACAXExpDicOwner Time:  5.312436331063509\n",
      "MakeWACAXExpDicUnknown Time:  30.197219403460622\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|         | 1/9 [00:39<05:14, 39.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 88\n",
      "len_exp2_user_47: 75\n",
      "MakeWACAXExpDicOwner Time:  2.0988353257998824\n",
      "MakeWACAXExpDicUnknown Time:  15.348136495798826\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|       | 2/9 [00:59<03:15, 27.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 500\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 44\n",
      "len_exp2_user_47: 37\n",
      "MakeWACAXExpDicOwner Time:  1.5875273365527391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|      | 3/9 [01:09<01:59, 19.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MakeWACAXExpDicUnknown Time:  8.307304156012833\n",
      "Done extracting features\n",
      "window_size: 750\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 29\n",
      "len_exp2_user_47: 25\n",
      "MakeWACAXExpDicOwner Time:  0.9138644058257341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 44%|     | 4/9 [01:16<01:14, 14.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MakeWACAXExpDicUnknown Time:  5.65815007314086\n",
      "Done extracting features\n",
      "window_size: 1000\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 22\n",
      "len_exp2_user_47: 18\n",
      "MakeWACAXExpDicOwner Time:  0.5903041576966643\n",
      "MakeWACAXExpDicUnknown Time:  5.408685807138681\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 56%|    | 5/9 [01:23<00:47, 11.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 17\n",
      "len_exp2_user_47: 15\n",
      "MakeWACAXExpDicOwner Time:  0.8007869580760598\n",
      "MakeWACAXExpDicUnknown Time:  4.314198703505099\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|   | 6/9 [01:28<00:28,  9.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1500\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 14\n",
      "len_exp2_user_47: 12\n",
      "MakeWACAXExpDicOwner Time:  0.6245483737438917\n",
      "MakeWACAXExpDicUnknown Time:  3.5519499722868204\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|  | 7/9 [01:33<00:15,  7.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1750\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 12\n",
      "len_exp2_user_47: 10\n",
      "MakeWACAXExpDicOwner Time:  0.6883842786774039\n",
      "MakeWACAXExpDicUnknown Time:  3.614115529693663\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 89%| | 8/9 [01:37<00:06,  6.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 2000\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 11\n",
      "len_exp2_user_47: 9\n",
      "MakeWACAXExpDicOwner Time:  0.6994125079363585\n",
      "MakeWACAXExpDicUnknown Time:  3.8005515662953258\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|| 9/9 [01:42<00:00, 11.42s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set: {0, 2, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 20, 21, 23, 26, 27, 29}\n",
      "test_set: {1, 3, 5, 6, 16, 19, 22, 24, 25, 28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 125\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 177\n",
      "len_exp2_user_47: 152\n",
      "MakeWACAXExpDicOwner Time:  15.290480955503881\n",
      "MakeWACAXExpDicUnknown Time:  113.0548558337614\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|         | 1/9 [04:33<36:24, 273.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 88\n",
      "len_exp2_user_47: 75\n",
      "MakeWACAXExpDicOwner Time:  9.471507349982858\n",
      "MakeWACAXExpDicUnknown Time:  54.19350157491863\n",
      "Done extracting features\n"
     ]
    }
   ],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": raw_dfList_exp1,\n",
    "            \"dfList_exp2\": raw_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": raw_dfList_exp2_user_47\n",
    "}\n",
    "\n",
    "for overlap in tqdm(OVERLAP_EXP_RANGE):\n",
    "\n",
    "    train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/overlap={overlap}_Mean_EER_df_train_dict.txt\"\n",
    "    test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/overlap={overlap}_Mean_EER_df_test_dict.txt\"\n",
    "    \n",
    "    with open(train_file_name, \"w\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    with open(test_file_name, \"w\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "\n",
    "    overlap*=0.01\n",
    "    print(f\"overlap: {overlap}\")\n",
    "    max_window_size=2000\n",
    "    step_width = int(max_window_size * (1-overlap))\n",
    "    max_num_windows=min(len(getIndices(sampleSize=max_window_size, step=step_width, numSamplePoints=P.num_sample_points_per_exp)), N_NEIGHBORS_PARAMS[-1]+1)\n",
    "    n_neighbors_params = np.arange(1, max_num_windows) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    key_column= [\"overlap\"]\n",
    "    #-----CV_FOLD-------\n",
    "    for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "        process_cv_fold_kNN(\\\n",
    "                            cv_fold_idx=cv_fold_idx, \n",
    "                            cv_sets=THREE_FOLD_CV, \n",
    "                            dfList_dict=dfList_dict, \n",
    "                            window_size_lst=WINDOW_SIZE_LST, \n",
    "                            exp_config=P, \n",
    "                            extract_features_dict=EXTRACT_WACA_features_DICT, \n",
    "                            overlap=overlap, \n",
    "                            n_neighbors_params=n_neighbors_params, \n",
    "                            train_file_name=train_file_name, \n",
    "                            test_file_name=test_file_name, \n",
    "                            preprocessing_params=overlap, \n",
    "                            key_column=key_column,\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "\n",
    "\n",
    "\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/overlap_Mean_EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/overlap_Mean_EER_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for overlap in tqdm(OVERLAP_EXP_RANGE):\n",
    "    overlap*=0.01\n",
    "    max_window_size=2000\n",
    "    step_width = int(max_window_size * (1-overlap))\n",
    "    max_num_windows=min(len(getIndices(sampleSize=max_window_size, step=step_width, numSamplePoints=P.num_sample_points_per_exp)), N_NEIGHBORS_PARAMS[-1]+1)\n",
    "    n_neighbors_params = np.arange(1, max_num_windows) \n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": raw_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": raw_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[overlap] = calculate_EER_different_window_sizes_optimize_num_neighbors(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                             extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                             n_neighbors_params=n_neighbors_params)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\overlap: {overlap}\\n\")\n",
    "        f.write(EER_df_train_dict[overlap].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "overlap=min_key\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                      extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                      best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"overlap\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EER_df_train_dict[0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "min_key=train_lst[0][0]\n",
    "overlap=min_key\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                      extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                      best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"overlap\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap=50\n",
    "overlap*=0.01\n",
    "max_window_size=2000\n",
    "step_width = int(max_window_size * (1-overlap))\n",
    "max_num_windows=len(getIndices(sampleSize=max_window_size, step=step_width, numSamplePoints=P.num_sample_points_per_exp))\n",
    "n_neighbors_params = np.arange(1, max_num_windows) \n",
    "N_NEIGHBORS_PARAMS[-1]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(1, 100, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_PATH_NAME\n",
    "%run ./Classification_utility-functions.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=return_and_save_final_result_df_as_json(final_exp_results_path=FINAL_EXP_RESULTS_PATH, exp_path_name=EXP_PATH_NAME, \n",
    "                                           window_size_lst=WINDOW_SIZE_LST)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\"></th>\n",
       "      <th colspan=\"5\" halign=\"left\">Window Size</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean rank</th>\n",
       "      <th>cut_off_freq</th>\n",
       "      <th>EMA_span</th>\n",
       "      <th>SMA_winsize</th>\n",
       "      <th>type</th>\n",
       "      <th>125</th>\n",
       "      <th>250</th>\n",
       "      <th>500</th>\n",
       "      <th>750</th>\n",
       "      <th>1000</th>\n",
       "      <th>mean</th>\n",
       "      <th>variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.193208</td>\n",
       "      <td>0.113839</td>\n",
       "      <td>0.067604</td>\n",
       "      <td>0.054209</td>\n",
       "      <td>0.048419</td>\n",
       "      <td>0.095456</td>\n",
       "      <td>0.003646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Naive</td>\n",
       "      <td>0.180459</td>\n",
       "      <td>0.103515</td>\n",
       "      <td>0.066354</td>\n",
       "      <td>0.047508</td>\n",
       "      <td>0.042005</td>\n",
       "      <td>0.087969</td>\n",
       "      <td>0.003253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.181438</td>\n",
       "      <td>0.104868</td>\n",
       "      <td>0.068005</td>\n",
       "      <td>0.048114</td>\n",
       "      <td>0.043948</td>\n",
       "      <td>0.089275</td>\n",
       "      <td>0.003235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>37</td>\n",
       "      <td>-</td>\n",
       "      <td>Naive</td>\n",
       "      <td>0.151440</td>\n",
       "      <td>0.100876</td>\n",
       "      <td>0.066600</td>\n",
       "      <td>0.053737</td>\n",
       "      <td>0.044941</td>\n",
       "      <td>0.083519</td>\n",
       "      <td>0.001894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>-</td>\n",
       "      <td>5</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.195923</td>\n",
       "      <td>0.117742</td>\n",
       "      <td>0.074052</td>\n",
       "      <td>0.051616</td>\n",
       "      <td>0.047922</td>\n",
       "      <td>0.097451</td>\n",
       "      <td>0.003802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>9</td>\n",
       "      <td>Naive</td>\n",
       "      <td>0.181971</td>\n",
       "      <td>0.112375</td>\n",
       "      <td>0.071977</td>\n",
       "      <td>0.053704</td>\n",
       "      <td>0.044490</td>\n",
       "      <td>0.092903</td>\n",
       "      <td>0.003158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.193560</td>\n",
       "      <td>0.113828</td>\n",
       "      <td>0.067604</td>\n",
       "      <td>0.054209</td>\n",
       "      <td>0.048419</td>\n",
       "      <td>0.095524</td>\n",
       "      <td>0.003663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>41</td>\n",
       "      <td>-</td>\n",
       "      <td>Naive</td>\n",
       "      <td>0.149418</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>0.068496</td>\n",
       "      <td>0.057273</td>\n",
       "      <td>0.046161</td>\n",
       "      <td>0.084409</td>\n",
       "      <td>0.001736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.184427</td>\n",
       "      <td>0.108904</td>\n",
       "      <td>0.068697</td>\n",
       "      <td>0.050875</td>\n",
       "      <td>0.044896</td>\n",
       "      <td>0.091560</td>\n",
       "      <td>0.003320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>43</td>\n",
       "      <td>-</td>\n",
       "      <td>34</td>\n",
       "      <td>Naive</td>\n",
       "      <td>0.164771</td>\n",
       "      <td>0.109658</td>\n",
       "      <td>0.078023</td>\n",
       "      <td>0.059966</td>\n",
       "      <td>0.050090</td>\n",
       "      <td>0.092502</td>\n",
       "      <td>0.002146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>-</td>\n",
       "      <td>2</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.181569</td>\n",
       "      <td>0.105811</td>\n",
       "      <td>0.066064</td>\n",
       "      <td>0.050269</td>\n",
       "      <td>0.045167</td>\n",
       "      <td>0.089776</td>\n",
       "      <td>0.003199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Window Size            \\\n",
       "   mean rank cut_off_freq EMA_span SMA_winsize   type         125       250   \n",
       "0          9            -        -           -      -    0.193208  0.113839   \n",
       "1          3           33        -           -  Naive    0.180459  0.103515   \n",
       "2          4           33        -           -   Real    0.181438  0.104868   \n",
       "3          1            -       37           -  Naive    0.151440  0.100876   \n",
       "4         11            -        5           -   Real    0.195923  0.117742   \n",
       "5          8            -        -           9  Naive    0.181971  0.112375   \n",
       "6         10            -        -           1   Real    0.193560  0.113828   \n",
       "7          2           47       41           -  Naive    0.149418  0.100699   \n",
       "8          6           49        4           -   Real    0.184427  0.108904   \n",
       "9          7           43        -          34  Naive    0.164771  0.109658   \n",
       "10         5           26        -           2   Real    0.181569  0.105811   \n",
       "\n",
       "                                                      \n",
       "         500       750      1000      mean  variance  \n",
       "0   0.067604  0.054209  0.048419  0.095456  0.003646  \n",
       "1   0.066354  0.047508  0.042005  0.087969  0.003253  \n",
       "2   0.068005  0.048114  0.043948  0.089275  0.003235  \n",
       "3   0.066600  0.053737  0.044941  0.083519  0.001894  \n",
       "4   0.074052  0.051616  0.047922  0.097451  0.003802  \n",
       "5   0.071977  0.053704  0.044490  0.092903  0.003158  \n",
       "6   0.067604  0.054209  0.048419  0.095524  0.003663  \n",
       "7   0.068496  0.057273  0.046161  0.084409  0.001736  \n",
       "8   0.068697  0.050875  0.044896  0.091560  0.003320  \n",
       "9   0.078023  0.059966  0.050090  0.092502  0.002146  \n",
       "10  0.066064  0.050269  0.045167  0.089776  0.003199  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=return_and_save_final_result_df_as_json(final_exp_results_path=FINAL_EXP_RESULTS_PATH, exp_path_name=EXP_PATH_NAME, window_size_lst=WINDOW_SIZE_LST_NN, save_file_suffix=\"-max1000\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean([df[i][\"('Window Size', 2000)\"] for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df=return_and_save_final_relative_result_df_as_json(df, base_case_index=0, final_exp_results_path=FINAL_EXP_RESULTS_PATH, exp_path_name=EXP_PATH_NAME, \n",
    "                                                             window_size_lst=WINDOW_SIZE_LST)\n",
    "relative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.style.format(STYLER_ERR_FORMAT_DICT).hide(axis='index').to_latex()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df.style.format(STYLER_IMPROVEMENT_FORMAT_DICT).hide(axis='index').to_latex()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_result_df=return_and_save_final_Gini_df_as_json(final_exp_results_path=FINAL_EXP_RESULTS_PATH, exp_path_name=EXP_PATH_NAME, window_size_lst=WINDOW_SIZE_LST)\n",
    "gini_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for winsize in WINDOW_SIZE_LST:\n",
    "#     print(np.mean([gini_result_df[i][('Window Size', winsize)] for i in range(3)]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_and_save_final_relative_gini_result_df_as_json(gini_result_df, base_case_index=0, final_exp_results_path=FINAL_EXP_RESULTS_PATH, \n",
    "                                                      exp_path_name=EXP_PATH_NAME, window_size_lst=WINDOW_SIZE_LST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Overlap exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 EER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th colspan=\"9\" halign=\"left\">Window Size</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean rank</th>\n",
       "      <th>overlap</th>\n",
       "      <th>125</th>\n",
       "      <th>250</th>\n",
       "      <th>500</th>\n",
       "      <th>750</th>\n",
       "      <th>1000</th>\n",
       "      <th>1250</th>\n",
       "      <th>1500</th>\n",
       "      <th>1750</th>\n",
       "      <th>2000</th>\n",
       "      <th>mean</th>\n",
       "      <th>variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.192919</td>\n",
       "      <td>0.118148</td>\n",
       "      <td>0.075220</td>\n",
       "      <td>0.053902</td>\n",
       "      <td>0.048765</td>\n",
       "      <td>0.042708</td>\n",
       "      <td>0.038492</td>\n",
       "      <td>0.040123</td>\n",
       "      <td>0.038333</td>\n",
       "      <td>0.072068</td>\n",
       "      <td>0.002720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.194099</td>\n",
       "      <td>0.114036</td>\n",
       "      <td>0.066835</td>\n",
       "      <td>0.053895</td>\n",
       "      <td>0.047980</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>0.045106</td>\n",
       "      <td>0.038735</td>\n",
       "      <td>0.040741</td>\n",
       "      <td>0.071727</td>\n",
       "      <td>0.002649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.195015</td>\n",
       "      <td>0.116727</td>\n",
       "      <td>0.064211</td>\n",
       "      <td>0.057778</td>\n",
       "      <td>0.048569</td>\n",
       "      <td>0.046708</td>\n",
       "      <td>0.038025</td>\n",
       "      <td>0.036610</td>\n",
       "      <td>0.042761</td>\n",
       "      <td>0.071823</td>\n",
       "      <td>0.002731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.200974</td>\n",
       "      <td>0.116975</td>\n",
       "      <td>0.066049</td>\n",
       "      <td>0.054861</td>\n",
       "      <td>0.049839</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.041975</td>\n",
       "      <td>0.044017</td>\n",
       "      <td>0.042256</td>\n",
       "      <td>0.073488</td>\n",
       "      <td>0.002848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.195357</td>\n",
       "      <td>0.115933</td>\n",
       "      <td>0.068370</td>\n",
       "      <td>0.054489</td>\n",
       "      <td>0.041704</td>\n",
       "      <td>0.043796</td>\n",
       "      <td>0.042477</td>\n",
       "      <td>0.033995</td>\n",
       "      <td>0.036420</td>\n",
       "      <td>0.070282</td>\n",
       "      <td>0.002842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.199957</td>\n",
       "      <td>0.118658</td>\n",
       "      <td>0.067308</td>\n",
       "      <td>0.055397</td>\n",
       "      <td>0.052493</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.034641</td>\n",
       "      <td>0.040741</td>\n",
       "      <td>0.037179</td>\n",
       "      <td>0.072137</td>\n",
       "      <td>0.002958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.193416</td>\n",
       "      <td>0.118502</td>\n",
       "      <td>0.068620</td>\n",
       "      <td>0.053654</td>\n",
       "      <td>0.048217</td>\n",
       "      <td>0.039394</td>\n",
       "      <td>0.039095</td>\n",
       "      <td>0.037160</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.071389</td>\n",
       "      <td>0.002742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.192982</td>\n",
       "      <td>0.117828</td>\n",
       "      <td>0.067816</td>\n",
       "      <td>0.053656</td>\n",
       "      <td>0.046552</td>\n",
       "      <td>0.040177</td>\n",
       "      <td>0.041910</td>\n",
       "      <td>0.036806</td>\n",
       "      <td>0.037434</td>\n",
       "      <td>0.070573</td>\n",
       "      <td>0.002759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.196179</td>\n",
       "      <td>0.117763</td>\n",
       "      <td>0.071416</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.042654</td>\n",
       "      <td>0.038272</td>\n",
       "      <td>0.040278</td>\n",
       "      <td>0.036710</td>\n",
       "      <td>0.037284</td>\n",
       "      <td>0.070679</td>\n",
       "      <td>0.002903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.194168</td>\n",
       "      <td>0.113729</td>\n",
       "      <td>0.068687</td>\n",
       "      <td>0.058829</td>\n",
       "      <td>0.046817</td>\n",
       "      <td>0.039031</td>\n",
       "      <td>0.037919</td>\n",
       "      <td>0.036831</td>\n",
       "      <td>0.038194</td>\n",
       "      <td>0.070467</td>\n",
       "      <td>0.002761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.193091</td>\n",
       "      <td>0.115519</td>\n",
       "      <td>0.068942</td>\n",
       "      <td>0.053100</td>\n",
       "      <td>0.052832</td>\n",
       "      <td>0.044925</td>\n",
       "      <td>0.040741</td>\n",
       "      <td>0.040058</td>\n",
       "      <td>0.036383</td>\n",
       "      <td>0.071732</td>\n",
       "      <td>0.002661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.193920</td>\n",
       "      <td>0.118969</td>\n",
       "      <td>0.066593</td>\n",
       "      <td>0.055519</td>\n",
       "      <td>0.046897</td>\n",
       "      <td>0.038186</td>\n",
       "      <td>0.041049</td>\n",
       "      <td>0.040035</td>\n",
       "      <td>0.035494</td>\n",
       "      <td>0.070740</td>\n",
       "      <td>0.002805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>17</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.195158</td>\n",
       "      <td>0.115289</td>\n",
       "      <td>0.070645</td>\n",
       "      <td>0.053669</td>\n",
       "      <td>0.049630</td>\n",
       "      <td>0.043345</td>\n",
       "      <td>0.039174</td>\n",
       "      <td>0.039057</td>\n",
       "      <td>0.041131</td>\n",
       "      <td>0.071900</td>\n",
       "      <td>0.002727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.192253</td>\n",
       "      <td>0.116105</td>\n",
       "      <td>0.068371</td>\n",
       "      <td>0.056258</td>\n",
       "      <td>0.045952</td>\n",
       "      <td>0.040850</td>\n",
       "      <td>0.036839</td>\n",
       "      <td>0.038194</td>\n",
       "      <td>0.037566</td>\n",
       "      <td>0.070265</td>\n",
       "      <td>0.002732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.193420</td>\n",
       "      <td>0.117478</td>\n",
       "      <td>0.070756</td>\n",
       "      <td>0.055115</td>\n",
       "      <td>0.049921</td>\n",
       "      <td>0.038038</td>\n",
       "      <td>0.038889</td>\n",
       "      <td>0.039672</td>\n",
       "      <td>0.036795</td>\n",
       "      <td>0.071121</td>\n",
       "      <td>0.002764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>11</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.193968</td>\n",
       "      <td>0.113863</td>\n",
       "      <td>0.068396</td>\n",
       "      <td>0.055767</td>\n",
       "      <td>0.049395</td>\n",
       "      <td>0.041689</td>\n",
       "      <td>0.040632</td>\n",
       "      <td>0.039719</td>\n",
       "      <td>0.040222</td>\n",
       "      <td>0.071517</td>\n",
       "      <td>0.002671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.197348</td>\n",
       "      <td>0.117108</td>\n",
       "      <td>0.072065</td>\n",
       "      <td>0.057075</td>\n",
       "      <td>0.048212</td>\n",
       "      <td>0.040258</td>\n",
       "      <td>0.036550</td>\n",
       "      <td>0.035880</td>\n",
       "      <td>0.036508</td>\n",
       "      <td>0.071223</td>\n",
       "      <td>0.002920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.196603</td>\n",
       "      <td>0.114026</td>\n",
       "      <td>0.071739</td>\n",
       "      <td>0.054461</td>\n",
       "      <td>0.047721</td>\n",
       "      <td>0.040812</td>\n",
       "      <td>0.038329</td>\n",
       "      <td>0.038940</td>\n",
       "      <td>0.038710</td>\n",
       "      <td>0.071260</td>\n",
       "      <td>0.002811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.194708</td>\n",
       "      <td>0.113811</td>\n",
       "      <td>0.072502</td>\n",
       "      <td>0.058654</td>\n",
       "      <td>0.046765</td>\n",
       "      <td>0.042844</td>\n",
       "      <td>0.037528</td>\n",
       "      <td>0.039928</td>\n",
       "      <td>0.041564</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.002702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.197111</td>\n",
       "      <td>0.117692</td>\n",
       "      <td>0.069434</td>\n",
       "      <td>0.056764</td>\n",
       "      <td>0.047348</td>\n",
       "      <td>0.043076</td>\n",
       "      <td>0.041033</td>\n",
       "      <td>0.037731</td>\n",
       "      <td>0.041093</td>\n",
       "      <td>0.072365</td>\n",
       "      <td>0.002814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.196639</td>\n",
       "      <td>0.117752</td>\n",
       "      <td>0.071715</td>\n",
       "      <td>0.056643</td>\n",
       "      <td>0.047345</td>\n",
       "      <td>0.043805</td>\n",
       "      <td>0.038728</td>\n",
       "      <td>0.035983</td>\n",
       "      <td>0.037001</td>\n",
       "      <td>0.071735</td>\n",
       "      <td>0.002862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>16</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.197071</td>\n",
       "      <td>0.117997</td>\n",
       "      <td>0.070154</td>\n",
       "      <td>0.056865</td>\n",
       "      <td>0.050622</td>\n",
       "      <td>0.041457</td>\n",
       "      <td>0.039187</td>\n",
       "      <td>0.037062</td>\n",
       "      <td>0.036603</td>\n",
       "      <td>0.071891</td>\n",
       "      <td>0.002866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.198626</td>\n",
       "      <td>0.118309</td>\n",
       "      <td>0.071613</td>\n",
       "      <td>0.057925</td>\n",
       "      <td>0.048953</td>\n",
       "      <td>0.043180</td>\n",
       "      <td>0.039963</td>\n",
       "      <td>0.039017</td>\n",
       "      <td>0.037292</td>\n",
       "      <td>0.072764</td>\n",
       "      <td>0.002882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.200856</td>\n",
       "      <td>0.120288</td>\n",
       "      <td>0.072010</td>\n",
       "      <td>0.059126</td>\n",
       "      <td>0.049202</td>\n",
       "      <td>0.043267</td>\n",
       "      <td>0.039992</td>\n",
       "      <td>0.036721</td>\n",
       "      <td>0.037145</td>\n",
       "      <td>0.073179</td>\n",
       "      <td>0.002990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Window Size                                          \\\n",
       "   mean rank overlap         125       250       500       750      1000   \n",
       "0         19    0.01    0.192919  0.118148  0.075220  0.053902  0.048765   \n",
       "1         12    0.05    0.194099  0.114036  0.066835  0.053895  0.047980   \n",
       "2         15    0.09    0.195015  0.116727  0.064211  0.057778  0.048569   \n",
       "3         24    0.13    0.200974  0.116975  0.066049  0.054861  0.049839   \n",
       "4          2    0.17    0.195357  0.115933  0.068370  0.054489  0.041704   \n",
       "5         20    0.21    0.199957  0.118658  0.067308  0.055397  0.052493   \n",
       "6         10    0.25    0.193416  0.118502  0.068620  0.053654  0.048217   \n",
       "7          4    0.29    0.192982  0.117828  0.067816  0.053656  0.046552   \n",
       "8          5    0.33    0.196179  0.117763  0.071416  0.055556  0.042654   \n",
       "9          3    0.37    0.194168  0.113729  0.068687  0.058829  0.046817   \n",
       "10        13    0.41    0.193091  0.115519  0.068942  0.053100  0.052832   \n",
       "11         6    0.45    0.193920  0.118969  0.066593  0.055519  0.046897   \n",
       "12        17    0.49    0.195158  0.115289  0.070645  0.053669  0.049630   \n",
       "13         1    0.53    0.192253  0.116105  0.068371  0.056258  0.045952   \n",
       "14         7    0.57    0.193420  0.117478  0.070756  0.055115  0.049921   \n",
       "15        11    0.61    0.193968  0.113863  0.068396  0.055767  0.049395   \n",
       "16         8    0.65    0.197348  0.117108  0.072065  0.057075  0.048212   \n",
       "17         9    0.69    0.196603  0.114026  0.071739  0.054461  0.047721   \n",
       "18        18    0.73    0.194708  0.113811  0.072502  0.058654  0.046765   \n",
       "19        21    0.77    0.197111  0.117692  0.069434  0.056764  0.047348   \n",
       "20        14    0.81    0.196639  0.117752  0.071715  0.056643  0.047345   \n",
       "21        16    0.85    0.197071  0.117997  0.070154  0.056865  0.050622   \n",
       "22        22    0.89    0.198626  0.118309  0.071613  0.057925  0.048953   \n",
       "23        23    0.93    0.200856  0.120288  0.072010  0.059126  0.049202   \n",
       "\n",
       "                                                                \n",
       "        1250      1500      1750      2000      mean  variance  \n",
       "0   0.042708  0.038492  0.040123  0.038333  0.072068  0.002720  \n",
       "1   0.044118  0.045106  0.038735  0.040741  0.071727  0.002649  \n",
       "2   0.046708  0.038025  0.036610  0.042761  0.071823  0.002731  \n",
       "3   0.044444  0.041975  0.044017  0.042256  0.073488  0.002848  \n",
       "4   0.043796  0.042477  0.033995  0.036420  0.070282  0.002842  \n",
       "5   0.042857  0.034641  0.040741  0.037179  0.072137  0.002958  \n",
       "6   0.039394  0.039095  0.037160  0.044444  0.071389  0.002742  \n",
       "7   0.040177  0.041910  0.036806  0.037434  0.070573  0.002759  \n",
       "8   0.038272  0.040278  0.036710  0.037284  0.070679  0.002903  \n",
       "9   0.039031  0.037919  0.036831  0.038194  0.070467  0.002761  \n",
       "10  0.044925  0.040741  0.040058  0.036383  0.071732  0.002661  \n",
       "11  0.038186  0.041049  0.040035  0.035494  0.070740  0.002805  \n",
       "12  0.043345  0.039174  0.039057  0.041131  0.071900  0.002727  \n",
       "13  0.040850  0.036839  0.038194  0.037566  0.070265  0.002732  \n",
       "14  0.038038  0.038889  0.039672  0.036795  0.071121  0.002764  \n",
       "15  0.041689  0.040632  0.039719  0.040222  0.071517  0.002671  \n",
       "16  0.040258  0.036550  0.035880  0.036508  0.071223  0.002920  \n",
       "17  0.040812  0.038329  0.038940  0.038710  0.071260  0.002811  \n",
       "18  0.042844  0.037528  0.039928  0.041564  0.072034  0.002702  \n",
       "19  0.043076  0.041033  0.037731  0.041093  0.072365  0.002814  \n",
       "20  0.043805  0.038728  0.035983  0.037001  0.071735  0.002862  \n",
       "21  0.041457  0.039187  0.037062  0.036603  0.071891  0.002866  \n",
       "22  0.043180  0.039963  0.039017  0.037292  0.072764  0.002882  \n",
       "23  0.043267  0.039992  0.036721  0.037145  0.073179  0.002990  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlap_df=return_and_save_final_result_df_as_json_overlap(final_exp_results_path=FINAL_EXP_RESULTS_PATH, exp_path_name=EXP_PATH_NAME, window_size_lst=WINDOW_SIZE_LST)\n",
    "overlap_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 EER relative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th colspan=\"9\" halign=\"left\">Window Size</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean rank</th>\n",
       "      <th>overlap</th>\n",
       "      <th>125</th>\n",
       "      <th>250</th>\n",
       "      <th>500</th>\n",
       "      <th>750</th>\n",
       "      <th>1000</th>\n",
       "      <th>1250</th>\n",
       "      <th>1500</th>\n",
       "      <th>1750</th>\n",
       "      <th>2000</th>\n",
       "      <th>mean</th>\n",
       "      <th>variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.611548</td>\n",
       "      <td>3.480336</td>\n",
       "      <td>11.147820</td>\n",
       "      <td>0.012693</td>\n",
       "      <td>1.611047</td>\n",
       "      <td>-3.299856</td>\n",
       "      <td>-17.182131</td>\n",
       "      <td>3.461538</td>\n",
       "      <td>-6.280193</td>\n",
       "      <td>0.473085</td>\n",
       "      <td>2.613778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-1.086270</td>\n",
       "      <td>1.202808</td>\n",
       "      <td>14.636322</td>\n",
       "      <td>-7.190184</td>\n",
       "      <td>0.402762</td>\n",
       "      <td>-9.364649</td>\n",
       "      <td>1.214204</td>\n",
       "      <td>8.757396</td>\n",
       "      <td>-11.550285</td>\n",
       "      <td>0.340692</td>\n",
       "      <td>-0.400215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-4.174938</td>\n",
       "      <td>0.992685</td>\n",
       "      <td>12.192263</td>\n",
       "      <td>-1.779141</td>\n",
       "      <td>-2.201431</td>\n",
       "      <td>-4.065041</td>\n",
       "      <td>-9.049256</td>\n",
       "      <td>-9.704142</td>\n",
       "      <td>-10.232762</td>\n",
       "      <td>-1.970118</td>\n",
       "      <td>-4.718302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-1.263376</td>\n",
       "      <td>1.874670</td>\n",
       "      <td>9.106682</td>\n",
       "      <td>-1.089422</td>\n",
       "      <td>14.481013</td>\n",
       "      <td>-2.547425</td>\n",
       "      <td>-10.352234</td>\n",
       "      <td>15.274725</td>\n",
       "      <td>4.991949</td>\n",
       "      <td>2.477867</td>\n",
       "      <td>-4.486993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-3.647815</td>\n",
       "      <td>-0.431774</td>\n",
       "      <td>10.519434</td>\n",
       "      <td>-2.773006</td>\n",
       "      <td>-7.643622</td>\n",
       "      <td>-0.348432</td>\n",
       "      <td>10.006064</td>\n",
       "      <td>-1.538461</td>\n",
       "      <td>3.010033</td>\n",
       "      <td>-0.095214</td>\n",
       "      <td>-8.747605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.257231</td>\n",
       "      <td>-0.299359</td>\n",
       "      <td>8.775445</td>\n",
       "      <td>0.460952</td>\n",
       "      <td>1.125176</td>\n",
       "      <td>7.760532</td>\n",
       "      <td>-1.565483</td>\n",
       "      <td>7.384615</td>\n",
       "      <td>-15.942029</td>\n",
       "      <td>0.942290</td>\n",
       "      <td>-0.801013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.032266</td>\n",
       "      <td>0.270974</td>\n",
       "      <td>9.843554</td>\n",
       "      <td>0.456190</td>\n",
       "      <td>4.539502</td>\n",
       "      <td>5.926712</td>\n",
       "      <td>-8.880449</td>\n",
       "      <td>8.269231</td>\n",
       "      <td>2.346446</td>\n",
       "      <td>2.073992</td>\n",
       "      <td>-1.435753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-1.689477</td>\n",
       "      <td>0.326019</td>\n",
       "      <td>5.058049</td>\n",
       "      <td>-3.067485</td>\n",
       "      <td>12.531646</td>\n",
       "      <td>10.388437</td>\n",
       "      <td>-4.639175</td>\n",
       "      <td>8.506787</td>\n",
       "      <td>2.737520</td>\n",
       "      <td>1.927472</td>\n",
       "      <td>-6.737848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>0.37</td>\n",
       "      <td>-0.647244</td>\n",
       "      <td>3.740543</td>\n",
       "      <td>8.685921</td>\n",
       "      <td>-9.139678</td>\n",
       "      <td>3.995253</td>\n",
       "      <td>8.609548</td>\n",
       "      <td>1.489118</td>\n",
       "      <td>8.205128</td>\n",
       "      <td>0.362319</td>\n",
       "      <td>2.221263</td>\n",
       "      <td>-1.529138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.089041</td>\n",
       "      <td>2.225264</td>\n",
       "      <td>8.347011</td>\n",
       "      <td>1.488397</td>\n",
       "      <td>-8.339538</td>\n",
       "      <td>-5.189200</td>\n",
       "      <td>-5.841924</td>\n",
       "      <td>0.161943</td>\n",
       "      <td>5.086672</td>\n",
       "      <td>0.465829</td>\n",
       "      <td>2.159431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.518487</td>\n",
       "      <td>-0.695017</td>\n",
       "      <td>11.470105</td>\n",
       "      <td>-2.998773</td>\n",
       "      <td>3.831680</td>\n",
       "      <td>10.587796</td>\n",
       "      <td>-6.643757</td>\n",
       "      <td>0.219780</td>\n",
       "      <td>7.407407</td>\n",
       "      <td>1.842521</td>\n",
       "      <td>-3.142661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>17</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-1.160599</td>\n",
       "      <td>2.419910</td>\n",
       "      <td>6.083105</td>\n",
       "      <td>0.431720</td>\n",
       "      <td>-1.772152</td>\n",
       "      <td>-1.490515</td>\n",
       "      <td>-1.771081</td>\n",
       "      <td>2.657343</td>\n",
       "      <td>-7.297229</td>\n",
       "      <td>0.233568</td>\n",
       "      <td>-0.265392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.345379</td>\n",
       "      <td>1.729421</td>\n",
       "      <td>9.105563</td>\n",
       "      <td>-4.370637</td>\n",
       "      <td>5.769797</td>\n",
       "      <td>4.351985</td>\n",
       "      <td>4.295533</td>\n",
       "      <td>4.807692</td>\n",
       "      <td>2.001380</td>\n",
       "      <td>2.501483</td>\n",
       "      <td>-0.433944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>0.57</td>\n",
       "      <td>-0.259396</td>\n",
       "      <td>0.567172</td>\n",
       "      <td>5.934936</td>\n",
       "      <td>-2.249489</td>\n",
       "      <td>-2.370051</td>\n",
       "      <td>10.935326</td>\n",
       "      <td>-1.030928</td>\n",
       "      <td>1.124260</td>\n",
       "      <td>4.011762</td>\n",
       "      <td>1.314818</td>\n",
       "      <td>-1.623186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>11</td>\n",
       "      <td>0.61</td>\n",
       "      <td>-0.543365</td>\n",
       "      <td>3.626989</td>\n",
       "      <td>9.072309</td>\n",
       "      <td>-3.460123</td>\n",
       "      <td>-1.290166</td>\n",
       "      <td>2.386146</td>\n",
       "      <td>-5.558925</td>\n",
       "      <td>1.007958</td>\n",
       "      <td>-4.927536</td>\n",
       "      <td>0.764992</td>\n",
       "      <td>1.795111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>0.65</td>\n",
       "      <td>-2.295557</td>\n",
       "      <td>0.880104</td>\n",
       "      <td>4.194567</td>\n",
       "      <td>-5.886424</td>\n",
       "      <td>1.134876</td>\n",
       "      <td>5.738188</td>\n",
       "      <td>5.046120</td>\n",
       "      <td>10.576923</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>1.173151</td>\n",
       "      <td>-7.351048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-1.909245</td>\n",
       "      <td>3.488912</td>\n",
       "      <td>4.628965</td>\n",
       "      <td>-1.037367</td>\n",
       "      <td>2.142162</td>\n",
       "      <td>4.440275</td>\n",
       "      <td>0.423559</td>\n",
       "      <td>2.948718</td>\n",
       "      <td>-0.981767</td>\n",
       "      <td>1.121219</td>\n",
       "      <td>-3.367140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.73</td>\n",
       "      <td>-0.926994</td>\n",
       "      <td>3.670745</td>\n",
       "      <td>3.613408</td>\n",
       "      <td>-8.816133</td>\n",
       "      <td>4.101266</td>\n",
       "      <td>-0.316935</td>\n",
       "      <td>2.503682</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>-8.427268</td>\n",
       "      <td>0.047507</td>\n",
       "      <td>0.667842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21</td>\n",
       "      <td>0.77</td>\n",
       "      <td>-2.172475</td>\n",
       "      <td>0.386482</td>\n",
       "      <td>7.692458</td>\n",
       "      <td>-5.309348</td>\n",
       "      <td>2.905639</td>\n",
       "      <td>-0.860139</td>\n",
       "      <td>-6.601556</td>\n",
       "      <td>5.961539</td>\n",
       "      <td>-7.200368</td>\n",
       "      <td>-0.411601</td>\n",
       "      <td>-3.453636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14</td>\n",
       "      <td>0.81</td>\n",
       "      <td>-1.928028</td>\n",
       "      <td>0.335466</td>\n",
       "      <td>4.660425</td>\n",
       "      <td>-5.085589</td>\n",
       "      <td>2.913781</td>\n",
       "      <td>-2.568073</td>\n",
       "      <td>-0.612580</td>\n",
       "      <td>10.318302</td>\n",
       "      <td>3.476366</td>\n",
       "      <td>0.462860</td>\n",
       "      <td>-5.221643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>16</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-2.151765</td>\n",
       "      <td>0.127962</td>\n",
       "      <td>6.735352</td>\n",
       "      <td>-5.497068</td>\n",
       "      <td>-3.806915</td>\n",
       "      <td>2.929897</td>\n",
       "      <td>-1.805111</td>\n",
       "      <td>7.629938</td>\n",
       "      <td>4.513889</td>\n",
       "      <td>0.245923</td>\n",
       "      <td>-5.385000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.89</td>\n",
       "      <td>-2.957782</td>\n",
       "      <td>-0.136154</td>\n",
       "      <td>4.795767</td>\n",
       "      <td>-7.462805</td>\n",
       "      <td>-0.385250</td>\n",
       "      <td>-1.104570</td>\n",
       "      <td>-3.820497</td>\n",
       "      <td>2.757045</td>\n",
       "      <td>2.715309</td>\n",
       "      <td>-0.965957</td>\n",
       "      <td>-5.942129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.93</td>\n",
       "      <td>-4.114155</td>\n",
       "      <td>-1.811558</td>\n",
       "      <td>4.267642</td>\n",
       "      <td>-9.691993</td>\n",
       "      <td>-0.894806</td>\n",
       "      <td>-1.307469</td>\n",
       "      <td>-3.897053</td>\n",
       "      <td>8.481013</td>\n",
       "      <td>3.099545</td>\n",
       "      <td>-1.541004</td>\n",
       "      <td>-9.922858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Window Size                                            \\\n",
       "   mean rank overlap         125       250        500       750       1000   \n",
       "0         19    0.01    0.000000  0.000000   0.000000  0.000000   0.000000   \n",
       "1         12    0.05   -0.611548  3.480336  11.147820  0.012693   1.611047   \n",
       "2         15    0.09   -1.086270  1.202808  14.636322 -7.190184   0.402762   \n",
       "3         24    0.13   -4.174938  0.992685  12.192263 -1.779141  -2.201431   \n",
       "4          2    0.17   -1.263376  1.874670   9.106682 -1.089422  14.481013   \n",
       "5         20    0.21   -3.647815 -0.431774  10.519434 -2.773006  -7.643622   \n",
       "6         10    0.25   -0.257231 -0.299359   8.775445  0.460952   1.125176   \n",
       "7          4    0.29   -0.032266  0.270974   9.843554  0.456190   4.539502   \n",
       "8          5    0.33   -1.689477  0.326019   5.058049 -3.067485  12.531646   \n",
       "9          3    0.37   -0.647244  3.740543   8.685921 -9.139678   3.995253   \n",
       "10        13    0.41   -0.089041  2.225264   8.347011  1.488397  -8.339538   \n",
       "11         6    0.45   -0.518487 -0.695017  11.470105 -2.998773   3.831680   \n",
       "12        17    0.49   -1.160599  2.419910   6.083105  0.431720  -1.772152   \n",
       "13         1    0.53    0.345379  1.729421   9.105563 -4.370637   5.769797   \n",
       "14         7    0.57   -0.259396  0.567172   5.934936 -2.249489  -2.370051   \n",
       "15        11    0.61   -0.543365  3.626989   9.072309 -3.460123  -1.290166   \n",
       "16         8    0.65   -2.295557  0.880104   4.194567 -5.886424   1.134876   \n",
       "17         9    0.69   -1.909245  3.488912   4.628965 -1.037367   2.142162   \n",
       "18        18    0.73   -0.926994  3.670745   3.613408 -8.816133   4.101266   \n",
       "19        21    0.77   -2.172475  0.386482   7.692458 -5.309348   2.905639   \n",
       "20        14    0.81   -1.928028  0.335466   4.660425 -5.085589   2.913781   \n",
       "21        16    0.85   -2.151765  0.127962   6.735352 -5.497068  -3.806915   \n",
       "22        22    0.89   -2.957782 -0.136154   4.795767 -7.462805  -0.385250   \n",
       "23        23    0.93   -4.114155 -1.811558   4.267642 -9.691993  -0.894806   \n",
       "\n",
       "                                                                    \n",
       "         1250       1500       1750       2000      mean  variance  \n",
       "0    0.000000   0.000000   0.000000   0.000000  0.000000  0.000000  \n",
       "1   -3.299856 -17.182131   3.461538  -6.280193  0.473085  2.613778  \n",
       "2   -9.364649   1.214204   8.757396 -11.550285  0.340692 -0.400215  \n",
       "3   -4.065041  -9.049256  -9.704142 -10.232762 -1.970118 -4.718302  \n",
       "4   -2.547425 -10.352234  15.274725   4.991949  2.477867 -4.486993  \n",
       "5   -0.348432  10.006064  -1.538461   3.010033 -0.095214 -8.747605  \n",
       "6    7.760532  -1.565483   7.384615 -15.942029  0.942290 -0.801013  \n",
       "7    5.926712  -8.880449   8.269231   2.346446  2.073992 -1.435753  \n",
       "8   10.388437  -4.639175   8.506787   2.737520  1.927472 -6.737848  \n",
       "9    8.609548   1.489118   8.205128   0.362319  2.221263 -1.529138  \n",
       "10  -5.189200  -5.841924   0.161943   5.086672  0.465829  2.159431  \n",
       "11  10.587796  -6.643757   0.219780   7.407407  1.842521 -3.142661  \n",
       "12  -1.490515  -1.771081   2.657343  -7.297229  0.233568 -0.265392  \n",
       "13   4.351985   4.295533   4.807692   2.001380  2.501483 -0.433944  \n",
       "14  10.935326  -1.030928   1.124260   4.011762  1.314818 -1.623186  \n",
       "15   2.386146  -5.558925   1.007958  -4.927536  0.764992  1.795111  \n",
       "16   5.738188   5.046120  10.576923   4.761905  1.173151 -7.351048  \n",
       "17   4.440275   0.423559   2.948718  -0.981767  1.121219 -3.367140  \n",
       "18  -0.316935   2.503682   0.487805  -8.427268  0.047507  0.667842  \n",
       "19  -0.860139  -6.601556   5.961539  -7.200368 -0.411601 -3.453636  \n",
       "20  -2.568073  -0.612580  10.318302   3.476366  0.462860 -5.221643  \n",
       "21   2.929897  -1.805111   7.629938   4.513889  0.245923 -5.385000  \n",
       "22  -1.104570  -3.820497   2.757045   2.715309 -0.965957 -5.942129  \n",
       "23  -1.307469  -3.897053   8.481013   3.099545 -1.541004 -9.922858  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_overlap_df=return_and_save_final_relative_result_df_as_json_overlap(overlap_df, base_case_index=0, final_exp_results_path=FINAL_EXP_RESULTS_PATH, \n",
    "                                                                     exp_path_name=EXP_PATH_NAME, window_size_lst=WINDOW_SIZE_LST)\n",
    "relative_overlap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
