{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "d975284c-a069-40d8-9847-9465d0ac4f47",
    "_uuid": "b6bb54c3-ff29-436c-8f5e-b3a13b9f5153",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "TEST_MODE = 0 # Testing macro\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "if TEST_MODE:\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "3ae86968-b273-4857-a4fd-537b178d854b",
    "_uuid": "bcec77b5-0738-4d15-8aad-f6b59b3ac023",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.9/site-packages (23.0.1)\n",
      "utility_functions imports setup complete\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "\n",
    "\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy import signal\n",
    "import numpy as np # linear algebraf\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "\n",
    "\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "%run ./utility-functions.ipynb\n",
    "%run ./SEED-CONSTANTS.ipynb\n",
    "\n",
    "print(f\"\\x1b[32mSEED: {SEED}\\x1b[0m\")\n",
    "# Global utitlity functions are in separate notebook\n",
    "if TEST_MODE: print(\"setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.10\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_windows(dfList_exp1, dfList_exp2, window_size, step_width, user_idx_set, scaler, num_sample_points_per_exp, EMA_per_win_span=None, SMA_per_win_winsize=None, \n",
    "                    Butter_per_win_argdict=None, verbose=1):\n",
    "\n",
    "    # preparing data\n",
    "    exp1_df_user_set_dict, exp2_df_user_set_dict = {user_key: dfList_exp1[user_key] for user_key in user_idx_set}, {user_key: dfList_exp2[user_key] for user_key in user_idx_set}\n",
    "    print(f\"len(exp1_df_user_set_dict): {len(exp1_df_user_set_dict)}\")\n",
    "    print(f\"len(exp2_df_user_set_dict): {len(exp2_df_user_set_dict)}\")\n",
    "    \n",
    "    # do not scale exp1 on its own, but instead use exp2 scaler from next steps to scale exp1\n",
    "    exp1_data_prep_dict = MakeRawXExpDict(users_exp_df_dict=exp1_df_user_set_dict, window_size=window_size, step=step_width, numSamplePoints=num_sample_points_per_exp, \n",
    "                                          scaler=None, exp_num=1, EMA_per_win_span=EMA_per_win_span, SMA_per_win_winsize=SMA_per_win_winsize,\n",
    "                                          Butter_per_win_argdict=Butter_per_win_argdict, verbose=verbose)\n",
    "    \n",
    "    \n",
    "    exp2_data_prep_dict = MakeRawXExpDict(users_exp_df_dict=exp2_df_user_set_dict, window_size=window_size, step=step_width, numSamplePoints=num_sample_points_per_exp, \n",
    "                                          scaler=scaler, exp_num=2, EMA_per_win_span=EMA_per_win_span, SMA_per_win_winsize=SMA_per_win_winsize, \n",
    "                                          Butter_per_win_argdict=Butter_per_win_argdict, verbose=verbose)\n",
    "\n",
    "    fitted_scaler_exp2_dict = exp2_data_prep_dict[\"fitted_scaler_exp_dic\"]\n",
    "    X_exp1_dict, X_exp2_dict = exp1_data_prep_dict[\"raw_X_exp_dict\"], exp2_data_prep_dict[\"raw_X_exp_dict\"]\n",
    "\n",
    "    if verbose: print(f\"Done getting raw wondows for window_size: {window_size}\")\n",
    "    return X_exp1_dict, X_exp2_dict, fitted_scaler_exp2_dict\n",
    "\n",
    "    \n",
    "def get_raw_windows_user_47(dfList_exp1_user_47, dfList_exp2_user_47, window_size, step_width, scaler, num_sample_points_per_exp, \n",
    "                            EMA_per_win_span=None, SMA_per_win_winsize=None, Butter_per_win_argdict=None, verbose=1):\n",
    "\n",
    "\n",
    "    exp1_df_dict_user_47, exp2_df_dict_user_47 = {47: dfList_exp1_user_47[0]}, {47: dfList_exp2_user_47[0]}\n",
    "    \n",
    "    \n",
    "    # Loading exp1 data:\n",
    "    # 47) accel_count: 22777, gyro_count: 22226\n",
    "    exp1_data_prep_dict_user_47 = MakeRawXExpDict(users_exp_df_dict=exp1_df_dict_user_47, window_size=window_size, step=step_width, \n",
    "                                                  numSamplePoints=num_sample_points_per_exp, scaler=None, exp_num=1, #+4000\n",
    "                                                  EMA_per_win_span=EMA_per_win_span, SMA_per_win_winsize=SMA_per_win_winsize, \n",
    "                                                  Butter_per_win_argdict=Butter_per_win_argdict, verbose=verbose)\n",
    "    \n",
    "    # Loading exp2 data:\n",
    "    # 47) accel_count: 17718, gyro_count: 18353\n",
    "    exp2_data_prep_dict_user_47 = MakeRawXExpDict(users_exp_df_dict=exp2_df_dict_user_47, window_size=window_size, step=step_width, \n",
    "                                                  numSamplePoints=18000, scaler=scaler, exp_num=2, # has to be manually chosen\n",
    "                                                  EMA_per_win_span=EMA_per_win_span, SMA_per_win_winsize=SMA_per_win_winsize, \n",
    "                                                  Butter_per_win_argdict=Butter_per_win_argdict, verbose=verbose)\n",
    "    \n",
    "    \n",
    "    fitted_scaler_exp2_dict_user_47 = exp2_data_prep_dict_user_47[\"fitted_scaler_exp_dic\"]\n",
    "    X_exp1_dict_user_47, X_exp2_dict_user_47 = exp1_data_prep_dict_user_47[\"raw_X_exp_dict\"], exp2_data_prep_dict_user_47[\"raw_X_exp_dict\"]\n",
    "    \n",
    "    # ---- adjusting the len_exp2_user_47\n",
    "    total_samples_per_user=len(X_exp1_dict_user_47[47])\n",
    "    print(f\"len(X_exp1_dict_user_47[47]): {len(X_exp1_dict_user_47[47])}\")\n",
    "    \n",
    "    len_exp2_user_47 = len(X_exp2_dict_user_47[47])\n",
    "    print(f\"len_exp2_user_47: {len_exp2_user_47}\")\n",
    "    np.random.seed(SEED+len_exp2_user_47)\n",
    "    X_exp2_dict_user_47[47] = X_exp2_dict_user_47[47] + [X_exp2_dict_user_47[47][idx] for idx in np.random.choice(len_exp2_user_47,\n",
    "                                                                                                      size=total_samples_per_user-len_exp2_user_47,\n",
    "                                                                                                      replace=False)]\n",
    "    # ----\n",
    "    \n",
    "    return X_exp1_dict_user_47, X_exp2_dict_user_47, fitted_scaler_exp2_dict_user_47\n",
    "\n",
    "\n",
    "\n",
    "def append_user_47_to_data(X_exp1_dict, X_exp2_dict, fitted_scaler_exp2_dict, all_user_set, X_exp1_dict_user_47, X_exp2_dict_user_47, fitted_scaler_exp2_dict_user_47, verbose=0):\n",
    "    user_47_idx = len(all_user_set)\n",
    "    \n",
    "    if user_47_idx not in all_user_set:\n",
    "        all_user_set.append(user_47_idx)\n",
    "        \n",
    "    user_47_key = user_47_idx\n",
    "    X_exp1_dict[user_47_key] = X_exp1_dict_user_47[47]\n",
    "    X_exp2_dict[user_47_key] = X_exp2_dict_user_47[47]\n",
    "    fitted_scaler_exp2_dict[user_47_key] = fitted_scaler_exp2_dict_user_47[47]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"user_47_idx: {user_47_idx}\")\n",
    "        print(f\"np.unique(all_user_set): {np.unique(all_user_set)}\")\n",
    "\n",
    "        print(f\"X_exp1_dict.keys(): {X_exp1_dict.keys()}\")\n",
    "        print(f\"X_exp2_dict.keys(): {X_exp2_dict.keys()}\")\n",
    "        print(f\"fitted_scaler_exp2_dict.keys(): {fitted_scaler_exp2_dict.keys()}\")\n",
    "        \n",
    "    return X_exp1_dict, X_exp2_dict, fitted_scaler_exp2_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_butter(df, filter_order=10, cut_off_freq=10, sampling_freq=100, plot=False, filtfilt=1):\n",
    "    N = np.array(df).shape[0]\n",
    "    # sample spacing\n",
    "    T = 1.0 / sampling_freq\n",
    "\n",
    "    sos = signal.butter(filter_order, cut_off_freq, 'lowpass', fs=sampling_freq, output='sos')\n",
    "    # sig = np.array(df)\n",
    "    sig = df.to_numpy()\n",
    "    \n",
    "    if filtfilt:\n",
    "        # this one eliminates the delay \n",
    "        y = signal.sosfiltfilt(sos, sig) #filtered time domain signal\n",
    "    else:\n",
    "        y = signal.sosfilt(sos, sig) #filtered time domain signal\n",
    "\n",
    "\n",
    "    yf = fft(y)\n",
    "    xf = fftfreq(N, T)[:N//2]\n",
    "    \n",
    "    \n",
    "    if plot:\n",
    "        plt.plot(xf, 2.0/N * np.abs(yf[0:N//2]))\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "def get_ffted_df(df, filter_order, cut_off_freq, sampling_freq, filtfilt):\n",
    "    \n",
    "    ffted_df = df.copy()\n",
    "    ffted_df['x_a'] = apply_butter(ffted_df['x_a'], filter_order=filter_order, cut_off_freq=cut_off_freq, sampling_freq=sampling_freq, filtfilt=filtfilt).astype(\"float32\")\n",
    "    ffted_df['y_a'] = apply_butter(ffted_df['y_a'], filter_order=filter_order, cut_off_freq=cut_off_freq, sampling_freq=sampling_freq, filtfilt=filtfilt).astype(\"float32\")\n",
    "    ffted_df['z_a'] = apply_butter(ffted_df['z_a'], filter_order=filter_order, cut_off_freq=cut_off_freq, sampling_freq=sampling_freq, filtfilt=filtfilt).astype(\"float32\")\n",
    "\n",
    "    ffted_df['x_g'] = apply_butter(ffted_df['x_g'], filter_order=filter_order, cut_off_freq=cut_off_freq, sampling_freq=sampling_freq, filtfilt=filtfilt).astype(\"float32\")\n",
    "    ffted_df['y_g'] = apply_butter(ffted_df['y_g'], filter_order=filter_order, cut_off_freq=cut_off_freq, sampling_freq=sampling_freq, filtfilt=filtfilt).astype(\"float32\")\n",
    "    ffted_df['z_g'] = apply_butter(ffted_df['z_g'], filter_order=filter_order, cut_off_freq=cut_off_freq, sampling_freq=sampling_freq, filtfilt=filtfilt).astype(\"float32\")\n",
    "    \n",
    "    return ffted_df\n",
    "\n",
    "def get_EMAed_df(df, span):\n",
    "    \n",
    "    EMAed_df = df.copy()\n",
    "    EMAed_df['x_a'] = EMAed_df['x_a'].ewm(span=span, adjust=False).mean().astype(\"float32\")\n",
    "    EMAed_df['y_a'] = EMAed_df['y_a'].ewm(span=span, adjust=False).mean().astype(\"float32\")\n",
    "    EMAed_df['z_a'] = EMAed_df['z_a'].ewm(span=span, adjust=False).mean().astype(\"float32\")\n",
    "\n",
    "    EMAed_df['x_g'] = EMAed_df['x_g'].ewm(span=span, adjust=False).mean().astype(\"float32\")\n",
    "    EMAed_df['y_g'] = EMAed_df['y_g'].ewm(span=span, adjust=False).mean().astype(\"float32\")\n",
    "    EMAed_df['z_g'] = EMAed_df['z_g'].ewm(span=span, adjust=False).mean().astype(\"float32\")\n",
    "    \n",
    "    return EMAed_df\n",
    "\n",
    "\n",
    "def get_SMAed_df(df, winsize):\n",
    "    \n",
    "    SMAed_df = df.copy()\n",
    "    # min_periods: Minimum number of observations in window required to have a value, \n",
    "    # min_periods=0 since we do not want the result to be shorter in length, and there is non nan value\n",
    "    SMAed_df['x_a'] = SMAed_df['x_a'].rolling(window=winsize, min_periods=0).mean()\n",
    "    SMAed_df['y_a'] = SMAed_df['y_a'].rolling(window=winsize, min_periods=0).mean()\n",
    "    SMAed_df['z_a'] = SMAed_df['z_a'].rolling(window=winsize, min_periods=0).mean()\n",
    "\n",
    "    SMAed_df['x_g'] = SMAed_df['x_g'].rolling(window=winsize, min_periods=0).mean()\n",
    "    SMAed_df['y_g'] = SMAed_df['y_g'].rolling(window=winsize, min_periods=0).mean()\n",
    "    SMAed_df['z_g'] = SMAed_df['z_g'].rolling(window=winsize, min_periods=0).mean()\n",
    "    \n",
    "    return SMAed_df\n",
    "\n",
    "\n",
    "def get_ffted_dfList(dfList_exp, filter_order=10, cut_off_freq=10, sampling_freq=100, filtfilt=1):\n",
    "    \n",
    "    ffted_dfList_exp = []\n",
    "    for df in dfList_exp:\n",
    "        \n",
    "        ffted_df=get_ffted_df(df, filter_order=filter_order, cut_off_freq=cut_off_freq, sampling_freq=sampling_freq, filtfilt=filtfilt)\n",
    "        \n",
    "        ffted_dfList_exp.append(ffted_df)\n",
    "        \n",
    "    return ffted_dfList_exp\n",
    "\n",
    "def get_EMAed_dfList(dfList_exp, span):\n",
    "    \n",
    "    EMAed_dfList_exp = []\n",
    "    for df in dfList_exp:\n",
    "        \n",
    "        EMAed_df=get_EMAed_df(df, span)\n",
    "        \n",
    "        EMAed_dfList_exp.append(EMAed_df)\n",
    "        \n",
    "    return EMAed_dfList_exp\n",
    "\n",
    "def get_SMAed_dfList(dfList_exp, winsize):\n",
    "    \n",
    "    SMAed_dfList_exp = []\n",
    "    for df in dfList_exp:\n",
    "        \n",
    "        SMAed_df=get_SMAed_df(df, winsize)\n",
    "        \n",
    "        SMAed_dfList_exp.append(SMAed_df)\n",
    "        \n",
    "    return SMAed_dfList_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "d7a8c48d-ce60-4891-a613-8cac6bcad885",
    "_uuid": "74529ec9-2ddf-43e8-8ef3-d2bba209f31b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def matchAccelGyroData(accel, gyro):\n",
    "    # Match the numbers by merge_asof to the higher length vector\n",
    "    accel_count = accel.count().time_stamp\n",
    "    gyro_count = gyro.count().time_stamp\n",
    "    names =['sensor_id', 'time_stamp', 'x', 'y', 'z']\n",
    "    if accel_count > gyro_count:\n",
    "        df = pd.merge_asof(accel, gyro, on=\"time_stamp\", direction='nearest')\n",
    "        df = df.sort_values(by=['time_stamp'])\n",
    "        df = df.dropna()\n",
    "        accel = df[[\"sensor_id_x\", \"time_stamp\", \"x_x\", \"y_x\", \"z_x\"]]\n",
    "        gyro = df[[\"sensor_id_y\", \"time_stamp\", \"x_y\", \"y_y\", \"z_y\"]]\n",
    "    else:\n",
    "        df = pd.merge_asof(gyro, accel, on=\"time_stamp\", direction='nearest')\n",
    "        df = df.sort_values(by=['time_stamp'])\n",
    "        df = df.dropna()\n",
    "        gyro = df[[\"sensor_id_x\", \"time_stamp\", \"x_x\", \"y_x\", \"z_x\"]]\n",
    "        accel = df[[\"sensor_id_y\", \"time_stamp\", \"x_y\", \"y_y\", \"z_y\"]]\n",
    "\n",
    "    accel.columns = names\n",
    "    gyro.columns = names\n",
    "    \n",
    "    return {'accel': accel, 'gyro': gyro}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_cell_guid": "d8b4d9b0-695e-4551-be65-38a6f1817241",
    "_uuid": "fb41768e-82f1-4fb4-ba68-e1dd6cfcfba3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getDataStats1(i, print_accel_gyro_array_size=1, print_na_df_array_size=0, begin_idx=500, end_idx=-500):\n",
    "    \n",
    "    #load the data\n",
    "    names =['sensor_id', 'time_stamp', 'x', 'y', 'z']\n",
    "#     if i!=8:\n",
    "#         data = pd.read_csv('../input/wearable-assisted-ca/user{}_1.csv'.format(i), error_bad_lines = False, header=None, usecols = range(len(names)))\n",
    "#     else:\n",
    "    \n",
    "    data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
    "    data.columns = names\n",
    "    data = data[(data.sensor_id == '10') | (data.sensor_id =='4')]\n",
    "    data.head(10)\n",
    "\n",
    "\n",
    "    types_dict = {'sensor_id': 'int32', 'time_stamp': 'float32', 'x': 'float32', 'y': 'float32', 'z': 'float32'}\n",
    "    for col, col_type in types_dict.items():\n",
    "        data[col] = data[col].astype(col_type)\n",
    "\n",
    "    # find how many NAN values in the data\n",
    "    data.isna().sum()\n",
    "\n",
    "    # since only 7 NAN is a very small amount, drop them\n",
    "    df = data.dropna()\n",
    "\n",
    "    # get rid of begin and end noise\n",
    "    #sort df and filter\n",
    "    df = df.sort_values(by=['time_stamp'])\n",
    "    df = df[begin_idx:end_idx]\n",
    "    \n",
    "    # cleaning extreme outliers\n",
    "    df = df[(df.x < 5) & ( -5 < df.x) & (df.y < 5) & ( -5 < df.y) & (df.z < 5) & ( -5 < df.z) ]\n",
    "\n",
    "    # Extract Accelerometer values and sort\n",
    "    accel = df[df.sensor_id == 10]\n",
    "    accel = accel.sort_values(by=['time_stamp'])\n",
    "\n",
    "    # Extract gyro values and sort\n",
    "    gyro = df[df.sensor_id == 4]\n",
    "    gyro = gyro.sort_values(by=['time_stamp'])\n",
    "    \n",
    "    if print_accel_gyro_array_size:\n",
    "        print(\"{}) accel_count: {}, gyro_count: {}\".format(i, accel.count().time_stamp, gyro.count().time_stamp))\n",
    "    \n",
    "    result = matchAccelGyroData(accel, gyro)\n",
    "    accel, gyro = result['accel'], result['gyro']\n",
    "    \n",
    "#     accel['x_a'] = accel['x'].ewm(span=40,adjust=False).mean()\n",
    "#     accel['y_a'] = accel['y'].ewm(span=40,adjust=False).mean()\n",
    "#     accel['z_a'] = accel['z'].ewm(span=40,adjust=False).mean()\n",
    "\n",
    "#     gyro['x_g'] = gyro['x'].ewm(span=40,adjust=False).mean()\n",
    "#     gyro['y_g'] = gyro['y'].ewm(span=40,adjust=False).mean()\n",
    "#     gyro['z_g'] = gyro['z'].ewm(span=40,adjust=False).mean()\n",
    "    \n",
    "    accel['x_a'] = accel['x']\n",
    "    accel['y_a'] = accel['y']\n",
    "    accel['z_a'] = accel['z']\n",
    "\n",
    "    gyro['x_g'] = gyro['x']\n",
    "    gyro['y_g'] = gyro['y']\n",
    "    gyro['z_g'] = gyro['z']\n",
    "    \n",
    "    left = accel[[\"time_stamp\", \"x_a\", \"y_a\", \"z_a\"]]\n",
    "    right = gyro[[\"time_stamp\", \"x_g\", \"y_g\", \"z_g\"]].set_index('time_stamp')\n",
    "    df = left.join(right, on='time_stamp')\n",
    "\n",
    "    if print_na_df_array_size:\n",
    "        print(\"{}) na_count: {}, df count: {}\".format(i, df.isna().sum().sum(), df.count().time_stamp))\n",
    "    \n",
    "    return {\"accel\":accel.count().time_stamp, \"gyro\": gyro.count().time_stamp, \"df\": df, \"userIdx\": i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "81b39c51-3e35-4210-a3ae-618543e164df",
    "_uuid": "287b6e21-fe46-4032-8e82-2ed1837e9f44",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getDataStats2(i, print_accel_gyro_array_size=1, print_na_df_array_size=0, begin_idx=500, end_idx=-500):\n",
    "    \n",
    "    #load the data\n",
    "    names =['sensor_id', 'time_stamp', 'x', 'y', 'z']\n",
    "#     if i!=8:\n",
    "#         data = pd.read_csv('../input/wearable-assisted-ca/user{}_1.csv'.format(i), error_bad_lines = False, header=None, usecols = range(len(names)))\n",
    "#     else:\n",
    "    \n",
    "    data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
    "    data.columns = names\n",
    "    data = data[(data.sensor_id == '10') | (data.sensor_id =='4')]\n",
    "    data.head(10)\n",
    "\n",
    "\n",
    "    types_dict = {'sensor_id': 'int32', 'time_stamp': 'float32', 'x': 'float32', 'y': 'float32', 'z': 'float32'}\n",
    "    for col, col_type in types_dict.items():\n",
    "        data[col] = data[col].astype(col_type)\n",
    "\n",
    "    # find how many NAN values in the data\n",
    "    data.isna().sum()\n",
    "\n",
    "    # since only 7 NAN is a very small amount, drop them\n",
    "    df = data.dropna()\n",
    "\n",
    "    # get rid of begin and end noise\n",
    "    #sort df and filter\n",
    "    df = df.sort_values(by=['time_stamp'])\n",
    "    df = df[begin_idx:end_idx]\n",
    "    \n",
    "    # cleaning extreme outliers\n",
    "    df = df[(df.x < 5) & ( -5 < df.x) & (df.y < 5) & ( -5 < df.y) & (df.z < 5) & ( -5 < df.z) ]\n",
    "\n",
    "    # Extract Accelerometer values and sort\n",
    "    accel = df[df.sensor_id == 10]\n",
    "    accel = accel.sort_values(by=['time_stamp'])\n",
    "\n",
    "    # Extract gyro values and sort\n",
    "    gyro = df[df.sensor_id == 4]\n",
    "    gyro = gyro.sort_values(by=['time_stamp'])\n",
    "    \n",
    "    if print_accel_gyro_array_size:\n",
    "        print(\"{}) accel_count: {}, gyro_count: {}\".format(i, accel.count().time_stamp, gyro.count().time_stamp))\n",
    "    \n",
    "    result = matchAccelGyroData(accel, gyro)\n",
    "    accel, gyro = result['accel'], result['gyro']\n",
    "    \n",
    "#     accel['x_a'] = accel['x'].ewm(span=40,adjust=False).mean()\n",
    "#     accel['y_a'] = accel['y'].ewm(span=40,adjust=False).mean()\n",
    "#     accel['z_a'] = accel['z'].ewm(span=40,adjust=False).mean()\n",
    "\n",
    "#     gyro['x_g'] = gyro['x'].ewm(span=40,adjust=False).mean()\n",
    "#     gyro['y_g'] = gyro['y'].ewm(span=40,adjust=False).mean()\n",
    "#     gyro['z_g'] = gyro['z'].ewm(span=40,adjust=False).mean()\n",
    "\n",
    "    accel['x_a'] = accel['x']\n",
    "    accel['y_a'] = accel['y']\n",
    "    accel['z_a'] = accel['z']\n",
    "\n",
    "    gyro['x_g'] = gyro['x']\n",
    "    gyro['y_g'] = gyro['y']\n",
    "    gyro['z_g'] = gyro['z']\n",
    "    \n",
    "    left = accel[[\"time_stamp\", \"x_a\", \"y_a\", \"z_a\"]]\n",
    "    right = gyro[[\"time_stamp\", \"x_g\", \"y_g\", \"z_g\"]].set_index('time_stamp')\n",
    "    df = left.join(right, on='time_stamp')\n",
    "\n",
    "    if print_na_df_array_size:\n",
    "        print(\"{}) na_count: {}, df count: {}\".format(i, df.isna().sum().sum(), df.count().time_stamp))\n",
    "    \n",
    "    return {\"accel\":accel.count().time_stamp, \"gyro\": gyro.count().time_stamp, \"df\": df, \"userIdx\": i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "852f6592-f958-4c9b-b8f4-73946030cfef",
    "_uuid": "52758db0-8d25-455e-91ec-b1bd76be8f7b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample_rate = 10 #Hz\n",
    "# #3352843.3\n",
    "# x = np.array([318.45,302.78,316.47,334.14,333.41,326.15,320.07,318.68,314.12,308.64,\n",
    "#               300.15,304.33,318.42,322.72,329.56,339.18,338.03,343.27,351.44,353.23,\n",
    "#               352.35,352.88,353.43,352.14,351.28,352.82,353.36,353.35,353.19,353.82])\n",
    "\n",
    "# mn=np.mean(x)\n",
    "# print(f' mean = {mn:.3f} unit')\n",
    "# print(f' sum x[i]**2  : {np.sum(x**2) :.1f} unit^2 ')\n",
    "\n",
    "\n",
    "# print(f' n *sum X[k]**2   : {spectral_energy(x) :.1f} unit^2 ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d09c240f-d000-4840-880c-260436115209",
    "_uuid": "aca85e4e-ed2f-4c54-bf0f-2f938ac45a07"
   },
   "source": [
    "# Using SVM in a real-world Scenario with multiple genuine users and intruders\n",
    "Source: https://datascience.stackexchange.com/questions/23623/what-is-the-best-way-to-classify-data-not-belonging-to-set-of-classes\n",
    "\n",
    "Stage 1: \n",
    "    Use one-class SVM to assign those images that do not belong to the set of predefined classes as the 9-th class.\n",
    "\n",
    "Stage 2:\n",
    "    For those images that passes through your filter, let the multi-class SVM assign them to one of the 8 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79b10ce2-42b2-4156-a1df-633bf52c05e6",
    "_uuid": "bf0780fa-46ed-4d13-90ee-3640fe7cc4aa"
   },
   "source": [
    "Loading data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_cell_guid": "72d60ae5-be59-4add-868d-4ba29d842b9f",
    "_uuid": "126e56a7-4e62-4f9d-adf7-89308283fccd",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def load_data_frames(user_ids, begin_idx, end_idx, min_len):\n",
    "    '''\n",
    "    input: \n",
    "        user_ids: list of approved user_ids after exploratory data analysis\n",
    "        begin_idx: the index before which data is discarded for user i\n",
    "        end_idx: the index after which data is discarded for user i\n",
    "        min_len: the minimum length that a dataframe has to be after cutting of both endings\n",
    "        \n",
    "    output:\n",
    "        {dfList_exp1, dfList_exp2}: return dfList for exp1 and exp2 of the selected user_ids\n",
    "    '''\n",
    "    print(\"Loading exp1 data:\")\n",
    "    dfList_exp1 = []\n",
    "    for i in user_ids:\n",
    "        dic = getDataStats1(i, begin_idx=begin_idx, end_idx=end_idx)\n",
    "\n",
    "        if(dic['accel']<min_len):\n",
    "            raise Exception(\"The Stream is shorter than {}\".format(min_len))\n",
    "\n",
    "        dfList_exp1 = dfList_exp1 + [dic['df'].reset_index(drop=True)]\n",
    "\n",
    "\n",
    "    print(\"Loading exp2 data:\")\n",
    "    dfList_exp2 = []\n",
    "    for i in user_ids:\n",
    "        dic = getDataStats2(i, begin_idx=begin_idx, end_idx=end_idx)\n",
    "\n",
    "        if(dic['accel']<min_len):\n",
    "            raise Exception(\"The Stream is shorter than {}\".format(min_len))\n",
    "\n",
    "        dfList_exp2 = dfList_exp2 + [dic['df'].reset_index(drop=True)]\n",
    "    #     dfList = dfList + [dic['df']]\n",
    "    \n",
    "    return {\"dfList_exp1\": dfList_exp1, \"dfList_exp2\": dfList_exp2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRawDataChunks(df, windows, scaler=None, user_key=None, exp_num=None, EMA_per_win_span=None, SMA_per_win_winsize=None, Butter_per_win_argdict=None, verbose=0):\n",
    "    a = []\n",
    "    df = df.copy().drop(columns=[\"time_stamp\"])\n",
    "    # df_array = df.to_numpy()\n",
    "    \n",
    "    if EMA_per_win_span!=None and SMA_per_win_winsize!=None:\n",
    "        raise Exception(\"cannot apply both EMA_per_win_span and SMA_per_win_winsize at the same time.\")\n",
    "        \n",
    "    if scaler!=None:\n",
    "        # print(df_array.shape)\n",
    "        scaler = get_new_scaler_dict[scaler]\n",
    "        # scaler = scaler().fit(df_array)\n",
    "        scaler = scaler().fit(df)\n",
    "        # df_array = scaler.transform(df_array)\n",
    "        df_array = scaler.transform(df)\n",
    "        # scaled_df = pd.DataFrame(data=df_array, columns = df.columns, dtype=df_array.dtype)\n",
    "        scaled_df = pd.DataFrame(data=df_array, columns = df.columns, dtype=df_array.dtype)\n",
    "        df = scaled_df\n",
    "        \n",
    "    if verbose: print(f\"getRawDataChunks - user_key: {user_key}, exp_num: {exp_num}, scaler: {scaler}\")\n",
    "    \n",
    "    verbose_c = verbose\n",
    "    for i in range(len(windows)):\n",
    "        # a.append(df_array[windows[i], :]) #CNN\n",
    "        \n",
    "        df_window=df.iloc[windows[i], :].copy()\n",
    "\n",
    "        # per window is currently only available for exp 1 \n",
    "        if scaler==None:\n",
    "            \n",
    "            if Butter_per_win_argdict!=None:\n",
    "                \n",
    "                \n",
    "                filter_order = Butter_per_win_argdict[\"filter_order\"]\n",
    "                cut_off_freq = Butter_per_win_argdict[\"cut_off_freq\"]\n",
    "                sampling_freq = Butter_per_win_argdict[\"sampling_freq\"]\n",
    "                filtfilt = Butter_per_win_argdict[\"filtfilt\"]\n",
    "                \n",
    "                # if verbose_c==1: df_window[\"x_a\"][0:500].plot()\n",
    "                    \n",
    "                df_window = get_ffted_df(df_window, filter_order=filter_order, cut_off_freq=cut_off_freq, sampling_freq=sampling_freq, filtfilt=filtfilt)\n",
    "\n",
    "                # if verbose_c: \n",
    "                #     df_window[\"x_a\"][0:500].plot()\n",
    "                #     print(f\"Butter_per_win_argdict: {Butter_per_win_argdict}\")\n",
    "                    \n",
    "            # exponential moving average\n",
    "            if EMA_per_win_span!=None:\n",
    "                \n",
    "                df_window = get_EMAed_df(df_window, span=EMA_per_win_span)\n",
    "                if verbose_c==1: df_window[\"x_a\"][0:500].plot()\n",
    "            # simple moving average\n",
    "            elif SMA_per_win_winsize!=None:\n",
    "\n",
    "                df_window = get_SMAed_df(df_window, winsize=SMA_per_win_winsize)\n",
    "                if verbose_c==1: df_window[\"x_a\"][0:500].plot()\n",
    "\n",
    "\n",
    "        verbose_c=0\n",
    "        a.append(df_window) #waca\n",
    "    \n",
    "#     print(len(a))\n",
    "#     print(len(a))\n",
    "#     return np.array([a])\n",
    "    # return np.array(a), scaler #CNN\n",
    "    \n",
    "    return a, scaler #waca\n",
    "\n",
    "\n",
    "def MakeRawXExpDict(users_exp_df_dict, window_size, step, numSamplePoints, scaler, exp_num, EMA_per_win_span=None, SMA_per_win_winsize=None, Butter_per_win_argdict=None,\n",
    "                    verbose=1):\n",
    "    '''\n",
    "    return \n",
    "    X_exp_dic\n",
    "    '''\n",
    "    \n",
    "    windows = getIndices(sampleSize=window_size, step=step, numSamplePoints=numSamplePoints)\n",
    "\n",
    "    X_exp_dic = {}\n",
    "    fitted_scaler_exp_dic={}\n",
    "    for user_key in users_exp_df_dict:\n",
    "        \n",
    "        encoding_array_exp, fitted_scaler_exp = getRawDataChunks(users_exp_df_dict[user_key], windows, scaler=scaler, user_key=user_key, exp_num=exp_num,\n",
    "                                                                 EMA_per_win_span=EMA_per_win_span, SMA_per_win_winsize=SMA_per_win_winsize, \n",
    "                                                                 Butter_per_win_argdict=Butter_per_win_argdict, verbose=verbose)\n",
    "        \n",
    "        X_exp_dic[user_key] = encoding_array_exp\n",
    "        \n",
    "        fitted_scaler_exp_dic[user_key]=fitted_scaler_exp\n",
    "        \n",
    "    return {\"raw_X_exp_dict\": X_exp_dic, \"fitted_scaler_exp_dic\": fitted_scaler_exp_dic}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_features_and_scale_for_parallel_call_Owner(X_exp_reg_df_dict, scaler_clip, scaler_type, feature_extractor, feature_extractor_transformer, owner, verbose=0):\n",
    "    \n",
    "    X_exp_dic = {}\n",
    "    \n",
    "    X_exp_dic[owner] = {\"profile_windows\": feature_extractor(X_exp_reg_df_dict[owner]), \"unknown_users_dict\": {}, \"fitted_feature_scaler\": None}\n",
    "        \n",
    "    # do minmax scaling here\n",
    "    if verbose: print(f\"fit_transform {feature_extractor.__name__} features using scaler of type: {scaler_type}, for profile_windows of owner: {owner}\")\n",
    "    if verbose: print(f\"owner: {owner}, exp_num: {2}, scaler: {scaler_type}, scaler_clip: {scaler_clip}\")\n",
    "\n",
    "    scaler = get_new_scaler_dict[scaler_type]\n",
    "    scaler = scaler(clip=scaler_clip).fit(X_exp_dic[owner][\"profile_windows\"])\n",
    "    X_exp_dic[owner][\"fitted_feature_scaler\"] = scaler\n",
    "\n",
    "    X_exp_dic[owner][\"profile_windows\"] = feature_extractor_transformer(X_exp_dic[owner][\"profile_windows\"], X_exp_dic[owner][\"fitted_feature_scaler\"])\n",
    "        \n",
    "        \n",
    "    return {\"owner\": owner, \"X_exp_dic\": X_exp_dic}\n",
    "\n",
    "\n",
    "def MakeXExpDicOwner(X_exp_reg_df_dict, scaler_clip=True, scaler_type=\"MinMaxScaler\", feature_extractor=None, feature_extractor_transformer=None, verbose=0):\n",
    "    '''k\n",
    "    ???\n",
    "    return \n",
    "    X_exp_dic\n",
    "    dfLists are of the same size.\n",
    "    '''\n",
    "\n",
    "    X_exp_dic = {}\n",
    "    \n",
    "#     for owner in X_exp_reg_df_dict.keys():\n",
    "\n",
    "#         if verbose: print(f\"raw exp2 samples of owner: {owner} are scaled already so we {feature_extractor.__name__}\")\n",
    "        \n",
    "#         X_exp_dic[owner] = {\"profile_windows\": feature_extractor(X_exp_reg_df_dict[owner]), \"unknown_users_dict\": {}, \"fitted_feature_scaler\": None}\n",
    "        \n",
    "# #         #------- DEBUG--------\n",
    "# #         if owner == 2: \n",
    "# #             X_exp_dic['debug']= X_exp_dic[owner], X_exp_reg_df_dict[owner]\n",
    "# #             return X_exp_dic\n",
    "        \n",
    "# #         #------- DEBUG--------\n",
    "        \n",
    "#         # do minmax scaling here\n",
    "#         if verbose: print(f\"fit_transform {feature_extractor.__name__} features using scaler of type: {scaler_type}, for profile_windows of owner: {owner}\")\n",
    "#         if verbose: print(f\"owner: {owner}, exp_num: {2}, scaler: {scaler_type}, scaler_clip: {scaler_clip}\")\n",
    "        \n",
    "#         scaler = get_new_scaler_dict[scaler_type]\n",
    "#         scaler = scaler(clip=scaler_clip).fit(X_exp_dic[owner][\"profile_windows\"])\n",
    "#         X_exp_dic[owner][\"fitted_feature_scaler\"] = scaler\n",
    "\n",
    "#         X_exp_dic[owner][\"profile_windows\"] = feature_extractor_transformer(X_exp_dic[owner][\"profile_windows\"], X_exp_dic[owner][\"fitted_feature_scaler\"])\n",
    "        \n",
    "        \n",
    "        \n",
    "    with parallel_backend('loky', n_jobs=-1):\n",
    "        results = Parallel(n_jobs=-1, verbose=100)(delayed(extract_features_and_scale_for_parallel_call_Owner)\n",
    "                                                   (X_exp_reg_df_dict, scaler_clip, scaler_type, feature_extractor, feature_extractor_transformer, owner, verbose=0) \n",
    "                                                   for owner in X_exp_reg_df_dict.keys())\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item[\"owner\"]\n",
    "        X_exp_dic[owner] = item[\"X_exp_dic\"][owner]\n",
    "        \n",
    "    \n",
    "    return X_exp_dic\n",
    "    \n",
    "\n",
    "\n",
    "def extract_features_and_scale_for_parallel_call_Unknown(X_exp_unknown_df_dict, X_exp_dic, fitted_raw_scaler_dict, feature_extractor, feature_extractor_transformer, owner, verbose=0):\n",
    "    if fitted_raw_scaler_dict:\n",
    "        \n",
    "        if verbose: print(f\"scaling exp1 unknown windows for when user: {owner}, is owner\")\n",
    "        \n",
    "        for unknown_user in X_exp_unknown_df_dict.keys():\n",
    "            # scale the unkown window using the scaler fitted on raw registeration data and then extract the unknown features\n",
    "            X_exp_dic[owner][\"unknown_users_dict\"][unknown_user] = feature_extractor(transform_user_windows(X_exp_unknown_df_dict[unknown_user], fitted_raw_scaler_dict[owner]))\n",
    "            \n",
    "            # get the scaler fitted on the registeration features\n",
    "            fitted_feature_scaler = X_exp_dic[owner][\"fitted_feature_scaler\"]\n",
    "            \n",
    "            # scale the unkown features using the registeration feature scaler\n",
    "            X_exp_dic[owner][\"unknown_users_dict\"][unknown_user] = feature_extractor_transformer(X_exp_dic[owner][\"unknown_users_dict\"][unknown_user], fitted_feature_scaler)\n",
    "            \n",
    "        if verbose: print(f\"done scaling exp1 unknown windows for when user: {owner}, is owner\")\n",
    "        \n",
    "    return {\"owner\": owner, \"unknown_users_dict\": X_exp_dic[owner][\"unknown_users_dict\"]}\n",
    "    \n",
    "def MakeXExpDicUnknown(X_exp_unknown_df_dict, X_exp_dic, fitted_raw_scaler_dict, feature_extractor, feature_extractor_transformer):\n",
    "    '''k\n",
    "    ???\n",
    "    return \n",
    "    X_exp_dic\n",
    "    dfLists are of the same size.\n",
    "    '''\n",
    "\n",
    "#     for owner in X_exp_unknown_df_dict.keys():\n",
    "#         if fitted_raw_scaler_dict:\n",
    "#             print(f\"scaling exp1 unknown windows for when user: {owner}, is owner\")\n",
    "#             for unknown_user in X_exp_unknown_df_dict.keys():\n",
    "#                 # scale the unkown window using the scaler fitted on raw registeration data and then extract the unknown features\n",
    "#                 X_exp_dic[owner][\"unknown_users_dict\"][unknown_user] = feature_extractor(transform_user_windows(X_exp_unknown_df_dict[unknown_user], fitted_raw_scaler_dict[owner]))\n",
    "                \n",
    "#                 # get the scaler fitted on the registeration features\n",
    "#                 fitted_feature_scaler = X_exp_dic[owner][\"fitted_feature_scaler\"]\n",
    "                \n",
    "#                 # scale the unkown features using the registeration feature scaler\n",
    "#                 X_exp_dic[owner][\"unknown_users_dict\"][unknown_user] = feature_extractor_transformer(X_exp_dic[owner][\"unknown_users_dict\"][unknown_user], fitted_feature_scaler)\n",
    "            \n",
    "            \n",
    "    with parallel_backend('loky', n_jobs=-1):\n",
    "        results = Parallel(n_jobs=-1, verbose=100)(delayed(extract_features_and_scale_for_parallel_call_Unknown)(X_exp_unknown_df_dict, X_exp_dic, fitted_raw_scaler_dict, \n",
    "                                                                                                                         feature_extractor, feature_extractor_transformer,\n",
    "                                                                                                                         owner) for owner in X_exp_unknown_df_dict.keys())\n",
    "\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item[\"owner\"]\n",
    "        unknown_users_dict = item[\"unknown_users_dict\"]\n",
    "        X_exp_dic[owner][\"unknown_users_dict\"] = unknown_users_dict\n",
    "    \n",
    "    \n",
    "    return X_exp_dic\n",
    "\n",
    "\n",
    "\n",
    "def transform_user_windows(X_exp, fitted_scaler):\n",
    "    \n",
    "    \n",
    "    transformed_X_exp = []\n",
    "    \n",
    "#     print(X_exp[0].shape)\n",
    "    for window in X_exp:\n",
    "        scaled_array = fitted_scaler.transform(window)\n",
    "        scaled_window_df = pd.DataFrame(data=scaled_array, columns = window.columns, dtype=scaled_array.dtype)\n",
    "        transformed_X_exp.append(scaled_window_df)\n",
    "        \n",
    "    return transformed_X_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\u001b[32mPreprocessing utility functions imported\u001b[0m--------------------\n"
     ]
    }
   ],
   "source": [
    "print(20*'-' + \"\\x1b[32mPreprocessing utility functions imported\\x1b[0m\" + 20*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
