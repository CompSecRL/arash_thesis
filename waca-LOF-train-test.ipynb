{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "EXP_PATH_NAME=\"WACA-LOF\"\n",
    "joblib.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "EER: 0.333, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.400, Threshold: 0.200 <-- Worse case\n",
      "EER: 0.167, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.333, Threshold: 1.000 <-- Worse case\n",
      "--------------------\u001b[32mUtility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mPreprocessing utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mNeural Networks utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "EER: 0.333, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.400, Threshold: 0.200 <-- Worse case\n",
      "EER: 0.167, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.333, Threshold: 1.000 <-- Worse case\n",
      "--------------------\u001b[32mUtility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mPreprocessing utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mWACA utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "Python 3.9.10\n",
      "--------------------\u001b[32mClassification utility functions imported\u001b[0m--------------------\n",
      "\u001b[32mSEED: 567\u001b[0m\n",
      "FINAL_EXP_RESULTS_PATH: clip=False_experiments_results\n",
      "--------------------\u001b[32mSEED and CONSTANTS imported\u001b[0m--------------------\n",
      "Numpy Seed was set to: 567\n",
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade pip\n",
    "\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import dataclasses\n",
    "from sklearn.svm import OneClassSVM\n",
    "from dataclasses import asdict\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import random\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold # Feature selector\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Global utitlity functions are in separate notebook\n",
    "%run ./Classification_utility-functions.ipynb\n",
    "%run ./SEED-CONSTANTS.ipynb\n",
    "\n",
    "np.random.seed(SEED)\n",
    "print(f\"Numpy Seed was set to: {SEED}\")\n",
    "\n",
    "print(\"Setup Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__dir__()\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class ExperimentParameters:\n",
    "    \"\"\"Contains all relevant parameters to run an experiment.\"\"\"\n",
    "\n",
    "    name: str  # Name of Parameter set. Used as identifier for charts etc.\n",
    "    frequency: int\n",
    "    max_subjects: int\n",
    "    max_test_subjects: int\n",
    "        \n",
    "    user_ids: list\n",
    "    num_sample_points_per_exp: int\n",
    "    exp_begin_cutoff_idx: int\n",
    "    exp_end_cutoff_idx: int\n",
    "        \n",
    "    \n",
    "    seconds_per_subject_train: float\n",
    "    seconds_per_subject_test: float\n",
    "    window_size: int  # After resampling\n",
    "    ocsvm_step_width: int  # After resampling\n",
    "    scaler: str  # StandardScaler, MinMaxScaler, Normalizer, MaxAbsScaler, RobustScaler, PowerTransformer\n",
    "    scaler_scope: str  # {\"subject\", \"session\"}\n",
    "    scaler_global: bool  # fit transform scale on all data (True) or fit on training only (False)\n",
    "    ocsvm_kernel: str # ocsvm kernel\n",
    "    ocsvm_nu: float  # Best value found in random search, used for final model\n",
    "    ocsvm_gamma: float  # Best value found in random search, used for final model\n",
    "    feature_cols: list  # Columns used as features\n",
    "    exclude_subjects: list  # Don't load data from those users\n",
    "        \n",
    "    # Calculated values\n",
    "    def __post_init__(self):\n",
    "        # HDF key of table:\n",
    "        self.table_name = f\"sensors_{self.frequency}hz\"\n",
    "\n",
    "        \n",
    "\n",
    "# INSTANCES\n",
    "# ===========================================================\n",
    "\n",
    "# NAIVE_APPROACH\n",
    "# -----------------------------------------------------------\n",
    "NAIVE_MINMAX_OCSVM = ExperimentParameters(\n",
    "    name=\"NAIVE-MINMAX_OCSVM\",\n",
    "    frequency=100,\n",
    "    max_subjects=29,\n",
    "    max_test_subjects=10,\n",
    "    user_ids = [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 28, 29, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49],\n",
    "    num_sample_points_per_exp=21000,\n",
    "    exp_begin_cutoff_idx=500,\n",
    "    exp_end_cutoff_idx=-500,\n",
    "    seconds_per_subject_train=210,\n",
    "    seconds_per_subject_test=210,    \n",
    "    window_size=250,\n",
    "    ocsvm_step_width=250,\n",
    "    scaler=\"minmax\",\n",
    "    scaler_scope=\"subject\",\n",
    "    scaler_global=True,\n",
    "    ocsvm_kernel=\"rbf\",\n",
    "    ocsvm_nu=None,\n",
    "    ocsvm_gamma=None,\n",
    "    feature_cols=[\n",
    "        \"x_a\",\n",
    "        \"y_a\",\n",
    "        \"z_a\",\n",
    "        \"x_g\",\n",
    "        \"y_g\",\n",
    "        \"z_g\",\n",
    "    ],\n",
    "    exclude_subjects=[],\n",
    ")\n",
    "\n",
    "# VALID_APPROACH\n",
    "# -----------------------------------------------------------\n",
    "VALID_MINMAX_OCSVM = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-MINMAX-OCSVM\",\n",
    "    scaler_global=False,\n",
    "    ocsvm_nu=0.165,\n",
    "    ocsvm_gamma=0.039,\n",
    ")\n",
    "\n",
    "# NAIVE_ROBUST_APPROACH\n",
    "# -----------------------------------------------------------\n",
    "NAIVE_ROBUST_OCSVM = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"NAIVE-ROBUST-OCSVM\",\n",
    "    scaler=\"robust\",\n",
    "    scaler_global=True,\n",
    "    ocsvm_nu=0.153,\n",
    "    ocsvm_gamma=0.091,  # below median, selected by chart\n",
    ")\n",
    "\n",
    "# ROBUST_APPROACH (VALID)\n",
    "# -----------------------------------------------------------\n",
    "VALID_ROBUST_OCSVM_125 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=125\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "\n",
    "VALID_ROBUST_OCSVM_250 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=250\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_500 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=500\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_750 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=750\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_1000 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=1000\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_1250 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=1250\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_1500 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=1500\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_1750 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=1750\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_OCSVM_2000 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-ROBUST-OCSVM\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=2000\n",
    "#     ocsvm_nu=0.037,\n",
    "#     ocsvm_gamma= 0.001,\n",
    ")\n",
    "\n",
    "# NORMALIZER_APPROACH (VALID)\n",
    "# -----------------------------------------------------------\n",
    "VALID_NORMALIZER_OCSVM = dataclasses.replace(\n",
    "    NAIVE_MINMAX_OCSVM,\n",
    "    name=\"VALID-NORMALIZER-OCSVM\",\n",
    "    scaler=\"Normalizer\",\n",
    "    scaler_global=False,\n",
    "    ocsvm_nu=0.074,\n",
    "    ocsvm_gamma= 0.029,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "P = VALID_ROBUST_OCSVM_2000\n",
    "P.ocsvm_step_width = int(P.window_size * .5)\n",
    "P.classifier=\"LOF\"\n",
    "\n",
    "P.train_cores=1 # 20 cores for every user and 1 core for the nested crossval function\n",
    "P.test_cores=2 # 10 cores for every user and 2 for the nested crossval function\n",
    "# P = VALID_NORMALIZER_OCSVM\n",
    "\n",
    "param_dist = {\n",
    "    'model__n_neighbors': np.arange(1, 20),\n",
    "    'model__contamination': np.linspace(0.0001, 0.1, 50),\n",
    "    #'metric': ['minkowski', 'manhattan', 'chebyshev', 'hamming', 'cosine']\n",
    "}\n",
    "P.LOF_metric=\"minkowski\"\n",
    "P.p=2\n",
    "\n",
    "P.scaler_clip=False\n",
    "P.is_NN=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dist['model__n_neighbors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>VALID-ROBUST-OCSVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frequency</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_subjects</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_test_subjects</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_ids</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_sample_points_per_exp</th>\n",
       "      <td>21000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp_begin_cutoff_idx</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp_end_cutoff_idx</th>\n",
       "      <td>-500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seconds_per_subject_train</th>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seconds_per_subject_test</th>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>window_size</th>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocsvm_step_width</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler</th>\n",
       "      <td>RobustScaler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler_scope</th>\n",
       "      <td>subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler_global</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocsvm_kernel</th>\n",
       "      <td>rbf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocsvm_nu</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocsvm_gamma</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_cols</th>\n",
       "      <td>[x_a, y_a, z_a, x_g, y_g, z_g]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exclude_subjects</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       Value\n",
       "name                                                      VALID-ROBUST-OCSVM\n",
       "frequency                                                                100\n",
       "max_subjects                                                              29\n",
       "max_test_subjects                                                         10\n",
       "user_ids                   [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 2...\n",
       "num_sample_points_per_exp                                              21000\n",
       "exp_begin_cutoff_idx                                                     500\n",
       "exp_end_cutoff_idx                                                      -500\n",
       "seconds_per_subject_train                                                210\n",
       "seconds_per_subject_test                                                 210\n",
       "window_size                                                             2000\n",
       "ocsvm_step_width                                                        1000\n",
       "scaler                                                          RobustScaler\n",
       "scaler_scope                                                         subject\n",
       "scaler_global                                                          False\n",
       "ocsvm_kernel                                                             rbf\n",
       "ocsvm_nu                                                                None\n",
       "ocsvm_gamma                                                             None\n",
       "feature_cols                                  [x_a, y_a, z_a, x_g, y_g, z_g]\n",
       "exclude_subjects                                                          []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils_ppp(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_scorer(utils_eer, greater_is_better=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils_eer_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading exp1 data:\n",
      "1) accel_count: 28388, gyro_count: 31997\n",
      "2) accel_count: 26010, gyro_count: 28954\n",
      "3) accel_count: 28227, gyro_count: 31814\n",
      "4) accel_count: 24860, gyro_count: 26105\n",
      "5) accel_count: 24270, gyro_count: 24347\n",
      "6) accel_count: 25012, gyro_count: 25060\n",
      "7) accel_count: 25301, gyro_count: 25382\n",
      "8) accel_count: 21975, gyro_count: 21658\n",
      "19) accel_count: 24110, gyro_count: 25050\n",
      "21) accel_count: 24326, gyro_count: 23809\n",
      "22) accel_count: 29123, gyro_count: 28724\n",
      "26) accel_count: 23148, gyro_count: 24291\n",
      "27) accel_count: 24299, gyro_count: 23589\n",
      "28) accel_count: 23807, gyro_count: 24523\n",
      "29) accel_count: 24030, gyro_count: 23457\n",
      "35) accel_count: 24388, gyro_count: 23673\n",
      "36) accel_count: 24228, gyro_count: 24208\n",
      "37) accel_count: 31945, gyro_count: 31816\n",
      "38) accel_count: 22135, gyro_count: 22327\n",
      "39) accel_count: 23573, gyro_count: 23459\n",
      "40) accel_count: 23057, gyro_count: 24296\n",
      "41) accel_count: 24102, gyro_count: 23681\n",
      "42) accel_count: 24074, gyro_count: 24328\n",
      "43) accel_count: 22631, gyro_count: 23835\n",
      "44) accel_count: 24473, gyro_count: 23749\n",
      "45) accel_count: 23974, gyro_count: 23229\n",
      "46) accel_count: 23614, gyro_count: 23827\n",
      "48) accel_count: 22828, gyro_count: 23904\n",
      "49) accel_count: 24183, gyro_count: 24633\n",
      "Loading exp2 data:\n",
      "1) accel_count: 24049, gyro_count: 26943\n",
      "2) accel_count: 24468, gyro_count: 27667\n",
      "3) accel_count: 24611, gyro_count: 27000\n",
      "4) accel_count: 24972, gyro_count: 26798\n",
      "5) accel_count: 23573, gyro_count: 23372\n",
      "6) accel_count: 23800, gyro_count: 23890\n",
      "7) accel_count: 23347, gyro_count: 24145\n",
      "8) accel_count: 22947, gyro_count: 22660\n",
      "19) accel_count: 26156, gyro_count: 25815\n",
      "21) accel_count: 23566, gyro_count: 24408\n",
      "22) accel_count: 23844, gyro_count: 24589\n",
      "26) accel_count: 23179, gyro_count: 23925\n",
      "27) accel_count: 25109, gyro_count: 25820\n",
      "28) accel_count: 23133, gyro_count: 24028\n",
      "29) accel_count: 23180, gyro_count: 24314\n",
      "35) accel_count: 23299, gyro_count: 23854\n",
      "36) accel_count: 25497, gyro_count: 25059\n",
      "37) accel_count: 25994, gyro_count: 25232\n",
      "38) accel_count: 21164, gyro_count: 21182\n",
      "39) accel_count: 24214, gyro_count: 23585\n",
      "40) accel_count: 23944, gyro_count: 23170\n",
      "41) accel_count: 23193, gyro_count: 24111\n",
      "42) accel_count: 26505, gyro_count: 25697\n",
      "43) accel_count: 22690, gyro_count: 23981\n",
      "44) accel_count: 23002, gyro_count: 23829\n",
      "45) accel_count: 23978, gyro_count: 23350\n",
      "46) accel_count: 21128, gyro_count: 21848\n",
      "48) accel_count: 27996, gyro_count: 27205\n",
      "49) accel_count: 23061, gyro_count: 24129\n"
     ]
    }
   ],
   "source": [
    "#include 47 later\n",
    "# user_ids = [9]\n",
    "df_exps_dict = load_data_frames(P.user_ids, P.exp_begin_cutoff_idx, P.exp_end_cutoff_idx, P.num_sample_points_per_exp)\n",
    "raw_dfList_exp1, raw_dfList_exp2 = df_exps_dict['dfList_exp1'], df_exps_dict['dfList_exp2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set: [7, 24, 8, 11, 13, 19, 28, 21, 26, 3, 20, 22, 6, 25, 16, 1, 17, 27, 5]\n",
      "test_set: [0, 12, 14, 9, 18, 23, 2, 15, 10, 4]\n",
      "train_set: [7, 24, 8, 11, 13, 19, 28, 21, 26, 3, 20, 22, 6, 25, 16, 1, 17, 27, 5]\n",
      "test_set: [0, 12, 14, 9, 18, 23, 2, 15, 10, 4]\n"
     ]
    }
   ],
   "source": [
    "randomized_data_idx = list(range(len(P.user_ids)))\n",
    "random.Random(SEED).shuffle(randomized_data_idx)\n",
    "split_idx = 2 * (len(randomized_data_idx)//3) + 1\n",
    "train_set = randomized_data_idx[: split_idx]\n",
    "test_set = randomized_data_idx[split_idx: ]\n",
    "# train_set = randomized_data_idx\n",
    "print(f\"train_set: {train_set}\\ntest_set: {test_set}\")\n",
    "# train_set = test_set\n",
    "# test_set = train_set\n",
    "print(f\"train_set: {train_set}\\ntest_set: {test_set}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading exp1 data:\n",
      "47) accel_count: 22777, gyro_count: 22226\n",
      "Loading exp2 data:\n",
      "47) accel_count: 17718, gyro_count: 18353\n"
     ]
    }
   ],
   "source": [
    "num_sample_points_per_exp_user_47 = 18000\n",
    "df_exps_dict_user_47 = load_data_frames([47], P.exp_begin_cutoff_idx, P.exp_end_cutoff_idx, num_sample_points_per_exp_user_47)\n",
    "dfList_exp1_user_47, dfList_exp2_user_47 = df_exps_dict_user_47['dfList_exp1'], df_exps_dict_user_47['dfList_exp2']\n",
    "\n",
    "raw_dfList_exp1_user_47 = dfList_exp1_user_47\n",
    "raw_dfList_exp2_user_47 = dfList_exp2_user_47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set: [7, 24, 8, 11, 13, 19, 28, 21, 26, 3, 20, 22, 6, 25, 16, 1, 17, 27, 5]\n"
     ]
    }
   ],
   "source": [
    "print(f\"train_set: {train_set}\")\n",
    "# print(f\"X_exp1_train_dic: {X_exp1_train_dic.keys()}\")\n",
    "# print(f\"X_exp2_train_dic: {X_exp2_train_dic.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_set: [0, 12, 14, 9, 18, 23, 2, 15, 10, 4]\n"
     ]
    }
   ],
   "source": [
    "print(f\"test_set: {test_set}\")\n",
    "# print(f\"X_exp1_test_dic: {X_exp1_test_dic.keys()}\")\n",
    "# print(f\"X_exp2_test_dic: {X_exp2_test_dic.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init_experiment_params(exp_config=P)\n",
    "\n",
    "# print(f\"train_set: {train_set+[47]}\")\n",
    "# print(f\"test_set: {test_set}\")\n",
    "# P.smoothing = None\n",
    "# # P.p=1 UNCOMMENT WHEN YOU RESET IT FOR THE REST OF EXPS\n",
    "\n",
    "\n",
    "# preprocessing_method=None\n",
    "# time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "# train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/Manhattan-dist_{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "# test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/Manhattan-dist_{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "# with open(train_file_name, \"w\") as f:\n",
    "#     f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "#     f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "# with open(test_file_name, \"w\") as f:\n",
    "#     f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "#     f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# EER_df_train_dict={}\n",
    "# EER_df_test_dict={}\n",
    "\n",
    "\n",
    "# dfList_dict={\n",
    "#             \"dfList_exp1\": raw_dfList_exp1,\n",
    "#             \"dfList_exp2\": raw_dfList_exp2,\n",
    "#             \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "#             \"dfList_exp2_user_47\": raw_dfList_exp2_user_47\n",
    "# }\n",
    "\n",
    "# test_dict_key=DASH_MACRO_NUM\n",
    "# EER_df_train_dict[test_dict_key] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "#                                                                                                 extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "#                                                                                                 param_dist=param_dist)\n",
    "\n",
    "# with open(train_file_name, \"a\") as f:\n",
    "#     f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "#     f.write(f\"\\ntest_dict_key: {test_dict_key}\\n\")\n",
    "#     f.write(EER_df_train_dict[test_dict_key].to_string())\n",
    "\n",
    "        \n",
    "# mean_EER_train_dict={}\n",
    "# for key in EER_df_train_dict:\n",
    "#     mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "# #-------\n",
    "# train_lst = list(mean_EER_train_dict.items())\n",
    "# train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "# with open(train_file_name, \"a\") as f:\n",
    "#     f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "#     f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "#     for i in range(len(train_lst)):\n",
    "#         f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "# min_key=train_lst[0][0]\n",
    "# EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "#                                                                                    extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "#                                                                                    best_param_df=EER_df_train_dict[min_key])\n",
    "# with open(test_file_name, \"a\") as f:\n",
    "#     f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "#     f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "#     f.write(EER_df_test_dict[min_key].to_string())\n",
    "# #-------\n",
    "# #-------\n",
    "# key_column= [\"cut_off_freq\"]\n",
    "# EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "# eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "# EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "# eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "# #-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. No Smoothing\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "P.smoothing = None\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=None\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": raw_dfList_exp1,\n",
    "            \"dfList_exp2\": raw_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": raw_dfList_exp2_user_47\n",
    "}\n",
    "\n",
    "\n",
    "# #-----CV_FOLD-------\n",
    "# for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "#     train_set, test_set = THREE_FOLD_CV[cv_fold_idx]\n",
    "#     print(f\"train_set: {train_set}\")\n",
    "#     print(f\"test_set: {test_set}\")\n",
    "    \n",
    "    \n",
    "#     test_dict_key=DASH_MACRO_NUM\n",
    "#     EER_df_train_dict[test_dict_key] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "#                                                                                                extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "#                                                                                                param_dist=param_dist)\n",
    "\n",
    "#     with open(train_file_name, \"a\") as f:\n",
    "#         f.write(\"\\n\" + \"-\"*22 + f\"Training results for cv_fold_idx: {cv_fold_idx}\" + \"-\"*22 + \"\\n\")  \n",
    "#         f.write(f\"\\nTest_dict_key: {test_dict_key}\\n\")\n",
    "#         f.write(EER_df_train_dict[test_dict_key].to_string())\n",
    "\n",
    "\n",
    "\n",
    "#     min_key=test_dict_key\n",
    "#     EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "#                                                                                        extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "#                                                                                        best_param_df=EER_df_train_dict[min_key])\n",
    "#     with open(test_file_name, \"a\") as f:\n",
    "#         f.write(\"\\n\" + \"-\"*22 + f\"Testing results for cv_fold_idx: {cv_fold_idx}\" + \"-\"*22 + \"\\n\")\n",
    "#         f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "#         f.write(EER_df_test_dict[min_key].to_string())\n",
    "#     #-------\n",
    "#     #-------\n",
    "#     key_column= [\"cut_off_freq\"]\n",
    "#     EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "#     eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "#     EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df_cv{cv_fold_idx}.json')\n",
    "#     eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df_cv{cv_fold_idx}.json')\n",
    "#     #-------\n",
    "    \n",
    "min_key=DASH_MACRO_NUM\n",
    "key_column= [\"cut_off_freq\"]\n",
    "#-----CV_FOLD-------\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_OCSVM_LOF_IF(\\\n",
    "                                cv_fold_idx=cv_fold_idx, \n",
    "                                cv_sets=THREE_FOLD_CV, \n",
    "                                dfList_dict=dfList_dict, \n",
    "                                window_size_lst=WINDOW_SIZE_LST, \n",
    "                                exp_config=P, \n",
    "                                extract_features_dict=EXTRACT_WACA_features_DICT, \n",
    "                                overlap=OVERLAP, \n",
    "                                param_dist=param_dist, \n",
    "                                train_file_name=train_file_name, \n",
    "                                test_file_name=test_file_name, \n",
    "                                preprocessing_params=min_key, \n",
    "                                key_column=key_column,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Butterworth frequency Cut-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Naive Approach\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "P.smoothing = \"Butterworth\"\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "old_test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/old_{time_of_execution}_{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "\n",
    "\n",
    "EER_df_test_dict={}\n",
    "    \n",
    "old_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "old_test_hyperparameters_df = pd.read_json(old_test_hyperparameters_file_name)\n",
    "old_test_hyperparameters_df.to_json(f'{old_test_file_name[:-4]}_raw_df.json')\n",
    "\n",
    "P.cut_off_freq=old_test_hyperparameters_df[\"cut_off_freq\"][0]\n",
    "min_key=P.cut_off_freq\n",
    "print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "\n",
    "ffted_dfList_exp1 = get_ffted_dfList(raw_dfList_exp1, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "ffted_dfList_exp1_user_47 = get_ffted_dfList(raw_dfList_exp1_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": ffted_dfList_exp1,\n",
    "            \"dfList_exp2\": ffted_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": ffted_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": ffted_dfList_exp2_user_47\n",
    "}\n",
    "    \n",
    "\n",
    "# #-------\n",
    "\n",
    "\n",
    "key_column= [\"cut_off_freq\"]\n",
    "#-----CV_FOLD-------\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_OCSVM_LOF_IF(\\\n",
    "                                cv_fold_idx=cv_fold_idx, \n",
    "                                cv_sets=THREE_FOLD_CV, \n",
    "                                dfList_dict=dfList_dict, \n",
    "                                window_size_lst=WINDOW_SIZE_LST, \n",
    "                                exp_config=P, \n",
    "                                extract_features_dict=EXTRACT_WACA_features_DICT, \n",
    "                                overlap=OVERLAP, \n",
    "                                param_dist=param_dist, \n",
    "                                train_file_name=train_file_name, \n",
    "                                test_file_name=test_file_name, \n",
    "                                preprocessing_params=min_key, \n",
    "                                key_column=key_column,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butterworth\"\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "old_test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/old_{time_of_execution}_{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "\n",
    "\n",
    "EER_df_test_dict={}\n",
    "    \n",
    "old_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "old_test_hyperparameters_df = pd.read_json(old_test_hyperparameters_file_name)\n",
    "old_test_hyperparameters_df.to_json(f'{old_test_file_name[:-4]}_raw_df.json')\n",
    "\n",
    "P.cut_off_freq=old_test_hyperparameters_df[\"cut_off_freq\"][0]\n",
    "min_key=P.cut_off_freq\n",
    "print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "\n",
    "ffted_dfList_exp1 = get_ffted_dfList(raw_dfList_exp1, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "ffted_dfList_exp1_user_47 = get_ffted_dfList(raw_dfList_exp1_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": ffted_dfList_exp1,\n",
    "            \"dfList_exp2\": ffted_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": ffted_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": ffted_dfList_exp2_user_47\n",
    "}\n",
    "    \n",
    "\n",
    "# #-------\n",
    "\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"cut_off_freq: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "# #-------\n",
    "# #-------\n",
    "key_column= [\"cut_off_freq\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "# #-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butterworth\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{train_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{test_file_name}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for cut_off_freq in tqdm(CUT_OFF_FREQ_RANGE):\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "\n",
    "    \n",
    "    ffted_dfList_exp1 = get_ffted_dfList(raw_dfList_exp1, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    \n",
    "    ffted_dfList_exp1_user_47 = get_ffted_dfList(raw_dfList_exp1_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": ffted_dfList_exp1,\n",
    "                \"dfList_exp2\": ffted_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": ffted_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": ffted_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.cut_off_freq] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                                    extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                                    param_dist=param_dist)\n",
    "        \n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq: {P.cut_off_freq}\\n\")\n",
    "        f.write(EER_df_train_dict[P.cut_off_freq].to_string())\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "P.smoothing = \"Butterworth\"\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "\n",
    "time_of_execution = time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "old_test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/old_{time_of_execution}_{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "\n",
    "\n",
    "\n",
    "P.Butter_per_win_argdict={\n",
    "    \"filter_order\": P.filter_order,\n",
    "    \"cut_off_freq\": None,\n",
    "    \"sampling_freq\": P.sampling_freq,\n",
    "    \"filtfilt\": P.filtfilt,\n",
    "    }\n",
    "\n",
    "\n",
    "EER_df_test_dict={}\n",
    "\n",
    "    \n",
    "old_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "old_test_hyperparameters_df = pd.read_json(old_test_hyperparameters_file_name)\n",
    "old_test_hyperparameters_df.to_json(f'{old_test_file_name[:-4]}_raw_df.json')\n",
    "\n",
    "P.cut_off_freq=old_test_hyperparameters_df[\"cut_off_freq\"][0]\n",
    "\n",
    "P.Butter_per_win_argdict[\"cut_off_freq\"]=P.cut_off_freq\n",
    "min_key=P.cut_off_freq\n",
    "print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "\n",
    "\n",
    "ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": raw_dfList_exp1,\n",
    "            \"dfList_exp2\": ffted_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": ffted_dfList_exp2_user_47\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "key_column= [\"cut_off_freq\"]\n",
    "#-----CV_FOLD-------\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_OCSVM_LOF_IF(\\\n",
    "                                cv_fold_idx=cv_fold_idx, \n",
    "                                cv_sets=THREE_FOLD_CV, \n",
    "                                dfList_dict=dfList_dict, \n",
    "                                window_size_lst=WINDOW_SIZE_LST, \n",
    "                                exp_config=P, \n",
    "                                extract_features_dict=EXTRACT_WACA_features_DICT, \n",
    "                                overlap=OVERLAP, \n",
    "                                param_dist=param_dist, \n",
    "                                train_file_name=train_file_name, \n",
    "                                test_file_name=test_file_name, \n",
    "                                preprocessing_params=min_key, \n",
    "                                key_column=key_column,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butterworth\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P.Butter_per_win_argdict={\n",
    "    \"filter_order\": P.filter_order,\n",
    "    \"cut_off_freq\": None,\n",
    "    \"sampling_freq\": P.sampling_freq,\n",
    "    \"filtfilt\": P.filtfilt,\n",
    "    }\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for cut_off_freq in tqdm(CUT_OFF_FREQ_RANGE):\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.Butter_per_win_argdict[\"cut_off_freq\"]=cut_off_freq\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "\n",
    "    \n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    \n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": ffted_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": ffted_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.cut_off_freq] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                                    extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                                    param_dist=param_dist)\n",
    "        \n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq: {P.cut_off_freq}\\n\")\n",
    "        f.write(EER_df_train_dict[P.cut_off_freq].to_string())\n",
    "\n",
    "\n",
    "\n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Butterworth frequency Cut-off + EMA span\n",
    "## 2.1 Naive Approach\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_2)\n",
    "P.smoothing = \"Butter+EMA\"\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "old_test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/old_{time_of_execution}_{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_test_dict={}\n",
    "\n",
    "    \n",
    "old_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "old_test_hyperparameters_df = pd.read_json(old_test_hyperparameters_file_name)\n",
    "old_test_hyperparameters_df.to_json(f'{old_test_file_name[:-4]}_raw_df.json')\n",
    "\n",
    "P.cut_off_freq=old_test_hyperparameters_df[\"cut_off_freq\"][0]\n",
    "P.span=old_test_hyperparameters_df[\"EMA_span\"][0]\n",
    "\n",
    "min_key= P.cut_off_freq, P.span\n",
    "print(f\"cut_off_freq: {P.cut_off_freq}, EMA span: {P.span}\")\n",
    "\n",
    "\n",
    "ffted_dfList_exp1 = get_ffted_dfList(raw_dfList_exp1, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "EMAed_dfList_exp1 = get_EMAed_dfList(ffted_dfList_exp1, span=P.span)\n",
    "EMAed_dfList_exp2 = get_EMAed_dfList(ffted_dfList_exp2, span=P.span)\n",
    "\n",
    "ffted_dfList_exp1_user_47 = get_ffted_dfList(raw_dfList_exp1_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "EMAed_dfList_exp1_user_47 = get_EMAed_dfList(ffted_dfList_exp1_user_47, span=P.span)\n",
    "EMAed_dfList_exp2_user_47 = get_EMAed_dfList(ffted_dfList_exp2_user_47, span=P.span)\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": EMAed_dfList_exp1,\n",
    "            \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": EMAed_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "key_column= [\"cut_off_freq\", \"EMA_span\"]\n",
    "#-----CV_FOLD-------\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_OCSVM_LOF_IF(\\\n",
    "                                cv_fold_idx=cv_fold_idx, \n",
    "                                cv_sets=THREE_FOLD_CV, \n",
    "                                dfList_dict=dfList_dict, \n",
    "                                window_size_lst=WINDOW_SIZE_LST, \n",
    "                                exp_config=P, \n",
    "                                extract_features_dict=EXTRACT_WACA_features_DICT, \n",
    "                                overlap=OVERLAP, \n",
    "                                param_dist=param_dist, \n",
    "                                train_file_name=train_file_name, \n",
    "                                test_file_name=test_file_name, \n",
    "                                preprocessing_params=min_key, \n",
    "                                key_column=key_column,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_2)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butter+EMA\"\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "indices = list(range(1, 50))\n",
    "mesh = np.array(np.meshgrid(indices, indices))\n",
    "index_pairs = mesh.T.reshape(-1, 2)\n",
    "\n",
    "print(f\"total cut_off_span_pairs: {index_pairs.shape}, choice_num: {CHOICE_NUM_PAIRS}\")\n",
    "cut_off_span_pairs = index_pairs[np.random.choice(index_pairs.shape[0], size=CHOICE_NUM_PAIRS, replace=False), :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for key_pair in tqdm(cut_off_span_pairs):\n",
    "    \n",
    "    key_pair = tuple(key_pair)\n",
    "    cut_off_freq, span = key_pair[0], key_pair[1]\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.span=span\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "    print(f\"span: {P.span}\")\n",
    "\n",
    "    \n",
    "    ffted_dfList_exp1 = get_ffted_dfList(raw_dfList_exp1, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    EMAed_dfList_exp1 = get_EMAed_dfList(ffted_dfList_exp1, span=P.span)\n",
    "    EMAed_dfList_exp2 = get_EMAed_dfList(ffted_dfList_exp2, span=P.span)\n",
    "    \n",
    "    ffted_dfList_exp1_user_47 = get_ffted_dfList(raw_dfList_exp1_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    EMAed_dfList_exp1_user_47 = get_EMAed_dfList(ffted_dfList_exp1_user_47, span=P.span)\n",
    "    EMAed_dfList_exp2_user_47 = get_EMAed_dfList(ffted_dfList_exp2_user_47, span=P.span)\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": EMAed_dfList_exp1,\n",
    "                \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": EMAed_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[key_pair] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                      extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                      param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq, span: {key_pair}\\n\")\n",
    "        f.write(EER_df_train_dict[key_pair].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\", \"EMA_span\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_2)\n",
    "P.smoothing = \"Butter+EMA\"\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "old_test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/old_{time_of_execution}_{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "\n",
    "\n",
    "\n",
    "P.Butter_per_win_argdict={\n",
    "    \"filter_order\": P.filter_order,\n",
    "    \"cut_off_freq\": None,\n",
    "    \"sampling_freq\": P.sampling_freq,\n",
    "    \"filtfilt\": P.filtfilt,\n",
    "    }\n",
    "\n",
    "EER_df_test_dict={}\n",
    "\n",
    "\n",
    "old_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "old_test_hyperparameters_df = pd.read_json(old_test_hyperparameters_file_name)\n",
    "old_test_hyperparameters_df.to_json(f'{old_test_file_name[:-4]}_raw_df.json')\n",
    "\n",
    "P.cut_off_freq=old_test_hyperparameters_df[\"cut_off_freq\"][0]\n",
    "P.span=old_test_hyperparameters_df[\"EMA_span\"][0]\n",
    "\n",
    "P.Butter_per_win_argdict[\"cut_off_freq\"]=P.cut_off_freq\n",
    "P.EMA_per_win_span=P.span\n",
    "\n",
    "min_key= P.cut_off_freq, P.span\n",
    "print(f\"cut_off_freq: {P.cut_off_freq}, EMA span: {P.span}\")\n",
    "\n",
    "\n",
    "ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "EMAed_dfList_exp2 = get_EMAed_dfList(ffted_dfList_exp2, span=P.span)\n",
    "\n",
    "ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "EMAed_dfList_exp2_user_47 = get_EMAed_dfList(ffted_dfList_exp2_user_47, span=P.span)\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": raw_dfList_exp1,\n",
    "            \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "key_column= [\"cut_off_freq\", \"EMA_span\"]\n",
    "#-----CV_FOLD-------\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_OCSVM_LOF_IF(\\\n",
    "                                cv_fold_idx=cv_fold_idx, \n",
    "                                cv_sets=THREE_FOLD_CV, \n",
    "                                dfList_dict=dfList_dict, \n",
    "                                window_size_lst=WINDOW_SIZE_LST, \n",
    "                                exp_config=P, \n",
    "                                extract_features_dict=EXTRACT_WACA_features_DICT, \n",
    "                                overlap=OVERLAP, \n",
    "                                param_dist=param_dist, \n",
    "                                train_file_name=train_file_name, \n",
    "                                test_file_name=test_file_name, \n",
    "                                preprocessing_params=min_key, \n",
    "                                key_column=key_column,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_2)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butter+EMA\"\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "indices = list(range(1, 50))\n",
    "mesh = np.array(np.meshgrid(indices, indices))\n",
    "index_pairs = mesh.T.reshape(-1, 2)\n",
    "\n",
    "print(f\"total cut_off_span_pairs: {index_pairs.shape}, choice_num: {CHOICE_NUM_PAIRS}\")\n",
    "cut_off_span_pairs = index_pairs[np.random.choice(index_pairs.shape[0], size=CHOICE_NUM_PAIRS, replace=False), :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P.Butter_per_win_argdict={\n",
    "    \"filter_order\": P.filter_order,\n",
    "    \"cut_off_freq\": None,\n",
    "    \"sampling_freq\": P.sampling_freq,\n",
    "    \"filtfilt\": P.filtfilt,\n",
    "    }\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for key_pair in tqdm(cut_off_span_pairs):\n",
    "    key_pair = tuple(key_pair)\n",
    "    cut_off_freq, span = key_pair[0], key_pair[1]\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.Butter_per_win_argdict[\"cut_off_freq\"]=cut_off_freq\n",
    "    \n",
    "    \n",
    "    P.span=span\n",
    "    P.EMA_per_win_span=span\n",
    "\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "    print(f\"span: {P.span}\")\n",
    "\n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    EMAed_dfList_exp2 = get_EMAed_dfList(ffted_dfList_exp2, span=P.span)\n",
    "    \n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    EMAed_dfList_exp2_user_47 = get_EMAed_dfList(ffted_dfList_exp2_user_47, span=P.span)\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[key_pair] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                      extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                      param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq, span: {key_pair}\\n\")\n",
    "        f.write(EER_df_train_dict[key_pair].to_string())\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\", \"EMA_span\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. EMA span\n",
    "## 3.1 Naive Approach\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "P.smoothing = \"EMA\"\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "old_test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/old_{time_of_execution}_{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "\n",
    "\n",
    "\n",
    "EER_df_test_dict={}\n",
    "    \n",
    "old_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "old_test_hyperparameters_df = pd.read_json(old_test_hyperparameters_file_name)\n",
    "old_test_hyperparameters_df.to_json(f'{old_test_file_name[:-4]}_raw_df.json')\n",
    "\n",
    "P.span=old_test_hyperparameters_df[\"EMA_span\"][0]\n",
    "min_key=P.span\n",
    "print(f\"EMA span: {P.span}\")\n",
    "\n",
    "\n",
    "EMAed_dfList_exp1 = get_EMAed_dfList(raw_dfList_exp1, span=P.span)\n",
    "EMAed_dfList_exp2 = get_EMAed_dfList(raw_dfList_exp2, span=P.span)\n",
    "\n",
    "EMAed_dfList_exp1_user_47 = get_EMAed_dfList(raw_dfList_exp1_user_47, span=P.span)\n",
    "EMAed_dfList_exp2_user_47 = get_EMAed_dfList(raw_dfList_exp2_user_47, span=P.span)\n",
    "\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": EMAed_dfList_exp1,\n",
    "            \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": EMAed_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "key_column= [\"EMA_span\"]\n",
    "#-----CV_FOLD-------\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_OCSVM_LOF_IF(\\\n",
    "                                cv_fold_idx=cv_fold_idx, \n",
    "                                cv_sets=THREE_FOLD_CV, \n",
    "                                dfList_dict=dfList_dict, \n",
    "                                window_size_lst=WINDOW_SIZE_LST, \n",
    "                                exp_config=P, \n",
    "                                extract_features_dict=EXTRACT_WACA_features_DICT, \n",
    "                                overlap=OVERLAP, \n",
    "                                param_dist=param_dist, \n",
    "                                train_file_name=train_file_name, \n",
    "                                test_file_name=test_file_name, \n",
    "                                preprocessing_params=min_key, \n",
    "                                key_column=key_column,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"EMA\"\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for span in tqdm(EMA_SPAN_RANGE):\n",
    "    P.span=span\n",
    "    print(f\"EMA span: {P.span}\")\n",
    "\n",
    "    \n",
    "    EMAed_dfList_exp1 = get_EMAed_dfList(raw_dfList_exp1, span=P.span)\n",
    "    EMAed_dfList_exp2 = get_EMAed_dfList(raw_dfList_exp2, span=P.span)\n",
    "    \n",
    "    EMAed_dfList_exp1_user_47 = get_EMAed_dfList(raw_dfList_exp1_user_47, span=P.span)\n",
    "    EMAed_dfList_exp2_user_47 = get_EMAed_dfList(raw_dfList_exp2_user_47, span=P.span)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": EMAed_dfList_exp1,\n",
    "                \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": EMAed_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.span] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                    extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                    param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\EMA span: {P.span}\\n\")\n",
    "        f.write(EER_df_train_dict[P.span].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"EMA_span\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "P.smoothing = \"EMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "old_test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/old_{time_of_execution}_{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "    \n",
    "\n",
    "EER_df_test_dict={}\n",
    "    \n",
    "old_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "old_test_hyperparameters_df = pd.read_json(old_test_hyperparameters_file_name)\n",
    "old_test_hyperparameters_df.to_json(f'{old_test_file_name[:-4]}_raw_df.json')\n",
    "\n",
    "P.span=old_test_hyperparameters_df[\"EMA_span\"][0]\n",
    "P.EMA_per_win_span=P.span\n",
    "min_key=P.span\n",
    "print(f\"EMA span: {P.span}\")\n",
    "\n",
    "\n",
    "EMAed_dfList_exp2 = get_EMAed_dfList(raw_dfList_exp2, span=P.span)\n",
    "\n",
    "EMAed_dfList_exp2_user_47 = get_EMAed_dfList(raw_dfList_exp2_user_47, span=P.span)\n",
    "\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": raw_dfList_exp1,\n",
    "            \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "key_column= [\"EMA_span\"]\n",
    "#-----CV_FOLD-------\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_OCSVM_LOF_IF(\\\n",
    "                                cv_fold_idx=cv_fold_idx, \n",
    "                                cv_sets=THREE_FOLD_CV, \n",
    "                                dfList_dict=dfList_dict, \n",
    "                                window_size_lst=WINDOW_SIZE_LST, \n",
    "                                exp_config=P, \n",
    "                                extract_features_dict=EXTRACT_WACA_features_DICT, \n",
    "                                overlap=OVERLAP, \n",
    "                                param_dist=param_dist, \n",
    "                                train_file_name=train_file_name, \n",
    "                                test_file_name=test_file_name, \n",
    "                                preprocessing_params=min_key, \n",
    "                                key_column=key_column,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"EMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for span in tqdm(EMA_SPAN_RANGE):\n",
    "    P.span=span\n",
    "    print(f\"EMA span: {P.span}\")\n",
    "\n",
    "    P.EMA_per_win_span=P.span\n",
    "\n",
    "    EMAed_dfList_exp2 = get_EMAed_dfList(raw_dfList_exp2, span=P.span)\n",
    "    \n",
    "    EMAed_dfList_exp2_user_47 = get_EMAed_dfList(raw_dfList_exp2_user_47, span=P.span)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": EMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": EMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.span] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                    extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                    param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\EMA span: {P.span}\\n\")\n",
    "        f.write(EER_df_train_dict[P.span].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"EMA_span\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SMA winsize\n",
    "## 4.1 Naive Approach\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "P.smoothing = \"SMA\"\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "old_test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/old_{time_of_execution}_{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "\n",
    "\n",
    "EER_df_test_dict={}\n",
    "\n",
    "old_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "old_test_hyperparameters_df = pd.read_json(old_test_hyperparameters_file_name)\n",
    "old_test_hyperparameters_df.to_json(f'{old_test_file_name[:-4]}_raw_df.json')\n",
    "\n",
    "P.winsize=old_test_hyperparameters_df[\"SMA_winsize\"][0]\n",
    "min_key=P.winsize\n",
    "print(f\"SMA winsize: {P.winsize}\")\n",
    "\n",
    "\n",
    "SMAed_dfList_exp1 = get_SMAed_dfList(raw_dfList_exp1, winsize=P.winsize)\n",
    "SMAed_dfList_exp2 = get_SMAed_dfList(raw_dfList_exp2, winsize=P.winsize)\n",
    "\n",
    "SMAed_dfList_exp1_user_47 = get_SMAed_dfList(raw_dfList_exp1_user_47, winsize=P.winsize)\n",
    "SMAed_dfList_exp2_user_47 = get_SMAed_dfList(raw_dfList_exp2_user_47, winsize=P.winsize)\n",
    "\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": SMAed_dfList_exp1,\n",
    "            \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": SMAed_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "}\n",
    "    \n",
    "\n",
    "\n",
    "key_column= [\"SMA_winsize\"]\n",
    "#-----CV_FOLD-------\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_OCSVM_LOF_IF(\\\n",
    "                                cv_fold_idx=cv_fold_idx, \n",
    "                                cv_sets=THREE_FOLD_CV, \n",
    "                                dfList_dict=dfList_dict, \n",
    "                                window_size_lst=WINDOW_SIZE_LST, \n",
    "                                exp_config=P, \n",
    "                                extract_features_dict=EXTRACT_WACA_features_DICT, \n",
    "                                overlap=OVERLAP, \n",
    "                                param_dist=param_dist, \n",
    "                                train_file_name=train_file_name, \n",
    "                                test_file_name=test_file_name, \n",
    "                                preprocessing_params=min_key, \n",
    "                                key_column=key_column,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"SMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for winsize in tqdm(SMA_WINSIZE_RANGE):\n",
    "    P.winsize=winsize\n",
    "    print(f\"SMA winsize: {P.winsize}\")\n",
    "\n",
    "\n",
    "    SMAed_dfList_exp1 = get_SMAed_dfList(raw_dfList_exp1, winsize=P.winsize)\n",
    "    SMAed_dfList_exp2 = get_SMAed_dfList(raw_dfList_exp2, winsize=P.winsize)\n",
    "    \n",
    "    SMAed_dfList_exp1_user_47 = get_SMAed_dfList(raw_dfList_exp1_user_47, winsize=P.winsize)\n",
    "    SMAed_dfList_exp2_user_47 = get_SMAed_dfList(raw_dfList_exp2_user_47, winsize=P.winsize)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": SMAed_dfList_exp1,\n",
    "                \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": SMAed_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.winsize] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                       extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                       param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\SMA winsize: {P.winsize}\\n\")\n",
    "        f.write(EER_df_train_dict[P.winsize].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"SMA_winsize\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "P.smoothing = \"SMA\"\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "old_test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/old_{time_of_execution}_{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "\n",
    "\n",
    "EER_df_test_dict={}\n",
    "\n",
    "old_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "old_test_hyperparameters_df = pd.read_json(old_test_hyperparameters_file_name)\n",
    "old_test_hyperparameters_df.to_json(f'{old_test_file_name[:-4]}_raw_df.json')\n",
    "\n",
    "P.winsize=old_test_hyperparameters_df[\"SMA_winsize\"][0]\n",
    "P.SMA_per_win_winsize=P.winsize\n",
    "min_key=P.winsize\n",
    "print(f\"SMA winsize: {P.winsize}\")\n",
    "\n",
    "\n",
    "SMAed_dfList_exp2 = get_SMAed_dfList(raw_dfList_exp2, winsize=P.winsize)\n",
    "\n",
    "SMAed_dfList_exp2_user_47 = get_SMAed_dfList(raw_dfList_exp2_user_47, winsize=P.winsize)\n",
    "\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": raw_dfList_exp1,\n",
    "            \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "}\n",
    "\n",
    "\n",
    "key_column= [\"SMA_winsize\"]\n",
    "#-----CV_FOLD-------\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_OCSVM_LOF_IF(\\\n",
    "                                cv_fold_idx=cv_fold_idx, \n",
    "                                cv_sets=THREE_FOLD_CV, \n",
    "                                dfList_dict=dfList_dict, \n",
    "                                window_size_lst=WINDOW_SIZE_LST, \n",
    "                                exp_config=P, \n",
    "                                extract_features_dict=EXTRACT_WACA_features_DICT, \n",
    "                                overlap=OVERLAP, \n",
    "                                param_dist=param_dist, \n",
    "                                train_file_name=train_file_name, \n",
    "                                test_file_name=test_file_name, \n",
    "                                preprocessing_params=min_key, \n",
    "                                key_column=key_column,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"SMA\"\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for winsize in tqdm(SMA_WINSIZE_RANGE):\n",
    "    P.winsize=winsize\n",
    "    P.SMA_per_win_winsize=P.winsize\n",
    "\n",
    "    print(f\"SMA winsize: {P.winsize}\")\n",
    "\n",
    "\n",
    "    SMAed_dfList_exp2 = get_SMAed_dfList(raw_dfList_exp2, winsize=P.winsize)\n",
    "    \n",
    "    SMAed_dfList_exp2_user_47 = get_SMAed_dfList(raw_dfList_exp2_user_47, winsize=P.winsize)\n",
    "\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[P.winsize] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                       extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                       param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\SMA winsize: {P.winsize}\\n\")\n",
    "        f.write(EER_df_train_dict[P.winsize].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"SMA_winsize\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Butterworth frequency Cut-off + SMA winsize\n",
    "## 5.1 Naive Approach\n",
    "### Optimizing and Testin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_5)\n",
    "P.smoothing = \"Butter+SMA\"\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "old_test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/old_{time_of_execution}_{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "\n",
    "\n",
    "EER_df_test_dict={}\n",
    "\n",
    "    \n",
    "old_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "old_test_hyperparameters_df = pd.read_json(old_test_hyperparameters_file_name)\n",
    "old_test_hyperparameters_df.to_json(f'{old_test_file_name[:-4]}_raw_df.json')\n",
    "\n",
    "P.winsize=old_test_hyperparameters_df[\"SMA_winsize\"][0]\n",
    "P.cut_off_freq=old_test_hyperparameters_df[\"cut_off_freq\"][0]\n",
    "\n",
    "min_key= P.cut_off_freq, P.winsize\n",
    "print(f\"cut_off_freq: {P.cut_off_freq}, winsize: {P.winsize}\")\n",
    "\n",
    "\n",
    "ffted_dfList_exp1 = get_ffted_dfList(raw_dfList_exp1, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "SMAed_dfList_exp1 = get_SMAed_dfList(ffted_dfList_exp1, winsize=P.winsize)\n",
    "SMAed_dfList_exp2 = get_SMAed_dfList(ffted_dfList_exp2, winsize=P.winsize)\n",
    "\n",
    "ffted_dfList_exp1_user_47 = get_ffted_dfList(raw_dfList_exp1_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "SMAed_dfList_exp1_user_47 = get_SMAed_dfList(ffted_dfList_exp1_user_47, winsize=P.winsize)\n",
    "SMAed_dfList_exp2_user_47 = get_SMAed_dfList(ffted_dfList_exp2_user_47, winsize=P.winsize)\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": SMAed_dfList_exp1,\n",
    "            \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": SMAed_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "key_column= [\"cut_off_freq\", \"SMA_winsize\"]\n",
    "#-----CV_FOLD-------\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_OCSVM_LOF_IF(\\\n",
    "                                cv_fold_idx=cv_fold_idx, \n",
    "                                cv_sets=THREE_FOLD_CV, \n",
    "                                dfList_dict=dfList_dict, \n",
    "                                window_size_lst=WINDOW_SIZE_LST, \n",
    "                                exp_config=P, \n",
    "                                extract_features_dict=EXTRACT_WACA_features_DICT, \n",
    "                                overlap=OVERLAP, \n",
    "                                param_dist=param_dist, \n",
    "                                train_file_name=train_file_name, \n",
    "                                test_file_name=test_file_name, \n",
    "                                preprocessing_params=min_key, \n",
    "                                key_column=key_column,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_5)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butter+SMA\"\n",
    "\n",
    "\n",
    "preprocessing_method=\"Naive\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "indices = list(range(1, 50))\n",
    "mesh = np.array(np.meshgrid(indices, indices))\n",
    "index_pairs = mesh.T.reshape(-1, 2)\n",
    "\n",
    "print(f\"total cut_off_winsize_pairs: {index_pairs.shape}, choice_num: {CHOICE_NUM_PAIRS}\")\n",
    "cut_off_winsize_pairs = index_pairs[np.random.choice(index_pairs.shape[0], size=CHOICE_NUM_PAIRS, replace=False), :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for key_pair in tqdm(cut_off_winsize_pairs):\n",
    "    \n",
    "    key_pair = tuple(key_pair)\n",
    "    cut_off_freq, winsize = key_pair[0], key_pair[1]\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.winsize=winsize\n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "    print(f\"winsize: {P.winsize}\")\n",
    "\n",
    "    \n",
    "    ffted_dfList_exp1 = get_ffted_dfList(raw_dfList_exp1, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    SMAed_dfList_exp1 = get_SMAed_dfList(ffted_dfList_exp1, winsize=P.winsize)\n",
    "    SMAed_dfList_exp2 = get_SMAed_dfList(ffted_dfList_exp2, winsize=P.winsize)\n",
    "    \n",
    "    ffted_dfList_exp1_user_47 = get_ffted_dfList(raw_dfList_exp1_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "\n",
    "    SMAed_dfList_exp1_user_47 = get_SMAed_dfList(ffted_dfList_exp1_user_47, winsize=P.winsize)\n",
    "    SMAed_dfList_exp2_user_47 = get_SMAed_dfList(ffted_dfList_exp2_user_47, winsize=P.winsize)\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": SMAed_dfList_exp1,\n",
    "                \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": SMAed_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[key_pair] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                      extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                      param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq, winsize: {key_pair}\\n\")\n",
    "        f.write(EER_df_train_dict[key_pair].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\", \"SMA_winsize\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Realworld Approach (per unknown window application of filter)\n",
    "### Optimizing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_5)\n",
    "P.smoothing = \"Butter+SMA\"\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/clip={P.scaler_clip}-Minkowski_P={P.p}-Smoothing={P.smoothing}-Prep={preprocessing_method}-EER_df_test_dict.txt\"\n",
    "old_test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/old_{time_of_execution}_{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "\n",
    "\n",
    "P.Butter_per_win_argdict={\n",
    "    \"filter_order\": P.filter_order,\n",
    "    \"cut_off_freq\": None,\n",
    "    \"sampling_freq\": P.sampling_freq,\n",
    "    \"filtfilt\": P.filtfilt,\n",
    "    }\n",
    "\n",
    "\n",
    "EER_df_test_dict={}\n",
    "\n",
    "    \n",
    "old_test_hyperparameters_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict_raw_df.json\"\n",
    "old_test_hyperparameters_df = pd.read_json(old_test_hyperparameters_file_name)\n",
    "old_test_hyperparameters_df.to_json(f'{old_test_file_name[:-4]}_raw_df.json')\n",
    "\n",
    "P.winsize=old_test_hyperparameters_df[\"SMA_winsize\"][0]\n",
    "P.cut_off_freq=old_test_hyperparameters_df[\"cut_off_freq\"][0]\n",
    "\n",
    "P.Butter_per_win_argdict[\"cut_off_freq\"]=P.cut_off_freq\n",
    "P.SMA_per_win_winsize=P.winsize\n",
    "\n",
    "min_key= P.cut_off_freq, P.winsize\n",
    "print(f\"cut_off_freq: {P.cut_off_freq}, winsize: {P.winsize}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "SMAed_dfList_exp2 = get_SMAed_dfList(ffted_dfList_exp2, winsize=P.winsize)\n",
    "\n",
    "ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "SMAed_dfList_exp2_user_47 = get_SMAed_dfList(ffted_dfList_exp2_user_47, winsize=P.winsize)\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": raw_dfList_exp1,\n",
    "            \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "}\n",
    "\n",
    "\n",
    "key_column= [\"cut_off_freq\", \"SMA_winsize\"]\n",
    "#-----CV_FOLD-------\n",
    "for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "    process_cv_fold_OCSVM_LOF_IF(\\\n",
    "                                cv_fold_idx=cv_fold_idx, \n",
    "                                cv_sets=THREE_FOLD_CV, \n",
    "                                dfList_dict=dfList_dict, \n",
    "                                window_size_lst=WINDOW_SIZE_LST, \n",
    "                                exp_config=P, \n",
    "                                extract_features_dict=EXTRACT_WACA_features_DICT, \n",
    "                                overlap=OVERLAP, \n",
    "                                param_dist=param_dist, \n",
    "                                train_file_name=train_file_name, \n",
    "                                test_file_name=test_file_name, \n",
    "                                preprocessing_params=min_key, \n",
    "                                key_column=key_column,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_experiment_params(exp_config=P, exp_num=EXP_NUM_5)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "P.smoothing = \"Butter+SMA\"\n",
    "\n",
    "\n",
    "preprocessing_method=\"Realworld-per_unknown_window\"\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/{P.smoothing}_Mean_EER_{preprocessing_method}_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "indices = list(range(1, 50))\n",
    "mesh = np.array(np.meshgrid(indices, indices))\n",
    "index_pairs = mesh.T.reshape(-1, 2)\n",
    "\n",
    "print(f\"total cut_off_winsize_pairs: {index_pairs.shape}, choice_num: {CHOICE_NUM_PAIRS}\")\n",
    "cut_off_winsize_pairs = index_pairs[np.random.choice(index_pairs.shape[0], size=CHOICE_NUM_PAIRS, replace=False), :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P.Butter_per_win_argdict={\n",
    "    \"filter_order\": P.filter_order,\n",
    "    \"cut_off_freq\": None,\n",
    "    \"sampling_freq\": P.sampling_freq,\n",
    "    \"filtfilt\": P.filtfilt,\n",
    "    }\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "\n",
    "for key_pair in tqdm(cut_off_winsize_pairs):\n",
    "    \n",
    "    key_pair = tuple(key_pair)\n",
    "    cut_off_freq, winsize = key_pair[0], key_pair[1]\n",
    "    P.cut_off_freq=cut_off_freq\n",
    "    P.Butter_per_win_argdict[\"cut_off_freq\"]=P.cut_off_freq\n",
    "    P.winsize=winsize\n",
    "    P.SMA_per_win_winsize=P.winsize\n",
    "    \n",
    "    print(f\"cut_off_freq: {P.cut_off_freq}\")\n",
    "    print(f\"winsize: {P.winsize}\")\n",
    "\n",
    "    \n",
    "    ffted_dfList_exp2 = get_ffted_dfList(raw_dfList_exp2, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    SMAed_dfList_exp2 = get_SMAed_dfList(ffted_dfList_exp2, winsize=P.winsize)\n",
    "    \n",
    "    ffted_dfList_exp2_user_47 = get_ffted_dfList(raw_dfList_exp2_user_47, cut_off_freq=P.cut_off_freq, filter_order=P.filter_order, sampling_freq=P.sampling_freq, filtfilt=P.filtfilt)\n",
    "    SMAed_dfList_exp2_user_47 = get_SMAed_dfList(ffted_dfList_exp2_user_47, winsize=P.winsize)\n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": SMAed_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": SMAed_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[key_pair] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                      extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                      param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\cut_off_freq, winsize: {key_pair}\\n\")\n",
    "        f.write(EER_df_train_dict[key_pair].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"cut_off_freq\", \"SMA_winsize\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. The effect of Varying Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reseting experiment params successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap: 0.5700000000000001\n",
      "train_set: {0, 1, 2, 3, 4, 5, 6, 9, 10, 12, 14, 15, 16, 18, 19, 22, 23, 24, 25, 28}\n",
      "test_set: {7, 8, 11, 13, 17, 20, 21, 26, 27, 29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 125\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  17.206865056417882\n",
      "MakeWACAXExpDicUnknown Time:  113.9773410698399\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 25191.02it/s]\n",
      "\n",
      "\n",
      " 33%|      | 1/3 [01:04<02:08, 64.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 17597.25it/s]\n",
      "\n",
      "\n",
      " 67%|   | 2/3 [02:13<01:06, 66.95s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 27648.68it/s]\n",
      "\n",
      "\n",
      "100%|| 3/3 [03:33<00:00, 71.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 21129.99it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:06<00:27,  6.85s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 13313.14it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:13<00:20,  6.79s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 22017.34it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:20<00:13,  6.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 20/20 [00:00<00:00, 20555.28it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:27<00:06,  6.81s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 20/20 [00:00<00:00, 16548.84it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:34<00:00,  6.80s/it]\u001b[A\u001b[A\n",
      "\n",
      " 11%|         | 1/9 [06:21<50:49, 381.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  8.347721057012677\n",
      "MakeWACAXExpDicUnknown Time:  57.64509401470423\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 16650.67it/s]\n",
      "\n",
      "\n",
      " 33%|      | 1/3 [00:27<00:54, 27.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 21896.65it/s]\n",
      "\n",
      "\n",
      " 67%|   | 2/3 [00:55<00:27, 27.81s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 24321.86it/s]\n",
      "\n",
      "\n",
      "100%|| 3/3 [01:23<00:00, 27.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 17443.56it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:03<00:13,  3.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 23831.27it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:06<00:09,  3.30s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 12877.81it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:09<00:06,  3.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 20/20 [00:00<00:00, 15589.31it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:13<00:03,  3.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 20/20 [00:00<00:00, 10724.38it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:16<00:00,  3.30s/it]\u001b[A\u001b[A\n",
      "\n",
      " 22%|       | 2/9 [09:08<29:49, 255.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 500\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  4.76131768617779\n",
      "MakeWACAXExpDicUnknown Time:  30.098450399935246\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 10619.84it/s]\n",
      "\n",
      "\n",
      " 33%|      | 1/3 [00:13<00:27, 13.95s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 14222.80it/s]\n",
      "\n",
      "\n",
      " 67%|   | 2/3 [00:28<00:14, 14.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 22262.76it/s]\n",
      "\n",
      "\n",
      "100%|| 3/3 [00:42<00:00, 14.25s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 11780.10it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:01<00:06,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 22556.08it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:03<00:04,  1.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 16591.39it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:04<00:03,  1.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 20/20 [00:00<00:00, 22598.62it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:06<00:01,  1.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 20/20 [00:00<00:00, 24686.90it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:08<00:00,  1.62s/it]\u001b[A\u001b[A\n",
      "\n",
      " 33%|      | 3/9 [10:35<17:50, 178.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 750\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  3.995430397801101\n",
      "MakeWACAXExpDicUnknown Time:  17.510494044981897\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 21675.99it/s]\n",
      "\n",
      "\n",
      " 33%|      | 1/3 [00:08<00:17,  8.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 26487.55it/s]\n",
      "\n",
      "\n",
      " 67%|   | 2/3 [00:16<00:08,  8.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 18682.87it/s]\n",
      "\n",
      "\n",
      "100%|| 3/3 [00:26<00:00,  8.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 21586.74it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:01<00:04,  1.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 15221.57it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:02<00:03,  1.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 16597.96it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:03<00:02,  1.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 20/20 [00:00<00:00, 25078.05it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:04<00:01,  1.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 20/20 [00:00<00:00, 16693.75it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:05<00:00,  1.17s/it]\u001b[A\u001b[A\n",
      "\n",
      " 44%|     | 4/9 [11:29<10:46, 129.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1000\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  3.0701964888721704\n",
      "MakeWACAXExpDicUnknown Time:  15.606147618032992\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 19226.70it/s]\n",
      "\n",
      "\n",
      " 33%|      | 1/3 [00:07<00:15,  7.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 20641.26it/s]\n",
      "\n",
      "\n",
      " 67%|   | 2/3 [00:18<00:09,  9.25s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 19991.92it/s]\n",
      "\n",
      "\n",
      "100%|| 3/3 [00:29<00:00,  9.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 19654.66it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:01<00:04,  1.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 21525.81it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:02<00:03,  1.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 20126.22it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:03<00:02,  1.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 20/20 [00:00<00:00, 17832.93it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:04<00:01,  1.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 20/20 [00:00<00:00, 16396.81it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:05<00:00,  1.09s/it]\u001b[A\u001b[A\n",
      "\n",
      " 56%|    | 5/9 [12:23<06:48, 102.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  2.6148091182112694\n",
      "MakeWACAXExpDicUnknown Time:  13.915659005753696\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 22751.85it/s]\n",
      "\n",
      "\n",
      " 33%|      | 1/3 [00:09<00:19,  9.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 23269.37it/s]\n",
      "\n",
      "\n",
      " 67%|   | 2/3 [00:20<00:10, 10.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 16601.24it/s]\n",
      "\n",
      "\n",
      "100%|| 3/3 [00:30<00:00, 10.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 20049.25it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:03,  1.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 15161.05it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:01<00:02,  1.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 15265.89it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:02<00:01,  1.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 20/20 [00:00<00:00, 19517.47it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:03<00:00,  1.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 20/20 [00:00<00:00, 24223.53it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:04<00:00,  1.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|   | 6/9 [13:15<04:15, 85.03s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1500\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  2.4741414673626423\n",
      "MakeWACAXExpDicUnknown Time:  15.233926696702838\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 16011.85it/s]\n",
      "\n",
      "\n",
      " 33%|      | 1/3 [00:06<00:13,  6.74s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 18872.01it/s]\n",
      "\n",
      "\n",
      " 67%|   | 2/3 [00:14<00:07,  7.30s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 25661.08it/s]\n",
      "\n",
      "\n",
      "100%|| 3/3 [00:20<00:00,  6.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 24973.53it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:03,  1.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 16581.55it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:01<00:02,  1.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 27971.35it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:02<00:01,  1.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 20/20 [00:00<00:00, 24230.53it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:03<00:00,  1.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 20/20 [00:00<00:00, 26124.60it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:04<00:00,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|  | 7/9 [13:59<02:23, 71.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1750\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  2.1934791123494506\n",
      "MakeWACAXExpDicUnknown Time:  11.04429057892412\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 21258.51it/s]\n",
      "\n",
      "\n",
      " 33%|      | 1/3 [00:08<00:17,  8.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 26852.14it/s]\n",
      "\n",
      "\n",
      " 67%|   | 2/3 [00:18<00:09,  9.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 22714.89it/s]\n",
      "\n",
      "\n",
      "100%|| 3/3 [00:25<00:00,  8.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 18546.56it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:03,  1.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 26181.67it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:01<00:02,  1.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 15953.99it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:02<00:01,  1.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 20/20 [00:00<00:00, 17719.92it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:03<00:00,  1.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 20/20 [00:00<00:00, 16390.40it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:04<00:00,  1.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%| | 8/9 [14:43<01:02, 62.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 2000\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 20\n",
      "len(exp2_df_user_set_dict): 20\n",
      "MakeWACAXExpDicOwner Time:  2.1472991723567247\n",
      "MakeWACAXExpDicUnknown Time:  9.717151207849383\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 22151.06it/s]\n",
      "\n",
      "\n",
      " 33%|      | 1/3 [00:08<00:16,  8.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 19590.40it/s]\n",
      "\n",
      "\n",
      " 67%|   | 2/3 [00:17<00:08,  8.81s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 19315.24it/s]\n",
      "\n",
      "\n",
      "100%|| 3/3 [00:25<00:00,  8.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 21692.81it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:03,  1.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 23211.42it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:01<00:02,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 14490.60it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:02<00:01,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 20/20 [00:00<00:00, 15221.57it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:03<00:00,  1.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 20/20 [00:00<00:00, 26083.98it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:04<00:00,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|| 9/9 [15:25<00:00, 102.81s/it][A\n",
      "\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 125\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 394\n",
      "len_exp2_user_47: 338\n",
      "MakeWACAXExpDicOwner Time:  6.580041076056659\n",
      "MakeWACAXExpDicUnknown Time:  40.59521010797471\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 8172.84it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:01<00:04,  1.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 9068.77it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:02<00:03,  1.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 8139.54it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:03<00:02,  1.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 9149.88it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:04<00:01,  1.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 9704.54it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:05<00:00,  1.12s/it]\u001b[A\u001b[A\n",
      "\n",
      " 11%|         | 1/9 [00:54<07:14, 54.31s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 194\n",
      "len_exp2_user_47: 166\n",
      "MakeWACAXExpDicOwner Time:  3.637112201191485\n",
      "MakeWACAXExpDicUnknown Time:  31.080385024659336\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 11719.21it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:03,  1.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 11573.69it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:01<00:02,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 14126.99it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:02<00:01,  1.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 13464.86it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:03<00:00,  1.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 7326.30it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:04<00:00,  1.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|       | 2/9 [01:34<05:20, 45.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 500\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 96\n",
      "len_exp2_user_47: 82\n",
      "MakeWACAXExpDicOwner Time:  2.27609524410218\n",
      "MakeWACAXExpDicUnknown Time:  18.020002147182822\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 10295.30it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:01,  2.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 11125.47it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:01,  2.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 14873.42it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:01<00:00,  2.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 10798.93it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:02<00:00,  1.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 11104.86it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:03<00:00,  1.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|      | 3/9 [01:58<03:35, 35.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 750\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 63\n",
      "len_exp2_user_47: 54\n",
      "MakeWACAXExpDicOwner Time:  3.905243707820773\n",
      "MakeWACAXExpDicUnknown Time:  25.31637274939567\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 7980.03it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:01<00:04,  1.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 14339.50it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:02<00:03,  1.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 11287.15it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:03<00:02,  1.25s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 9747.39it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:04<00:01,  1.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 8130.07it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:06<00:00,  1.22s/it]\u001b[A\u001b[A\n",
      "\n",
      " 44%|     | 4/9 [02:34<03:00, 36.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1000\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 47\n",
      "len_exp2_user_47: 40\n",
      "MakeWACAXExpDicOwner Time:  3.8883924279361963\n",
      "MakeWACAXExpDicUnknown Time:  21.136013022623956\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 5283.16it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:01<00:04,  1.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 10300.35it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:02<00:03,  1.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 6561.80it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:03<00:02,  1.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 8505.99it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:04<00:01,  1.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 8907.00it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:05<00:00,  1.13s/it]\u001b[A\u001b[A\n",
      "\n",
      " 56%|    | 5/9 [03:06<02:17, 34.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 37\n",
      "len_exp2_user_47: 32\n",
      "MakeWACAXExpDicOwner Time:  3.62245090585202\n",
      "MakeWACAXExpDicUnknown Time:  6.2483347076922655\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 8662.34it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:01,  3.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 8442.64it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:01,  2.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 9267.13it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:01<00:00,  2.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 8709.10it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:01<00:00,  2.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 7812.08it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:02<00:00,  2.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|   | 6/9 [03:18<01:21, 27.08s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1500\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 31\n",
      "len_exp2_user_47: 26\n",
      "MakeWACAXExpDicOwner Time:  1.4797136653214693\n",
      "MakeWACAXExpDicUnknown Time:  8.833459904417396\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 7214.15it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:01,  2.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 11069.69it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:01<00:01,  1.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 9974.56it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:01<00:01,  1.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 9631.01it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:02<00:00,  1.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 11848.32it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:02<00:00,  1.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|  | 7/9 [03:32<00:45, 22.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1750\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 26\n",
      "len_exp2_user_47: 22\n",
      "MakeWACAXExpDicOwner Time:  2.011291641741991\n",
      "MakeWACAXExpDicUnknown Time:  12.076810038648546\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 9834.24it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:02,  1.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 7918.26it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:01<00:02,  1.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 8335.26it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:02<00:01,  1.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 204.87it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:02<00:00,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 12969.40it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:03<00:00,  1.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%| | 8/9 [03:50<00:21, 21.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 2000\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 9\n",
      "len(exp2_df_user_set_dict): 9\n",
      "len(X_exp1_dict_user_47[47]): 23\n",
      "len_exp2_user_47: 19\n",
      "MakeWACAXExpDicOwner Time:  1.759348377585411\n",
      "MakeWACAXExpDicUnknown Time:  7.5285140024498105\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 8633.81it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:01,  3.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 10315.55it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:00,  3.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 8420.61it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:01<00:00,  2.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 2696.61it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:02<00:00,  1.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 5143.86it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:03<00:00,  1.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|| 9/9 [04:03<00:00, 27.04s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set: {0, 2, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 20, 21, 23, 26, 27, 29}\n",
      "test_set: {1, 3, 5, 6, 16, 19, 22, 24, 25, 28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 125\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 394\n",
      "len_exp2_user_47: 338\n",
      "MakeWACAXExpDicOwner Time:  63.70132458303124\n",
      "MakeWACAXExpDicUnknown Time:  231.16149803437293\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 18102.30it/s]\n",
      "\n",
      "\n",
      " 33%|      | 1/3 [02:40<05:21, 160.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 20550.24it/s]\n",
      "\n",
      "\n",
      " 67%|   | 2/3 [06:19<03:14, 194.93s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 15090.14it/s]\n",
      "\n",
      "\n",
      "100%|| 3/3 [10:35<00:00, 211.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 12246.14it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:16<01:04, 16.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 15363.75it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:29<00:43, 14.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 18176.83it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:40<00:25, 12.89s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 20/20 [00:00<00:00, 18220.26it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:56<00:14, 14.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 20/20 [00:00<00:00, 13709.12it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [01:07<00:00, 13.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 11%|         | 1/9 [16:48<2:14:30, 1008.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 194\n",
      "len_exp2_user_47: 166\n",
      "MakeWACAXExpDicOwner Time:  19.360864027403295\n",
      "MakeWACAXExpDicUnknown Time:  122.35135687049478\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 9434.94it/s]\n",
      "\n",
      "\n",
      " 33%|      | 1/3 [00:57<01:55, 57.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 15352.50it/s]\n",
      "\n",
      "\n",
      " 67%|   | 2/3 [01:36<00:46, 46.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 21242.36it/s]\n",
      "\n",
      "\n",
      "100%|| 3/3 [02:23<00:00, 47.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 19143.33it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:07<00:30,  7.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 16939.84it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:14<00:21,  7.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 12442.31it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:21<00:14,  7.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 20/20 [00:00<00:00, 21421.37it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:27<00:06,  6.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 20/20 [00:00<00:00, 12599.29it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:34<00:00,  6.85s/it]\u001b[A\u001b[A\n",
      "\n",
      " 22%|       | 2/9 [22:10<1:10:31, 604.52s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 500\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 96\n",
      "len_exp2_user_47: 82\n",
      "MakeWACAXExpDicOwner Time:  13.503367246128619\n",
      "MakeWACAXExpDicUnknown Time:  104.30809350218624\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 20223.26it/s]\n",
      "\n",
      "\n",
      " 33%|      | 1/3 [00:39<01:19, 39.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 17001.64it/s]\n",
      "\n",
      "\n",
      " 67%|   | 2/3 [01:27<00:44, 44.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 12068.20it/s]\n",
      "\n",
      "\n",
      "100%|| 3/3 [02:28<00:00, 49.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 19445.08it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:05<00:22,  5.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 13003.58it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:11<00:16,  5.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 24001.74it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:16<00:11,  5.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 20/20 [00:00<00:00, 23994.87it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:19<00:04,  4.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 20/20 [00:00<00:00, 22483.54it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:22<00:00,  4.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 33%|      | 3/9 [27:00<46:06, 461.08s/it]  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 750\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 63\n",
      "len_exp2_user_47: 54\n",
      "MakeWACAXExpDicOwner Time:  8.60724260378629\n",
      "MakeWACAXExpDicUnknown Time:  49.406780424527824\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 9733.82it/s]\n",
      "\n",
      "\n",
      " 33%|      | 1/3 [00:24<00:48, 24.25s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 22813.73it/s]\n",
      "\n",
      "\n",
      " 67%|   | 2/3 [00:46<00:23, 23.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 15732.57it/s]\n",
      "\n",
      "\n",
      "100%|| 3/3 [01:12<00:00, 24.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 17060.42it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:02<00:08,  2.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 15321.66it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:04<00:05,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 19355.35it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:05<00:03,  1.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 20/20 [00:00<00:00, 15595.11it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:06<00:01,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 20/20 [00:00<00:00, 20400.31it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:08<00:00,  1.62s/it]\u001b[A\u001b[A\n",
      "\n",
      " 44%|     | 4/9 [29:20<27:50, 334.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1000\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 47\n",
      "len_exp2_user_47: 40\n",
      "MakeWACAXExpDicOwner Time:  3.330050602555275\n",
      "MakeWACAXExpDicUnknown Time:  27.881362412124872\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 19723.98it/s]\n",
      "\n",
      "\n",
      " 33%|      | 1/3 [00:22<00:44, 22.29s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 13920.69it/s]\n",
      "\n",
      "\n",
      " 67%|   | 2/3 [00:46<00:23, 23.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 15414.57it/s]\n",
      "\n",
      "\n",
      "100%|| 3/3 [01:09<00:00, 23.30s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 20034.89it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:02<00:08,  2.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 14543.36it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:04<00:07,  2.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 16203.61it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:06<00:04,  2.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 20/20 [00:00<00:00, 22221.48it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:09<00:02,  2.30s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 20/20 [00:00<00:00, 23134.61it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:11<00:00,  2.20s/it]\u001b[A\u001b[A\n",
      "\n",
      " 56%|    | 5/9 [31:12<16:56, 254.21s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 37\n",
      "len_exp2_user_47: 32\n",
      "MakeWACAXExpDicOwner Time:  5.320429272018373\n",
      "MakeWACAXExpDicUnknown Time:  33.536738839931786\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 23051.96it/s]\n",
      "\n",
      "\n",
      " 33%|      | 1/3 [00:16<00:33, 16.88s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 15926.73it/s]\n",
      "\n",
      "\n",
      " 67%|   | 2/3 [00:31<00:15, 15.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 17130.10it/s]\n",
      "\n",
      "\n",
      "100%|| 3/3 [00:52<00:00, 17.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 21437.79it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:01<00:07,  1.81s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 13516.93it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:03<00:05,  1.73s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 20296.66it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:04<00:03,  1.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 20/20 [00:00<00:00, 13895.33it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:06<00:01,  1.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 20/20 [00:00<00:00, 17844.31it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:08<00:00,  1.80s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|   | 6/9 [32:53<10:06, 202.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1500\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 31\n",
      "len_exp2_user_47: 26\n",
      "MakeWACAXExpDicOwner Time:  6.421873078681529\n",
      "MakeWACAXExpDicUnknown Time:  33.8069309014827\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 12420.21it/s]\n",
      "\n",
      "\n",
      " 33%|      | 1/3 [00:21<00:42, 21.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 14755.69it/s]\n",
      "\n",
      "\n",
      " 67%|   | 2/3 [00:42<00:21, 21.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 25458.60it/s]\n",
      "\n",
      "\n",
      "100%|| 3/3 [01:04<00:00, 21.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 17056.95it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:04<00:19,  4.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 10388.37it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:09<00:13,  4.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 20184.33it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:12<00:08,  4.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 20/20 [00:00<00:00, 22869.71it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:17<00:04,  4.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 20/20 [00:00<00:00, 20867.18it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:22<00:00,  4.42s/it]\u001b[A\u001b[A\n",
      "\n",
      " 78%|  | 7/9 [35:01<05:55, 177.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1750\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 26\n",
      "len_exp2_user_47: 22\n",
      "MakeWACAXExpDicOwner Time:  9.99949957150966\n",
      "MakeWACAXExpDicUnknown Time:  47.005152026191354\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 15213.29it/s]\n",
      "\n",
      "\n",
      " 33%|      | 1/3 [00:35<01:10, 35.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 16516.26it/s]\n",
      "\n",
      "\n",
      " 67%|   | 2/3 [01:09<00:34, 34.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 22127.69it/s]\n",
      "\n",
      "\n",
      "100%|| 3/3 [01:29<00:00, 29.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 13041.99it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:02<00:09,  2.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 15471.43it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:04<00:07,  2.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 16871.70it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:07<00:04,  2.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 20/20 [00:00<00:00, 21492.72it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:09<00:02,  2.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 20/20 [00:00<00:00, 23790.72it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:11<00:00,  2.24s/it]\u001b[A\u001b[A\n",
      "\n",
      " 89%| | 8/9 [37:40<02:51, 171.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 2000\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 23\n",
      "len_exp2_user_47: 19\n",
      "MakeWACAXExpDicOwner Time:  3.846177263185382\n",
      "MakeWACAXExpDicUnknown Time:  25.950133375823498\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 13792.52it/s]\n",
      "\n",
      "\n",
      " 33%|      | 1/3 [00:28<00:56, 28.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 15821.59it/s]\n",
      "\n",
      "\n",
      " 67%|   | 2/3 [01:05<00:33, 33.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 10909.88it/s]\n",
      "\n",
      "\n",
      "100%|| 3/3 [01:33<00:00, 31.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 20/20 [00:00<00:00, 15372.20it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:04<00:18,  4.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 20/20 [00:00<00:00, 20595.65it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:09<00:13,  4.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 20/20 [00:00<00:00, 20836.09it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:13<00:08,  4.48s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 20/20 [00:00<00:00, 18695.36it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:17<00:04,  4.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 20/20 [00:00<00:00, 18094.50it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:21<00:00,  4.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|| 9/9 [40:06<00:00, 267.41s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 125\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  17.922545137815177\n",
      "MakeWACAXExpDicUnknown Time:  73.47802932932973\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 9825.03it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:01<00:07,  1.95s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 8015.10it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:03<00:05,  1.94s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 10315.55it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:05<00:03,  1.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 8916.46it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:06<00:01,  1.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 12111.76it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:07<00:00,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 11%|         | 1/9 [01:45<14:07, 105.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  4.1964131006971\n",
      "MakeWACAXExpDicUnknown Time:  31.93518601357937\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 8893.77it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:03,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 8062.87it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:01<00:02,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 9273.28it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:02<00:01,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 7833.96it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:03<00:00,  1.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 13005.59it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:03<00:00,  1.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|       | 2/9 [02:26<07:52, 67.55s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 500\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  3.1785020008683205\n",
      "MakeWACAXExpDicUnknown Time:  17.939189607277513\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 10694.30it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:02,  1.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 5960.36it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:01<00:01,  1.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 8365.19it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:01<00:01,  1.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 7788.87it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:02<00:00,  1.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 11994.01it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:02<00:00,  1.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|      | 3/9 [02:50<04:46, 47.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 750\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  1.9936387790367007\n",
      "MakeWACAXExpDicUnknown Time:  14.874823888763785\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 8451.15it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:02,  1.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 8528.47it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:01,  2.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 10190.24it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:01<00:00,  2.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 16384.00it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:01<00:00,  2.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 7355.85it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:02<00:00,  2.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|     | 4/9 [03:10<03:03, 36.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1000\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  1.9378726482391357\n",
      "MakeWACAXExpDicUnknown Time:  12.126787717454135\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 9532.51it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:01,  2.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 8884.36it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:01,  2.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 13050.11it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:01<00:00,  2.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 11403.76it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:01<00:00,  2.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 9984.06it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:02<00:00,  2.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|    | 5/9 [03:26<01:57, 29.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1250\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  1.8972551068291068\n",
      "MakeWACAXExpDicUnknown Time:  8.967617691494524\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 4255.15it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:01,  2.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 10951.19it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:01,  2.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 4980.77it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:01<00:00,  2.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 16403.22it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:01<00:00,  2.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 9057.02it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:01<00:00,  2.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|   | 6/9 [03:40<01:11, 23.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1500\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  2.3729104064404964\n",
      "MakeWACAXExpDicUnknown Time:  8.454355494119227\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 10363.98it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:01,  2.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 9293.83it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:01,  2.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 12122.27it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:01<00:00,  2.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 7430.12it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:01<00:00,  2.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 8182.41it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:01<00:00,  2.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|  | 7/9 [03:52<00:40, 20.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 1750\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  1.2418386796489358\n",
      "MakeWACAXExpDicUnknown Time:  6.773800622671843\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 6980.04it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:01,  2.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 12729.30it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:01,  2.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 6137.41it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:01<00:00,  2.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 7027.99it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:01<00:00,  2.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 10594.35it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:02<00:00,  2.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%| | 8/9 [04:03<00:17, 17.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 2000\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 10\n",
      "len(exp2_df_user_set_dict): 10\n",
      "MakeWACAXExpDicOwner Time:  1.2952119894325733\n",
      "MakeWACAXExpDicUnknown Time:  5.796888489276171\n",
      "Done extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 0: 100%|| 10/10 [00:00<00:00, 10060.70it/s]\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [00:00<00:01,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 1: 100%|| 10/10 [00:00<00:00, 10675.25it/s]\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [00:00<00:01,  2.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 2: 100%|| 10/10 [00:00<00:00, 7873.67it/s]\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [00:00<00:00,  3.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 3: 100%|| 10/10 [00:00<00:00, 8937.36it/s]\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [00:01<00:00,  3.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "run: 4: 100%|| 10/10 [00:00<00:00, 16339.32it/s]\n",
      "\n",
      "\n",
      "100%|| 5/5 [00:01<00:00,  3.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|| 9/9 [04:12<00:00, 28.04s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set: {1, 3, 5, 6, 7, 8, 11, 13, 16, 17, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29}\n",
      "test_set: {0, 2, 4, 9, 10, 12, 14, 15, 18, 23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 125\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n",
      "len(X_exp1_dict_user_47[47]): 394\n",
      "len_exp2_user_47: 338\n",
      "MakeWACAXExpDicOwner Time:  32.48487980943173\n"
     ]
    }
   ],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "\n",
    "dfList_dict={\n",
    "            \"dfList_exp1\": raw_dfList_exp1,\n",
    "            \"dfList_exp2\": raw_dfList_exp2,\n",
    "            \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "            \"dfList_exp2_user_47\": raw_dfList_exp2_user_47\n",
    "}\n",
    "\n",
    "for overlap in tqdm(OVERLAP_EXP_RANGE[14:]):\n",
    "\n",
    "    train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/overlap={overlap}_Mean_EER_df_train_dict.txt\"\n",
    "    test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/overlap={overlap}_Mean_EER_df_test_dict.txt\"\n",
    "    \n",
    "    with open(train_file_name, \"w\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    with open(test_file_name, \"w\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "\n",
    "    overlap*=0.01\n",
    "    print(f\"overlap: {overlap}\")\n",
    "    max_window_size=2000\n",
    "    step_width = int(max_window_size * (1-overlap))\n",
    "    max_num_windows=min(len(getIndices(sampleSize=max_window_size, step=step_width, numSamplePoints=P.num_sample_points_per_exp)), param_dist['model__n_neighbors'][-1]+1)\n",
    "    n_neighbors_params = np.arange(1, max_num_windows) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    key_column= [\"overlap\"]\n",
    "    #-----CV_FOLD-------\n",
    "    for cv_fold_idx in range(len(THREE_FOLD_CV)):\n",
    "        process_cv_fold_OCSVM_LOF_IF(\\\n",
    "                                    cv_fold_idx=cv_fold_idx, \n",
    "                                    cv_sets=THREE_FOLD_CV, \n",
    "                                    dfList_dict=dfList_dict, \n",
    "                                    window_size_lst=WINDOW_SIZE_LST, \n",
    "                                    exp_config=P, \n",
    "                                    extract_features_dict=EXTRACT_WACA_features_DICT, \n",
    "                                    overlap=overlap, \n",
    "                                    param_dist=param_dist, \n",
    "                                    train_file_name=train_file_name, \n",
    "                                    test_file_name=test_file_name, \n",
    "                                    preprocessing_params=overlap, \n",
    "                                    key_column=key_column,\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reseting experiment params successful!\n",
      "train_set: [7, 24, 8, 11, 13, 19, 28, 21, 26, 3, 20, 22, 6, 25, 16, 1, 17, 27, 5, 47]\n",
      "test_set: [0, 12, 14, 9, 18, 23, 2, 15, 10, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 125\n",
      "WACA preprocessing\n",
      "len(exp1_df_user_set_dict): 19\n",
      "len(exp2_df_user_set_dict): 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process LokyProcess-81:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 451, in _process_worker\n",
      "    _process_reference_size = _get_memory_usage(pid, force_gc=True)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 117, in _get_memory_usage\n",
      "    gc.collect()\n",
      "KeyboardInterrupt\n",
      "  0%|          | 0/9 [00:15<?, ?it/s]\n",
      "  0%|          | 0/11 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m n_neighbors_params \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, max_num_windows) \n\u001b[1;32m     28\u001b[0m dfList_dict\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdfList_exp1\u001b[39m\u001b[38;5;124m\"\u001b[39m: raw_dfList_exp1,\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdfList_exp2\u001b[39m\u001b[38;5;124m\"\u001b[39m: raw_dfList_exp2,\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdfList_exp1_user_47\u001b[39m\u001b[38;5;124m\"\u001b[39m: raw_dfList_exp1_user_47,\n\u001b[1;32m     32\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdfList_exp2_user_47\u001b[39m\u001b[38;5;124m\"\u001b[39m: raw_dfList_exp2_user_47\n\u001b[1;32m     33\u001b[0m }\n\u001b[0;32m---> 36\u001b[0m EER_df_train_dict[overlap] \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_EER_different_window_sizes_train_OCSVM_IF_LOF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfList_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size_lst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWINDOW_SIZE_LST\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                                                                                 \u001b[49m\u001b[43mextract_features_func_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEXTRACT_WACA_features_DICT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOVERLAP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                                                                                 \u001b[49m\u001b[43mparam_dist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_dist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(train_file_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     40\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m22\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/tmp/ipykernel_2296350/1337764819.py:446\u001b[0m, in \u001b[0;36mcalculate_EER_different_window_sizes_train_OCSVM_IF_LOF\u001b[0;34m(dfList_dict, window_size_lst, train_set, exp_config, extract_features_func_dict, overlap, param_dist, verbose)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m#         X_train_exp1_dict, X_train_exp2_dict, fitted_scaler_train_exp2_dict=get_raw_windows(dfList_exp1, dfList_exp2, window_size, step_width, train_set, exp_config.scaler, \u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;66;03m#                                                                                             exp_config.num_sample_points_per_exp, \u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m#                                                                                             EMA_per_win_span=exp_config.EMA_per_win_span, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \n\u001b[1;32m    441\u001b[0m \u001b[38;5;66;03m# ###added----\u001b[39;00m\n\u001b[1;32m    444\u001b[0m         extract_features_func\u001b[38;5;241m=\u001b[39mextract_features_func_dict[window_size]\n\u001b[0;32m--> 446\u001b[0m         X_exp_train_dic \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_X_exp_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfList_exp1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdfList_exp1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mdfList_exp2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdfList_exp2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mdfList_exp1_user_47\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdfList_exp1_user_47\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mdfList_exp2_user_47\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdfList_exp2_user_47\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mstep_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43muser_idx_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mexp_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mextract_features_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_features_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m         \u001b[38;5;66;03m# X_exp_train_dic = extract_features_func(X_train_exp1_dict, X_train_exp2_dict, fitted_scaler_classifier_exp2_dic=fitted_scaler_train_exp2_dict, \u001b[39;00m\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;66;03m#                                         fitted_min_max_scaler_exp2_dict=fitted_min_max_scaler_train_exp2_dict,#added\u001b[39;00m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;66;03m#                                         scaler_clip=exp_config.scaler_clip)\u001b[39;00m\n\u001b[1;32m    462\u001b[0m         result_dict \u001b[38;5;241m=\u001b[39m parallel_find_best_hyperparams(X_exp_train_dic, param_dist, exp_config, cores\u001b[38;5;241m=\u001b[39mexp_config\u001b[38;5;241m.\u001b[39mtrain_cores, verbose\u001b[38;5;241m=\u001b[39mverbose)\n",
      "File \u001b[0;32m/tmp/ipykernel_2296350/1337764819.py:342\u001b[0m, in \u001b[0;36mcalculate_X_exp_dict\u001b[0;34m(dfList_exp1, dfList_exp2, dfList_exp1_user_47, dfList_exp2_user_47, window_size, step_width, user_idx_set, exp_config, extract_features_func, verbose)\u001b[0m\n\u001b[1;32m    321\u001b[0m         X_exp1_dict_user_47, X_exp2_dict_user_47,\\\n\u001b[1;32m    322\u001b[0m         fitted_scaler_exp2_dict_user_47\u001b[38;5;241m=\u001b[39mget_raw_windows_user_47(dfList_exp1_user_47\u001b[38;5;241m=\u001b[39mdfList_exp1_user_47, \n\u001b[1;32m    323\u001b[0m                                                                 dfList_exp2_user_47\u001b[38;5;241m=\u001b[39mdfList_exp2_user_47, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m                                                                 Butter_per_win_argdict\u001b[38;5;241m=\u001b[39mexp_config\u001b[38;5;241m.\u001b[39mButter_per_win_argdict, \n\u001b[1;32m    331\u001b[0m                                                                 verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m    333\u001b[0m         X_exp1_dict, X_exp2_dict, fitted_scaler_exp2_dict\u001b[38;5;241m=\u001b[39mappend_user_47_to_data(X_exp1_dict\u001b[38;5;241m=\u001b[39mX_exp1_dict, \n\u001b[1;32m    334\u001b[0m                                                                                  X_exp2_dict\u001b[38;5;241m=\u001b[39mX_exp2_dict, \n\u001b[1;32m    335\u001b[0m                                                                                  fitted_scaler_exp2_dict\u001b[38;5;241m=\u001b[39mfitted_scaler_exp2_dict, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    339\u001b[0m                                                                                  fitted_scaler_exp2_dict_user_47\u001b[38;5;241m=\u001b[39mfitted_scaler_exp2_dict_user_47, \n\u001b[1;32m    340\u001b[0m                                                                                  verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m--> 342\u001b[0m     X_features_dict \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_exp1_dic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_exp1_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mX_exp2_dic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_exp2_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mfitted_scaler_classifier_exp2_dic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfitted_scaler_exp2_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mscaler_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler_clip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_features_dict\n",
      "File \u001b[0;32m/tmp/ipykernel_2296350/1016762008.py:6\u001b[0m, in \u001b[0;36mextract_WACA_features\u001b[0;34m(X_exp1_dic, X_exp2_dic, fitted_scaler_classifier_exp2_dic, scaler_clip, verbose)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_WACA_features\u001b[39m(X_exp1_dic, X_exp2_dic, fitted_scaler_classifier_exp2_dic, scaler_clip, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      5\u001b[0m     start \u001b[38;5;241m=\u001b[39m timeit\u001b[38;5;241m.\u001b[39mdefault_timer()\n\u001b[0;32m----> 6\u001b[0m     X_exp_dic \u001b[38;5;241m=\u001b[39m \u001b[43mMakeWACAXExpDicOwner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_exp2_dic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler_clip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMinMaxScaler\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     stop \u001b[38;5;241m=\u001b[39m timeit\u001b[38;5;241m.\u001b[39mdefault_timer()\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMakeWACAXExpDicOwner Time: \u001b[39m\u001b[38;5;124m'\u001b[39m, stop \u001b[38;5;241m-\u001b[39m start)\n",
      "File \u001b[0;32m/tmp/ipykernel_2296350/2385131817.py:9\u001b[0m, in \u001b[0;36mMakeWACAXExpDicOwner\u001b[0;34m(X_exp_reg_df_dict, scaler_clip, scaler_type)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mMakeWACAXExpDicOwner\u001b[39m(X_exp_reg_df_dict, scaler_clip, scaler_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMinMaxScaler\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124;03m'''k\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    ???\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    return \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    X_exp_dic\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    dfLists are of the same size.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMakeXExpDicOwner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_exp_reg_df_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler_clip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMinMaxScaler\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWACA_feature_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_extractor_transformer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform_user_WACA_windows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/ipykernel_2296350/1902086158.py:146\u001b[0m, in \u001b[0;36mMakeXExpDicOwner\u001b[0;34m(X_exp_reg_df_dict, scaler_clip, scaler_type, feature_extractor, feature_extractor_transformer, verbose)\u001b[0m\n\u001b[1;32m    142\u001b[0m X_exp_dic \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloky\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 146\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_features_and_scale_for_parallel_call_Owner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_exp_reg_df_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_clip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_extractor_transformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mowner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mowner\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX_exp_reg_df_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m    151\u001b[0m     owner \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mowner\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init_experiment_params(exp_config=P)\n",
    "\n",
    "print(f\"train_set: {train_set+[47]}\")\n",
    "print(f\"test_set: {test_set}\")\n",
    "\n",
    "\n",
    "\n",
    "time_of_execution = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "train_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/overlap_Mean_EER_df_train_dict.txt\"\n",
    "test_file_name=f\"{FINAL_EXP_RESULTS_PATH}/{EXP_PATH_NAME}/overlap_Mean_EER_df_test_dict.txt\"\n",
    "with open(train_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "with open(test_file_name, \"w\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*40 + f\"{time_of_execution}\" + \"-\"*40 + \"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "EER_df_train_dict={}\n",
    "EER_df_test_dict={}\n",
    "for overlap in tqdm(OVERLAP_EXP_RANGE):\n",
    "    overlap*=0.01\n",
    "    max_window_size=2000\n",
    "    step_width = int(max_window_size * (1-overlap))\n",
    "    max_num_windows=min(len(getIndices(sampleSize=max_window_size, step=step_width, numSamplePoints=P.num_sample_points_per_exp)), param_dist['model__n_neighbors'][-1]+1)\n",
    "    n_neighbors_params = np.arange(1, max_num_windows) \n",
    "    \n",
    "    dfList_dict={\n",
    "                \"dfList_exp1\": raw_dfList_exp1,\n",
    "                \"dfList_exp2\": raw_dfList_exp2,\n",
    "                \"dfList_exp1_user_47\": raw_dfList_exp1_user_47,\n",
    "                \"dfList_exp2_user_47\": raw_dfList_exp2_user_47\n",
    "    }\n",
    "    \n",
    "\n",
    "    EER_df_train_dict[overlap] = calculate_EER_different_window_sizes_train_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, train_set=train_set, exp_config=P, \n",
    "                                                                                     extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                     param_dist=param_dist)\n",
    "    with open(train_file_name, \"a\") as f:\n",
    "        f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "        f.write(f\"\\overlap: {overlap}\\n\")\n",
    "        f.write(EER_df_train_dict[overlap].to_string())\n",
    "\n",
    "\n",
    "        \n",
    "mean_EER_train_dict={}\n",
    "for key in EER_df_train_dict:\n",
    "    mean_EER_train_dict[key] = EER_df_train_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "#-------\n",
    "train_lst = list(mean_EER_train_dict.items())\n",
    "train_lst.sort(key=lambda i: i[1], reverse=False) #sort ascending as it is an error rate\n",
    "\n",
    "with open(train_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(train_lst)):\n",
    "        f.write(f\"{i+1}) {train_lst[i]}\\n\")\n",
    "        \n",
    "\n",
    "min_key=train_lst[0][0]\n",
    "overlap=min_key\n",
    "EER_df_test_dict[min_key] = calculate_EER_different_window_sizes_test_OCSVM_IF_LOF(dfList_dict, window_size_lst=WINDOW_SIZE_LST, test_set=test_set, exp_config=P, \n",
    "                                                                                   extract_features_func_dict=EXTRACT_WACA_features_DICT, overlap=OVERLAP, \n",
    "                                                                                   best_param_df=EER_df_train_dict[min_key])\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\Top smoothing parameter/s: {min_key}\\n\")\n",
    "    f.write(EER_df_test_dict[min_key].to_string())\n",
    "#-------\n",
    "#-------\n",
    "key_column= [\"overlap\"]\n",
    "EER_df_test_dict_df=make_raw_exp_df_results(EER_df_test_dict, key_column)\n",
    "eer_per_window_size_col_df =make_eer_per_window_size_col_df(EER_df_test_dict_df, key_column, window_size_lst=WINDOW_SIZE_LST)\n",
    "\n",
    "EER_df_test_dict_df.to_json(f'{test_file_name[:-4]}_raw_df.json')\n",
    "eer_per_window_size_col_df.to_json(f'{test_file_name[:-4]}_eer_per_window_size_col_df.json')\n",
    "#-------\n",
    "\n",
    "mean_EER_test_dict={}\n",
    "for key in EER_df_test_dict:\n",
    "    mean_EER_test_dict[key] = EER_df_test_dict[key][\"Mean_EER\"].mean()\n",
    "    \n",
    "l = list(mean_EER_test_dict.items())\n",
    "l.sort(key=lambda i: i[1])\n",
    "\n",
    "with open(test_file_name, \"a\") as f:\n",
    "    f.write(\"\\n\" + \"-\"*22 + \"\\n\")\n",
    "    f.write(f\"\\nSorting based on Mean EER among windows\\n\")\n",
    "    for i in range(len(l)):\n",
    "        f.write(f\"{i+1}) {l[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OVERLAP_EXP_RANGE.index(57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap=0.01\n",
    "max_window_size=2000\n",
    "step_width = int(max_window_size * (1-overlap))\n",
    "max_num_windows=min(len(getIndices(sampleSize=max_window_size, step=step_width, numSamplePoints=P.num_sample_points_per_exp)), param_dist['model__n_neighbors'][-1]+1)\n",
    "n_neighbors_params = np.arange(1, max_num_windows) \n",
    "n_neighbors_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip=True\n",
    "df=return_and_save_final_result_df_as_json(final_exp_results_path=FINAL_EXP_RESULTS_PATH, exp_path_name=EXP_PATH_NAME, window_size_lst=WINDOW_SIZE_LST)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\"></th>\n",
       "      <th colspan=\"5\" halign=\"left\">Window Size</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean rank</th>\n",
       "      <th>cut_off_freq</th>\n",
       "      <th>EMA_span</th>\n",
       "      <th>SMA_winsize</th>\n",
       "      <th>type</th>\n",
       "      <th>125</th>\n",
       "      <th>250</th>\n",
       "      <th>500</th>\n",
       "      <th>750</th>\n",
       "      <th>1000</th>\n",
       "      <th>mean</th>\n",
       "      <th>variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.159600</td>\n",
       "      <td>0.102140</td>\n",
       "      <td>0.066176</td>\n",
       "      <td>0.054276</td>\n",
       "      <td>0.046387</td>\n",
       "      <td>0.085716</td>\n",
       "      <td>0.002161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Naive</td>\n",
       "      <td>0.151841</td>\n",
       "      <td>0.094810</td>\n",
       "      <td>0.061223</td>\n",
       "      <td>0.047542</td>\n",
       "      <td>0.042864</td>\n",
       "      <td>0.079656</td>\n",
       "      <td>0.002041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.156094</td>\n",
       "      <td>0.095819</td>\n",
       "      <td>0.062963</td>\n",
       "      <td>0.047710</td>\n",
       "      <td>0.046477</td>\n",
       "      <td>0.081813</td>\n",
       "      <td>0.002120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>49</td>\n",
       "      <td>-</td>\n",
       "      <td>Naive</td>\n",
       "      <td>0.129547</td>\n",
       "      <td>0.090563</td>\n",
       "      <td>0.062048</td>\n",
       "      <td>0.050370</td>\n",
       "      <td>0.041057</td>\n",
       "      <td>0.074717</td>\n",
       "      <td>0.001286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.159600</td>\n",
       "      <td>0.102140</td>\n",
       "      <td>0.066176</td>\n",
       "      <td>0.054276</td>\n",
       "      <td>0.046387</td>\n",
       "      <td>0.085716</td>\n",
       "      <td>0.002161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>49</td>\n",
       "      <td>Naive</td>\n",
       "      <td>0.137119</td>\n",
       "      <td>0.096684</td>\n",
       "      <td>0.071598</td>\n",
       "      <td>0.054411</td>\n",
       "      <td>0.044399</td>\n",
       "      <td>0.080842</td>\n",
       "      <td>0.001383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.159611</td>\n",
       "      <td>0.101519</td>\n",
       "      <td>0.065819</td>\n",
       "      <td>0.054242</td>\n",
       "      <td>0.046387</td>\n",
       "      <td>0.085516</td>\n",
       "      <td>0.002161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>48</td>\n",
       "      <td>-</td>\n",
       "      <td>Naive</td>\n",
       "      <td>0.130591</td>\n",
       "      <td>0.091905</td>\n",
       "      <td>0.061089</td>\n",
       "      <td>0.049764</td>\n",
       "      <td>0.042367</td>\n",
       "      <td>0.075143</td>\n",
       "      <td>0.001318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.157539</td>\n",
       "      <td>0.100299</td>\n",
       "      <td>0.064101</td>\n",
       "      <td>0.049596</td>\n",
       "      <td>0.043315</td>\n",
       "      <td>0.082970</td>\n",
       "      <td>0.002226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>43</td>\n",
       "      <td>-</td>\n",
       "      <td>34</td>\n",
       "      <td>Naive</td>\n",
       "      <td>0.145928</td>\n",
       "      <td>0.100455</td>\n",
       "      <td>0.074431</td>\n",
       "      <td>0.054646</td>\n",
       "      <td>0.044670</td>\n",
       "      <td>0.084026</td>\n",
       "      <td>0.001651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>-</td>\n",
       "      <td>2</td>\n",
       "      <td>Real</td>\n",
       "      <td>0.154874</td>\n",
       "      <td>0.096507</td>\n",
       "      <td>0.064190</td>\n",
       "      <td>0.048822</td>\n",
       "      <td>0.042864</td>\n",
       "      <td>0.081451</td>\n",
       "      <td>0.002117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Window Size            \\\n",
       "   mean rank cut_off_freq EMA_span SMA_winsize   type         125       250   \n",
       "0         10            -        -           -      -    0.159600  0.102140   \n",
       "1          3           37        -           -  Naive    0.151841  0.094810   \n",
       "2          6           33        -           -   Real    0.156094  0.095819   \n",
       "3          1            -       49           -  Naive    0.129547  0.090563   \n",
       "4         10            -        1           -   Real    0.159600  0.102140   \n",
       "5          4            -        -          49  Naive    0.137119  0.096684   \n",
       "6          9            -        -           1   Real    0.159611  0.101519   \n",
       "7          2           41       48           -  Naive    0.130591  0.091905   \n",
       "8          7           40        3           -   Real    0.157539  0.100299   \n",
       "9          8           43        -          34  Naive    0.145928  0.100455   \n",
       "10         5           26        -           2   Real    0.154874  0.096507   \n",
       "\n",
       "                                                      \n",
       "         500       750      1000      mean  variance  \n",
       "0   0.066176  0.054276  0.046387  0.085716  0.002161  \n",
       "1   0.061223  0.047542  0.042864  0.079656  0.002041  \n",
       "2   0.062963  0.047710  0.046477  0.081813  0.002120  \n",
       "3   0.062048  0.050370  0.041057  0.074717  0.001286  \n",
       "4   0.066176  0.054276  0.046387  0.085716  0.002161  \n",
       "5   0.071598  0.054411  0.044399  0.080842  0.001383  \n",
       "6   0.065819  0.054242  0.046387  0.085516  0.002161  \n",
       "7   0.061089  0.049764  0.042367  0.075143  0.001318  \n",
       "8   0.064101  0.049596  0.043315  0.082970  0.002226  \n",
       "9   0.074431  0.054646  0.044670  0.084026  0.001651  \n",
       "10  0.064190  0.048822  0.042864  0.081451  0.002117  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=return_and_save_final_result_df_as_json(final_exp_results_path=FINAL_EXP_RESULTS_PATH, exp_path_name=EXP_PATH_NAME, window_size_lst=WINDOW_SIZE_LST_NN, save_file_suffix=\"-max1000\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip=False\n",
    "df=return_and_save_final_result_df_as_json(final_exp_results_path=FINAL_EXP_RESULTS_PATH, exp_path_name=EXP_PATH_NAME, window_size_lst=WINDOW_SIZE_LST)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df=return_and_save_final_relative_result_df_as_json(df, base_case_index=0, final_exp_results_path=FINAL_EXP_RESULTS_PATH, exp_path_name=EXP_PATH_NAME, \n",
    "                                                             window_size_lst=WINDOW_SIZE_LST)\n",
    "relative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df=return_and_save_final_relative_result_df_as_json(df, base_case_index=0, final_exp_results_path=FINAL_EXP_RESULTS_PATH, exp_path_name=EXP_PATH_NAME, \n",
    "                                                             window_size_lst=WINDOW_SIZE_LST)\n",
    "relative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.style.format(STYLER_ERR_FORMAT_DICT).hide(axis='index').to_latex()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_df.style.format(STYLER_IMPROVEMENT_FORMAT_DICT).hide(axis='index').to_latex()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_result_df=return_and_save_final_Gini_df_as_json(final_exp_results_path=FINAL_EXP_RESULTS_PATH, exp_path_name=EXP_PATH_NAME, window_size_lst=WINDOW_SIZE_LST)\n",
    "gini_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_and_save_final_relative_gini_result_df_as_json(gini_result_df, base_case_index=0, final_exp_results_path=FINAL_EXP_RESULTS_PATH, \n",
    "                                                      exp_path_name=EXP_PATH_NAME, window_size_lst=WINDOW_SIZE_LST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 EER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th colspan=\"9\" halign=\"left\">Window Size</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean rank</th>\n",
       "      <th>overlap</th>\n",
       "      <th>125</th>\n",
       "      <th>250</th>\n",
       "      <th>500</th>\n",
       "      <th>750</th>\n",
       "      <th>1000</th>\n",
       "      <th>1250</th>\n",
       "      <th>1500</th>\n",
       "      <th>1750</th>\n",
       "      <th>2000</th>\n",
       "      <th>mean</th>\n",
       "      <th>variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.156656</td>\n",
       "      <td>0.106776</td>\n",
       "      <td>0.069004</td>\n",
       "      <td>0.051389</td>\n",
       "      <td>0.047354</td>\n",
       "      <td>0.041551</td>\n",
       "      <td>0.036905</td>\n",
       "      <td>0.038889</td>\n",
       "      <td>0.039815</td>\n",
       "      <td>0.065371</td>\n",
       "      <td>0.001664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.157470</td>\n",
       "      <td>0.103136</td>\n",
       "      <td>0.064983</td>\n",
       "      <td>0.054087</td>\n",
       "      <td>0.046549</td>\n",
       "      <td>0.042048</td>\n",
       "      <td>0.043386</td>\n",
       "      <td>0.039198</td>\n",
       "      <td>0.041111</td>\n",
       "      <td>0.065774</td>\n",
       "      <td>0.001586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.163023</td>\n",
       "      <td>0.110668</td>\n",
       "      <td>0.064412</td>\n",
       "      <td>0.055247</td>\n",
       "      <td>0.049242</td>\n",
       "      <td>0.047119</td>\n",
       "      <td>0.037407</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.040505</td>\n",
       "      <td>0.067185</td>\n",
       "      <td>0.001807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.168213</td>\n",
       "      <td>0.101948</td>\n",
       "      <td>0.064892</td>\n",
       "      <td>0.050347</td>\n",
       "      <td>0.048390</td>\n",
       "      <td>0.042885</td>\n",
       "      <td>0.040494</td>\n",
       "      <td>0.039601</td>\n",
       "      <td>0.042357</td>\n",
       "      <td>0.066570</td>\n",
       "      <td>0.001842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.165919</td>\n",
       "      <td>0.107572</td>\n",
       "      <td>0.066519</td>\n",
       "      <td>0.052581</td>\n",
       "      <td>0.042815</td>\n",
       "      <td>0.043056</td>\n",
       "      <td>0.038657</td>\n",
       "      <td>0.033995</td>\n",
       "      <td>0.036420</td>\n",
       "      <td>0.065281</td>\n",
       "      <td>0.001950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.164616</td>\n",
       "      <td>0.106569</td>\n",
       "      <td>0.063746</td>\n",
       "      <td>0.053810</td>\n",
       "      <td>0.050356</td>\n",
       "      <td>0.044180</td>\n",
       "      <td>0.037582</td>\n",
       "      <td>0.037434</td>\n",
       "      <td>0.036895</td>\n",
       "      <td>0.066132</td>\n",
       "      <td>0.001840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.157811</td>\n",
       "      <td>0.108959</td>\n",
       "      <td>0.064949</td>\n",
       "      <td>0.051401</td>\n",
       "      <td>0.047051</td>\n",
       "      <td>0.038047</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.037531</td>\n",
       "      <td>0.040883</td>\n",
       "      <td>0.064852</td>\n",
       "      <td>0.001737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.164045</td>\n",
       "      <td>0.105163</td>\n",
       "      <td>0.066794</td>\n",
       "      <td>0.052564</td>\n",
       "      <td>0.049936</td>\n",
       "      <td>0.040660</td>\n",
       "      <td>0.042203</td>\n",
       "      <td>0.032986</td>\n",
       "      <td>0.036772</td>\n",
       "      <td>0.065680</td>\n",
       "      <td>0.001842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.161809</td>\n",
       "      <td>0.107704</td>\n",
       "      <td>0.066458</td>\n",
       "      <td>0.052981</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.040201</td>\n",
       "      <td>0.039259</td>\n",
       "      <td>0.035512</td>\n",
       "      <td>0.035926</td>\n",
       "      <td>0.064983</td>\n",
       "      <td>0.001842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.160925</td>\n",
       "      <td>0.104511</td>\n",
       "      <td>0.063664</td>\n",
       "      <td>0.056288</td>\n",
       "      <td>0.049074</td>\n",
       "      <td>0.039744</td>\n",
       "      <td>0.038272</td>\n",
       "      <td>0.035185</td>\n",
       "      <td>0.036690</td>\n",
       "      <td>0.064928</td>\n",
       "      <td>0.001767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.160820</td>\n",
       "      <td>0.103482</td>\n",
       "      <td>0.068624</td>\n",
       "      <td>0.051248</td>\n",
       "      <td>0.048747</td>\n",
       "      <td>0.043347</td>\n",
       "      <td>0.040016</td>\n",
       "      <td>0.037719</td>\n",
       "      <td>0.036819</td>\n",
       "      <td>0.065647</td>\n",
       "      <td>0.001720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.158228</td>\n",
       "      <td>0.103168</td>\n",
       "      <td>0.064519</td>\n",
       "      <td>0.053481</td>\n",
       "      <td>0.043944</td>\n",
       "      <td>0.038825</td>\n",
       "      <td>0.037577</td>\n",
       "      <td>0.039506</td>\n",
       "      <td>0.032613</td>\n",
       "      <td>0.063540</td>\n",
       "      <td>0.001730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.155823</td>\n",
       "      <td>0.103365</td>\n",
       "      <td>0.066278</td>\n",
       "      <td>0.053395</td>\n",
       "      <td>0.046713</td>\n",
       "      <td>0.041262</td>\n",
       "      <td>0.037251</td>\n",
       "      <td>0.038805</td>\n",
       "      <td>0.038207</td>\n",
       "      <td>0.064566</td>\n",
       "      <td>0.001616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.155288</td>\n",
       "      <td>0.104234</td>\n",
       "      <td>0.067235</td>\n",
       "      <td>0.055651</td>\n",
       "      <td>0.044961</td>\n",
       "      <td>0.041340</td>\n",
       "      <td>0.036111</td>\n",
       "      <td>0.035571</td>\n",
       "      <td>0.039418</td>\n",
       "      <td>0.064423</td>\n",
       "      <td>0.001637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.152811</td>\n",
       "      <td>0.103751</td>\n",
       "      <td>0.067650</td>\n",
       "      <td>0.053762</td>\n",
       "      <td>0.047872</td>\n",
       "      <td>0.038989</td>\n",
       "      <td>0.038351</td>\n",
       "      <td>0.037607</td>\n",
       "      <td>0.035105</td>\n",
       "      <td>0.063989</td>\n",
       "      <td>0.001583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.158110</td>\n",
       "      <td>0.101350</td>\n",
       "      <td>0.061356</td>\n",
       "      <td>0.053466</td>\n",
       "      <td>0.047044</td>\n",
       "      <td>0.041418</td>\n",
       "      <td>0.038072</td>\n",
       "      <td>0.041252</td>\n",
       "      <td>0.041185</td>\n",
       "      <td>0.064806</td>\n",
       "      <td>0.001609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.156200</td>\n",
       "      <td>0.102975</td>\n",
       "      <td>0.066133</td>\n",
       "      <td>0.055247</td>\n",
       "      <td>0.050032</td>\n",
       "      <td>0.040862</td>\n",
       "      <td>0.037427</td>\n",
       "      <td>0.035012</td>\n",
       "      <td>0.037169</td>\n",
       "      <td>0.064562</td>\n",
       "      <td>0.001637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.155926</td>\n",
       "      <td>0.099575</td>\n",
       "      <td>0.066376</td>\n",
       "      <td>0.052736</td>\n",
       "      <td>0.046695</td>\n",
       "      <td>0.042343</td>\n",
       "      <td>0.038587</td>\n",
       "      <td>0.036626</td>\n",
       "      <td>0.038292</td>\n",
       "      <td>0.064128</td>\n",
       "      <td>0.001584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.151752</td>\n",
       "      <td>0.102145</td>\n",
       "      <td>0.067093</td>\n",
       "      <td>0.056674</td>\n",
       "      <td>0.048938</td>\n",
       "      <td>0.043377</td>\n",
       "      <td>0.041308</td>\n",
       "      <td>0.039205</td>\n",
       "      <td>0.038580</td>\n",
       "      <td>0.065453</td>\n",
       "      <td>0.001453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>17</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.154925</td>\n",
       "      <td>0.102917</td>\n",
       "      <td>0.065442</td>\n",
       "      <td>0.054473</td>\n",
       "      <td>0.047222</td>\n",
       "      <td>0.043103</td>\n",
       "      <td>0.042820</td>\n",
       "      <td>0.040201</td>\n",
       "      <td>0.040564</td>\n",
       "      <td>0.065741</td>\n",
       "      <td>0.001516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>12</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.152507</td>\n",
       "      <td>0.103645</td>\n",
       "      <td>0.066447</td>\n",
       "      <td>0.053186</td>\n",
       "      <td>0.048725</td>\n",
       "      <td>0.044929</td>\n",
       "      <td>0.042941</td>\n",
       "      <td>0.037133</td>\n",
       "      <td>0.038526</td>\n",
       "      <td>0.065338</td>\n",
       "      <td>0.001490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.152545</td>\n",
       "      <td>0.104400</td>\n",
       "      <td>0.065714</td>\n",
       "      <td>0.054410</td>\n",
       "      <td>0.053192</td>\n",
       "      <td>0.045685</td>\n",
       "      <td>0.040677</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.038108</td>\n",
       "      <td>0.066141</td>\n",
       "      <td>0.001468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.151519</td>\n",
       "      <td>0.104231</td>\n",
       "      <td>0.067442</td>\n",
       "      <td>0.057595</td>\n",
       "      <td>0.050101</td>\n",
       "      <td>0.049349</td>\n",
       "      <td>0.044849</td>\n",
       "      <td>0.042483</td>\n",
       "      <td>0.040911</td>\n",
       "      <td>0.067609</td>\n",
       "      <td>0.001373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.152609</td>\n",
       "      <td>0.106455</td>\n",
       "      <td>0.069861</td>\n",
       "      <td>0.059706</td>\n",
       "      <td>0.054368</td>\n",
       "      <td>0.054029</td>\n",
       "      <td>0.047892</td>\n",
       "      <td>0.046695</td>\n",
       "      <td>0.046553</td>\n",
       "      <td>0.070907</td>\n",
       "      <td>0.001292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Window Size                                          \\\n",
       "   mean rank overlap         125       250       500       750      1000   \n",
       "0         13    0.01    0.156656  0.106776  0.069004  0.051389  0.047354   \n",
       "1         18    0.05    0.157470  0.103136  0.064983  0.054087  0.046549   \n",
       "2         22    0.09    0.163023  0.110668  0.064412  0.055247  0.049242   \n",
       "3         21    0.13    0.168213  0.101948  0.064892  0.050347  0.048390   \n",
       "4         11    0.17    0.165919  0.107572  0.066519  0.052581  0.042815   \n",
       "5         19    0.21    0.164616  0.106569  0.063746  0.053810  0.050356   \n",
       "6          8    0.25    0.157811  0.108959  0.064949  0.051401  0.047051   \n",
       "7         16    0.29    0.164045  0.105163  0.066794  0.052564  0.049936   \n",
       "8         10    0.33    0.161809  0.107704  0.066458  0.052981  0.045000   \n",
       "9          9    0.37    0.160925  0.104511  0.063664  0.056288  0.049074   \n",
       "10        15    0.41    0.160820  0.103482  0.068624  0.051248  0.048747   \n",
       "11         1    0.45    0.158228  0.103168  0.064519  0.053481  0.043944   \n",
       "12         6    0.49    0.155823  0.103365  0.066278  0.053395  0.046713   \n",
       "13         4    0.53    0.155288  0.104234  0.067235  0.055651  0.044961   \n",
       "14         2    0.57    0.152811  0.103751  0.067650  0.053762  0.047872   \n",
       "15         7    0.61    0.158110  0.101350  0.061356  0.053466  0.047044   \n",
       "16         5    0.65    0.156200  0.102975  0.066133  0.055247  0.050032   \n",
       "17         3    0.69    0.155926  0.099575  0.066376  0.052736  0.046695   \n",
       "18        14    0.73    0.151752  0.102145  0.067093  0.056674  0.048938   \n",
       "19        17    0.77    0.154925  0.102917  0.065442  0.054473  0.047222   \n",
       "20        12    0.81    0.152507  0.103645  0.066447  0.053186  0.048725   \n",
       "21        20    0.85    0.152545  0.104400  0.065714  0.054410  0.053192   \n",
       "22        23    0.89    0.151519  0.104231  0.067442  0.057595  0.050101   \n",
       "23        24    0.93    0.152609  0.106455  0.069861  0.059706  0.054368   \n",
       "\n",
       "                                                                \n",
       "        1250      1500      1750      2000      mean  variance  \n",
       "0   0.041551  0.036905  0.038889  0.039815  0.065371  0.001664  \n",
       "1   0.042048  0.043386  0.039198  0.041111  0.065774  0.001586  \n",
       "2   0.047119  0.037407  0.037037  0.040505  0.067185  0.001807  \n",
       "3   0.042885  0.040494  0.039601  0.042357  0.066570  0.001842  \n",
       "4   0.043056  0.038657  0.033995  0.036420  0.065281  0.001950  \n",
       "5   0.044180  0.037582  0.037434  0.036895  0.066132  0.001840  \n",
       "6   0.038047  0.037037  0.037531  0.040883  0.064852  0.001737  \n",
       "7   0.040660  0.042203  0.032986  0.036772  0.065680  0.001842  \n",
       "8   0.040201  0.039259  0.035512  0.035926  0.064983  0.001842  \n",
       "9   0.039744  0.038272  0.035185  0.036690  0.064928  0.001767  \n",
       "10  0.043347  0.040016  0.037719  0.036819  0.065647  0.001720  \n",
       "11  0.038825  0.037577  0.039506  0.032613  0.063540  0.001730  \n",
       "12  0.041262  0.037251  0.038805  0.038207  0.064566  0.001616  \n",
       "13  0.041340  0.036111  0.035571  0.039418  0.064423  0.001637  \n",
       "14  0.038989  0.038351  0.037607  0.035105  0.063989  0.001583  \n",
       "15  0.041418  0.038072  0.041252  0.041185  0.064806  0.001609  \n",
       "16  0.040862  0.037427  0.035012  0.037169  0.064562  0.001637  \n",
       "17  0.042343  0.038587  0.036626  0.038292  0.064128  0.001584  \n",
       "18  0.043377  0.041308  0.039205  0.038580  0.065453  0.001453  \n",
       "19  0.043103  0.042820  0.040201  0.040564  0.065741  0.001516  \n",
       "20  0.044929  0.042941  0.037133  0.038526  0.065338  0.001490  \n",
       "21  0.045685  0.040677  0.040541  0.038108  0.066141  0.001468  \n",
       "22  0.049349  0.044849  0.042483  0.040911  0.067609  0.001373  \n",
       "23  0.054029  0.047892  0.046695  0.046553  0.070907  0.001292  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlap_df=return_and_save_final_result_df_as_json_overlap(final_exp_results_path=FINAL_EXP_RESULTS_PATH, exp_path_name=EXP_PATH_NAME, window_size_lst=WINDOW_SIZE_LST)\n",
    "overlap_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 EER relative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th colspan=\"9\" halign=\"left\">Window Size</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean rank</th>\n",
       "      <th>overlap</th>\n",
       "      <th>125</th>\n",
       "      <th>250</th>\n",
       "      <th>500</th>\n",
       "      <th>750</th>\n",
       "      <th>1000</th>\n",
       "      <th>1250</th>\n",
       "      <th>1500</th>\n",
       "      <th>1750</th>\n",
       "      <th>2000</th>\n",
       "      <th>mean</th>\n",
       "      <th>variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.519871</td>\n",
       "      <td>3.409091</td>\n",
       "      <td>5.826314</td>\n",
       "      <td>-5.250078</td>\n",
       "      <td>1.701371</td>\n",
       "      <td>-1.196133</td>\n",
       "      <td>-17.562724</td>\n",
       "      <td>-0.793651</td>\n",
       "      <td>-3.255814</td>\n",
       "      <td>-0.616937</td>\n",
       "      <td>4.680369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-4.064484</td>\n",
       "      <td>-3.645662</td>\n",
       "      <td>6.653702</td>\n",
       "      <td>-7.507508</td>\n",
       "      <td>-3.986795</td>\n",
       "      <td>-13.401424</td>\n",
       "      <td>-1.362007</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>-1.733615</td>\n",
       "      <td>-2.774603</td>\n",
       "      <td>-8.602594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-7.377503</td>\n",
       "      <td>4.520974</td>\n",
       "      <td>5.958466</td>\n",
       "      <td>2.027027</td>\n",
       "      <td>-2.186058</td>\n",
       "      <td>-3.210673</td>\n",
       "      <td>-9.725209</td>\n",
       "      <td>-1.831502</td>\n",
       "      <td>-6.384778</td>\n",
       "      <td>-1.833885</td>\n",
       "      <td>-10.676362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-5.912871</td>\n",
       "      <td>-0.746261</td>\n",
       "      <td>3.601278</td>\n",
       "      <td>-2.320502</td>\n",
       "      <td>9.586592</td>\n",
       "      <td>-3.621170</td>\n",
       "      <td>-4.749104</td>\n",
       "      <td>12.585034</td>\n",
       "      <td>8.527132</td>\n",
       "      <td>0.136741</td>\n",
       "      <td>-17.171179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-5.081211</td>\n",
       "      <td>0.193645</td>\n",
       "      <td>7.618580</td>\n",
       "      <td>-4.710425</td>\n",
       "      <td>-6.338633</td>\n",
       "      <td>-6.327099</td>\n",
       "      <td>-1.834282</td>\n",
       "      <td>3.741497</td>\n",
       "      <td>7.334526</td>\n",
       "      <td>-1.164139</td>\n",
       "      <td>-10.541425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.737238</td>\n",
       "      <td>-2.044812</td>\n",
       "      <td>5.875109</td>\n",
       "      <td>-0.024349</td>\n",
       "      <td>0.641423</td>\n",
       "      <td>8.432515</td>\n",
       "      <td>-0.358423</td>\n",
       "      <td>3.492064</td>\n",
       "      <td>-2.683363</td>\n",
       "      <td>0.793445</td>\n",
       "      <td>-4.394260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-4.716541</td>\n",
       "      <td>1.510069</td>\n",
       "      <td>3.201498</td>\n",
       "      <td>-2.286902</td>\n",
       "      <td>-5.451743</td>\n",
       "      <td>2.143636</td>\n",
       "      <td>-14.355782</td>\n",
       "      <td>15.178571</td>\n",
       "      <td>7.641196</td>\n",
       "      <td>-0.473575</td>\n",
       "      <td>-10.701531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-3.289669</td>\n",
       "      <td>-0.869210</td>\n",
       "      <td>3.689581</td>\n",
       "      <td>-3.098220</td>\n",
       "      <td>4.972067</td>\n",
       "      <td>3.249768</td>\n",
       "      <td>-6.379928</td>\n",
       "      <td>8.683473</td>\n",
       "      <td>9.767442</td>\n",
       "      <td>0.592915</td>\n",
       "      <td>-10.681302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.37</td>\n",
       "      <td>-2.724937</td>\n",
       "      <td>2.120635</td>\n",
       "      <td>7.737438</td>\n",
       "      <td>-9.532789</td>\n",
       "      <td>-3.631285</td>\n",
       "      <td>4.349689</td>\n",
       "      <td>-3.703704</td>\n",
       "      <td>9.523810</td>\n",
       "      <td>7.848837</td>\n",
       "      <td>0.677414</td>\n",
       "      <td>-6.189105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-2.658033</td>\n",
       "      <td>3.084596</td>\n",
       "      <td>0.549521</td>\n",
       "      <td>0.274187</td>\n",
       "      <td>-2.941176</td>\n",
       "      <td>-4.322707</td>\n",
       "      <td>-8.430731</td>\n",
       "      <td>3.007519</td>\n",
       "      <td>7.523940</td>\n",
       "      <td>-0.422425</td>\n",
       "      <td>-3.332773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-1.003473</td>\n",
       "      <td>3.379010</td>\n",
       "      <td>6.499680</td>\n",
       "      <td>-4.072072</td>\n",
       "      <td>7.202174</td>\n",
       "      <td>6.560369</td>\n",
       "      <td>-1.821983</td>\n",
       "      <td>-1.587302</td>\n",
       "      <td>18.087855</td>\n",
       "      <td>2.800566</td>\n",
       "      <td>-3.970424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.531407</td>\n",
       "      <td>3.194221</td>\n",
       "      <td>3.949828</td>\n",
       "      <td>-3.903904</td>\n",
       "      <td>1.354749</td>\n",
       "      <td>0.696379</td>\n",
       "      <td>-0.937414</td>\n",
       "      <td>0.216450</td>\n",
       "      <td>4.039168</td>\n",
       "      <td>1.230548</td>\n",
       "      <td>2.894216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.873066</td>\n",
       "      <td>2.380046</td>\n",
       "      <td>2.563172</td>\n",
       "      <td>-8.294501</td>\n",
       "      <td>5.053917</td>\n",
       "      <td>0.507947</td>\n",
       "      <td>2.150538</td>\n",
       "      <td>8.531746</td>\n",
       "      <td>0.996678</td>\n",
       "      <td>1.449497</td>\n",
       "      <td>1.619451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>0.57</td>\n",
       "      <td>2.454486</td>\n",
       "      <td>2.832264</td>\n",
       "      <td>1.960863</td>\n",
       "      <td>-4.618905</td>\n",
       "      <td>-1.093546</td>\n",
       "      <td>6.165776</td>\n",
       "      <td>-3.919528</td>\n",
       "      <td>3.296703</td>\n",
       "      <td>11.830131</td>\n",
       "      <td>2.114180</td>\n",
       "      <td>4.897507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>0.61</td>\n",
       "      <td>-0.928188</td>\n",
       "      <td>5.081359</td>\n",
       "      <td>11.083248</td>\n",
       "      <td>-4.041184</td>\n",
       "      <td>0.655350</td>\n",
       "      <td>0.319315</td>\n",
       "      <td>-3.162555</td>\n",
       "      <td>-6.075534</td>\n",
       "      <td>-3.441860</td>\n",
       "      <td>0.864386</td>\n",
       "      <td>3.313933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.291251</td>\n",
       "      <td>3.559089</td>\n",
       "      <td>4.159853</td>\n",
       "      <td>-7.507507</td>\n",
       "      <td>-5.654017</td>\n",
       "      <td>1.659198</td>\n",
       "      <td>-1.414827</td>\n",
       "      <td>9.970238</td>\n",
       "      <td>6.644518</td>\n",
       "      <td>1.237653</td>\n",
       "      <td>1.635189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.465893</td>\n",
       "      <td>6.743900</td>\n",
       "      <td>3.807162</td>\n",
       "      <td>-2.620803</td>\n",
       "      <td>1.392351</td>\n",
       "      <td>-1.907007</td>\n",
       "      <td>-4.559473</td>\n",
       "      <td>5.820106</td>\n",
       "      <td>3.825956</td>\n",
       "      <td>1.900598</td>\n",
       "      <td>4.805962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14</td>\n",
       "      <td>0.73</td>\n",
       "      <td>3.130040</td>\n",
       "      <td>4.337166</td>\n",
       "      <td>2.768623</td>\n",
       "      <td>-10.284542</td>\n",
       "      <td>-3.344506</td>\n",
       "      <td>-4.395449</td>\n",
       "      <td>-11.930364</td>\n",
       "      <td>-0.813008</td>\n",
       "      <td>3.100775</td>\n",
       "      <td>-0.124901</td>\n",
       "      <td>12.656322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>17</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.104805</td>\n",
       "      <td>3.613464</td>\n",
       "      <td>5.160809</td>\n",
       "      <td>-6.000916</td>\n",
       "      <td>0.279330</td>\n",
       "      <td>-3.734205</td>\n",
       "      <td>-16.028422</td>\n",
       "      <td>-3.373016</td>\n",
       "      <td>-1.882614</td>\n",
       "      <td>-0.565914</td>\n",
       "      <td>8.884454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>12</td>\n",
       "      <td>0.81</td>\n",
       "      <td>2.648640</td>\n",
       "      <td>2.931898</td>\n",
       "      <td>3.705195</td>\n",
       "      <td>-3.496503</td>\n",
       "      <td>-2.893433</td>\n",
       "      <td>-8.131052</td>\n",
       "      <td>-16.357592</td>\n",
       "      <td>4.515599</td>\n",
       "      <td>3.237574</td>\n",
       "      <td>0.050884</td>\n",
       "      <td>10.437049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20</td>\n",
       "      <td>0.85</td>\n",
       "      <td>2.624272</td>\n",
       "      <td>2.224654</td>\n",
       "      <td>4.767613</td>\n",
       "      <td>-5.878254</td>\n",
       "      <td>-12.328025</td>\n",
       "      <td>-9.949020</td>\n",
       "      <td>-10.221234</td>\n",
       "      <td>-4.247104</td>\n",
       "      <td>4.287791</td>\n",
       "      <td>-1.178383</td>\n",
       "      <td>11.791116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.89</td>\n",
       "      <td>3.279268</td>\n",
       "      <td>2.383049</td>\n",
       "      <td>2.263662</td>\n",
       "      <td>-12.076449</td>\n",
       "      <td>-5.799126</td>\n",
       "      <td>-18.766689</td>\n",
       "      <td>-21.526460</td>\n",
       "      <td>-9.240924</td>\n",
       "      <td>-2.753275</td>\n",
       "      <td>-3.423262</td>\n",
       "      <td>17.489001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2.583283</td>\n",
       "      <td>0.300311</td>\n",
       "      <td>-1.242033</td>\n",
       "      <td>-16.183876</td>\n",
       "      <td>-14.810248</td>\n",
       "      <td>-30.029810</td>\n",
       "      <td>-29.771982</td>\n",
       "      <td>-20.072333</td>\n",
       "      <td>-16.924122</td>\n",
       "      <td>-8.469408</td>\n",
       "      <td>22.383186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Window Size                                             \\\n",
       "   mean rank overlap         125       250        500        750       1000   \n",
       "0         13    0.01    0.000000  0.000000   0.000000   0.000000   0.000000   \n",
       "1         18    0.05   -0.519871  3.409091   5.826314  -5.250078   1.701371   \n",
       "2         22    0.09   -4.064484 -3.645662   6.653702  -7.507508  -3.986795   \n",
       "3         21    0.13   -7.377503  4.520974   5.958466   2.027027  -2.186058   \n",
       "4         11    0.17   -5.912871 -0.746261   3.601278  -2.320502   9.586592   \n",
       "5         19    0.21   -5.081211  0.193645   7.618580  -4.710425  -6.338633   \n",
       "6          8    0.25   -0.737238 -2.044812   5.875109  -0.024349   0.641423   \n",
       "7         16    0.29   -4.716541  1.510069   3.201498  -2.286902  -5.451743   \n",
       "8         10    0.33   -3.289669 -0.869210   3.689581  -3.098220   4.972067   \n",
       "9          9    0.37   -2.724937  2.120635   7.737438  -9.532789  -3.631285   \n",
       "10        15    0.41   -2.658033  3.084596   0.549521   0.274187  -2.941176   \n",
       "11         1    0.45   -1.003473  3.379010   6.499680  -4.072072   7.202174   \n",
       "12         6    0.49    0.531407  3.194221   3.949828  -3.903904   1.354749   \n",
       "13         4    0.53    0.873066  2.380046   2.563172  -8.294501   5.053917   \n",
       "14         2    0.57    2.454486  2.832264   1.960863  -4.618905  -1.093546   \n",
       "15         7    0.61   -0.928188  5.081359  11.083248  -4.041184   0.655350   \n",
       "16         5    0.65    0.291251  3.559089   4.159853  -7.507507  -5.654017   \n",
       "17         3    0.69    0.465893  6.743900   3.807162  -2.620803   1.392351   \n",
       "18        14    0.73    3.130040  4.337166   2.768623 -10.284542  -3.344506   \n",
       "19        17    0.77    1.104805  3.613464   5.160809  -6.000916   0.279330   \n",
       "20        12    0.81    2.648640  2.931898   3.705195  -3.496503  -2.893433   \n",
       "21        20    0.85    2.624272  2.224654   4.767613  -5.878254 -12.328025   \n",
       "22        23    0.89    3.279268  2.383049   2.263662 -12.076449  -5.799126   \n",
       "23        24    0.93    2.583283  0.300311  -1.242033 -16.183876 -14.810248   \n",
       "\n",
       "                                                                     \n",
       "         1250       1500       1750       2000      mean   variance  \n",
       "0    0.000000   0.000000   0.000000   0.000000  0.000000   0.000000  \n",
       "1   -1.196133 -17.562724  -0.793651  -3.255814 -0.616937   4.680369  \n",
       "2  -13.401424  -1.362007   4.761905  -1.733615 -2.774603  -8.602594  \n",
       "3   -3.210673  -9.725209  -1.831502  -6.384778 -1.833885 -10.676362  \n",
       "4   -3.621170  -4.749104  12.585034   8.527132  0.136741 -17.171179  \n",
       "5   -6.327099  -1.834282   3.741497   7.334526 -1.164139 -10.541425  \n",
       "6    8.432515  -0.358423   3.492064  -2.683363  0.793445  -4.394260  \n",
       "7    2.143636 -14.355782  15.178571   7.641196 -0.473575 -10.701531  \n",
       "8    3.249768  -6.379928   8.683473   9.767442  0.592915 -10.681302  \n",
       "9    4.349689  -3.703704   9.523810   7.848837  0.677414  -6.189105  \n",
       "10  -4.322707  -8.430731   3.007519   7.523940 -0.422425  -3.332773  \n",
       "11   6.560369  -1.821983  -1.587302  18.087855  2.800566  -3.970424  \n",
       "12   0.696379  -0.937414   0.216450   4.039168  1.230548   2.894216  \n",
       "13   0.507947   2.150538   8.531746   0.996678  1.449497   1.619451  \n",
       "14   6.165776  -3.919528   3.296703  11.830131  2.114180   4.897507  \n",
       "15   0.319315  -3.162555  -6.075534  -3.441860  0.864386   3.313933  \n",
       "16   1.659198  -1.414827   9.970238   6.644518  1.237653   1.635189  \n",
       "17  -1.907007  -4.559473   5.820106   3.825956  1.900598   4.805962  \n",
       "18  -4.395449 -11.930364  -0.813008   3.100775 -0.124901  12.656322  \n",
       "19  -3.734205 -16.028422  -3.373016  -1.882614 -0.565914   8.884454  \n",
       "20  -8.131052 -16.357592   4.515599   3.237574  0.050884  10.437049  \n",
       "21  -9.949020 -10.221234  -4.247104   4.287791 -1.178383  11.791116  \n",
       "22 -18.766689 -21.526460  -9.240924  -2.753275 -3.423262  17.489001  \n",
       "23 -30.029810 -29.771982 -20.072333 -16.924122 -8.469408  22.383186  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_overlap_df=return_and_save_final_relative_result_df_as_json_overlap(overlap_df, base_case_index=0, final_exp_results_path=FINAL_EXP_RESULTS_PATH, \n",
    "                                                                     exp_path_name=EXP_PATH_NAME, window_size_lst=WINDOW_SIZE_LST)\n",
    "relative_overlap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
