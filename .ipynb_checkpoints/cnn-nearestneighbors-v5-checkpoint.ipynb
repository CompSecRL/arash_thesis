{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Dataset [link](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fdrive.google.com%2Fdrive%2Ffolders%2F1-6sd6E3cL8uWdNeO4xiXxLbTSYXSTiVr%3Fusp%3Dsharing&data=04%7C01%7Carash.gholami%40queensu.ca%7C92b180b5b67d475fb70e08d87b7f4305%7Cd61ecb3b38b142d582c4efb2838b925c%7C1%7C0%7C637395133804367020%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=1ieR3E8llMblC0QMBtx1%2FpHS71RQuCxzdNQLmH0HJVU%3D&reserved=0)\n\nExplanation: There are **49** users' data here. Naming is in the format of Username_ExperimentNumber. Please also note that it is unprocessed raw data, which includes a lot of noise etc. \n\nExperiment-1: **20** participants are involved in this task and the participants were asked to type **a story** from **a set** of short and simple stories from the American Literature https://americanliterature.com/100-great-short-stories for **four minutes**. The story was chosen randomly by the participants.\n\nExperiment-2: 20 participants are involved in this task and for this data set, all the participants were asked to type the **same text** https://en.wikipedia.org/wiki/The_Adventures_of_Tom_Sawyer for **four minutes**. For each participant, almost the same amount of data is collected as Experiment-1. This dataset is important to be able to measure the quality of the features. The collection of this dataset is quite realistic since in a real case the users could be also asked to type a given text during their CA sessions. \n\nExperiment-3: The participants are asked to type **10 different pangram sentences**. Pangrams sentences contain** all 26 letters** of the alphabet. This data set is used to test how the **letter distribution** of text affects the result.\n\nExperiment-4: **34** participants are involved in this task and the participants were instructed to imitate someone else's typing pattern by watching the prerecorded video of the other person. For these experiments, one of the participants was recorded on video while typing a short and simple sentence for **15 seconds** from a perspective that the hand motions, smartwatch, keyboard, and the screen could be seen. Although it was not required, the perspective allowed to infer what the victim was typing by watching. This dataset was primarily used to analyze the **attacking scenarios**. It is left to the participant to type the exact same text or imitate the hand movement of the victim.\n\n-----------------------\n\nI realized that you have mostly 4s and 10s as values on column A, but I also found 2s. What does column A represent? \nA represents the sensor type:\n \n**10 for the accelerometer\n4 for the gyroscope**\n2 for the magnetic field sensor\n\nYou can find more info about sensors and constant values that are used to define the sensor type in these links:\n- https://developer.android.com/guide/topics/sensors/sensors_overview\n- https://developer.android.com/reference/android/hardware/Sensor#TYPE_LINEAR_ACCELERATION\n\nNormally, I only collected and used accelerometer and gyroscope sensor data. Magnetometer data should not be in the dataset. If there is, that means it is an **outlier**.\n \nIs column B the time stamp, is it recorded at 100HZ?\n\nIn an android app, you can set the sampling rate \"SENSOR_DELAY_NORMAL, SENSOR_DELAY_UI, SENSOR_DELAY_GAME, or SENSOR_DELAY_FASTEST \" \n\nAs far as I remember I used **\"SENSOR_DELAY_FASTEST\" for all sensors**, but in practice, I had a lot of issues regarding this. They **don't give a constant sampling rate**. Every time I used they were giving different sampling rates. They were giving different sampling rates for two sessions of the same user, for different users, or for different sensors. Therefore, I would recommend using the one in the practice, which you can calculate using the timestamp and the sample count. \n\nOn the paper you said you have recorded both accelerometer and gyro data; but you seem to only have one of them at a time on columns C, D, and F. I am guessing 10 on column A represents accelerometer and 4 gyro, but I could be wrong. \nAlso, some rows have more columns, and start with 11 on column A. What do those represent?\nPlease see the explanation above. \n \n\nIn short, I would really appreciate it if you could let me know information I need to know for working with the data.\n\nAlso, as these sensors were giving me a lot of headaches because they are really not stable, I needed to perform a lot of preprocessing. I **cleaned the outliers** before further processing. I also applied some other filtering techniques such as **moving average** to obtain more smooth data. Moreover, I needed to **clean the part the users are not performing typing operation from the beginning and end**, which you can see if you plot the values, i.e., the **user stops meaning no value and then big deviations, where the user takes off the smartwatch**.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nCORES = -1\nSEED = 567\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-19T16:21:32.858548Z","iopub.status.idle":"2022-04-19T16:21:32.859373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade pip\n# !pip install python-docx\n# !pip install antropy\n\nfrom sklearn.neighbors import NearestNeighbors\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport dataclasses\nimport math as math\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.model_selection import cross_validate, RandomizedSearchCV\nimport statsmodels.stats.api as sms\nfrom tqdm.auto import tqdm\nfrom dataclasses import asdict\nfrom sklearn import svm\nfrom tqdm import tqdm\nimport warnings\nimport random\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import roc_curve, accuracy_score, make_scorer, auc\nfrom scipy.optimize import brentq\nfrom scipy.interpolate import interp1d\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import VarianceThreshold # Feature selector\nfrom sklearn.model_selection import KFold\nfrom sklearn import metrics\n# import tensorflow as tfw\nfrom keras import backend as K\nfrom keras.models import Model\nfrom keras.layers import (\n    Dense,\n    Input,\n    Conv1D,\n    MaxPooling1D,\n    Flatten,\n    Lambda,\n    Conv2D,\n    MaxPooling2D,\n    Dropout,\n    BatchNormalization,\n    GlobalAveragePooling1D,\n    Activation\n)\nfrom keras.utils import plot_model\nfrom keras.optimizers import Adam, SGD,RMSprop\nfrom keras.models import load_model\nfrom keras.callbacks import Callback\n\n\nimport pandas as pd\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import kurtosis\nfrom scipy.stats import skew\nfrom scipy.stats import iqr\nfrom scipy.stats import median_absolute_deviation\nfrom scipy.stats import mode\nfrom scipy.signal import find_peaks\nfrom scipy.signal import peak_widths\n# from scipy.special import entr\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, MaxAbsScaler, RobustScaler, PowerTransformer\nget_new_scaler_dict = {\"StandardScaler\": StandardScaler, \"MinMaxScaler\": MinMaxScaler, \"Normalizer\": Normalizer, \n                       \"MaxAbsScaler\": MaxAbsScaler, \"RobustScaler\": RobustScaler, \"PowerTransformer\": PowerTransformer}\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import auc\n# import antropy as ant\nimport time\n# import docx\n\n# Global utitlity functions are in separate notebook\n%run ../usr/lib/utility_functions/utility_functions.ipynb\n\nnp.random.seed(SEED)\nprint(f\"Seed was set to: {SEED}\")\n\nprint(\"Setup Complete\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:21:32.860726Z","iopub.status.idle":"2022-04-19T16:21:32.861404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclasses.dataclass\nclass ExperimentParameters:\n    \"\"\"Contains all relevant parameters to run an experiment.\"\"\"\n\n    name: str  # Name of Experiments Parameter set. Used as identifier for charts etc.\n\n    # Data / Splitting:\n    frequency: int\n    feature_cols: list  # Columns used as features\n    max_subjects: int\n    user_ids: list\n    num_sample_points_per_exp: int\n    exp_begin_cutoff_idx: int\n    exp_end_cutoff_idx: int\n        \n#     exclude_subjects: list  # Don't load data from those users\n    n_valid_train_subjects: int\n    n_valid_test_subjects: int\n    n_test_train_subjects: int\n    n_test_test_subjects: int\n    seconds_per_subject_train: float\n    seconds_per_subject_test: float\n    task_types: list  # Limit scenarios to [1, 3, 5] for sitting or [2, 4, 6] for walking, or don't limit (None)\n\n    # Reshaping\n    window_size: int  # After resampling\n    nn_step_width: int\n    ocsvm_step_width: int\n\n    # Normalization\n    scaler: str  # {\"std\", \"robust\", \"minmax\"}\n    scaler_scope: str  # {\"subject\", \"session\"}\n    scaler_global: bool  # scale training and testing sets at once (True), or fit scaler on training only (False)\n\n    # Siamese Network\n    max_pairs_per_session: int  # Max. number of pairs per session\n    margin: float  # Contrastive Loss Margin\n    model_variant: str  # {\"1d\", \"2d\"} Type of architecture\n    filters: list  # List of length 4, containing number of filters for conv layers\n    epochs_best: int  # Train epochs to for final model\n    epochs_max: int\n    batch_size: int\n    optimizer: str  # Optimizer to use for Siamese Network\n    optimizer_lr: float  # Learning Rate\n    optimizer_decay: float\n\n    # OCSVM\n    ocsvm_kernel: str # ocsvm kernel\n    ocsvm_nu: float  # Best value found in random search, used for final model\n    ocsvm_gamma: float  # Best value found in random search, used for final model\n\n    # Calculated values\n    def __post_init__(self):\n        # HDF key of table:\n        self.table_name = f\"sensors_{self.frequency}hz\"\n\n        # Number of samples per _session_ used for training:\n        self.samples_per_subject_train = math.ceil(\n            (self.seconds_per_subject_train * 100)\n            / (100 / self.frequency)\n            / self.window_size\n        )\n\n        # Number of samples per _session_ used for testing:\n        self.samples_per_subject_test = math.ceil(\n            (self.seconds_per_subject_test * 100)\n            / (100 / self.frequency)\n            / self.window_size\n        )\n\n\n# INSTANCES\n# ===========================================================\n\n# NAIVE_MINMAX (2D Filters)\n# -----------------------------------------------------------\nNAIVE_MINMAX_2D = ExperimentParameters(\n    name=\"NAIVE-MINMAX-2D\",\n#     # Data / Splitting\n    frequency=100,\n    feature_cols=[\n        \"acc_x\",\n        \"acc_y\",\n        \"acc_z\",\n        \"gyr_x\",\n        \"gyr_y\",\n        \"gyr_z\"\n    ],\n    max_subjects=29,\n    user_ids = [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 28, 29, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49],\n    num_sample_points_per_exp=21000,\n    exp_begin_cutoff_idx=500,\n    exp_end_cutoff_idx=-500,\n    n_valid_train_subjects=40,\n    n_valid_test_subjects=10,\n    n_test_train_subjects=10,\n    n_test_test_subjects=30,\n    seconds_per_subject_train=67.5,\n    seconds_per_subject_test=67.5,\n    task_types=None,\n    # Reshaping\n    window_size=100 * 1,  #1 sec\n    nn_step_width=500,\n    ocsvm_step_width=125,\n    # Normalization\n    scaler=\"minmax\",\n    scaler_scope=\"subject\",\n    scaler_global=True,\n    # Siamese Network\n    model_variant=\"2d\",\n    filters=[32, 64, 128, 32],\n#     filters=[32, 64, 128, 64],\n    epochs_best=35,\n    epochs_max=40,\n    batch_size=200,\n    optimizer=\"sgd\",\n    optimizer_lr=0.01,\n    optimizer_decay=0,\n    max_pairs_per_session=60,  # => 4min\n    margin=0.2,\n    # OCSVM\n    ocsvm_kernel=\"rbf\",\n    ocsvm_nu=0.092,\n    ocsvm_gamma=1.151,\n)  # <END NAIVE_APPROACH>\n\n# VALID_MINMAX (2D)\n# -----------------------------------------------------------\nVALID_MINMAX_2D = dataclasses.replace(\n    NAIVE_MINMAX_2D,\n    name=\"VALID-MINMAX-2D\",\n    task_types=None,\n    scaler_global=False,\n    epochs_max=40,\n    ocsvm_nu=0.110,\n    ocsvm_gamma=59.636,\n)\n\n# NAIVE_ROBUST (2D)\n# -----------------------------------------------------------\nNAIVE_ROBUST_2D = dataclasses.replace(\n    NAIVE_MINMAX_2D,\n    name=\"NAIVE-ROBUST-2D\",\n    scaler=\"robust\",\n    optimizer=\"sgd\",\n    optimizer_lr=0.05, # Decreased, to avoid \"all zeros\" prediction\n    optimizer_decay=0.002,\n    epochs_best=5,\n    ocsvm_nu=0.214,\n    ocsvm_gamma=2.354,\n)\n\n# VALID_ROBUST (2D)\n# -----------------------------------------------------------\nVALID_ROBUST_2D = dataclasses.replace(\n    NAIVE_MINMAX_2D,\n    name=\"VALID-ROBUST-2D\",\n    scaler=\"robust\",\n    scaler_global=False,\n    epochs_best=6,\n    epochs_max=20,\n    optimizer=\"sgd\",\n    optimizer_lr=0.05,  # Decrease LR, to avoid \"all zeros\" prediction\n    optimizer_decay=0.002,\n    ocsvm_nu=0.190,\n    ocsvm_gamma=0.069,\n)\n\n# VALID_ROBUST (1D)\n# -----------------------------------------------------------\nVALID_ROBUST_1D = dataclasses.replace(\n    NAIVE_MINMAX_2D,\n    name=\"VALID-ROBUST-1D\",\n    scaler=\"robust\",\n    scaler_global=False,\n    model_variant=\"1d\", \n    filters=[32, 64, 128, 64],    \n    epochs_best=9,\n    epochs_max=20,\n    ocsvm_nu=0.156,\n    ocsvm_gamma=33.932,\n)\n\n# FCN_ROBUST (1D)\n# -----------------------------------------------------------\nVALID_FCN_ROBUST_125 = dataclasses.replace(\n    NAIVE_MINMAX_2D,\n    name=\"VALID-FCN-ROBUST-FINAL\",\n    task_types=[1, 2],\n    feature_cols=[\"acc_x\", \"acc_y\", \"acc_z\", 'gyr_x', 'gyr_y', 'gyr_z'], \n    frequency=100,\n    window_size=250,\n\n    scaler=\"RobustScaler\",\n    scaler_global=False,\n    seconds_per_subject_train=60 * 10,\n    seconds_per_subject_test=60 * 10,\n    max_pairs_per_session=60 * 10,\n    model_variant=\"fcn\",\n    filters=[32, 64, 32],\n    optimizer=\"adam\",\n    optimizer_lr=0.00001,\n#     optimizer_lr=0.0001,\n#     optimizer_lr=0.001,\n#     optimizer_lr=0.01,\n    optimizer_decay=None,\n    batch_size=300,\n    margin=1,\n#     margin=.2,\n    epochs_best=40,\n    epochs_max=80,\n    ocsvm_nu=0.007,\n    ocsvm_gamma=0.002,\n)\n\nVALID_FCN_ROBUST_250 = dataclasses.replace(\n    NAIVE_MINMAX_2D,\n    name=\"VALID-FCN-ROBUST-FINAL\",\n    task_types=[1, 2],\n    feature_cols=[\"acc_x\", \"acc_y\", \"acc_z\", 'gyr_x', 'gyr_y', 'gyr_z'], \n    frequency=100,\n    window_size=250,\n\n    scaler=\"RobustScaler\",\n    scaler_global=False,\n    seconds_per_subject_train=60 * 10,\n    seconds_per_subject_test=60 * 10,\n    max_pairs_per_session=60 * 10,\n    model_variant=\"fcn\",\n    filters=[32, 64, 32],\n    optimizer=\"adam\",\n    optimizer_lr=0.00001,\n#     optimizer_lr=0.0001,\n#     optimizer_lr=0.001,\n#     optimizer_lr=0.01,\n    optimizer_decay=None,\n    batch_size=300,\n    margin=1,\n#     margin=.2,\n    epochs_best=40,\n    epochs_max=80,\n    ocsvm_nu=0.007,\n    ocsvm_gamma=0.002,\n)\n\nVALID_FCN_ROBUST_500 = dataclasses.replace(\n    NAIVE_MINMAX_2D,\n    name=\"VALID-FCN-ROBUST-FINAL\",\n    task_types=[1, 2],\n    feature_cols=[\"acc_x\", \"acc_y\", \"acc_z\", 'gyr_x', 'gyr_y', 'gyr_z'], \n    frequency=100,\n    window_size=500,\n\n    scaler=\"RobustScaler\",\n    scaler_global=False,\n    seconds_per_subject_train=60 * 10,\n    seconds_per_subject_test=60 * 10,\n    max_pairs_per_session=60 * 10,\n    model_variant=\"fcn\",\n    filters=[32, 64, 32],\n    optimizer=\"adam\",\n    optimizer_lr=0.00001,\n#     optimizer_lr=0.0001,\n#     optimizer_lr=0.001,\n#     optimizer_lr=0.01,\n    optimizer_decay=None,\n    batch_size=300,\n    margin=1,\n#     margin=.2,\n    epochs_best=40,\n    epochs_max=80,\n    ocsvm_nu=0.007,\n    ocsvm_gamma=0.002,\n)\n\nVALID_FCN_ROBUST_750 = dataclasses.replace(\n    NAIVE_MINMAX_2D,\n    name=\"VALID-FCN-ROBUST-FINAL\",\n    task_types=[1, 2],\n    feature_cols=[\"acc_x\", \"acc_y\", \"acc_z\", 'gyr_x', 'gyr_y', 'gyr_z'], \n    frequency=100,\n    window_size=750,\n\n    scaler=\"RobustScaler\",\n    scaler_global=False,\n    seconds_per_subject_train=60 * 10,\n    seconds_per_subject_test=60 * 10,\n    max_pairs_per_session=60 * 10,\n    model_variant=\"fcn\",\n    filters=[32, 64, 32],\n    optimizer=\"adam\",\n    optimizer_lr=0.00001,\n#     optimizer_lr=0.0001,\n#     optimizer_lr=0.001,\n#     optimizer_lr=0.01,\n    optimizer_decay=None,\n    batch_size=300,\n    margin=1,\n#     margin=.2,\n    epochs_best=40,\n    epochs_max=80,\n    ocsvm_nu=0.007,\n    ocsvm_gamma=0.002,\n)\n\nVALID_FCN_ROBUST_1000 = dataclasses.replace(\n    NAIVE_MINMAX_2D,\n    name=\"VALID-FCN-ROBUST-FINAL\",\n    task_types=[1, 2],\n    feature_cols=[\"acc_x\", \"acc_y\", \"acc_z\", 'gyr_x', 'gyr_y', 'gyr_z'], \n    frequency=100,\n    window_size=1000,\n\n    scaler=\"RobustScaler\",\n    scaler_global=False,\n    seconds_per_subject_train=60 * 10,\n    seconds_per_subject_test=60 * 10,\n    max_pairs_per_session=60 * 10,\n    model_variant=\"fcn\",\n    filters=[32, 64, 32],\n    optimizer=\"adam\",\n    optimizer_lr=0.00001,\n#     optimizer_lr=0.0001,\n#     optimizer_lr=0.001,\n#     optimizer_lr=0.01,\n    optimizer_decay=None,\n    batch_size=300,\n    margin=1,\n#     margin=.2,\n    epochs_best=40,\n    epochs_max=80,\n    ocsvm_nu=0.007,\n    ocsvm_gamma=0.002,\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:21:32.862857Z","iopub.status.idle":"2022-04-19T16:21:32.863564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"P = VALID_FCN_ROBUST_125\n# utils_ppp(P)\n# P = VALID_ROBUST_1D\n# utils_ppp(P)\n# P = NAIVE_MINMAX_2D","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:21:32.864992Z","iopub.status.idle":"2022-04-19T16:21:32.865726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"utils_ppp(P)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:21:32.867999Z","iopub.status.idle":"2022-04-19T16:21:32.868851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.4 Select Approach <a id='1.4'>&nbsp;</a> \nSelect the parameters to use for current notebook execution here!","metadata":{}},{"cell_type":"markdown","source":"# Split Dataset for Valid/Test  \nIn two splits: one used during hyperparameter optimization, and one used during testing.\n\nThe split is done along the subjects: All sessions of a single subject will either be in the validation split or in the testing split, never in both.\n\nThey did a 30 60 split.","metadata":{}},{"cell_type":"markdown","source":"# Reshaping Raw Features.\nWe have our own function of windows for this. Do this for both training and testing.\n\n# Extracting time and frequency based features.\nAgain, we have a function for this. Do this for both training and testing.","metadata":{}},{"cell_type":"markdown","source":"# Hyperparameter Optimization \n\nI do not find any reaqsonable explaination how to use a cross-validation as we are talking about anomaly detection.\n\nI am using the experiment 1 data as train, and experiment 2 data as validation.","metadata":{}},{"cell_type":"markdown","source":"# Using SVM in a real-world Scenario with multiple genuine users and intruders\nSource: https://datascience.stackexchange.com/questions/23623/what-is-the-best-way-to-classify-data-not-belonging-to-set-of-classes\n\nStage 1: \n    Use one-class SVM to assign those images that do not belong to the set of predefined classes as the 9-th class.\n\nStage 2:\n    For those images that passes through your filter, let the multi-class SVM assign them to one of the 8 classes.","metadata":{}},{"cell_type":"markdown","source":"Loading data:","metadata":{}},{"cell_type":"code","source":"df_exps_dict = load_data_frames(P.user_ids, P.exp_begin_cutoff_idx, P.exp_end_cutoff_idx, P.num_sample_points_per_exp)\ndfList_exp1, dfList_exp2 = df_exps_dict['dfList_exp1'], df_exps_dict['dfList_exp2']","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:21:32.870185Z","iopub.status.idle":"2022-04-19T16:21:32.871032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.array(dfList_exp2[0]['EMA_x_a']).shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:21:32.872324Z","iopub.status.idle":"2022-04-19T16:21:32.873154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yf.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:21:32.52017Z","iopub.execute_input":"2022-04-19T16:21:32.520759Z","iopub.status.idle":"2022-04-19T16:21:32.850468Z","shell.execute_reply.started":"2022-04-19T16:21:32.52072Z","shell.execute_reply":"2022-04-19T16:21:32.848982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.fft import fft, ifft\nfrom scipy.fft import fft, fftfreq\nimport matplotlib.pyplot as plt\n\n\n# Number of sample points\n# for i in range(len(dfList_exp2)):\ni =1\nN = np.array(dfList_exp2[i]['EMA_x_a']).shape[0]\n# sample spacing\nT = 1.0 / 100\nx = np.linspace(0.0, N*T, N, endpoint=False)\n# y = np.array(np.sqrt(dfList_exp2[i]['EMA_x_a']**2 + dfList_exp2[i]['EMA_y_a']**2 + dfList_exp2[i]['EMA_z_a']**2))\ny = np.array(dfList_exp2[i]['EMA_x_a'])\n\nyf = fft(y)\nxf = fftfreq(N, T)[:N//2]\nplt.plot(xf, 2.0/N * np.abs(yf[0:N//2]))\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:21:32.85178Z","iopub.status.idle":"2022-04-19T16:21:32.852627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import signal\ni=1\nN = np.array(dfList_exp2[i]['EMA_x_a']).shape[0]\n# sample spacing\nT = 1.0 / 100\n\nsos = signal.butter(10, 20, 'lowpass', fs=100, output='sos')\nsig = np.array(dfList_exp2[i]['EMA_x_a'])\ny = signal.sosfilt(sos, sig) #filtered time domain signal\n\n\nyf = fft(y)\nxf = fftfreq(N, T)[:N//2]\nplt.plot(xf, 2.0/N * np.abs(yf[0:N//2]))\nplt.grid()\nplt.show()\n\n# N = np.array(dfList_exp2[i]['EMA_x_g']).shape[0]\n# # sample spacing\n# T = 1.0 / 100\n\n# t = np.linspace(0.0, N*T, N, endpoint=False)\n\n# plt.plot(t, filtered)\n# # plt.set_title('After 10 Hz high-pass filter')\n# plt.axis([0, 1, -2, 2])\n# # plt.set_xlabel('Time [seconds]')\n# plt.tight_layout()\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:21:32.85416Z","iopub.status.idle":"2022-04-19T16:21:32.855099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfList_exp2[i]['EMA_x_a'][500:1000].plot()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:21:32.856306Z","iopub.status.idle":"2022-04-19T16:21:32.85715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(y[500:1000].shape[0]), y[500:1000])\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:14:29.224532Z","iopub.status.idle":"2022-04-04T13:14:29.225304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfList_exp2[i]['EMA_x_a'].ewm(span=5,adjust=False).mean()[500:1000].plot()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T21:00:38.55673Z","iopub.execute_input":"2022-04-01T21:00:38.557083Z","iopub.status.idle":"2022-04-01T21:00:38.689165Z","shell.execute_reply.started":"2022-04-01T21:00:38.55705Z","shell.execute_reply":"2022-04-01T21:00:38.688332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"randomized_data_idx = list(range(len(P.user_ids)))\nrandom.Random(SEED).shuffle(randomized_data_idx)\nsplit_idx = 2 * (len(randomized_data_idx)//3) + 1\ntrain_set = randomized_data_idx[: split_idx]\ntest_set = randomized_data_idx[split_idx: ]\nprint(f\"train_set: {train_set}\\ntest_set: {test_set}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:21:32.874751Z","iopub.status.idle":"2022-04-19T16:21:32.875728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preparing train data\n# train_set = r\n# dfList_exp1_train, dfList_exp2_train = [dfList_exp1[i] for i in train_set], [dfList_exp2[i] for i in train_set]\n# print(f\"len(dfList_exp1_train): {len(dfList_exp1_train)}\")\n# print(f\"len(dfList_exp2_train): {len(dfList_exp2_train)}\")\n# XExpTrainDict = MakeXExpDic(dfList_exp1_train, dfList_exp2_train, window_size = 250, step = 251, numSamplePoints= 18000)\n# X_exp1_train_dic, X_exp2_train_dic = XExpTrainDict[\"X_exp1_dic\"], XExpTrainDict[\"X_exp2_dic\"]\n\n# # preparing test data\n# dfList_exp1_test, dfList_exp2_test = [dfList_exp1[i] for i in test_set], [dfList_exp2[i] for i in test_set]\n# print(f\"len(dfList_exp1_test): {len(dfList_exp1_test)}\")\n# print(f\"len(dfList_exp2_test): {len(dfList_exp2_test)}\")\n# XExpTestDict = MakeXExpDic(dfList_exp1_test, dfList_exp2_test, window_size = 250, step = 251, numSamplePoints= 18000)\n# X_exp1_test_dic, X_exp2_test_dic = XExpTestDict[\"X_exp1_dic\"], XExpTestDict[\"X_exp2_dic\"]","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:21:32.876975Z","iopub.status.idle":"2022-04-19T16:21:32.878037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN","metadata":{}},{"cell_type":"code","source":"# preparing train data\n\n# train_set = r\ndfList_exp1_train, dfList_exp2_train = [dfList_exp1[i] for i in train_set], [dfList_exp2[i] for i in train_set]\nprint(f\"len(dfList_exp1_train): {len(dfList_exp1_train)}\")\nprint(f\"len(dfList_exp2_train): {len(dfList_exp2_train)}\")\nRaw_X_Exp_Train_Dict = MakeRawXExpDic(dfList_exp1_train, dfList_exp2_train, window_size=P.window_size, step=P.nn_step_width, numSamplePoints=P.num_sample_points_per_exp, \n                                      scale_exp1=True, scale_exp2=True, scaler=P.scaler)\nRaw_X_exp1_train_dic, Raw_X_exp2_train_dic = Raw_X_Exp_Train_Dict[\"Raw_X_exp1_dic\"], Raw_X_Exp_Train_Dict[\"Raw_X_exp2_dic\"]\n\n# preparing test data\ndfList_exp1_test, dfList_exp2_test = [dfList_exp1[i] for i in test_set], [dfList_exp2[i] for i in test_set]\nprint(f\"len(dfList_exp1_test): {len(dfList_exp1_test)}\")\nprint(f\"len(dfList_exp2_test): {len(dfList_exp2_test)}\")\nRaw_X_Exp_Test_Dict = MakeRawXExpDic(dfList_exp1_test, dfList_exp2_test, window_size=P.window_size, step=P.ocsvm_step_width, numSamplePoints=P.num_sample_points_per_exp, \n                                     scale_exp1=False, scale_exp2=True, scaler=P.scaler)\n\nfitted_scaler_ocsvm_exp2_dic = Raw_X_Exp_Test_Dict[\"fitted_scaler_exp2_dic\"]\nRaw_X_exp1_test_dic, Raw_X_exp2_test_dic = Raw_X_Exp_Test_Dict[\"Raw_X_exp1_dic\"], Raw_X_Exp_Test_Dict[\"Raw_X_exp2_dic\"]","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:21:32.8796Z","iopub.status.idle":"2022-04-19T16:21:32.880498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Raw_X_exp1_train_dic[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:21:32.881751Z","iopub.status.idle":"2022-04-19T16:21:32.882574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dfList_exp1","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:21:32.883816Z","iopub.status.idle":"2022-04-19T16:21:32.884796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Raw_X_exp1_train_dic[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:21:32.886093Z","iopub.status.idle":"2022-04-19T16:21:32.886896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dfList_exp1_test","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:21:32.888122Z","iopub.status.idle":"2022-04-19T16:21:32.889057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# np.max(Raw_X_exp1_test_dic[0][0])","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:22:56.935639Z","iopub.execute_input":"2022-04-19T15:22:56.935947Z","iopub.status.idle":"2022-04-19T15:22:56.941143Z","shell.execute_reply.started":"2022-04-19T15:22:56.935919Z","shell.execute_reply":"2022-04-19T15:22:56.938706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Overview of current Experiment Parameters:**","metadata":{}},{"cell_type":"markdown","source":"#### Euclidead distance","metadata":{}},{"cell_type":"code","source":"def k_euclidean_dist(t):\n    x = t[0]\n    y = t[1]    \n    return K.sqrt(K.sum(K.square(x - y), axis=-1, keepdims=True))","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:22:57.87698Z","iopub.execute_input":"2022-04-19T15:22:57.877329Z","iopub.status.idle":"2022-04-19T15:22:57.88354Z","shell.execute_reply.started":"2022-04-19T15:22:57.877293Z","shell.execute_reply":"2022-04-19T15:22:57.882198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Loss Function","metadata":{}},{"cell_type":"code","source":"def k_contrastive_loss(y_true, dist):\n    \"\"\"Contrastive loss from Hadsell-et-al.'06\n    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n    \"\"\"\n    margin = P.margin\n    return K.mean(y_true * K.square(dist) + (1 - y_true) * K.square(K.maximum(margin - dist, 0)))","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:22:59.025546Z","iopub.execute_input":"2022-04-19T15:22:59.027389Z","iopub.status.idle":"2022-04-19T15:22:59.036533Z","shell.execute_reply.started":"2022-04-19T15:22:59.027342Z","shell.execute_reply":"2022-04-19T15:22:59.035412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def contrastive_loss_test(y_true, dist):\n    \"\"\"Test function above using implementation with numpy instead tensors.\"\"\"\n    margin = P.margin\n    return y_true * np.square(dist) + (1 - y_true) * np.square(np.max(margin - dist, 0))","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:23:00.309822Z","iopub.execute_input":"2022-04-19T15:23:00.310206Z","iopub.status.idle":"2022-04-19T15:23:00.315623Z","shell.execute_reply.started":"2022-04-19T15:23:00.310171Z","shell.execute_reply":"2022-04-19T15:23:00.314526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Positive: class=1, distance=0,     loss:\", contrastive_loss_test(1, 0))\nprint(\"Positive: class=1, distance=0.01,  loss:\", contrastive_loss_test(1, 0.01))\nprint(\"Positive: class=1, distance=0.3,   loss:\", contrastive_loss_test(1, 0.3))\nprint(\"Positive: class=1, distance=0.5,   loss:\", contrastive_loss_test(1, 0.5))\nprint(\"Positive: class=1, distance=1,     loss:\", contrastive_loss_test(1, 1))","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:23:14.442114Z","iopub.execute_input":"2022-04-19T15:23:14.442489Z","iopub.status.idle":"2022-04-19T15:23:14.450384Z","shell.execute_reply.started":"2022-04-19T15:23:14.442457Z","shell.execute_reply":"2022-04-19T15:23:14.44931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Negative: class=0, distance=0,     loss:\", contrastive_loss_test(0, 0))\nprint(\"Negative: class=0, distance=0.01,  loss:\", contrastive_loss_test(0, 0.01))\nprint(\"Negative: class=0, distance=0.3,   loss:\", contrastive_loss_test(0, 0.3))\nprint(\"Negative: class=0, distance=0.5,   loss:\", contrastive_loss_test(0, 0.5))\nprint(\"Negative: class=0, distance=1,     loss:\", contrastive_loss_test(0, 1))","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:23:14.804948Z","iopub.execute_input":"2022-04-19T15:23:14.805275Z","iopub.status.idle":"2022-04-19T15:23:14.813026Z","shell.execute_reply.started":"2022-04-19T15:23:14.805242Z","shell.execute_reply":"2022-04-19T15:23:14.811936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Siamese Model with 2D Filters, as derived from Centeno et al. (2018)","metadata":{}},{"cell_type":"code","source":"# cite: Siamese Model with 2D Filters, as derived from Centeno et al. (2018)\n# the thesis I found\ndef build_model_2d(input_shape, filters):\n    \"\"\"\n        Siamese CNN architecture with 3D input and 2D filters\n    \"\"\"\n    # Define the tensors for the two input images\n    left_inputs = Input(input_shape, name=\"left_inputs\")\n    right_inputs = Input(input_shape, name=\"right_inputs\")\n\n    # Convolutional Neural Network\n    inputs = Input(input_shape, name=\"input\")\n    x = Conv2D(filters[0], (7, 7), padding=\"same\", activation=\"tanh\", name=\"conv1\")(inputs)\n    x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp1\")(x)\n    x = Conv2D(filters[1], (5, 5), padding=\"same\", activation=\"tanh\", name=\"conv2\")(x)\n    x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp2\")(x)\n    x = Conv2D(filters[2], (3, 3), padding=\"same\", activation=\"tanh\", name=\"conv3\")(x)\n    x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp3\")(x)\n    x = Conv2D(filters[3], (3, 3), padding=\"same\", activation=\"tanh\", name=\"conv4\")(x)\n    x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp4\")(x)\n    x = Flatten(name=\"flat\")(x)\n    \n    # Basemodel instance\n    basemodel = Model(inputs, x, name=\"basemodel\")\n\n    # using same instance of \"basemodel\" to share weights between left/right networks\n    encoded_l = basemodel(left_inputs)\n    encoded_r = basemodel(right_inputs)\n\n    # Add a customized layer to compute the distance between the encodings\n    distance_layer = Lambda(k_euclidean_dist, name=\"distance\")([encoded_l, encoded_r])\n\n    # Combine into one net\n    siamese_net = Model(inputs=[left_inputs, right_inputs], outputs=distance_layer)\n\n    # return the model\n    return siamese_net, basemodel","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:23:15.306606Z","iopub.execute_input":"2022-04-19T15:23:15.306923Z","iopub.status.idle":"2022-04-19T15:23:15.32117Z","shell.execute_reply.started":"2022-04-19T15:23:15.306892Z","shell.execute_reply":"2022-04-19T15:23:15.320337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Siamese Model with 1D Filters, as derived from Centeno et al. (2018)","metadata":{}},{"cell_type":"code","source":"def build_model_1d(input_shape, filters):\n    \"\"\"\n        Model architecture\n    \"\"\"\n    # Define the tensors for the two input images\n    left_inputs = Input(input_shape, name=\"left_inputs\")\n    right_inputs = Input(input_shape, name=\"right_inputs\")\n\n    # Convolutional Neural Network\n    inputs = Input(input_shape, name=\"input\")\n    x = Conv1D(filters[0], 7, activation=\"elu\", padding=\"same\", name=\"conv1\")(inputs)\n    x = MaxPooling1D(pool_size=2, name=\"mp1\")(x)\n    x = Conv1D(filters[1], 5, activation=\"elu\", padding=\"same\", name=\"conv2\")(x)\n    x = MaxPooling1D(pool_size=2, name=\"mp2\")(x)\n    x = Conv1D(filters[2], 3, activation=\"elu\", padding=\"same\", name=\"conv3\")(x)\n    x = MaxPooling1D(pool_size=2, name=\"mp3\")(x)\n    x = Conv1D(filters[3], 3, activation=\"elu\", padding=\"same\", name=\"conv4\")(x)\n    x = MaxPooling1D(pool_size=2, name=\"mp5\")(x)\n    x = Flatten(name=\"flat\")(x)\n\n    # Generate the encodings (feature vectors) for the two images\n    basemodel = Model(inputs, x, name=\"basemodel\")\n\n    # using same instance of \"basemodel\" to share weights between left/right networks\n    encoded_l = basemodel(left_inputs)\n    encoded_r = basemodel(right_inputs)\n\n    # Add a customized layer to compute the absolute difference between the encodings\n    distance_layer = Lambda(k_euclidean_dist, name=\"distance\")([encoded_l, encoded_r])\n\n    siamese_net = Model(inputs=[left_inputs, right_inputs], outputs=distance_layer)\n\n    # return the model\n    return siamese_net, basemodel","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:23:15.85305Z","iopub.execute_input":"2022-04-19T15:23:15.853505Z","iopub.status.idle":"2022-04-19T15:23:15.865953Z","shell.execute_reply.started":"2022-04-19T15:23:15.853463Z","shell.execute_reply":"2022-04-19T15:23:15.864984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Siamese Model with FCN architecture","metadata":{}},{"cell_type":"code","source":"def build_model_fcn(input_shape, filters):\n    # Define the tensors for the two input images\n    left_inputs = Input(input_shape, name=\"left_inputs\")\n    right_inputs = Input(input_shape, name=\"right_inputs\")\n\n    # Convolutional Neural Network\n    inputs = Input(input_shape, name=\"input\")\n    x = Conv1D(filters=filters[0], kernel_size=8, strides=1, activation=None, padding=\"same\", name=\"conv1\")(inputs)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Dropout(0.1, name=\"drop1\")(x)\n    x = Conv1D(filters=filters[1], kernel_size=5, strides=1, activation=None, padding=\"same\", name=\"conv2\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Dropout(0.1, name=\"drop2\")(x)\n    x = Conv1D(filters=filters[2], kernel_size=3, strides=1, activation=None, padding=\"same\", name=\"conv3\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = GlobalAveragePooling1D()(x)\n    x = Dense(32, activation=\"sigmoid\", name=\"dense\")(x) # <--- !!!!!!!!!!!!\n#     x = Dense(64, activation=\"sigmoid\", name=\"dense\")(x) # <--- !!!!!!!!!!!!\n\n    # Basemodel instance\n    basemodel = Model(inputs, x, name=\"basemodel\")\n\n    # using same instance of \"basemodel\" to share weights between left/right networks\n    encoded_l = basemodel(left_inputs)\n    encoded_r = basemodel(right_inputs)\n\n    # Add a customized layer to compute the distance between the encodings\n    distance_layer = Lambda(k_euclidean_dist, name=\"distance\")([encoded_l, encoded_r])\n\n    # Combine into one net\n    siamese_net = Model(inputs=[left_inputs, right_inputs], outputs=distance_layer)\n\n    # return the model\n    return siamese_net, basemodel","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:23:16.401045Z","iopub.execute_input":"2022-04-19T15:23:16.401383Z","iopub.status.idle":"2022-04-19T15:23:16.414088Z","shell.execute_reply.started":"2022-04-19T15:23:16.401351Z","shell.execute_reply":"2022-04-19T15:23:16.413039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model(name, window_size, feature_cols, filters):\n    print(f\"Using Model variant {name}...\")\n    if name == \"1d\":\n        model, basemodel = build_model_1d((window_size, len(feature_cols)), filters)\n    elif name == \"2d\":\n        model, basemodel = build_model_2d((window_size, len(feature_cols), 1), filters)\n    elif name == \"fcn\":\n        model, basemodel = build_model_fcn((window_size, len(feature_cols)), filters)\n    else:\n        raise BaseException(\"Error: Not a valid model name: {1d, 2d, fcn}\")\n\n    return model, basemodel","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:23:16.68468Z","iopub.execute_input":"2022-04-19T15:23:16.684986Z","iopub.status.idle":"2022-04-19T15:23:16.692406Z","shell.execute_reply.started":"2022-04-19T15:23:16.684956Z","shell.execute_reply":"2022-04-19T15:23:16.69133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_optimizer(name, lr=None, decay=None):\n    if name == \"sgd\":\n        lr = lr if lr != None else 0.01\n        decay = decay if decay != None else 0\n        optimizer = SGD(lr=lr, decay=decay)\n    elif name == \"adam\":\n        lr = lr if lr != None else 0.001\n        decay = decay if decay != None else 0\n        optimizer = Adam(lr=lr, decay=decay)\n    elif name == \"rmsprop\":\n        lr = lr if lr != None else 0.001\n        optimizer = RMSprop(lr=lr)\n    else:\n        raise BaseException(\"Error: Not a valid model name: 1d or 2d.\")\n    return optimizer","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:23:17.09999Z","iopub.execute_input":"2022-04-19T15:23:17.100337Z","iopub.status.idle":"2022-04-19T15:23:17.107971Z","shell.execute_reply.started":"2022-04-19T15:23:17.100303Z","shell.execute_reply":"2022-04-19T15:23:17.106824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Inspect Model Architecture","metadata":{}},{"cell_type":"code","source":"temp_model, temp_basemodel = get_model(P.model_variant, P.window_size, P.feature_cols, P.filters)\n\ntemp_basemodel.summary()\ntemp_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:06:57.92849Z","iopub.execute_input":"2022-04-05T16:06:57.928952Z","iopub.status.idle":"2022-04-05T16:07:00.391466Z","shell.execute_reply.started":"2022-04-05T16:06:57.928912Z","shell.execute_reply":"2022-04-05T16:07:00.390676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from itertools import product\nimport numpy as np\n# c = list(product(a, b))\na1 = [1, 2, 5, 6]\na2 = [1, 2, 5, 6]\n# a2 = [11,22,33,44]\n# a2 = [3, 4, 5]\nmesh = np.array(np.meshgrid(a1, a2))\nprint(mesh)\nmesh = mesh.T.reshape(-1, 2)\n# print(np.array([mesh[:, 1], mesh[:, 0]]).T) good for u1u2 or u2u1 reversing the generated pairs\nmesh = mesh.reshape(4,4,2)\n# print(mesh)\ndef deleteDiagonal(array):\n    depth = array.shape[-1]\n    m = array.shape[1]\n    strided = np.lib.stride_tricks.as_strided\n    s0,s1,s2 = array.strides\n    return strided(array.ravel()[depth:], shape=(m-1, m, depth), strides=(s0+s1,s1, s2)).reshape(m, m-1, depth)\nprint(mesh.shape)\ndeleteDiagonal(mesh).reshape(12,2)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:23:19.988459Z","iopub.execute_input":"2022-04-19T15:23:19.988783Z","iopub.status.idle":"2022-04-19T15:23:20.004931Z","shell.execute_reply.started":"2022-04-19T15:23:19.988753Z","shell.execute_reply":"2022-04-19T15:23:20.003972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_negative_index_pairs(diagonal_size):\n    '''\n    return an array of indices for negative pairs for user u1 and u2, where u1 != u2\n    u1 is the genuine user and u2 is the imposter.\n    '''\n    indices = list(range(diagonal_size))\n    mesh = np.array(np.meshgrid(indices, indices))\n    negative_index_pairs = mesh.T.reshape(-1, 2)\n    \n    return negative_index_pairs\n\ndef generate_positive_index_pairs(diagonal_size):\n    '''\n    return an array of tuple indices for the positive pairs.\n    '''\n    indices = list(range(diagonal_size))\n    mesh = np.array(np.meshgrid(indices, indices))\n    positive_index_pairs = mesh.T.reshape(-1, 2)\n    positive_index_pairs = positive_index_pairs.reshape(diagonal_size, diagonal_size, 2)\n\n    new_array_size = diagonal_size * diagonal_size - diagonal_size\n\n    positive_index_pairs = deleteDiagonal(positive_index_pairs).reshape(new_array_size, 2)\n    return positive_index_pairs\n\ndef generate_pairs(X_exp_dic):\n    '''\n    return positive and negative pairs\n    '''\n    num_users = len(X_exp_dic)\n#     X_neg[np.random.choice(X_neg.shape[0], size=X_pos.shape[0], replace=False), :]\n    negative_pairs_dic = {}\n    positive_pairs_dic = {}\n    \n    total_samples_per_user = len(X_exp_dic[list(X_exp_dic.keys())[0]])\n    positive_pairs_indices = generate_positive_index_pairs(total_samples_per_user)\n    negative_pairs_indices = generate_negative_index_pairs(total_samples_per_user)\n    \n    positive_left = positive_pairs_indices[:, 0]\n    positive_right = positive_pairs_indices[:, 1]\n    \n    negative_left = negative_pairs_indices[:, 0]\n    negative_right = negative_pairs_indices[:, 1]\n    \n    neg_fraction =  (total_samples_per_user-1) / (total_samples_per_user * (num_users-1))\n    for u1_key in tqdm(X_exp_dic):\n        \n        positive_pairs_dic[u1_key] = np.array(list(zip(X_exp_dic[u1_key][positive_left], X_exp_dic[u1_key][positive_right])))\n        \n        for u2_key in X_exp_dic:\n            \n            # should i eliminate less informative pairs like (w_n, w_m) vs (w_m, w_n)?\n            if u2_key != u1_key:\n                negative_pairs_dic[(u1_key, u2_key)] = np.array(list(zip(X_exp_dic[u1_key][negative_left], X_exp_dic[u2_key][negative_right])))\n                \n                new_size = int(negative_pairs_dic[(u1_key, u2_key)].shape[0] * neg_fraction)\n                negative_pairs_dic[(u1_key, u2_key)] = negative_pairs_dic[(u1_key, u2_key)][np.random.choice(\n                    negative_pairs_dic[(u1_key, u2_key)].shape[0], size=new_size, replace=False), :]\n            \n    return {\"positive_pairs_dic\": positive_pairs_dic, \"negative_pairs_dic\": negative_pairs_dic}","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:23:20.475971Z","iopub.execute_input":"2022-04-19T15:23:20.47629Z","iopub.status.idle":"2022-04-19T15:23:20.492619Z","shell.execute_reply.started":"2022-04-19T15:23:20.476253Z","shell.execute_reply":"2022-04-19T15:23:20.491642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"3/2","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:23:22.324806Z","iopub.execute_input":"2022-04-19T15:23:22.325122Z","iopub.status.idle":"2022-04-19T15:23:22.331259Z","shell.execute_reply.started":"2022-04-19T15:23:22.325092Z","shell.execute_reply":"2022-04-19T15:23:22.330365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import gc\n# # del d\n# gc.collect()\n# # del d2\n# # gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:23:22.688084Z","iopub.execute_input":"2022-04-19T15:23:22.68846Z","iopub.status.idle":"2022-04-19T15:23:22.692928Z","shell.execute_reply.started":"2022-04-19T15:23:22.688426Z","shell.execute_reply":"2022-04-19T15:23:22.691927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test\n# X_exp_dic = {\n#     1: np.array(['u1_1', 'u1_2', 'u1_3', 'u1_4']),\n#     2: np.array(['u2_1', 'u2_2', \"u2_3\", 'u2_4'])\n# }\n\n# negative_pairs_indices = generate_positive_index_pairs(4)\n# print(negative_pairs_indices)\n# negative_left = negative_pairs_indices[:, 0]\n# negative_right = negative_pairs_indices[:, 1]\n# print(list(zip(X_exp_dic[1][negative_left], X_exp_dic[2][negative_right])))\n# # print('-----------------')\n# # print(generate_positive_index_pairs(5))\n# d = generate_pairs(Raw_X_exp1_train_dic)\n# # generate_pairs(X_exp_dic, len(X_exp_dic[1]))['positive_pairs_dic'][1].shape\n# # len(Raw_X_exp1_train_dic[0])","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:23:23.000607Z","iopub.execute_input":"2022-04-19T15:23:23.000934Z","iopub.status.idle":"2022-04-19T15:23:23.007155Z","shell.execute_reply.started":"2022-04-19T15:23:23.000902Z","shell.execute_reply":"2022-04-19T15:23:23.006227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# d['negative_pairs_dic'][1,2][:, 1, :, :].shape","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:23:23.469159Z","iopub.execute_input":"2022-04-19T15:23:23.469616Z","iopub.status.idle":"2022-04-19T15:23:23.476228Z","shell.execute_reply.started":"2022-04-19T15:23:23.469574Z","shell.execute_reply":"2022-04-19T15:23:23.474989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"342 * 284","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:23:23.847181Z","iopub.execute_input":"2022-04-19T15:23:23.847524Z","iopub.status.idle":"2022-04-19T15:23:23.85312Z","shell.execute_reply.started":"2022-04-19T15:23:23.847492Z","shell.execute_reply":"2022-04-19T15:23:23.852047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"5112*19 ","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:23:24.304318Z","iopub.execute_input":"2022-04-19T15:23:24.30463Z","iopub.status.idle":"2022-04-19T15:23:24.310113Z","shell.execute_reply.started":"2022-04-19T15:23:24.3046Z","shell.execute_reply":"2022-04-19T15:23:24.309053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spliter = 2*len(Raw_X_exp2_train_dic)//3\ncnn_train = {key: Raw_X_exp2_train_dic[key] for key in list(Raw_X_exp2_train_dic.keys())[:spliter]}\ncnn_valid = {key: Raw_X_exp2_train_dic[key] for key in list(Raw_X_exp2_train_dic.keys())[spliter:]}","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:51:45.344286Z","iopub.execute_input":"2022-04-19T15:51:45.344633Z","iopub.status.idle":"2022-04-19T15:51:45.353983Z","shell.execute_reply.started":"2022-04-19T15:51:45.344604Z","shell.execute_reply":"2022-04-19T15:51:45.353108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cnn_train = {key: Raw_X_exp2_train_dic[key] for key in list(Raw_X_exp2_train_dic.keys())}\n# cnn_valid = {key: Raw_X_exp2_test_dic[key] for key in list(Raw_X_exp2_test_dic.keys())}","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:51:49.672802Z","iopub.execute_input":"2022-04-19T15:51:49.673118Z","iopub.status.idle":"2022-04-19T15:51:49.677595Z","shell.execute_reply.started":"2022-04-19T15:51:49.673086Z","shell.execute_reply":"2022-04-19T15:51:49.676444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"train set: {cnn_train.keys()}\\nvalidation set: {cnn_valid.keys()}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:51:50.047089Z","iopub.execute_input":"2022-04-19T15:51:50.047447Z","iopub.status.idle":"2022-04-19T15:51:50.052355Z","shell.execute_reply.started":"2022-04-19T15:51:50.047415Z","shell.execute_reply":"2022-04-19T15:51:50.051399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# d2 = get_pos_array(d[\"negative_pairs_dic\"])","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:51:50.405908Z","iopub.execute_input":"2022-04-19T15:51:50.406195Z","iopub.status.idle":"2022-04-19T15:51:50.409927Z","shell.execute_reply.started":"2022-04-19T15:51:50.406168Z","shell.execute_reply":"2022-04-19T15:51:50.409026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# d2.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:51:50.69801Z","iopub.execute_input":"2022-04-19T15:51:50.698311Z","iopub.status.idle":"2022-04-19T15:51:50.702078Z","shell.execute_reply.started":"2022-04-19T15:51:50.698282Z","shell.execute_reply":"2022-04-19T15:51:50.700948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_pos_array(X_dic):\n    \n    return np.concatenate([X_dic[key] for key in X_dic])\n\ndef get_neg_array(X_dic):\n    \n    return np.concatenate([X_dic[key] for key in X_dic])","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:51:50.915554Z","iopub.execute_input":"2022-04-19T15:51:50.915908Z","iopub.status.idle":"2022-04-19T15:51:50.921325Z","shell.execute_reply.started":"2022-04-19T15:51:50.915876Z","shell.execute_reply":"2022-04-19T15:51:50.920111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prep_X_y_pair(data_dic):\n    \n    X_dic = generate_pairs(data_dic)\n    pos_X, neg_X = get_pos_array(X_dic['positive_pairs_dic']), get_neg_array(X_dic['negative_pairs_dic'])\n    \n    pos_y = np.repeat(1., pos_X.shape[0]).reshape((pos_X.shape[0], 1))\n    neg_y = np.repeat(0., neg_X.shape[0]).reshape((neg_X.shape[0], 1))\n    \n    pos_X_left = pos_X[:, 0, :, :]\n    pos_X_right = pos_X[:, 1, :, :]\n    \n    neg_X_left = neg_X[:, 0, :, :]\n    neg_X_right = neg_X[:, 1, :, :]\n    \n    X_left = np.concatenate([pos_X_left, neg_X_left])\n    X_right = np.concatenate([pos_X_right, neg_X_right])\n    \n    X = [X_left, X_right]\n    y = np.concatenate([pos_y, neg_y])\n    \n    return X, y","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:51:51.135643Z","iopub.execute_input":"2022-04-19T15:51:51.135911Z","iopub.status.idle":"2022-04-19T15:51:51.145349Z","shell.execute_reply.started":"2022-04-19T15:51:51.135884Z","shell.execute_reply":"2022-04-19T15:51:51.14428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_train[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:51:51.673562Z","iopub.execute_input":"2022-04-19T15:51:51.673872Z","iopub.status.idle":"2022-04-19T15:51:51.679745Z","shell.execute_reply.started":"2022-04-19T15:51:51.673843Z","shell.execute_reply":"2022-04-19T15:51:51.678746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, y_train = prep_X_y_pair(cnn_train)\nX_valid, y_valid = prep_X_y_pair(cnn_valid)\n\n# 2D Filter Model needs flat 4th dimension\nif P.model_variant == \"2d\":\n    X_train[0] = X_train[0].reshape((*X_train[0].shape, 1))\n    X_train[1] = X_train[1].reshape((*X_train[1].shape, 1))\n    X_valid[0] = X_valid[0].reshape((*X_valid[0].shape, 1))\n    X_valid[1] = X_valid[1].reshape((*X_valid[1].shape, 1))\n\nprint(\n    f\"Training samples:   {y_train.shape[0]}, shape: {X_train[0].shape},\"\n    + f\" class balance: {np.unique(y_train, return_counts=True)}\"\n)\nprint(\n    f\"Validation samples: {y_valid.shape[0]}, shape: {X_valid[0].shape},\"\n    + f\" class balance: {np.unique(y_valid, return_counts=True)}\"\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:51:52.492017Z","iopub.execute_input":"2022-04-19T15:51:52.492443Z","iopub.status.idle":"2022-04-19T15:51:59.086839Z","shell.execute_reply.started":"2022-04-19T15:51:52.492399Z","shell.execute_reply":"2022-04-19T15:51:59.08595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"utils_ppp(P)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:52:16.038344Z","iopub.execute_input":"2022-04-19T15:52:16.038678Z","iopub.status.idle":"2022-04-19T15:52:16.057717Z","shell.execute_reply.started":"2022-04-19T15:52:16.038646Z","shell.execute_reply":"2022-04-19T15:52:16.056754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(y_train[0][0])","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:07:24.414784Z","iopub.execute_input":"2022-04-05T16:07:24.415049Z","iopub.status.idle":"2022-04-05T16:07:24.422611Z","shell.execute_reply.started":"2022-04-05T16:07:24.415011Z","shell.execute_reply":"2022-04-05T16:07:24.421636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.4 Search optimal Epoch <a id='4.4'>&nbsp;</a> ","metadata":{}},{"cell_type":"code","source":"class MetricsCallback(Callback):\n    \"\"\"\n    Custom Keras Callback function.\n    \n    Used to predict and plot distances for positive and negative pairs\n    after each n-th epoch, along with some 'classification' metrics. \n    'Classification' here means to ability to distinguish between positive \n    and negative pairs using a threshold for the distance.\n    \n    Arguments:\n        payload {tuple}           -- Datasets used for evaluation: (X_valid, y_valid, X_train, y_train)\n        epoch_evaluate_freq {int} -- Frequency for evaluation. After every n-th epoch, \n                                     the results are evaluated and printed\n        save_plots {boolean}      -- Do you want to save plots as PDF? Path is configured via global\n                                     parameter REPORT_PATH.\n    \"\"\"\n\n    def __init__(self, payload, epoch_evaluate_freq=1, loss_record_dict={}, save_plots=False):\n\n        self.X_valid, self.y_valid, self.X_train, self.y_train = payload\n        self.save_plots = save_plots\n        self.epoch_evaluate_freq = epoch_evaluate_freq\n        self.loss_record_dict = loss_record_dict\n        self.epoch = []\n        self.history = {}\n        \n        # Do we have train and valid set?\n        self.sets = []\n        if self.X_train:\n            self.sets.append([self.X_train, self.y_train, \"Train\"])\n        if self.X_valid:\n            self.sets.append([self.X_valid, self.y_valid, \"Valid\"])\n\n    def on_train_begin(self, logs={}):\n\n        print(32 * \"=\" + f\"[ Initial State ]\" + 32 * \"=\", end=\"\")\n        for X, y, desc in self.sets:\n            self.evaluate(X, y, logs, desc, -1)\n            \n\n    def on_train_end(self, logs={}):\n        print(32 * \"=\" + f\"[ Final State ]\" + 32 * \"=\", end=\"\")\n        for X, y, desc in self.sets:\n            self.evaluate(X, y, logs, desc, -1)\n\n    def on_epoch_end(self, epoch, logs={}):\n        print(32 * \"=\" + f\"[   Epoch {epoch}   ]\" + 32 * \"=\", end=\"\")\n        if epoch % self.epoch_evaluate_freq == 0:  # Evaluate only every n-th epoch\n            for X, y, desc in self.sets:\n                self.evaluate(X, y, logs, desc, epoch)\n        else:\n            print(f\"\\n{ ', '.join([k + ': ' + f'{v:.3f}' for k,v in logs.items()]) }\")\n            \n\n        logs = logs or {}\n        self.epoch.append(epoch)\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n            \n        self.loss_record_dict['loss'].append(logs['loss'])\n\n        self.loss_record_dict['val_loss'].append(logs['val_loss'])\n\n            \n    def evaluate(self, X, y, logs, desc, epoch):\n        # Predict\n        y_score = self.model.predict(X)\n        y_score_neg = y_score * -1  # lower distance means closer to positive class\n\n        # Calc Metrics\n        roc_val = metrics.roc_auc_score(y, y_score_neg)\n        eer_val, thres = utils_eer(y, y_score_neg, True)\n        y_pred = np.where(y_score_neg > thres, 1, 0)\n        acc_val = metrics.accuracy_score(y, y_pred)\n        f1_val = metrics.f1_score(y, y_pred)\n\n        print(\n            f\"\\n{desc.upper()}: roc_auc: {roc_val:.4f}, \"\n            + f\"eer: {eer_val:.4f}, thres: {thres*-1:.4f} => \"\n            + f\"acc: {acc_val:.4f}, f1: {f1_val:.4f}\\n\"\n            + f\"{ ', '.join([k + ': ' + f'{v:.3f}' for k,v in logs.items()]) }\"\n        )\n\n        # Plot distances\n        mask = np.where(y == 1, True, False)\n        dist_positive = y_score[mask]\n        dist_negative = y_score[~mask]\n        plt = utils_plot_distance_hist(\n            dist_positive, dist_negative, thres * -1, desc=desc, fig_size=(12, 2), margin=P.margin\n        )\n\n#         if self.save_plots:\n#             utils_save_plot(\n#                 plt,\n#                 REPORT_PATH\n#                 / f\"buech2019-siamese-{P.name.lower()}-epoch-{epoch+1}-{desc.lower()}.pdf\",\n#             )\n\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:52:20.083056Z","iopub.execute_input":"2022-04-19T15:52:20.083417Z","iopub.status.idle":"2022-04-19T15:52:20.106171Z","shell.execute_reply.started":"2022-04-19T15:52:20.083383Z","shell.execute_reply":"2022-04-19T15:52:20.104813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:52:20.386379Z","iopub.execute_input":"2022-04-19T15:52:20.386695Z","iopub.status.idle":"2022-04-19T15:52:20.412246Z","shell.execute_reply.started":"2022-04-19T15:52:20.386666Z","shell.execute_reply":"2022-04-19T15:52:20.410652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select model architecture\nmodel, basemodel = get_model(P.model_variant, P.window_size, P.feature_cols, P.filters)\nprint(P)\nbasemodel.summary()\nmodel.summary()\n# Select Optimizer\nP.optimizer_lr=0.00001\noptimizer = get_optimizer(P.optimizer, P.optimizer_lr)\n\n# Compile\nwarnings.filterwarnings(\"ignore\")\nmodel.compile(loss=k_contrastive_loss, optimizer=optimizer)\nloss_record_dict = {'loss': [], 'val_loss': []}\n\n\n# Train\nfitted_model = model.fit(\n    x=X_train,\n    y=y_train,\n    batch_size=P.batch_size,\n#     epochs=P.epochs_max,\n    epochs=P.epochs_best,\n    verbose=0,\n    validation_data=(X_valid, y_valid),\n    callbacks=[MetricsCallback((X_valid, y_valid, X_train, y_train), epoch_evaluate_freq=5, loss_record_dict=loss_record_dict, save_plots=True)],\n)\n# print(fitted_model.history)\nprint(loss_record_dict)\nprint(\"Training History:\")\n# plt = utils_plot_training_loss(fitted_model)\nplt = utils_plot_training_loss(loss_record_dict)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T15:57:16.125373Z","iopub.execute_input":"2022-04-19T15:57:16.125725Z","iopub.status.idle":"2022-04-19T15:59:49.840197Z","shell.execute_reply.started":"2022-04-19T15:57:16.125692Z","shell.execute_reply":"2022-04-19T15:59:49.839266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_record_dict","metadata":{"execution":{"iopub.status.busy":"2022-04-05T01:13:46.065753Z","iopub.execute_input":"2022-04-05T01:13:46.066097Z","iopub.status.idle":"2022-04-05T01:13:46.07239Z","shell.execute_reply.started":"2022-04-05T01:13:46.066061Z","shell.execute_reply":"2022-04-05T01:13:46.071395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt = utils_plot_training_loss(loss_record_dict)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T01:13:47.007949Z","iopub.execute_input":"2022-04-05T01:13:47.008274Z","iopub.status.idle":"2022-04-05T01:13:47.173068Z","shell.execute_reply.started":"2022-04-05T01:13:47.008243Z","shell.execute_reply":"2022-04-05T01:13:47.172169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.5 Check Distances <a id='4.5'>&nbsp;</a> ","metadata":{}},{"cell_type":"code","source":"# Predic validation set\ndists = model.predict(X_valid)\n\n# Stats\nprint(f\"Mean distance: {dists.mean():.5f}\")\nprint(f\"Max distance: {dists.max():.5f}\")\nprint(f\"Min distance: {dists.min():.5f}\\n\")\n\n# Histrogram\nprint(\"\\nHistogram of Pair Distances:\")\neer_val, thres = utils_eer(y_valid, dists, True)\nmask = np.where(y_valid == 1, True, False)\ndist_positive = dists[mask]\ndist_negative = dists[~mask]\nplt = utils_plot_distance_hist(dist_positive, dist_negative, thres, \"Valid\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T01:41:21.357486Z","iopub.execute_input":"2022-04-05T01:41:21.357822Z","iopub.status.idle":"2022-04-05T01:41:23.906625Z","shell.execute_reply.started":"2022-04-05T01:41:21.357791Z","shell.execute_reply":"2022-04-05T01:41:23.905778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eer_val","metadata":{"execution":{"iopub.status.busy":"2022-04-05T01:14:04.070182Z","iopub.execute_input":"2022-04-05T01:14:04.070545Z","iopub.status.idle":"2022-04-05T01:14:04.077694Z","shell.execute_reply.started":"2022-04-05T01:14:04.07051Z","shell.execute_reply":"2022-04-05T01:14:04.076402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.6 Rebuild and train to optimal Epoch  <a id='4.6'>&nbsp;</a> \nNow, that we know the learning curve, we can rebuild the model and train it until the best Epoch.\n\nAlso, we will include the validation data to have more training data. \n\n**Note:** This also means, that the training metrics are not valid anymore, because we don't have any validation data left to test against...","metadata":{}},{"cell_type":"code","source":"P.epochs_best = 16","metadata":{"execution":{"iopub.status.busy":"2022-03-31T04:49:19.648857Z","iopub.execute_input":"2022-03-31T04:49:19.649181Z","iopub.status.idle":"2022-03-31T04:49:19.653305Z","shell.execute_reply.started":"2022-03-31T04:49:19.649151Z","shell.execute_reply":"2022-03-31T04:49:19.652339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-03-31T04:50:29.34497Z","iopub.execute_input":"2022-03-31T04:50:29.345366Z","iopub.status.idle":"2022-03-31T04:50:29.351733Z","shell.execute_reply.started":"2022-03-31T04:50:29.34533Z","shell.execute_reply":"2022-03-31T04:50:29.35039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concat train & valid data\nX_train_valid = [[], []]\nX_train_valid[0] = np.vstack([X_train[0], X_valid[0]])\nX_train_valid[1] = np.vstack([X_train[1], X_valid[1]])\ny_train_valid = np.vstack([y_train, y_valid])\n\n# Select model architecture\nmodel, basemodel = get_model(P.model_variant, P.window_size, P.feature_cols, P.filters)\n\n# Select Optimizer\noptimizer = get_optimizer(P.optimizer, P.optimizer_lr)\n    \n# Compile\nmodel.compile(loss=k_contrastive_loss, optimizer=optimizer)\n\n# Train\nhistory = model.fit(\n    x=X_train_valid,\n    y=y_train_valid,\n    batch_size=P.batch_size,\n    epochs=P.epochs_best,\n    verbose=0,\n    callbacks=[MetricsCallback((None, None, X_train, y_train), epoch_evaluate_freq=10, save_plots=False)],\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T04:50:43.978743Z","iopub.execute_input":"2022-03-31T04:50:43.979074Z","iopub.status.idle":"2022-03-31T04:50:56.450417Z","shell.execute_reply.started":"2022-03-31T04:50:43.979042Z","shell.execute_reply":"2022-03-31T04:50:56.448977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i in range(len(train_set)):\n#     a = OneClassSVMSets(i, X_exp1_dic,X_exp2_dic)\n#     a[\"X_train\"].shape\n#     a['X_test_regular'][90, 23]\n#     print(np.argwhere(np.isnan(a['X_test_anomalous'])))","metadata":{"execution":{"iopub.status.busy":"2022-02-22T13:43:20.789438Z","iopub.execute_input":"2022-02-22T13:43:20.789855Z","iopub.status.idle":"2022-02-22T13:43:20.79574Z","shell.execute_reply.started":"2022-02-22T13:43:20.789814Z","shell.execute_reply":"2022-02-22T13:43:20.794817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(a[\"X_train\"].shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T13:43:20.797175Z","iopub.execute_input":"2022-02-22T13:43:20.797858Z","iopub.status.idle":"2022-02-22T13:43:20.801908Z","shell.execute_reply.started":"2022-02-22T13:43:20.79777Z","shell.execute_reply":"2022-02-22T13:43:20.800962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pca_fs = pca_feature_selector(5)\n# pca_fs.add_user_pca(0, pca)\n# pca_fs.add_user_pca(1, pca)\n# pca_fs.add_user_pca(2, pca)\n# pca_fs.add_user_pca(3, pca)\n# pca_fs.add_user_pca(4, pca)\n# pca_fs.get_comparison_matrix()\n# pca_fs.find_top_n_features()\n# pca_fs._top_feature_matrix","metadata":{"execution":{"iopub.status.busy":"2022-02-22T13:43:20.803329Z","iopub.execute_input":"2022-02-22T13:43:20.803824Z","iopub.status.idle":"2022-02-22T13:43:20.809799Z","shell.execute_reply.started":"2022-02-22T13:43:20.803784Z","shell.execute_reply":"2022-02-22T13:43:20.808893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = getIndices2(sampleSize=10, step=2, numSamplePoints= 30)\n# n = n % 10\nn","metadata":{"execution":{"iopub.status.busy":"2022-04-05T15:14:22.96183Z","iopub.execute_input":"2022-04-05T15:14:22.962216Z","iopub.status.idle":"2022-04-05T15:14:22.970052Z","shell.execute_reply.started":"2022-04-05T15:14:22.962179Z","shell.execute_reply":"2022-04-05T15:14:22.968921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.bincount(n[:,0])","metadata":{"execution":{"iopub.status.busy":"2022-02-22T13:43:20.821537Z","iopub.execute_input":"2022-02-22T13:43:20.82198Z","iopub.status.idle":"2022-02-22T13:43:20.829069Z","shell.execute_reply.started":"2022-02-22T13:43:20.82194Z","shell.execute_reply":"2022-02-22T13:43:20.828178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.argmax(np.bincount(n[:,0])[3:])","metadata":{"execution":{"iopub.status.busy":"2022-02-22T13:43:20.831776Z","iopub.execute_input":"2022-02-22T13:43:20.832043Z","iopub.status.idle":"2022-02-22T13:43:20.839073Z","shell.execute_reply.started":"2022-02-22T13:43:20.832018Z","shell.execute_reply":"2022-02-22T13:43:20.838162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.linspace(0, .5)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T13:43:20.840709Z","iopub.execute_input":"2022-02-22T13:43:20.841332Z","iopub.status.idle":"2022-02-22T13:43:20.849521Z","shell.execute_reply.started":"2022-02-22T13:43:20.841296Z","shell.execute_reply":"2022-02-22T13:43:20.848211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.repeat(1, 10)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-22T13:43:20.851572Z","iopub.execute_input":"2022-02-22T13:43:20.852287Z","iopub.status.idle":"2022-02-22T13:43:20.86057Z","shell.execute_reply.started":"2022-02-22T13:43:20.852239Z","shell.execute_reply":"2022-02-22T13:43:20.859166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# spliter = 2*len(Raw_X_exp2_test_dic)//3\nspliter=len(Raw_X_exp2_test_dic)\nkNN_train_train = {key: Raw_X_exp2_test_dic[key] for key in list(Raw_X_exp2_test_dic.keys())[:spliter]}\nkNN_train_valid = {key: Raw_X_exp1_test_dic[key] for key in list(Raw_X_exp1_test_dic.keys())[:spliter]}\nkNN_test_train = {key: Raw_X_exp2_test_dic[key] for key in list(Raw_X_exp2_test_dic.keys())[spliter:]}\nkNN_test_test = {key: Raw_X_exp1_test_dic[key] for key in list(Raw_X_exp1_test_dic.keys())[spliter:]}","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:06:44.631897Z","iopub.execute_input":"2022-04-19T16:06:44.632265Z","iopub.status.idle":"2022-04-19T16:06:44.639912Z","shell.execute_reply.started":"2022-04-19T16:06:44.632227Z","shell.execute_reply":"2022-04-19T16:06:44.638807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"train set: {OCSVM_train_train.keys()}\\nvalidation set: {OCSVM_train_valid.keys()}\")\nprint(f\"test_train set: {OCSVM_test_train.keys()}\\ntest_test set: {OCSVM_test_test.keys()}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:03:38.526532Z","iopub.execute_input":"2022-04-19T16:03:38.526862Z","iopub.status.idle":"2022-04-19T16:03:38.532228Z","shell.execute_reply.started":"2022-04-19T16:03:38.526831Z","shell.execute_reply":"2022-04-19T16:03:38.531184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.get_layer(\"basemodel\").get_output_at(1)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:02:26.683529Z","iopub.execute_input":"2022-04-19T16:02:26.684001Z","iopub.status.idle":"2022-04-19T16:02:26.691835Z","shell.execute_reply.started":"2022-04-19T16:02:26.683959Z","shell.execute_reply":"2022-04-19T16:02:26.690799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.layers","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:02:24.169117Z","iopub.execute_input":"2022-04-19T16:02:24.169456Z","iopub.status.idle":"2022-04-19T16:02:24.174765Z","shell.execute_reply.started":"2022-04-19T16:02:24.169423Z","shell.execute_reply":"2022-04-19T16:02:24.173927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract one of the child networks\ndeep_feature_model = Model(\n    inputs=model.layers[0].get_input_at(0),  # get_layer(\"left_inputs\").input,\n    outputs=model.get_layer(\"basemodel\").get_output_at(0),\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:02:16.564606Z","iopub.execute_input":"2022-04-19T16:02:16.564991Z","iopub.status.idle":"2022-04-19T16:02:16.581754Z","shell.execute_reply.started":"2022-04-19T16:02:16.564952Z","shell.execute_reply":"2022-04-19T16:02:16.580867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deep_feature_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:02:18.161962Z","iopub.execute_input":"2022-04-19T16:02:18.162299Z","iopub.status.idle":"2022-04-19T16:02:18.17226Z","shell.execute_reply.started":"2022-04-19T16:02:18.162265Z","shell.execute_reply":"2022-04-19T16:02:18.171065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kNN_train_train[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:06:54.767751Z","iopub.execute_input":"2022-04-19T16:06:54.768187Z","iopub.status.idle":"2022-04-19T16:06:54.777331Z","shell.execute_reply.started":"2022-04-19T16:06:54.768142Z","shell.execute_reply":"2022-04-19T16:06:54.776202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fitted_scaler_ocsvm_exp2_dic.keys()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T15:57:27.32041Z","iopub.execute_input":"2022-04-05T15:57:27.320763Z","iopub.status.idle":"2022-04-05T15:57:27.327448Z","shell.execute_reply.started":"2022-04-05T15:57:27.320732Z","shell.execute_reply":"2022-04-05T15:57:27.325645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OCSVM_train_valid[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-04-05T15:57:28.143923Z","iopub.execute_input":"2022-04-05T15:57:28.144331Z","iopub.status.idle":"2022-04-05T15:57:28.151662Z","shell.execute_reply.started":"2022-04-05T15:57:28.144296Z","shell.execute_reply":"2022-04-05T15:57:28.15057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OCSVM_train_train_WACA_features_dic = MakeDeepXExpDic(kNN_train_train)\nOCSVM_train_valid_WACA_features_dic = MakeDeepXExpDic(kNN_train_valid, fitted_scaler_dic=fitted_scaler_ocsvm_exp2_train_dic)\n# OCSVM_test_train_WACA_features_dic = MakeDeepXExpDic(kNN_train_train)\n# OCSVM_test_test_WACA_features_dic = MakeDeepXExpDic(kNN_train_train, fitted_scaler_dic=fitted_scaler_ocsvm_exp2_test_dic)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:18:59.596844Z","iopub.execute_input":"2022-04-19T16:18:59.59723Z","iopub.status.idle":"2022-04-19T16:18:59.635302Z","shell.execute_reply.started":"2022-04-19T16:18:59.5972Z","shell.execute_reply":"2022-04-19T16:18:59.633748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"OCSVM_train_train_deep_features_dic[0].shape: {OCSVM_train_train_deep_features_dic[0].shape}\")\nprint(f\"OCSVM_train_valid_deep_features_dic[0].shape: {OCSVM_train_valid_deep_features_dic[0].shape}\")\nprint(f\"len(OCSVM_train_train_deep_features_dic): {len(OCSVM_train_train_deep_features_dic)}\")\nprint(f\"len(OCSVM_train_valid_deep_features_dic): {len(OCSVM_train_valid_deep_features_dic)}\")\nprint(f\"OCSVM_train_train_deep_features_dic.keys(): {OCSVM_train_train_deep_features_dic.keys()}\")\nprint(f\"OCSVM_train_valid_deep_features_dic.keys(): {OCSVM_train_valid_deep_features_dic.keys()}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-05T15:57:33.036872Z","iopub.execute_input":"2022-04-05T15:57:33.037248Z","iopub.status.idle":"2022-04-05T15:57:33.045165Z","shell.execute_reply.started":"2022-04-05T15:57:33.037211Z","shell.execute_reply":"2022-04-05T15:57:33.043826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_exp_dic_sample['X_exp2_dic'][0].shape","metadata":{"execution":{"iopub.status.busy":"2022-04-05T15:57:35.605976Z","iopub.execute_input":"2022-04-05T15:57:35.606362Z","iopub.status.idle":"2022-04-05T15:57:35.637323Z","shell.execute_reply.started":"2022-04-05T15:57:35.606329Z","shell.execute_reply":"2022-04-05T15:57:35.636115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = ['gen_user_id', \"adv_user_id\", \"EER\"]\nEER_distro_df = pd.DataFrame(columns = columns)\nfor gen_user_id in OCSVM_train_train_WACA_features_dic.keys():\n    nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(OCSVM_train_train_WACA_features_dic[gen_user_id])\n    curr_EER_distro_lst = [] \n    df = pd.DataFrame()\n    \n    distFRR, FRR_indices = nbrs.kneighbors(OCSVM_train_valid_WACA_features_dic[gen_user_id])\n    distFRR = distFRR.mean(axis=1).reshape(distFRR.shape[0], 1)\n    for adv_user_id in OCSVM_train_valid_WACA_features_dic.keys():\n        if adv_user_id != 16 and adv_user_id != gen_user_id and gen_user_id != 16:\n            distFAR, FAR_indices = nbrs.kneighbors(OCSVM_train_valid_WACA_features_dic[adv_user_id])\n            distFAR = distFAR.mean(axis=1).reshape(distFAR.shape[0], 1)\n            df[adv_user_id] = distFAR.mean(axis=1)\n            \n            \n            all_dist = np.concatenate([distFAR, distFRR])\n#             print(all_dist.shape)\n            scaler = MinMaxScaler().fit(all_dist)\n            scaled_distFRR, scaled_distFAR = scaler.transform(distFRR), scaler.transform(distFAR)\n\n\n            \n            y = [1]*distFRR.shape[0] + [-1]*distFAR.shape[0]\n            y_pred = 1-np.concatenate([scaled_distFRR, scaled_distFAR]).ravel() \n            \n            temp_eer, tres = utils_eer(y, y_pred, return_threshold=True)\n            curr_EER_distro_lst.append(temp_eer)\n            print(f\"EER: {temp_eer:.9f}, gen_user_id: {gen_user_id}, adv_user_id: {adv_user_id}, Threshold: {tres:.9f}\")\n            \n            new_row = pd.DataFrame([[gen_user_id, adv_user_id, temp_eer]], columns=columns)\n            EER_distro_df = pd.concat([EER_distro_df, new_row])\n#     if gen_user_id != 16:\n#         print(len(curr_EER_distro_lst))\n# #         EER_distro_df[\"Owner\"] = gen_user_id\n#         EER_distro_df[gen_user_id] = np.array(curr_EER_distro_lst)\n\n    if gen_user_id != 16:\n        df.plot(figsize=(30, 6), title=f\"gen_user_id: {gen_user_id}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EER_distro_df\ny_col = \"EER\"\nfigsize=(30, 6)\nfontsize=11\nn_subject = len(EER_distro_df.columns)\nmean_col = EER_distro_df[y_col].mean()\n\nfig = plt.figure(figsize=figsize, dpi=180)\nax = sns.boxplot(x=\"gen_user_id\", y=y_col, data=EER_distro_df)#, **utils_boxplot_style)\nax.set_ylim((0, 1))\nsns.swarmplot(x=\"gen_user_id\", y=y_col, data=EER_distro_df, color=\".25\")\n\nplt.plot(\n    [-0.6, figsize[0] + 0.6],\n    [mean_col, mean_col],\n    linestyle=\"dashed\",\n    linewidth=1,\n    color=MAGENTA,\n    alpha=0.7,\n)\nplt.text(n_subject + 0.6, mean_col, f\"mean\", fontsize=fontsize, color=MAGENTA)\nplt.text(\n    n_subject + 0.6, mean_col - 0.04, f\"{mean_col:.3f}\", fontsize=fontsize, color=MAGENTA\n)\nplt.xticks(rotation=45)\nfig.tight_layout()\nplt.title(f'EER distrobution for window size of {P.window_size}')\n\nprint(f\"Overall mean: {mean_col:.4f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deep_feature_model.predict(OCSVM_train_train[0]).shape","metadata":{"execution":{"iopub.status.busy":"2022-04-05T15:57:36.794887Z","iopub.execute_input":"2022-04-05T15:57:36.795279Z","iopub.status.idle":"2022-04-05T15:57:36.845157Z","shell.execute_reply.started":"2022-04-05T15:57:36.795242Z","shell.execute_reply":"2022-04-05T15:57:36.844218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T15:57:38.734554Z","iopub.execute_input":"2022-04-05T15:57:38.734906Z","iopub.status.idle":"2022-04-05T15:57:38.747188Z","shell.execute_reply.started":"2022-04-05T15:57:38.734875Z","shell.execute_reply":"2022-04-05T15:57:38.746092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.logspace(-3, -1)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T15:57:40.791198Z","iopub.execute_input":"2022-04-05T15:57:40.791551Z","iopub.status.idle":"2022-04-05T15:57:40.799213Z","shell.execute_reply.started":"2022-04-05T15:57:40.791521Z","shell.execute_reply":"2022-04-05T15:57:40.798199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_dist = {\n    \"model__gamma\": np.logspace(-3, 1), \n    \"model__nu\": np.linspace(0.0001, 0.1),\n#     'scaler': [StandardScaler(), MinMaxScaler(),\n#         Normalizer(), MaxAbsScaler()],\n#     \"selector__threshold\": np.linspace(0, 2, num=1000),\n\n}\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndf_results = None  # Will be filled with randomsearch scores\n\n\n# Don't need to loop over to find the best PCA.\nfor run in tqdm(range(3)):\n#     df_results[run] = {}\n    \n    for owner_idx in tqdm(\n        range(len(OCSVM_train_train_deep_features_dic)),\n        desc=\"Owner\",\n    ):\n        run_seed = SEED + run\n        X_vals_owner_idx = utils_create_cv_splits(owner_idx, OCSVM_train_train_deep_features_dic, OCSVM_train_valid_deep_features_dic, seed=run_seed)\n#         X_train = X_vals_owner_idx['X_train']\n#          = X.reshape(X.shape[-3], -1)  # flatten windows\n        X_test_regular = X_vals_owner_idx['X_test_regular'].copy()\n        X_test_anomalous = X_vals_owner_idx['X_test_anomalous'].copy()\n        \n#         print(X_vals_owner_idx['cv_splits'])\n        cv_splits = X_vals_owner_idx['cv_splits']\n        \n        pca = PCA(n_components = run)\n#         X_train = pca.fit_transform(X_train)\n#         X_test_regular = pca.transform(X_test_regular)\n#         X_test_anomalous = pca.transform(X_test_anomalous)\n        \n#         pca_fs.add_user_pca(owner_idx, pca)\n        \n        clf = svm.OneClassSVM(kernel=P.ocsvm_kernel)\n        \n        pipeline = Pipeline([\n#                             ('scaler', StandardScaler()), \n                            ('scaler', get_new_scaler_dict[\"RobustScaler\"]()), \n#                              ('pca', pca), \n#                              ('selector', VarianceThreshold()), \n                             ('model', clf)\n                            ])\n        \n        # random serach usees 5-folld cv by default. Probably need to remove it.\n        random_search = RandomizedSearchCV(\n            pipeline,\n            param_distributions=param_dist,\n            cv=cv_splits,\n            n_iter=80,\n            n_jobs=CORES,\n            refit=False,\n            scoring={\"eer\": utils_eer_scorer, \"accuracy\": \"accuracy\"},\n            verbose=1,\n            return_train_score=False,\n            iid=False,\n            error_score=\"raise\",\n            random_state=run_seed\n        )\n#         print(X_train.shape)\n#         print(X_test_anomalous.shape)\n\n        \n        random_search.fit(X_vals_owner_idx['X_train'], X_vals_owner_idx['y_train'])\n        impostors = [idx for idx in range(len(OCSVM_train_train_deep_features_dic)) if idx != owner_idx]\n\n        df_report = utils_cv_report(random_search, owner_idx, impostors)\n        df_report[\"run\"] = run\n        df_results = pd.concat([df_results, df_report], sort=False)\n        \n\n\ndf_results","metadata":{"execution":{"iopub.status.busy":"2022-04-05T15:58:40.653132Z","iopub.execute_input":"2022-04-05T15:58:40.653499Z","iopub.status.idle":"2022-04-05T16:01:10.649798Z","shell.execute_reply.started":"2022-04-05T15:58:40.653468Z","shell.execute_reply":"2022-04-05T16:01:10.648734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_results[df_results[\"rank_test_eer\"] == 1])","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:01:10.652672Z","iopub.execute_input":"2022-04-05T16:01:10.653001Z","iopub.status.idle":"2022-04-05T16:01:10.660767Z","shell.execute_reply.started":"2022-04-05T16:01:10.652967Z","shell.execute_reply":"2022-04-05T16:01:10.659876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_results[df_results[\"owner\"] == 14])","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:01:10.662065Z","iopub.execute_input":"2022-04-05T16:01:10.662657Z","iopub.status.idle":"2022-04-05T16:01:10.671179Z","shell.execute_reply.started":"2022-04-05T16:01:10.662604Z","shell.execute_reply":"2022-04-05T16:01:10.67006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(\n    df_results[df_results[\"rank_test_eer\"] == 1]\n    .sort_values(\"mean_test_eer\")\n    .head(30)\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:01:10.672847Z","iopub.execute_input":"2022-04-05T16:01:10.673786Z","iopub.status.idle":"2022-04-05T16:01:10.717702Z","shell.execute_reply.started":"2022-04-05T16:01:10.673739Z","shell.execute_reply":"2022-04-05T16:01:10.716865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(\n    df_results[df_results[\"rank_test_eer\"] == 1][\n        [\n            \"mean_fit_time\",\n            \"param_model__nu\",\n            \"param_model__gamma\",\n            \"mean_test_accuracy\",\n            \"std_test_accuracy\",\n            \"mean_test_eer\",\n            \"std_test_eer\",\n        ]\n    ].describe()\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:01:10.720977Z","iopub.execute_input":"2022-04-05T16:01:10.721244Z","iopub.status.idle":"2022-04-05T16:01:10.749483Z","shell.execute_reply.started":"2022-04-05T16:01:10.721219Z","shell.execute_reply":"2022-04-05T16:01:10.748417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = utils_plot_randomsearch_results(df_results, n_top=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:01:10.751521Z","iopub.execute_input":"2022-04-05T16:01:10.752116Z","iopub.status.idle":"2022-04-05T16:01:11.264044Z","shell.execute_reply.started":"2022-04-05T16:01:10.752064Z","shell.execute_reply":"2022-04-05T16:01:11.263077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df_plot = df_results.rename(\n#     columns={\"train_accuracy\": \"train Accuracy\", \"test_eer\": \"Test EER\", \"owner\": \"Owner\"}\n# ).astype({\"Owner\": str})","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:01:11.265428Z","iopub.execute_input":"2022-04-05T16:01:11.265954Z","iopub.status.idle":"2022-04-05T16:01:11.269483Z","shell.execute_reply.started":"2022-04-05T16:01:11.26591Z","shell.execute_reply":"2022-04-05T16:01:11.26856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig = utils_plot_acc_eer_dist(train_df_plot, \"Test Accuracy\")","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:01:11.270759Z","iopub.execute_input":"2022-04-05T16:01:11.27127Z","iopub.status.idle":"2022-04-05T16:01:11.279181Z","shell.execute_reply.started":"2022-04-05T16:01:11.271233Z","shell.execute_reply":"2022-04-05T16:01:11.278212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig = utils_plot_acc_eer_dist(test_df_plot, \"Test Accuracy\")","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:01:11.280303Z","iopub.execute_input":"2022-04-05T16:01:11.280797Z","iopub.status.idle":"2022-04-05T16:01:11.287564Z","shell.execute_reply.started":"2022-04-05T16:01:11.280757Z","shell.execute_reply":"2022-04-05T16:01:11.286205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k = list(OCSVM_test_train_deep_features_dic.keys())[0]\nprint(f\"OCSVM_test_train_deep_features_dic[{k}].shape: {OCSVM_test_train_deep_features_dic[k].shape}\")\nprint(f\"OCSVM_test_test_deep_features_dic[{k}].shape: {OCSVM_test_test_deep_features_dic[k].shape}\")\nprint(f\"len(OCSVM_test_train_deep_features_dic): {len(OCSVM_test_train_deep_features_dic)}\")\nprint(f\"len(OCSVM_test_test_deep_features_dic): {len(OCSVM_test_test_deep_features_dic)}\")\nprint(f\"OCSVM_train_train_deep_features_dic.keys(): {OCSVM_test_train_deep_features_dic.keys()}\")\nprint(f\"OCSVM_test_test_deep_features_dic.keys(): {OCSVM_test_test_deep_features_dic.keys()}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:01:11.288672Z","iopub.execute_input":"2022-04-05T16:01:11.2892Z","iopub.status.idle":"2022-04-05T16:01:11.299113Z","shell.execute_reply.started":"2022-04-05T16:01:11.28916Z","shell.execute_reply":"2022-04-05T16:01:11.29796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# P.ocsvm_nu, P.ocsvm_gamma = 0.015, 0.003","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:01:11.300508Z","iopub.execute_input":"2022-04-05T16:01:11.301245Z","iopub.status.idle":"2022-04-05T16:01:11.30672Z","shell.execute_reply.started":"2022-04-05T16:01:11.301205Z","shell.execute_reply":"2022-04-05T16:01:11.305924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# P.ocsvm_nu, P.ocsvm_gamma = 0.007, 0.002","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:01:11.308434Z","iopub.execute_input":"2022-04-05T16:01:11.309192Z","iopub.status.idle":"2022-04-05T16:01:11.315006Z","shell.execute_reply.started":"2022-04-05T16:01:11.309152Z","shell.execute_reply":"2022-04-05T16:01:11.314264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# P.ocsvm_nu, P.ocsvm_gamma = 0.018, 0.001","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:01:11.316388Z","iopub.execute_input":"2022-04-05T16:01:11.317311Z","iopub.status.idle":"2022-04-05T16:01:11.323891Z","shell.execute_reply.started":"2022-04-05T16:01:11.317273Z","shell.execute_reply":"2022-04-05T16:01:11.322939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# P.ocsvm_nu, P.ocsvm_gamma = 0.025, 0.003","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:01:11.325555Z","iopub.execute_input":"2022-04-05T16:01:11.326256Z","iopub.status.idle":"2022-04-05T16:01:11.33075Z","shell.execute_reply.started":"2022-04-05T16:01:11.326218Z","shell.execute_reply":"2022-04-05T16:01:11.329714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"P.ocsvm_nu, P.ocsvm_gamma = 0.025, 0.001","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:01:11.332392Z","iopub.execute_input":"2022-04-05T16:01:11.33326Z","iopub.status.idle":"2022-04-05T16:01:11.338794Z","shell.execute_reply.started":"2022-04-05T16:01:11.333212Z","shell.execute_reply":"2022-04-05T16:01:11.33787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"utils_ppp(P)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:01:11.340393Z","iopub.execute_input":"2022-04-05T16:01:11.341148Z","iopub.status.idle":"2022-04-05T16:01:11.360875Z","shell.execute_reply.started":"2022-04-05T16:01:11.341037Z","shell.execute_reply":"2022-04-05T16:01:11.359824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OCSVM_test_train_deep_features_dic.keys()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:01:11.362874Z","iopub.execute_input":"2022-04-05T16:01:11.363563Z","iopub.status.idle":"2022-04-05T16:01:11.370602Z","shell.execute_reply.started":"2022-04-05T16:01:11.363513Z","shell.execute_reply":"2022-04-05T16:01:11.369414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_results = None  # Will be filled with randomsearch scores\nfor run in tqdm(range(5)):\n\n\n    for owner_idx in tqdm(\n        OCSVM_test_train_deep_features_dic.keys(),\n        desc=\"Owner\",\n    ):\n\n        \n        run_seed = SEED + run\n        X_vals_owner_idx = utils_create_cv_splits(owner_idx, OCSVM_test_train_deep_features_dic, OCSVM_test_test_deep_features_dic, seed=run_seed)\n#         X_train = X_vals_owner_idx['X_train']\n#          = X.reshape(X.shape[-3], -1)  # flatten windows\n        X_test_regular = X_vals_owner_idx['X_test_regular']\n        X_test_anomalous = X_vals_owner_idx['X_test_anomalous']\n#         print(X_vals_owner_idx)\n#         break\n#         np.random.shuffle(X_train)\n#         np.random.shuffle(X_test_regular)\n#         np.random.shuffle(X_test_anomalous)\n#         print(X_vals_owner_idx['cv_splits'])\n        train_test_cv_splits = X_vals_owner_idx['cv_splits']\n        \n        \n#         pca = PCA(n_components = run)\n#         X_train = pca.fit_transform(X_train)\n#         X_test_regular = pca.transform(X_test_regular)\n#         X_test_anomalous = pca.transform(X_test_anomalous)\n        \n        clf = svm.OneClassSVM(kernel=P.ocsvm_kernel, nu=P.ocsvm_nu, gamma=P.ocsvm_gamma)\n        pipeline = Pipeline([\n#             Normalizer, MaxAbsScaler,\n            ('scaler', get_new_scaler_dict[\"RobustScaler\"]()), \n#                             ('scaler', StandardScaler()), \n#                              ('scaler', RobustScaler()),\n#                              ('scaler', MaxAbsScaler()),\n#                             ('scaler', Normalizer()),#best result\n#                             ('scaler', PowerTransformer()),\n#                              ('pca', pca), \n#                              ('selector', VarianceThreshold()), \n                             ('model', clf)\n                            ])\n        \n        scores = cross_validate(\n            pipeline,\n            X_vals_owner_idx['X_train'],\n            X_vals_owner_idx['y_train'],\n            cv=train_test_cv_splits,\n            scoring={\n                \"eer\": utils_eer_scorer,\n                \"accuracy\": \"accuracy\",\n                \"precision\": \"precision\",\n                \"recall\": \"recall\",\n            },\n            n_jobs=CORES,\n            verbose=1,\n            return_train_score=True,\n        )\n        \n        df_score = pd.DataFrame(scores)\n        df_score[\"owner\"] = owner_idx\n        df_score[\"train_eer\"] = df_score[\"train_eer\"].abs()  # Revert scorer's signflip\n        df_score[\"test_eer\"] = df_score[\"test_eer\"].abs()\n        test_df_results = pd.concat([test_df_results, df_score], axis=0)\n        \n\ntest_df_results.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:01:11.372257Z","iopub.execute_input":"2022-04-05T16:01:11.372876Z","iopub.status.idle":"2022-04-05T16:01:14.156814Z","shell.execute_reply.started":"2022-04-05T16:01:11.372827Z","shell.execute_reply":"2022-04-05T16:01:14.155697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_plot = test_df_results.rename(\n    columns={\"test_accuracy\": \"Test Accuracy\", \"test_eer\": \"Test EER\", \"owner\": \"Owner\"}\n).astype({\"Owner\": str})","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:01:14.158579Z","iopub.execute_input":"2022-04-05T16:01:14.159267Z","iopub.status.idle":"2022-04-05T16:01:14.170927Z","shell.execute_reply.started":"2022-04-05T16:01:14.159223Z","shell.execute_reply":"2022-04-05T16:01:14.170052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = utils_plot_acc_eer_dist(test_df_plot, \"Test Accuracy\")","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:01:14.172943Z","iopub.execute_input":"2022-04-05T16:01:14.173741Z","iopub.status.idle":"2022-04-05T16:01:14.361911Z","shell.execute_reply.started":"2022-04-05T16:01:14.173691Z","shell.execute_reply":"2022-04-05T16:01:14.361025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = utils_plot_acc_eer_dist(test_df_plot, \"Test EER\")","metadata":{"execution":{"iopub.status.busy":"2022-04-05T16:01:14.363285Z","iopub.execute_input":"2022-04-05T16:01:14.363744Z","iopub.status.idle":"2022-04-05T16:01:14.544241Z","shell.execute_reply.started":"2022-04-05T16:01:14.363697Z","shell.execute_reply":"2022-04-05T16:01:14.543162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}