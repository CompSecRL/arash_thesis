{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset [link](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fdrive.google.com%2Fdrive%2Ffolders%2F1-6sd6E3cL8uWdNeO4xiXxLbTSYXSTiVr%3Fusp%3Dsharing&data=04%7C01%7Carash.gholami%40queensu.ca%7C92b180b5b67d475fb70e08d87b7f4305%7Cd61ecb3b38b142d582c4efb2838b925c%7C1%7C0%7C637395133804367020%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=1ieR3E8llMblC0QMBtx1%2FpHS71RQuCxzdNQLmH0HJVU%3D&reserved=0)\n",
    "\n",
    "Explanation: There are **49** users' data here. Naming is in the format of Username_ExperimentNumber. Please also note that it is unprocessed raw data, which includes a lot of noise etc. \n",
    "\n",
    "Experiment-1: **20** participants are involved in this task and the participants were asked to type **a story** from **a set** of short and simple stories from the American Literature https://americanliterature.com/100-great-short-stories for **four minutes**. The story was chosen randomly by the participants.\n",
    "\n",
    "Experiment-2: 20 participants are involved in this task and for this data set, all the participants were asked to type the **same text** https://en.wikipedia.org/wiki/The_Adventures_of_Tom_Sawyer for **four minutes**. For each participant, almost the same amount of data is collected as Experiment-1. This dataset is important to be able to measure the quality of the features. The collection of this dataset is quite realistic since in a real case the users could be also asked to type a given text during their CA sessions. \n",
    "\n",
    "Experiment-3: The participants are asked to type **10 different pangram sentences**. Pangrams sentences contain** all 26 letters** of the alphabet. This data set is used to test how the **letter distribution** of text affects the result.\n",
    "\n",
    "Experiment-4: **34** participants are involved in this task and the participants were instructed to imitate someone else's typing pattern by watching the prerecorded video of the other person. For these experiments, one of the participants was recorded on video while typing a short and simple sentence for **15 seconds** from a perspective that the hand motions, smartwatch, keyboard, and the screen could be seen. Although it was not required, the perspective allowed to infer what the victim was typing by watching. This dataset was primarily used to analyze the **attacking scenarios**. It is left to the participant to type the exact same text or imitate the hand movement of the victim.\n",
    "\n",
    "-----------------------\n",
    "\n",
    "I realized that you have mostly 4s and 10s as values on column A, but I also found 2s. What does column A represent? \n",
    "A represents the sensor type:\n",
    " \n",
    "**10 for the accelerometer\n",
    "4 for the gyroscope**\n",
    "2 for the magnetic field sensor\n",
    "\n",
    "You can find more info about sensors and constant values that are used to define the sensor type in these links:\n",
    "- https://developer.android.com/guide/topics/sensors/sensors_overview\n",
    "- https://developer.android.com/reference/android/hardware/Sensor#TYPE_LINEAR_ACCELERATION\n",
    "\n",
    "Normally, I only collected and used accelerometer and gyroscope sensor data. Magnetometer data should not be in the dataset. If there is, that means it is an **outlier**.\n",
    " \n",
    "Is column B the time stamp, is it recorded at 100HZ?\n",
    "\n",
    "In an android app, you can set the sampling rate \"SENSOR_DELAY_NORMAL, SENSOR_DELAY_UI, SENSOR_DELAY_GAME, or SENSOR_DELAY_FASTEST \" \n",
    "\n",
    "As far as I remember I used **\"SENSOR_DELAY_FASTEST\" for all sensors**, but in practice, I had a lot of issues regarding this. They **don't give a constant sampling rate**. Every time I used they were giving different sampling rates. They were giving different sampling rates for two sessions of the same user, for different users, or for different sensors. Therefore, I would recommend using the one in the practice, which you can calculate using the timestamp and the sample count. \n",
    "\n",
    "On the paper you said you have recorded both accelerometer and gyro data; but you seem to only have one of them at a time on columns C, D, and F. I am guessing 10 on column A represents accelerometer and 4 gyro, but I could be wrong. \n",
    "Also, some rows have more columns, and start with 11 on column A. What do those represent?\n",
    "Please see the explanation above. \n",
    " \n",
    "\n",
    "In short, I would really appreciate it if you could let me know information I need to know for working with the data.\n",
    "\n",
    "Also, as these sensors were giving me a lot of headaches because they are really not stable, I needed to perform a lot of preprocessing. I **cleaned the outliers** before further processing. I also applied some other filtering techniques such as **moving average** to obtain more smooth data. Moreover, I needed to **clean the part the users are not performing typing operation from the beginning and end**, which you can see if you plot the values, i.e., the **user stops meaning no value and then big deviations, where the user takes off the smartwatch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "CORES = -1\n",
    "SEED = 567\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.9/site-packages (22.3.1)\n",
      "utility_functions imports setup complete\n",
      "Python 3.9.10\n",
      "EER: 0.333, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.400, Threshold: 0.200 <-- Worse case\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utility functions imported\n",
      "Numpy Seed was set to: 567\n",
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install python-docx\n",
    "# !pip install antropy\n",
    "\n",
    "from math import sqrt\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import dataclasses\n",
    "import math as math\n",
    "from sklearn.model_selection import cross_validate, RandomizedSearchCV, GridSearchCV\n",
    "import statsmodels.stats.api as sms\n",
    "from tqdm.auto import tqdm\n",
    "from dataclasses import asdict\n",
    "from sklearn import svm\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import random\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_curve, accuracy_score, make_scorer, auc\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold # Feature selector\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import iqr\n",
    "from scipy.stats import median_absolute_deviation\n",
    "from scipy.stats import mode\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.signal import peak_widths\n",
    "# from scipy.special import entr\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, MaxAbsScaler, RobustScaler, PowerTransformer\n",
    "get_new_scaler_dict = {\"StandardScaler\": StandardScaler, \"MinMaxScaler\": MinMaxScaler, \"Normalizer\": Normalizer, \n",
    "                       \"MaxAbsScaler\": MaxAbsScaler, \"RobustScaler\": RobustScaler, \"PowerTransformer\": PowerTransformer}\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import auc\n",
    "# import antropy as ant\n",
    "import time\n",
    "# import docx\n",
    "\n",
    "# Global utitlity functions are in separate notebook\n",
    "# import utility_functions as uf\n",
    "%run ./utility-functions.ipynb\n",
    "\n",
    "np.random.seed(SEED)\n",
    "print(f\"Numpy Seed was set to: {SEED}\")\n",
    "\n",
    "print(\"Setup Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class ExperimentParameters:\n",
    "    \"\"\"Contains all relevant parameters to run an experiment.\"\"\"\n",
    "\n",
    "    name: str  # Name of Parameter set. Used as identifier for charts etc.\n",
    "    frequency: int\n",
    "    max_subjects: int\n",
    "    max_test_subjects: int\n",
    "        \n",
    "    user_ids: list\n",
    "    num_sample_points_per_exp: int\n",
    "    exp_begin_cutoff_idx: int\n",
    "    exp_end_cutoff_idx: int\n",
    "        \n",
    "    \n",
    "    seconds_per_subject_train: float\n",
    "    seconds_per_subject_test: float\n",
    "    window_size: int  # After resampling\n",
    "    IF_step_width: int  # After resampling\n",
    "    scaler: str  # StandardScaler, MinMaxScaler, Normalizer, MaxAbsScaler, RobustScaler, PowerTransformer\n",
    "    scaler_scope: str  # {\"subject\", \"session\"}\n",
    "    scaler_global: bool  # fit transform scale on all data (True) or fit on training only (False)\n",
    "    IF_kernel: str # IF kernel\n",
    "    IF_nu: float  # Best value found in random search, used for final model\n",
    "    IF_gamma: float  # Best value found in random search, used for final model\n",
    "    feature_cols: list  # Columns used as features\n",
    "    exclude_subjects: list  # Don't load data from those users\n",
    "        \n",
    "    # Calculated values\n",
    "    def __post_init__(self):\n",
    "        # HDF key of table:\n",
    "        self.table_name = f\"sensors_{self.frequency}hz\"\n",
    "\n",
    "        \n",
    "\n",
    "# INSTANCES\n",
    "# ===========================================================\n",
    "\n",
    "# NAIVE_APPROACH\n",
    "# -----------------------------------------------------------\n",
    "NAIVE_MINMAX_IF = ExperimentParameters(\n",
    "    name=\"NAIVE-MINMAX_IF\",\n",
    "    frequency=100,\n",
    "    max_subjects=29,\n",
    "    max_test_subjects=10,\n",
    "    user_ids = [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 28, 29, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49],\n",
    "    num_sample_points_per_exp=21000,\n",
    "    exp_begin_cutoff_idx=500,\n",
    "    exp_end_cutoff_idx=-500,\n",
    "    seconds_per_subject_train=210,\n",
    "    seconds_per_subject_test=210,    \n",
    "    window_size=250,\n",
    "    IF_step_width=250,\n",
    "    scaler=\"minmax\",\n",
    "    scaler_scope=\"subject\",\n",
    "    scaler_global=True,\n",
    "    IF_kernel=\"rbf\",\n",
    "    IF_nu=None,\n",
    "    IF_gamma=None,\n",
    "    feature_cols=[\n",
    "        \"EMA_x_a\",\n",
    "        \"EMA_y_a\",\n",
    "        \"EMA_z_a\",\n",
    "        \"EMA_x_g\",\n",
    "        \"EMA_y_g\",\n",
    "        \"EMA_z_g\",\n",
    "    ],\n",
    "    exclude_subjects=[],\n",
    ")\n",
    "\n",
    "# VALID_APPROACH\n",
    "# -----------------------------------------------------------\n",
    "VALID_MINMAX_IF = dataclasses.replace(\n",
    "    NAIVE_MINMAX_IF,\n",
    "    name=\"VALID-MINMAX-IF\",\n",
    "    scaler_global=False,\n",
    "    IF_nu=0.165,\n",
    "    IF_gamma=0.039,\n",
    ")\n",
    "\n",
    "# NAIVE_ROBUST_APPROACH\n",
    "# -----------------------------------------------------------\n",
    "NAIVE_ROBUST_IF = dataclasses.replace(\n",
    "    NAIVE_MINMAX_IF,\n",
    "    name=\"NAIVE-ROBUST-IF\",\n",
    "    scaler=\"robust\",\n",
    "    scaler_global=True,\n",
    "    IF_nu=0.153,\n",
    "    IF_gamma=0.091,  # below median, selected by chart\n",
    ")\n",
    "\n",
    "# ROBUST_APPROACH (VALID)\n",
    "# -----------------------------------------------------------\n",
    "VALID_ROBUST_IF_125 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_IF,\n",
    "    name=\"VALID-ROBUST-IF\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=125\n",
    "#     IF_nu=0.037,\n",
    "#     IF_gamma= 0.001,\n",
    ")\n",
    "\n",
    "\n",
    "VALID_ROBUST_IF_250 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_IF,\n",
    "    name=\"VALID-ROBUST-IF\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=250\n",
    "#     IF_nu=0.037,\n",
    "#     IF_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_IF_500 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_IF,\n",
    "    name=\"VALID-ROBUST-IF\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=500\n",
    "#     IF_nu=0.037,\n",
    "#     IF_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_IF_750 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_IF,\n",
    "    name=\"VALID-ROBUST-IF\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=750\n",
    "#     IF_nu=0.037,\n",
    "#     IF_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_IF_1000 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_IF,\n",
    "    name=\"VALID-ROBUST-IF\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=1000\n",
    "#     IF_nu=0.037,\n",
    "#     IF_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_IF_1250 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_IF,\n",
    "    name=\"VALID-ROBUST-IF\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=1250\n",
    "#     IF_nu=0.037,\n",
    "#    IF_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_IF_1500 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_IF,\n",
    "    name=\"VALID-ROBUST-IF\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=1500\n",
    "#     IF_nu=0.037,\n",
    "#     IF_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_IF_1750 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_IF,\n",
    "    name=\"VALID-ROBUST-IF\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=1750\n",
    "#     IF_nu=0.037,\n",
    "#     IF_gamma= 0.001,\n",
    ")\n",
    "\n",
    "VALID_ROBUST_IF_2000 = dataclasses.replace(\n",
    "    NAIVE_MINMAX_IF,\n",
    "    name=\"VALID-ROBUST-IF\",\n",
    "    scaler=\"RobustScaler\",\n",
    "    scaler_global=False,\n",
    "    window_size=2000\n",
    "#     IF_nu=0.037,\n",
    "#     IF_gamma= 0.001,\n",
    ")\n",
    "\n",
    "# NORMALIZER_APPROACH (VALID)\n",
    "# -----------------------------------------------------------\n",
    "VALID_NORMALIZER_IF = dataclasses.replace(\n",
    "    NAIVE_MINMAX_IF,\n",
    "    name=\"VALID-NORMALIZER-IF\",\n",
    "    scaler=\"Normalizer\",\n",
    "    scaler_global=False,\n",
    "    IF_nu=0.074,\n",
    "    IF_gamma= 0.029,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "P = VALID_ROBUST_IF_500\n",
    "P.IF_step_width = int(P.window_size * .5)\n",
    "# P = VALID_NORMALIZER_IF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>VALID-ROBUST-IF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frequency</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_subjects</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_test_subjects</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_ids</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_sample_points_per_exp</th>\n",
       "      <td>21000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp_begin_cutoff_idx</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp_end_cutoff_idx</th>\n",
       "      <td>-500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seconds_per_subject_train</th>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seconds_per_subject_test</th>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>window_size</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IF_step_width</th>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler</th>\n",
       "      <td>RobustScaler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler_scope</th>\n",
       "      <td>subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler_global</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IF_kernel</th>\n",
       "      <td>rbf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IF_nu</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IF_gamma</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_cols</th>\n",
       "      <td>[EMA_x_a, EMA_y_a, EMA_z_a, EMA_x_g, EMA_y_g, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exclude_subjects</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       Value\n",
       "name                                                         VALID-ROBUST-IF\n",
       "frequency                                                                100\n",
       "max_subjects                                                              29\n",
       "max_test_subjects                                                         10\n",
       "user_ids                   [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 2...\n",
       "num_sample_points_per_exp                                              21000\n",
       "exp_begin_cutoff_idx                                                     500\n",
       "exp_end_cutoff_idx                                                      -500\n",
       "seconds_per_subject_train                                                210\n",
       "seconds_per_subject_test                                                 210\n",
       "window_size                                                              500\n",
       "IF_step_width                                                            250\n",
       "scaler                                                          RobustScaler\n",
       "scaler_scope                                                         subject\n",
       "scaler_global                                                          False\n",
       "IF_kernel                                                                rbf\n",
       "IF_nu                                                                   None\n",
       "IF_gamma                                                                None\n",
       "feature_cols               [EMA_x_a, EMA_y_a, EMA_z_a, EMA_x_g, EMA_y_g, ...\n",
       "exclude_subjects                                                          []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils_ppp(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 28, 29, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49][17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_scorer(utils_eer, greater_is_better=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils_eer_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset for Valid/Test  \n",
    "In two splits: one used during hyperparameter optimization, and one used during testing.\n",
    "\n",
    "The split is done along the subjects: All sessions of a single subject will either be in the validation split or in the testing split, never in both.\n",
    "\n",
    "They did a 30 60 split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping Raw Features.\n",
    "We have our own function of windows for this. Do this for both training and testing.\n",
    "\n",
    "# Extracting time and frequency based features.\n",
    "Again, we have a function for this. Do this for both training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization \n",
    "\n",
    "I do not find any reaqsonable explaination how to use a cross-validation as we are talking about anomaly detection.\n",
    "\n",
    "I am using the experiment 1 data as train, and experiment 2 data as validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SVM in a real-world Scenario with multiple genuine users and intruders\n",
    "Source: https://datascience.stackexchange.com/questions/23623/what-is-the-best-way-to-classify-data-not-belonging-to-set-of-classes\n",
    "\n",
    "Stage 1: \n",
    "    Use one-class SVM to assign those images that do not belong to the set of predefined classes as the 9-th class.\n",
    "\n",
    "Stage 2:\n",
    "    For those images that passes through your filter, let the multi-class SVM assign them to one of the 8 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 19,\n",
       " 21,\n",
       " 22,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 48,\n",
       " 49]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading exp1 data:\n",
      "1) accel_count: 28388, gyro_count: 31997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2) accel_count: 26015, gyro_count: 28954\n",
      "3) accel_count: 28227, gyro_count: 31814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4) accel_count: 24862, gyro_count: 26103\n",
      "5) accel_count: 24270, gyro_count: 24347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6) accel_count: 25012, gyro_count: 25060\n",
      "7) accel_count: 25301, gyro_count: 25382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8) accel_count: 21985, gyro_count: 21658\n",
      "19) accel_count: 24122, gyro_count: 25050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21) accel_count: 24367, gyro_count: 23830\n",
      "22) accel_count: 29284, gyro_count: 28863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26) accel_count: 23148, gyro_count: 24291\n",
      "27) accel_count: 24299, gyro_count: 23589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28) accel_count: 23807, gyro_count: 24523\n",
      "29) accel_count: 24030, gyro_count: 23475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35) accel_count: 24388, gyro_count: 23673\n",
      "36) accel_count: 24244, gyro_count: 24208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37) accel_count: 32022, gyro_count: 31843\n",
      "38) accel_count: 22138, gyro_count: 22327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39) accel_count: 23573, gyro_count: 23459\n",
      "40) accel_count: 23057, gyro_count: 24296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41) accel_count: 24102, gyro_count: 23681\n",
      "42) accel_count: 24074, gyro_count: 24328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43) accel_count: 22631, gyro_count: 23835\n",
      "44) accel_count: 24474, gyro_count: 23749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45) accel_count: 23974, gyro_count: 23229\n",
      "46) accel_count: 23614, gyro_count: 23827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48) accel_count: 22828, gyro_count: 23904\n",
      "49) accel_count: 24183, gyro_count: 24633\n",
      "Loading exp2 data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/2372085147.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_1.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) accel_count: 24049, gyro_count: 26943\n",
      "2) accel_count: 24472, gyro_count: 27667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3) accel_count: 24611, gyro_count: 27000\n",
      "4) accel_count: 24972, gyro_count: 26798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5) accel_count: 23573, gyro_count: 23372\n",
      "6) accel_count: 23800, gyro_count: 23890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7) accel_count: 23347, gyro_count: 24145\n",
      "8) accel_count: 22964, gyro_count: 22660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19) accel_count: 26160, gyro_count: 25846\n",
      "21) accel_count: 23642, gyro_count: 24444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22) accel_count: 23863, gyro_count: 24589\n",
      "26) accel_count: 23179, gyro_count: 23925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27) accel_count: 25121, gyro_count: 25829\n",
      "28) accel_count: 23133, gyro_count: 24039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29) accel_count: 23180, gyro_count: 24314\n",
      "35) accel_count: 23299, gyro_count: 23854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36) accel_count: 25500, gyro_count: 25059\n",
      "37) accel_count: 25994, gyro_count: 25232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38) accel_count: 21168, gyro_count: 21182\n",
      "39) accel_count: 24214, gyro_count: 23585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40) accel_count: 23944, gyro_count: 23170\n",
      "41) accel_count: 23195, gyro_count: 24111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42) accel_count: 26505, gyro_count: 25697\n",
      "43) accel_count: 22692, gyro_count: 23982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44) accel_count: 23002, gyro_count: 23829\n",
      "45) accel_count: 23978, gyro_count: 23350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46) accel_count: 21128, gyro_count: 21848\n",
      "48) accel_count: 28000, gyro_count: 27205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n",
      "/tmp/ipykernel_2624/1994135355.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(f'{os.getcwd()}/WACA_dataset/user{i}_2.csv', error_bad_lines = False, header=None, usecols = range(len(names)), dtype = str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49) accel_count: 23071, gyro_count: 24129\n"
     ]
    }
   ],
   "source": [
    "#include 47 later\n",
    "# user_ids = [9]\n",
    "df_exps_dict = load_data_frames(P.user_ids, P.exp_begin_cutoff_idx, P.exp_end_cutoff_idx, P.num_sample_points_per_exp)\n",
    "dfList_exp1, dfList_exp2 = df_exps_dict['dfList_exp1'], df_exps_dict['dfList_exp2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfList_exp1[i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i=17\n",
    "# dfList_exp1[i][['EMA_x_a']].plot(figsize=(30, 6))\n",
    "# dfList_exp1[i][['EMA_y_a']].plot(figsize=(30, 6))\n",
    "# dfList_exp1[i][['EMA_z_a']].plot(figsize=(30, 6))\n",
    "# dfList_exp1[i][['EMA_x_g']].plot(figsize=(30, 6))\n",
    "# dfList_exp1[i][['EMA_y_g']].plot(figsize=(30, 6))\n",
    "# dfList_exp1[i][['EMA_z_g']].plot(figsize=(30, 6))\n",
    "\n",
    "# dfList_exp2[i][['EMA_x_a']].plot(figsize=(30, 6))\n",
    "# dfList_exp2[i][['EMA_y_a']].plot(figsize=(30, 6))\n",
    "# dfList_exp2[i][['EMA_z_a']].plot(figsize=(30, 6))\n",
    "# dfList_exp2[i][['EMA_x_g']].plot(figsize=(30, 6))\n",
    "# dfList_exp2[i][['EMA_y_g']].plot(figsize=(30, 6))\n",
    "# dfList_exp2[i][['EMA_z_g']].plot(figsize=(30, 6))\n",
    "# # # exp1 idx 10 has corrupted data frist 50 sec\n",
    "# # # exp2 idx 12 has some artifacts first 12.5 sec\n",
    "# # # exp1 idx 17 has some artifacts first 75 sec\n",
    "# # # exp2 idx 23 has some artifacts last 6.5 sec\n",
    "\n",
    "# dfList_exp1[i] = dfList_exp1[i][7500:].reset_index(drop=True)\n",
    "\n",
    "# # dfList_exp1[i][['EMA_x_a']].plot(figsize=(30, 6))\n",
    "# dfList_exp1[i][['EMA_y_a']].plot(figsize=(30, 6))\n",
    "# dfList_exp1[i][['EMA_z_a']].plot(figsize=(30, 6))\n",
    "# dfList_exp1[i][['EMA_x_g']].plot(figsize=(30, 6))\n",
    "# dfList_exp1[i][['EMA_y_g']].plot(figsize=(30, 6))\n",
    "# dfList_exp1[i][['EMA_z_g']].plot(figsize=(30, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32022, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfList_exp1[i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set: [7, 24, 8, 11, 13, 19, 28, 21, 26, 3, 20, 22, 6, 25, 16, 1, 17, 27, 5]\n",
      "test_set: [0, 12, 14, 9, 18, 23, 2, 15, 10, 4]\n"
     ]
    }
   ],
   "source": [
    "randomized_data_idx = list(range(len(P.user_ids)))\n",
    "random.Random(SEED).shuffle(randomized_data_idx)\n",
    "split_idx = 2 * (len(randomized_data_idx)//3) + 1\n",
    "train_set = randomized_data_idx[: split_idx]\n",
    "test_set = randomized_data_idx[split_idx: ]\n",
    "print(f\"train_set: {train_set}\\ntest_set: {test_set}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # preparing train data\n",
    "# # train_set = r\n",
    "# dfList_exp1_train, dfList_exp2_train = [dfList_exp1[i] for i in train_set], [dfList_exp2[i] for i in train_set]\n",
    "# print(f\"len(dfList_exp1_train): {len(dfList_exp1_train)}\")\n",
    "# print(f\"len(dfList_exp2_train): {len(dfList_exp2_train)}\")\n",
    "# XExpTrainDict = MakeRawXExpDic(dfList_exp1_train, dfList_exp2_train, window_size = P.window_size, step=P.IF_step_width, numSamplePoints=P.num_sample_points_per_exp, \n",
    "#                                       scale_exp1=False, scale_exp2=True, scaler=P.scaler)\n",
    "\n",
    "# fitted_scaler_IF_exp2_train_dic = XExpTrainDict[\"fitted_scaler_exp2_dic\"]\n",
    "# X_exp1_train_dic, X_exp2_train_dic = XExpTrainDict[\"Raw_X_exp1_dic\"], XExpTrainDict[\"Raw_X_exp2_dic\"]\n",
    "\n",
    "# # preparing test data\n",
    "# dfList_exp1_test, dfList_exp2_test = [dfList_exp1[i] for i in test_set], [dfList_exp2[i] for i in test_set]\n",
    "# print(f\"len(dfList_exp1_test): {len(dfList_exp1_test)}\")\n",
    "# print(f\"len(dfList_exp2_test): {len(dfList_exp2_test)}\")\n",
    "# XExpTestDict = MakeRawXExpDic(dfList_exp1_test, dfList_exp2_test, window_size = P.window_size, step=P.IF_step_width, numSamplePoints=P.num_sample_points_per_exp, \n",
    "#                                       scale_exp1=False, scale_exp2=True, scaler=P.scaler)\n",
    "\n",
    "# fitted_scaler_IF_exp2_test_dic = XExpTestDict[\"fitted_scaler_exp2_dic\"]\n",
    "# X_exp1_test_dic, X_exp2_test_dic = XExpTestDict[\"Raw_X_exp1_dic\"], XExpTestDict[\"Raw_X_exp2_dic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_exp2_train_dic[0].shape\n",
    "# temporary\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def get_raw_windows(window_size, step_width):\n",
    "    P.window_size = window_size\n",
    "    P.IF_step_width = step_width\n",
    "    # preparing train data\n",
    "    # train_set = r\n",
    "    dfList_exp1_train, dfList_exp2_train = [dfList_exp1[i] for i in train_set], [dfList_exp2[i] for i in train_set]\n",
    "    print(f\"len(dfList_exp1_train): {len(dfList_exp1_train)}\")\n",
    "    print(f\"len(dfList_exp2_train): {len(dfList_exp2_train)}\")\n",
    "    XExpTrainDict = MakeRawXExpDic(dfList_exp1_train, dfList_exp2_train, window_size = P.window_size, step=P.IF_step_width, numSamplePoints=P.num_sample_points_per_exp, \n",
    "                                          scale_exp1=False, scale_exp2=True, scaler=P.scaler)\n",
    "\n",
    "    fitted_scaler_IF_exp2_train_dic = XExpTrainDict[\"fitted_scaler_exp2_dic\"]\n",
    "    X_exp1_train_dic, X_exp2_train_dic = XExpTrainDict[\"Raw_X_exp1_dic\"], XExpTrainDict[\"Raw_X_exp2_dic\"]\n",
    "\n",
    "    # preparing test data\n",
    "    dfList_exp1_test, dfList_exp2_test = [dfList_exp1[i] for i in test_set], [dfList_exp2[i] for i in test_set]\n",
    "    print(f\"len(dfList_exp1_test): {len(dfList_exp1_test)}\")\n",
    "    print(f\"len(dfList_exp2_test): {len(dfList_exp2_test)}\")\n",
    "    XExpTestDict = MakeRawXExpDic(dfList_exp1_test, dfList_exp2_test, window_size = P.window_size, step=P.IF_step_width, numSamplePoints=P.num_sample_points_per_exp, \n",
    "                                          scale_exp1=False, scale_exp2=True, scaler=P.scaler)\n",
    "\n",
    "    fitted_scaler_IF_exp2_test_dic = XExpTestDict[\"fitted_scaler_exp2_dic\"]\n",
    "    X_exp1_test_dic, X_exp2_test_dic = XExpTestDict[\"Raw_X_exp1_dic\"], XExpTestDict[\"Raw_X_exp2_dic\"]\n",
    "    \n",
    "    #----\n",
    "    return X_exp1_train_dic, X_exp2_train_dic, fitted_scaler_IF_exp2_train_dic, X_exp1_test_dic, X_exp2_test_dic, fitted_scaler_IF_exp2_test_dic\n",
    "\n",
    "def extract_features(X_exp1_train_dic, X_exp2_train_dic, fitted_scaler_IF_exp2_train_dic, X_exp1_test_dic, X_exp2_test_dic, fitted_scaler_IF_exp2_test_dic):\n",
    "    X_exp_train_dic = MakeWACAXExpDicOwner(X_exp2_train_dic, scaler_clip=True, scaler_type=\"MinMaxScaler\")\n",
    "    X_exp_train_dic = MakeWACAXExpDicUnknown(X_exp1_train_dic, X_exp_train_dic, fitted_raw_scaler_dict=fitted_scaler_IF_exp2_train_dic)\n",
    "    X_exp_test_dic = MakeWACAXExpDicOwner(X_exp2_test_dic, scaler_clip=True, scaler_type=\"MinMaxScaler\")\n",
    "    X_exp_test_dic = MakeWACAXExpDicUnknown(X_exp1_test_dic, X_exp_test_dic, fitted_raw_scaler_dict=fitted_scaler_IF_exp2_test_dic)\n",
    "    \n",
    "    return X_exp_train_dic, X_exp_test_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfList_exp1[0].shape\n",
    "# a = dfList_exp1[0].copy()\n",
    "# a = a.drop(columns=[\"time_stamp\"])\n",
    "# a\n",
    "# # df.drop(columns=[\"time_stamp\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(f\"train_set: {train_set}\")\n",
    "# print(f\"X_exp1_train_dic: {X_exp1_train_dic.keys()}\")\n",
    "# print(f\"X_exp2_train_dic: {X_exp2_train_dic.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(f\"test_set: {test_set}\")\n",
    "# print(f\"X_exp1_test_dic: {X_exp1_test_dic.keys()}\")\n",
    "# print(f\"X_exp2_test_dic: {X_exp2_test_dic.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.9/site-packages (22.3.1)\n",
      "utility_functions imports setup complete\n",
      "Python 3.9.10\n",
      "EER: 0.333, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.400, Threshold: 0.200 <-- Worse case\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utility functions imported\n"
     ]
    }
   ],
   "source": [
    "# print(len(X_exp2_train_dic[0]))\n",
    "# print(X_exp2_train_dic[0][0].shape)\n",
    "%run ./utility-functions.ipynb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import timeit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dfList_exp1_train): 19\n",
      "len(dfList_exp2_train): 19\n",
      "len(dfList_exp1_test): 10\n",
      "len(dfList_exp2_test): 10\n"
     ]
    }
   ],
   "source": [
    "X_exp1_train_dic, X_exp2_train_dic, fitted_scaler_IF_exp2_train_dic, X_exp1_test_dic, X_exp2_test_dic, fitted_scaler_IF_exp2_test_dic = \\\n",
    "            get_raw_windows(P.window_size, P.IF_step_width)\n",
    "# X_exp_train_dic, X_exp_test_dic = \\\n",
    "#     extract_features(X_exp1_train_dic, X_exp2_train_dic, fitted_scaler_IF_exp2_train_dic, X_exp1_test_dic, X_exp2_test_dic, fitted_scaler_IF_exp2_test_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  19 | elapsed:   15.8s remaining:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  19 | elapsed:   15.9s remaining:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  19 | elapsed:   15.9s remaining:   59.7s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  19 | elapsed:   16.1s remaining:   45.1s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  19 | elapsed:   16.5s remaining:   35.8s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  19 | elapsed:   16.6s remaining:   28.4s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  19 | elapsed:   17.0s remaining:   23.4s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  19 | elapsed:   17.2s remaining:   19.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  19 | elapsed:   17.3s remaining:   15.6s\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  19 | elapsed:   17.6s remaining:   12.8s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  19 | elapsed:   17.7s remaining:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  19 | elapsed:   18.0s remaining:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done  14 out of  19 | elapsed:   18.2s remaining:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  19 | elapsed:   18.4s remaining:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  19 | elapsed:   18.5s remaining:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  19 | elapsed:   18.5s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  19 | elapsed:   19.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  19 | elapsed:   19.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  10 | elapsed:    8.2s remaining:   32.7s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:    8.2s remaining:   19.2s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  10 | elapsed:    8.4s remaining:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:    8.5s remaining:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    8.6s remaining:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    8.6s remaining:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed:    8.8s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    9.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    9.0s finished\n",
      "Time:  48.22052407823503\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "\n",
    "X_exp_train_dic, X_exp_test_dic = \\\n",
    "    extract_features(X_exp1_train_dic, X_exp2_train_dic, fitted_scaler_IF_exp2_train_dic, X_exp1_test_dic, X_exp2_test_dic, fitted_scaler_IF_exp2_test_dic)\n",
    "\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ( X_exp_train_dic[owner][\"profile_windows\"] != temp[owner][\"profile_windows\"]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  48.22052407823503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'profile_windows': array([[1.        , 1.        , 0.841263  , ..., 0.54676413, 0.90860453,\n",
       "          0.94925259],\n",
       "         [0.46631697, 0.50107812, 0.98771071, ..., 0.1648182 , 0.91807802,\n",
       "          0.88233504],\n",
       "         [0.29437031, 0.35101914, 0.95830691, ..., 0.67351772, 0.87798087,\n",
       "          0.81107603],\n",
       "         ...,\n",
       "         [0.06180678, 0.35834818, 0.51474601, ..., 0.39082   , 0.77257338,\n",
       "          0.882709  ],\n",
       "         [0.26867882, 0.56960125, 0.26194329, ..., 0.70026938, 0.83300424,\n",
       "          0.71091735],\n",
       "         [0.43546688, 0.49556883, 0.55363003, ..., 0.60069661, 0.95463234,\n",
       "          0.64685727]]),\n",
       "  'unknown_users_dict': {0: array([[0.37764643, 0.34579465, 0.82071148, ..., 0.88997195, 0.78822576,\n",
       "           1.        ],\n",
       "          [0.37860256, 0.44786279, 0.7651379 , ..., 0.96379906, 0.78454977,\n",
       "           0.73335218],\n",
       "          [0.46371488, 0.5441181 , 0.58193057, ..., 0.84133219, 0.82478893,\n",
       "           0.78399423],\n",
       "          ...,\n",
       "          [0.19628951, 0.31747747, 0.26454433, ..., 0.71429335, 0.2725525 ,\n",
       "           0.23124952],\n",
       "          [0.26732589, 0.25478602, 0.46473318, ..., 0.        , 0.23370589,\n",
       "           0.08659048],\n",
       "          [0.29902365, 0.28429614, 0.28543991, ..., 0.        , 0.50703387,\n",
       "           0.53298126]]),\n",
       "   1: array([[1.        , 0.37944606, 0.        , ..., 0.99584823, 1.        ,\n",
       "           1.        ],\n",
       "          [1.        , 0.40565791, 0.        , ..., 0.64033805, 0.99632858,\n",
       "           1.        ],\n",
       "          [1.        , 0.29474297, 0.        , ..., 0.68214044, 1.        ,\n",
       "           1.        ],\n",
       "          ...,\n",
       "          [1.        , 0.47563852, 0.        , ..., 0.99190943, 0.86733617,\n",
       "           1.        ],\n",
       "          [1.        , 0.44453085, 0.        , ..., 1.        , 0.80862474,\n",
       "           0.93545474],\n",
       "          [1.        , 0.27630776, 0.        , ..., 0.78694102, 0.52775213,\n",
       "           0.32871608]]),\n",
       "   2: array([[1.        , 0.        , 1.        , ..., 0.26752431, 0.24418914,\n",
       "           0.        ],\n",
       "          [1.        , 0.29892444, 1.        , ..., 0.64930742, 0.8413577 ,\n",
       "           0.76000518],\n",
       "          [1.        , 0.64134549, 1.        , ..., 0.74843047, 0.78069425,\n",
       "           0.50329488],\n",
       "          ...,\n",
       "          [0.33966364, 0.55454144, 1.        , ..., 0.58405312, 0.70066713,\n",
       "           0.66401305],\n",
       "          [0.65923575, 0.50193033, 1.        , ..., 1.        , 1.        ,\n",
       "           0.99420661],\n",
       "          [0.98375259, 0.60250454, 1.        , ..., 1.        , 0.80497711,\n",
       "           0.91196104]]),\n",
       "   3: array([[1.        , 0.19578207, 0.36160307, ..., 0.61824082, 0.83308508,\n",
       "           1.        ],\n",
       "          [1.        , 0.24305003, 0.20655097, ..., 0.676554  , 0.9608424 ,\n",
       "           0.83076959],\n",
       "          [1.        , 0.1676023 , 0.49694165, ..., 0.77712619, 0.89908036,\n",
       "           0.88018144],\n",
       "          ...,\n",
       "          [1.        , 0.08710139, 0.01104353, ..., 0.34703295, 1.        ,\n",
       "           0.99076842],\n",
       "          [1.        , 0.22416265, 0.        , ..., 0.58933007, 0.95551817,\n",
       "           0.85690423],\n",
       "          [1.        , 0.07452929, 0.        , ..., 0.69650886, 0.86640631,\n",
       "           0.30369275]]),\n",
       "   4: array([[1.        , 0.        , 1.        , ..., 1.        , 0.9640238 ,\n",
       "           0.61722068],\n",
       "          [1.        , 0.        , 0.78716068, ..., 0.62219605, 0.56531836,\n",
       "           0.628502  ],\n",
       "          [1.        , 0.        , 0.89909384, ..., 0.89012528, 0.66379185,\n",
       "           0.68013866],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.00308433, ..., 0.92181711, 0.73262202,\n",
       "           0.82798543],\n",
       "          [1.        , 0.        , 0.10678154, ..., 0.81883035, 0.6297622 ,\n",
       "           0.94158496],\n",
       "          [1.        , 0.        , 0.51755814, ..., 0.83119228, 0.39733127,\n",
       "           0.86947632]]),\n",
       "   5: array([[1.        , 0.        , 1.        , ..., 0.3274392 , 0.63555056,\n",
       "           0.24904202],\n",
       "          [1.        , 0.        , 1.        , ..., 0.34253994, 0.75748443,\n",
       "           0.46961033],\n",
       "          [1.        , 0.        , 1.        , ..., 0.38027069, 0.7212826 ,\n",
       "           0.37586802],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.93233041, ..., 0.        , 0.77609621,\n",
       "           0.98779022],\n",
       "          [1.        , 0.        , 0.98252953, ..., 0.10226682, 1.        ,\n",
       "           0.86073378],\n",
       "          [1.        , 0.        , 0.96046468, ..., 0.28989551, 0.84824185,\n",
       "           0.74647093]]),\n",
       "   6: array([[0.        , 0.        , 1.        , ..., 0.        , 0.25406546,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.61626577, 0.17540896,\n",
       "           1.        ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.13495422, 0.40743293,\n",
       "           0.92187745],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.79061478, 0.66071886,\n",
       "           0.56955603],\n",
       "          [0.        , 0.        , 1.        , ..., 0.31538253, 0.58223054,\n",
       "           0.60575181],\n",
       "          [0.        , 0.        , 1.        , ..., 0.32684833, 0.48172273,\n",
       "           0.62268941]]),\n",
       "   7: array([[0.        , 0.51985661, 1.        , ..., 0.71124624, 0.67677014,\n",
       "           0.74411989],\n",
       "          [0.54538377, 0.84918863, 1.        , ..., 0.88728271, 0.82344332,\n",
       "           0.84336243],\n",
       "          [0.66705138, 0.72084538, 1.        , ..., 0.57270036, 0.79135485,\n",
       "           1.        ],\n",
       "          ...,\n",
       "          [0.56100078, 0.91542721, 1.        , ..., 0.41422994, 0.60228043,\n",
       "           0.94737102],\n",
       "          [0.57611185, 0.85747595, 1.        , ..., 0.77868057, 0.67710604,\n",
       "           1.        ],\n",
       "          [0.40600548, 0.85018821, 1.        , ..., 0.71092593, 0.52790974,\n",
       "           0.83474106]]),\n",
       "   8: array([[1.        , 0.        , 0.99249163, ..., 0.70379597, 0.72397012,\n",
       "           0.67329568],\n",
       "          [1.        , 0.        , 0.84775952, ..., 0.83456774, 0.47726719,\n",
       "           0.39619938],\n",
       "          [1.        , 0.        , 0.90237084, ..., 1.        , 0.62538743,\n",
       "           0.67928007],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.7153745 , ..., 0.57987984, 0.66807076,\n",
       "           0.71385871],\n",
       "          [1.        , 0.        , 0.52814947, ..., 0.59281894, 0.64904967,\n",
       "           0.80567941],\n",
       "          [1.        , 0.        , 0.47677082, ..., 0.26429709, 0.46172   ,\n",
       "           0.53050851]]),\n",
       "   9: array([[1.        , 1.        , 0.80505244, ..., 0.50472574, 0.86317599,\n",
       "           0.5968625 ],\n",
       "          [1.        , 1.        , 0.99668808, ..., 0.5273827 , 0.7874761 ,\n",
       "           0.58077548],\n",
       "          [1.        , 0.66296923, 1.        , ..., 0.87266566, 0.77775559,\n",
       "           0.67453473],\n",
       "          ...,\n",
       "          [1.        , 0.74917807, 0.23850385, ..., 0.        , 0.46485919,\n",
       "           0.02825726],\n",
       "          [1.        , 0.70300517, 0.23959336, ..., 0.03424227, 0.78574348,\n",
       "           0.4937695 ],\n",
       "          [0.99247598, 0.49058965, 0.20603458, ..., 0.16259334, 0.71176121,\n",
       "           0.49983399]]),\n",
       "   10: array([[0.88477112, 0.4136811 , 0.7494331 , ..., 0.        , 0.56020281,\n",
       "           0.38806699],\n",
       "          [0.6656659 , 0.19187781, 1.        , ..., 0.29905348, 0.42513023,\n",
       "           0.29676917],\n",
       "          [0.7033988 , 0.19808014, 1.        , ..., 1.        , 0.69968892,\n",
       "           0.82554003],\n",
       "          ...,\n",
       "          [0.36811371, 0.12323846, 1.        , ..., 0.43286211, 0.74388166,\n",
       "           0.43594704],\n",
       "          [0.34644056, 0.18492387, 0.9784911 , ..., 0.07895819, 0.47792531,\n",
       "           0.09632897],\n",
       "          [0.36926897, 0.13729481, 0.97518848, ..., 0.18971037, 0.64180602,\n",
       "           0.33387884]]),\n",
       "   11: array([[0.87184768, 0.93413989, 1.        , ..., 0.43099193, 0.49761133,\n",
       "           0.60801527],\n",
       "          [0.80265437, 0.42219313, 1.        , ..., 0.50086225, 0.54199738,\n",
       "           0.66431322],\n",
       "          [0.79082098, 0.41401744, 1.        , ..., 0.60102422, 0.49294097,\n",
       "           0.61173976],\n",
       "          ...,\n",
       "          [0.74573889, 0.46869254, 0.83424371, ..., 0.58880798, 0.96217375,\n",
       "           0.86092441],\n",
       "          [0.76787428, 0.46467895, 0.89475837, ..., 0.64713306, 0.57306979,\n",
       "           0.63365092],\n",
       "          [0.83858685, 0.37158826, 1.        , ..., 0.93691706, 0.61401388,\n",
       "           0.67295089]]),\n",
       "   12: array([[0.65416755, 1.        , 1.        , ..., 0.16114428, 0.71813825,\n",
       "           0.        ],\n",
       "          [0.85604478, 0.42499466, 0.89065217, ..., 0.        , 0.24142433,\n",
       "           0.1194451 ],\n",
       "          [0.60005266, 0.52114848, 0.8825769 , ..., 0.        , 0.08157523,\n",
       "           0.19720019],\n",
       "          ...,\n",
       "          [0.46103833, 0.18153791, 1.        , ..., 0.2620004 , 0.        ,\n",
       "           0.        ],\n",
       "          [0.62089929, 0.29240967, 1.        , ..., 0.13317927, 0.        ,\n",
       "           0.        ],\n",
       "          [0.82769451, 0.25876995, 0.9731812 , ..., 0.08187651, 0.05622158,\n",
       "           0.49696885]]),\n",
       "   13: array([[0.        , 0.74138714, 1.        , ..., 1.        , 0.59678036,\n",
       "           0.55482605],\n",
       "          [0.2699352 , 1.        , 1.        , ..., 0.65084044, 0.43161896,\n",
       "           0.067063  ],\n",
       "          [0.24168129, 1.        , 1.        , ..., 0.76407212, 0.09197727,\n",
       "           0.        ],\n",
       "          ...,\n",
       "          [0.16176609, 1.        , 0.95506604, ..., 0.38751826, 0.31669406,\n",
       "           0.32433813],\n",
       "          [0.36431532, 1.        , 1.        , ..., 0.80688703, 0.60438774,\n",
       "           0.45168032],\n",
       "          [0.28464826, 1.        , 1.        , ..., 0.82679072, 0.60443518,\n",
       "           0.17046942]]),\n",
       "   14: array([[0.        , 0.        , 0.81005912, ..., 0.35851572, 0.58844101,\n",
       "           0.49680975],\n",
       "          [0.        , 0.        , 1.        , ..., 0.56704951, 0.81269805,\n",
       "           0.56568787],\n",
       "          [0.        , 0.        , 1.        , ..., 0.71170414, 0.61863371,\n",
       "           0.61450209],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.9340305 , ..., 1.        , 0.74735946,\n",
       "           0.97083455],\n",
       "          [0.        , 0.        , 1.        , ..., 0.76803067, 0.36764079,\n",
       "           0.59727033],\n",
       "          [0.        , 0.        , 1.        , ..., 0.33523488, 0.16723536,\n",
       "           0.166358  ]]),\n",
       "   15: array([[1.        , 0.        , 1.        , ..., 0.15826613, 0.89620976,\n",
       "           1.        ],\n",
       "          [1.        , 0.        , 1.        , ..., 0.77487469, 0.99130523,\n",
       "           0.83837481],\n",
       "          [1.        , 0.        , 1.        , ..., 0.37404644, 0.93647713,\n",
       "           0.5420714 ],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.76742182, ..., 0.        , 0.76851841,\n",
       "           0.64573812],\n",
       "          [1.        , 0.        , 1.        , ..., 0.        , 0.71080069,\n",
       "           0.57362113],\n",
       "          [1.        , 0.        , 1.        , ..., 0.        , 0.67631907,\n",
       "           0.49699652]]),\n",
       "   16: array([[0.        , 0.        , 0.        , ..., 0.25324135, 0.        ,\n",
       "           0.36961175],\n",
       "          [0.        , 0.        , 1.        , ..., 0.        , 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 1.        , ..., 1.        , 0.        ,\n",
       "           0.10565811],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.62127807, ..., 0.59152357, 0.5608537 ,\n",
       "           0.08134894],\n",
       "          [0.        , 0.        , 0.71797954, ..., 0.6037034 , 0.61376431,\n",
       "           0.21721158],\n",
       "          [0.        , 0.        , 0.73433569, ..., 0.56331702, 0.63586955,\n",
       "           0.37790263]]),\n",
       "   17: array([[0.        , 0.        , 1.        , ..., 0.        , 0.26035578,\n",
       "           0.58958243],\n",
       "          [0.        , 0.        , 1.        , ..., 0.22987341, 0.67650923,\n",
       "           0.49980166],\n",
       "          [0.        , 0.        , 1.        , ..., 0.51909709, 0.70289385,\n",
       "           1.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.019464  , 0.61272548,\n",
       "           0.97805565],\n",
       "          [0.        , 0.        , 1.        , ..., 0.47716589, 0.34653855,\n",
       "           0.81494959],\n",
       "          [0.        , 0.        , 1.        , ..., 0.68577119, 0.38934066,\n",
       "           0.72017521]]),\n",
       "   18: array([[0.02399418, 0.58053992, 1.        , ..., 0.49275329, 0.81078846,\n",
       "           0.73928313],\n",
       "          [0.08137787, 0.15072251, 1.        , ..., 0.87981788, 0.60050178,\n",
       "           0.73848835],\n",
       "          [0.31464499, 0.2792705 , 0.87732917, ..., 0.67492812, 0.48628532,\n",
       "           0.61331128],\n",
       "          ...,\n",
       "          [0.34320432, 0.16716296, 0.78031286, ..., 0.49529609, 0.87337394,\n",
       "           0.40626256],\n",
       "          [0.3547028 , 0.37810089, 0.7155554 , ..., 0.38990594, 0.72846107,\n",
       "           0.04319119],\n",
       "          [0.23149838, 0.35605202, 0.87714749, ..., 0.03439792, 0.62080522,\n",
       "           0.19622518]])},\n",
       "  'fitted_feature_scaler': MinMaxScaler(clip=True)},\n",
       " 1: {'profile_windows': array([[0.76982177, 0.37496014, 0.8334941 , ..., 0.11025979, 0.96822752,\n",
       "          1.        ],\n",
       "         [0.77913903, 0.        , 1.        , ..., 0.86554313, 1.        ,\n",
       "          0.83845617],\n",
       "         [0.45795248, 0.50885993, 0.69041966, ..., 0.64443131, 0.80922204,\n",
       "          0.56604254],\n",
       "         ...,\n",
       "         [0.41714354, 0.38721526, 0.5752334 , ..., 0.67175569, 0.76479439,\n",
       "          0.77354376],\n",
       "         [0.3490123 , 0.48256015, 0.39313385, ..., 0.64862724, 0.44243332,\n",
       "          0.3531618 ],\n",
       "         [0.26230244, 0.58922413, 0.33098218, ..., 0.79487805, 0.47748045,\n",
       "          0.26114807]]),\n",
       "  'unknown_users_dict': {0: array([[0.        , 0.37753163, 1.        , ..., 0.81214202, 0.7277151 ,\n",
       "           0.90897521],\n",
       "          [0.        , 0.56993491, 1.        , ..., 0.85596018, 0.72451234,\n",
       "           0.64369966],\n",
       "          [0.        , 0.75138073, 1.        , ..., 0.78327316, 0.75957129,\n",
       "           0.69058367],\n",
       "          ...,\n",
       "          [0.        , 0.3241524 , 1.        , ..., 0.70787256, 0.27842733,\n",
       "           0.17885697],\n",
       "          [0.        , 0.20597603, 1.        , ..., 0.07409195, 0.24458167,\n",
       "           0.04493277],\n",
       "          [0.        , 0.26160402, 1.        , ..., 0.14064877, 0.48272261,\n",
       "           0.45819784]]),\n",
       "   1: array([[0.93732002, 0.44096612, 1.        , ..., 0.87498214, 0.92468719,\n",
       "           0.9703783 ],\n",
       "          [0.93821518, 0.4903767 , 1.        , ..., 0.66397831, 0.90902768,\n",
       "           0.90996845],\n",
       "          [1.        , 0.2812968 , 1.        , ..., 0.68878903, 0.97961692,\n",
       "           0.91838958],\n",
       "          ...,\n",
       "          [0.5274483 , 0.62229348, 0.71157218, ..., 0.87264436, 0.79664118,\n",
       "           1.        ],\n",
       "          [0.42018539, 0.56365403, 0.71589364, ..., 0.87862196, 0.745488  ,\n",
       "           0.83080461],\n",
       "          [0.3420726 , 0.24654554, 0.83700351, ..., 0.75099069, 0.5007737 ,\n",
       "           0.26909074]]),\n",
       "   2: array([[0.        , 0.        , 1.        , ..., 0.4427044 , 0.25371535,\n",
       "           0.        ],\n",
       "          [0.        , 0.28917905, 1.        , ..., 0.66930184, 0.77400707,\n",
       "           0.66837479],\n",
       "          [0.        , 0.93465897, 1.        , ..., 0.72813376, 0.72115316,\n",
       "           0.43071442],\n",
       "          ...,\n",
       "          [0.        , 0.77102923, 1.        , ..., 0.63057185, 0.65142838,\n",
       "           0.57950604],\n",
       "          [0.        , 0.67185479, 1.        , ..., 0.88639817, 0.96734997,\n",
       "           0.88519663],\n",
       "          [0.        , 0.86144193, 1.        , ..., 0.94322722, 0.74230995,\n",
       "           0.80905433]]),\n",
       "   3: array([[0.88568917, 0.0947508 , 1.        , ..., 0.65086307, 0.76679943,\n",
       "           0.96689729],\n",
       "          [0.77545118, 0.18385315, 1.        , ..., 0.68547335, 0.87810983,\n",
       "           0.73388792],\n",
       "          [0.50171727, 0.04163059, 1.        , ..., 0.74516536, 0.82429876,\n",
       "           0.77963303],\n",
       "          ...,\n",
       "          [0.70352329, 0.        , 1.        , ..., 0.48989469, 0.91296443,\n",
       "           0.88201358],\n",
       "          [0.52037544, 0.14824954, 1.        , ..., 0.63370385, 0.87347102,\n",
       "           0.75808317],\n",
       "          [0.66086386, 0.        , 1.        , ..., 0.69731703, 0.79583103,\n",
       "           0.24592434]]),\n",
       "   4: array([[0.        , 0.        , 1.        , ..., 0.9142829 , 0.88088167,\n",
       "           0.53618603],\n",
       "          [0.03045457, 0.        , 1.        , ..., 0.6532106 , 0.53350382,\n",
       "           0.54663019],\n",
       "          [0.        , 0.        , 1.        , ..., 0.81223303, 0.61930026,\n",
       "           0.59443499],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.83104289, 0.67926954,\n",
       "           0.73131037],\n",
       "          [0.        , 0.        , 1.        , ..., 0.76991778, 0.58965144,\n",
       "           0.83647992],\n",
       "          [0.        , 0.        , 1.        , ..., 0.77725487, 0.38714264,\n",
       "           0.76972231]]),\n",
       "   5: array([[0.        , 0.        , 1.        , ..., 0.47826533, 0.59469462,\n",
       "           0.19532913],\n",
       "          [0.        , 0.        , 1.        , ..., 0.48722798, 0.70093127,\n",
       "           0.39952953],\n",
       "          [0.        , 0.        , 1.        , ..., 0.50962208, 0.66938991,\n",
       "           0.31274363],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.16045926, 0.71714705,\n",
       "           0.87925639],\n",
       "          [0.        , 0.        , 1.        , ..., 0.34462011, 0.99831652,\n",
       "           0.76162854],\n",
       "          [0.        , 0.        , 1.        , ..., 0.45598225, 0.78000498,\n",
       "           0.65584489]]),\n",
       "   6: array([[0.        , 0.        , 1.        , ..., 0.03977165, 0.26232024,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.64969083, 0.19378963,\n",
       "           0.97034636],\n",
       "          [0.        , 0.        , 1.        , ..., 0.36402087, 0.39594386,\n",
       "           0.81823486],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.75317115, 0.61662287,\n",
       "           0.49205847],\n",
       "          [0.        , 0.        , 1.        , ..., 0.4711094 , 0.54823879,\n",
       "           0.52556824],\n",
       "          [0.        , 0.        , 1.        , ..., 0.47791463, 0.46066992,\n",
       "           0.54124894]]),\n",
       "   7: array([[0.        , 0.70564666, 1.        , ..., 0.70606402, 0.63060779,\n",
       "           0.65366832],\n",
       "          [0.        , 1.        , 1.        , ..., 0.81054589, 0.75839891,\n",
       "           0.74554628],\n",
       "          [0.        , 1.        , 1.        , ..., 0.62383372, 0.73044136,\n",
       "           0.8960262 ],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.52977771, 0.56570755,\n",
       "           0.84183661],\n",
       "          [0.        , 1.        , 1.        , ..., 0.74608792, 0.63090044,\n",
       "           0.93715126],\n",
       "          [0.        , 1.        , 1.        , ..., 0.70587391, 0.50091101,\n",
       "           0.73756468]]),\n",
       "   8: array([[0.        , 0.        , 1.        , ..., 0.70164211, 0.67173144,\n",
       "           0.58809982],\n",
       "          [0.        , 0.        , 1.        , ..., 0.77925829, 0.45678796,\n",
       "           0.33156625],\n",
       "          [0.        , 0.        , 1.        , ..., 0.88427885, 0.58583985,\n",
       "           0.59364012],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.62809491, 0.62302832,\n",
       "           0.62565276],\n",
       "          [0.        , 0.        , 1.        , ..., 0.63577458, 0.60645592,\n",
       "           0.71065963],\n",
       "          [0.        , 0.        , 1.        , ..., 0.44078897, 0.44324225,\n",
       "           0.45590858]]),\n",
       "   9: array([[0.        , 1.        , 1.        , ..., 0.58348915, 0.79301657,\n",
       "           0.51733858],\n",
       "          [0.        , 1.        , 1.        , ..., 0.5969366 , 0.72706194,\n",
       "           0.50244535],\n",
       "          [0.        , 0.97542074, 1.        , ..., 0.80187032, 0.71859281,\n",
       "           0.58924692],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.0270091 , 0.44597731,\n",
       "           0.        ],\n",
       "          [0.        , 1.        , 1.        , ..., 0.30424591, 0.72555238,\n",
       "           0.4218959 ],\n",
       "          [0.        , 0.65047706, 1.        , ..., 0.38042535, 0.66109426,\n",
       "           0.42751035]]),\n",
       "   10: array([[0.        , 0.50550081, 1.        , ..., 0.26841702, 0.52904682,\n",
       "           0.32403734],\n",
       "          [0.        , 0.08739109, 1.        , ..., 0.46141772, 0.41136288,\n",
       "           0.23951454],\n",
       "          [0.        , 0.09908277, 1.        , ..., 0.92550384, 0.6505761 ,\n",
       "           0.72904644],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.54083635, 0.68907966,\n",
       "           0.36836432],\n",
       "          [0.        , 0.07428257, 1.        , ..., 0.33078588, 0.45736136,\n",
       "           0.05394858],\n",
       "          [0.        , 0.        , 1.        , ..., 0.39651996, 0.60014479,\n",
       "           0.27387039]]),\n",
       "   11: array([[0.        , 1.        , 1.        , ..., 0.53972636, 0.47451309,\n",
       "           0.52766374],\n",
       "          [0.        , 0.52154637, 1.        , ..., 0.58119607, 0.51318507,\n",
       "           0.57978393],\n",
       "          [0.        , 0.50613482, 1.        , ..., 0.64064461, 0.47044397,\n",
       "           0.53111183],\n",
       "          ...,\n",
       "          [0.        , 0.60919997, 1.        , ..., 0.63339397, 0.87926979,\n",
       "           0.76180502],\n",
       "          [0.        , 0.60163417, 1.        , ..., 0.66801131, 0.54025736,\n",
       "           0.55139701],\n",
       "          [0.        , 0.4261538 , 1.        , ..., 0.84000507, 0.57593048,\n",
       "           0.58778061]]),\n",
       "   12: array([[0.        , 1.        , 1.        , ..., 0.3795653 , 0.66665034,\n",
       "           0.        ],\n",
       "          [0.        , 0.5268274 , 1.        , ..., 0.16806621, 0.25130648,\n",
       "           0.07534932],\n",
       "          [0.        , 0.70808191, 1.        , ..., 0.234698  , 0.11203564,\n",
       "           0.14733436],\n",
       "          ...,\n",
       "          [0.        , 0.06789988, 1.        , ..., 0.43942583, 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.2768984 , 1.        , ..., 0.3629674 , 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.21348593, 1.        , ..., 0.33251797, 0.08994591,\n",
       "           0.42485783]]),\n",
       "   13: array([[0.        , 1.        , 1.        , ..., 1.        , 0.56091553,\n",
       "           0.47842157],\n",
       "          [0.        , 1.        , 1.        , ..., 0.67021173, 0.41701629,\n",
       "           0.02685439],\n",
       "          [0.        , 1.        , 1.        , ..., 0.73741745, 0.12109857,\n",
       "           0.        ],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.51392369, 0.3168863 ,\n",
       "           0.26503767],\n",
       "          [0.        , 1.        , 1.        , ..., 0.76282913, 0.56754358,\n",
       "           0.38293007],\n",
       "          [0.        , 1.        , 1.        , ..., 0.77464245, 0.5675849 ,\n",
       "           0.12258723]]),\n",
       "   14: array([[0.        , 0.        , 1.        , ..., 0.49670999, 0.55364975,\n",
       "           0.42471054],\n",
       "          [0.        , 0.        , 1.        , ..., 0.6204798 , 0.74903694,\n",
       "           0.48847736],\n",
       "          [0.        , 0.        , 1.        , ..., 0.7063358 , 0.57995558,\n",
       "           0.53366918],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.91915532, 0.69210975,\n",
       "           0.86355896],\n",
       "          [0.        , 0.        , 1.        , ..., 0.73976694, 0.36127438,\n",
       "           0.51771615],\n",
       "          [0.        , 0.        , 1.        , ..., 0.48289225, 0.18666826,\n",
       "           0.11878091]]),\n",
       "   15: array([[0.        , 0.        , 1.        , ..., 0.37785705, 0.82179772,\n",
       "           0.90343672],\n",
       "          [0.        , 0.        , 1.        , ..., 0.74382903, 0.90465101,\n",
       "           0.74092878],\n",
       "          [0.        , 0.        , 1.        , ..., 0.50592784, 0.85688124,\n",
       "           0.46661341],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.2791301 , 0.71054478,\n",
       "           0.56258725],\n",
       "          [0.        , 0.        , 1.        , ..., 0.        , 0.66025739,\n",
       "           0.49582191],\n",
       "          [0.        , 0.        , 1.        , ..., 0.        , 0.63021478,\n",
       "           0.42488344]]),\n",
       "   16: array([[0.        , 0.        , 0.        , ..., 0.43422712, 0.00191665,\n",
       "           0.30695163],\n",
       "          [0.        , 0.        , 1.        , ..., 0.        , 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.90549827, 0.        ,\n",
       "           0.06258543],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.63500574, 0.52961391,\n",
       "           0.0400802 ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.64223476, 0.57571305,\n",
       "           0.16586075],\n",
       "          [0.        , 0.        , 1.        , ..., 0.61826448, 0.59497255,\n",
       "           0.31462726]]),\n",
       "   17: array([[0.        , 0.        , 1.        , ..., 0.06069725, 0.26780077,\n",
       "           0.51059876],\n",
       "          [0.        , 0.        , 1.        , ..., 0.42035769, 0.63038046,\n",
       "           0.42748043],\n",
       "          [0.        , 0.        , 1.        , ..., 0.59201888, 0.65336844,\n",
       "           0.95971759],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.29547465, 0.57480795,\n",
       "           0.8702442 ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.56713171, 0.34288875,\n",
       "           0.7192419 ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.69094397, 0.38018071,\n",
       "           0.63150052]]),\n",
       "   18: array([[0.        , 0.82003759, 1.        , ..., 0.57638321, 0.74737317,\n",
       "           0.64919048],\n",
       "          [0.        , 0.0098114 , 1.        , ..., 0.80611534, 0.56415788,\n",
       "           0.64845468],\n",
       "          [0.        , 0.25213045, 1.        , ..., 0.68450835, 0.46464514,\n",
       "           0.53256674],\n",
       "          ...,\n",
       "          [0.        , 0.04080243, 1.        , ..., 0.57789242, 0.80190167,\n",
       "           0.34088267],\n",
       "          [0.        , 0.43843041, 1.        , ..., 0.51534083, 0.67564425,\n",
       "           0.00475405],\n",
       "          [0.        , 0.39686725, 1.        , ..., 0.30433829, 0.58184754,\n",
       "           0.14643171]])},\n",
       "  'fitted_feature_scaler': MinMaxScaler(clip=True)},\n",
       " 2: {'profile_windows': array([[0.4606129 , 0.38624686, 0.29090394, ..., 0.60329007, 0.07989236,\n",
       "          0.41567683],\n",
       "         [0.85141026, 0.4999653 , 0.49743657, ..., 0.91254778, 0.82756815,\n",
       "          0.71557741],\n",
       "         [0.52163992, 0.74026724, 0.37003572, ..., 0.89229614, 0.77665985,\n",
       "          0.5554798 ],\n",
       "         ...,\n",
       "         [0.37435626, 0.65868479, 0.29234083, ..., 0.75863431, 0.72118062,\n",
       "          0.55874327],\n",
       "         [0.44225454, 0.56711594, 0.14750707, ..., 0.88302052, 0.83132015,\n",
       "          0.74735996],\n",
       "         [0.4431816 , 0.86671939, 0.46108835, ..., 0.7326844 , 0.84775274,\n",
       "          0.83531284]]),\n",
       "  'unknown_users_dict': {0: array([[0.23576986, 0.56830179, 0.04370091, ..., 0.87249667, 0.63410412,\n",
       "           0.77856602],\n",
       "          [0.2364972 , 0.6477076 , 0.01596033, ..., 0.90329125, 0.63040697,\n",
       "           0.5664346 ],\n",
       "          [0.30124298, 0.72259122, 0.        , ..., 0.85220817, 0.67087777,\n",
       "           0.60392607],\n",
       "          ...,\n",
       "          [0.09780987, 0.54627191, 0.        , ..., 0.79921804, 0.11546247,\n",
       "           0.19471644],\n",
       "          [0.15184795, 0.49749993, 0.        , ..., 0.35380893, 0.07639225,\n",
       "           0.08762203],\n",
       "          [0.17596075, 0.52045788, 0.        , ..., 0.40058381, 0.35129359,\n",
       "           0.4180954 ]]),\n",
       "   1: array([[1.        , 0.59448153, 0.        , ..., 0.91665952, 0.86148162,\n",
       "           0.82766789],\n",
       "          [1.        , 0.61487353, 0.        , ..., 0.76836999, 0.84340485,\n",
       "           0.77936028],\n",
       "          [1.        , 0.52858519, 0.        , ..., 0.7858065 , 0.92489054,\n",
       "           0.78609436],\n",
       "          ...,\n",
       "          [1.        , 0.66931625, 0.        , ..., 0.91501658, 0.71366992,\n",
       "           0.86061962],\n",
       "          [1.        , 0.64511545, 0.        , ..., 0.91921752, 0.65462052,\n",
       "           0.71605577],\n",
       "          [1.        , 0.51424317, 0.        , ..., 0.82952066, 0.37213112,\n",
       "           0.26687318]]),\n",
       "   2: array([[1.        , 0.16618081, 0.97964007, ..., 0.61286284, 0.08693585,\n",
       "           0.        ],\n",
       "          [0.84892781, 0.53183823, 0.83869705, ..., 0.77211127, 0.68754191,\n",
       "           0.58616642],\n",
       "          [0.73803088, 0.79823107, 0.30165934, ..., 0.81345724, 0.62652926,\n",
       "           0.39611789],\n",
       "          ...,\n",
       "          [0.20687599, 0.73070025, 0.39046371, ..., 0.74489256, 0.54604147,\n",
       "           0.51510125],\n",
       "          [0.44997763, 0.68977046, 0.36558311, ..., 0.9246825 , 0.91073001,\n",
       "           0.75955114],\n",
       "          [0.69684079, 0.76801403, 0.41452588, ..., 0.96462089, 0.6509519 ,\n",
       "           0.69866285]]),\n",
       "   3: array([[1.        , 0.45159671, 0.        , ..., 0.75915285, 0.67922167,\n",
       "           0.82488425],\n",
       "          [1.        , 0.4883697 , 0.        , ..., 0.7834763 , 0.8077144 ,\n",
       "           0.63855495],\n",
       "          [1.        , 0.42967373, 0.        , ..., 0.82542672, 0.74559684,\n",
       "           0.67513568],\n",
       "          ...,\n",
       "          [1.        , 0.36704655, 0.        , ..., 0.64602729, 0.84794929,\n",
       "           0.75700576],\n",
       "          [1.        , 0.47367591, 0.        , ..., 0.74709367, 0.80235952,\n",
       "           0.65790303],\n",
       "          [1.        , 0.35726585, 0.        , ..., 0.79179982, 0.71273471,\n",
       "           0.24834783]]),\n",
       "   4: array([[1.        , 0.        , 0.17933221, ..., 0.94427936, 0.81091411,\n",
       "           0.48045978],\n",
       "          [1.        , 0.        , 0.02695341, ..., 0.76080265, 0.4099136 ,\n",
       "           0.4888116 ],\n",
       "          [1.        , 0.        , 0.08282691, ..., 0.87256063, 0.50895393,\n",
       "           0.5270394 ],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.        , ..., 0.88577984, 0.57818031,\n",
       "           0.63649377],\n",
       "          [1.        , 0.        , 0.        , ..., 0.84282226, 0.47472839,\n",
       "           0.72059412],\n",
       "          [1.        , 0.        , 0.        , ..., 0.84797864, 0.24095951,\n",
       "           0.66721043]]),\n",
       "   5: array([[0.83765943, 0.        , 0.24810482, ..., 0.63785439, 0.48055007,\n",
       "           0.20788864],\n",
       "          [1.        , 0.        , 0.41666302, ..., 0.64415317, 0.60318584,\n",
       "           0.37118044],\n",
       "          [1.        , 0.        , 0.40511081, ..., 0.65989133, 0.56677562,\n",
       "           0.30178084],\n",
       "          ...,\n",
       "          [0.9599033 , 0.        , 0.09941755, ..., 0.41450625, 0.62190475,\n",
       "           0.75480094],\n",
       "          [0.99109345, 0.        , 0.12447537, ..., 0.54393104, 0.94647668,\n",
       "           0.66073813],\n",
       "          [0.76876484, 0.        , 0.11346129, ..., 0.62219426, 0.69446569,\n",
       "           0.57614671]]),\n",
       "   6: array([[0.        , 0.        , 1.        , ..., 0.32968926, 0.09686902,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.75832902, 0.01775975,\n",
       "           0.82764234],\n",
       "          [0.        , 0.        , 1.        , ..., 0.55756553, 0.25111933,\n",
       "           0.70600419],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.83105305, 0.50586325,\n",
       "           0.44517254],\n",
       "          [0.        , 0.        , 1.        , ..., 0.63282534, 0.42692313,\n",
       "           0.47196911],\n",
       "          [0.        , 0.        , 1.        , ..., 0.63760793, 0.32583676,\n",
       "           0.4845084 ]]),\n",
       "   7: array([[0.        , 0.70371653, 0.49162564, ..., 0.79794704, 0.52200693,\n",
       "           0.57440619],\n",
       "          [0.36336931, 0.95992652, 0.50491078, ..., 0.87137494, 0.66952441,\n",
       "           0.64787772],\n",
       "          [0.45592305, 0.86007949, 0.58280139, ..., 0.74015712, 0.63725123,\n",
       "           0.76821116],\n",
       "          ...,\n",
       "          [0.37524932, 1.        , 0.30710608, ..., 0.67405633, 0.44708843,\n",
       "           0.72487767],\n",
       "          [0.38674447, 0.96637379, 0.28751475, ..., 0.82607508, 0.52234476,\n",
       "           0.8010974 ],\n",
       "          [0.25734287, 0.96070416, 0.18288834, ..., 0.79781344, 0.37228963,\n",
       "           0.64149512]]),\n",
       "   8: array([[1.        , 0.        , 0.12944814, ..., 0.7948394 , 0.5694786 ,\n",
       "           0.52197339],\n",
       "          [1.        , 0.        , 0.05720244, ..., 0.8493866 , 0.32135557,\n",
       "           0.31683262],\n",
       "          [1.        , 0.        , 0.08446268, ..., 0.92319308, 0.47032844,\n",
       "           0.52640377],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.        , ..., 0.74315181, 0.51325747,\n",
       "           0.55200314],\n",
       "          [1.        , 0.        , 0.        , ..., 0.74854894, 0.49412689,\n",
       "           0.61998011],\n",
       "          [1.        , 0.        , 0.        , ..., 0.61151671, 0.30571889,\n",
       "           0.41626476]]),\n",
       "   9: array([[1.        , 1.        , 0.03588441, ..., 0.71180372, 0.70948579,\n",
       "           0.46538814],\n",
       "          [0.93723824, 1.        , 0.13154287, ..., 0.72125433, 0.63335014,\n",
       "           0.45347856],\n",
       "          [0.83243266, 0.81505366, 0.15605754, ..., 0.86527791, 0.62357368,\n",
       "           0.52289069],\n",
       "          ...,\n",
       "          [0.88995076, 0.88212144, 0.        , ..., 0.32071998, 0.30887615,\n",
       "           0.0444366 ],\n",
       "          [0.86476552, 0.84620037, 0.        , ..., 0.51555681, 0.63160755,\n",
       "           0.38906603],\n",
       "          [0.70347675, 0.68094776, 0.        , ..., 0.56909429, 0.55719942,\n",
       "           0.39355571]]),\n",
       "   10: array([[0.62154461, 0.62111532, 0.00812099, ..., 0.49037694, 0.4047686 ,\n",
       "           0.31081202],\n",
       "          [0.45486911, 0.44855932, 0.36576989, ..., 0.62601421, 0.26891849,\n",
       "           0.24322215],\n",
       "          [0.4835729 , 0.45338453, 0.59961443, ..., 0.95216523, 0.54505764,\n",
       "           0.63468338],\n",
       "          ...,\n",
       "          [0.22851824, 0.39516005, 0.15526602, ..., 0.68182813, 0.58950477,\n",
       "           0.34625873],\n",
       "          [0.21203125, 0.44314937, 0.12245951, ..., 0.5342086 , 0.32201748,\n",
       "           0.09483165],\n",
       "          [0.22939705, 0.40609546, 0.12081095, ..., 0.58040528, 0.48684154,\n",
       "           0.27069529]]),\n",
       "   11: array([[0.61171362, 1.        , 0.35777388, ..., 0.68104805, 0.34181682,\n",
       "           0.4736448 ],\n",
       "          [0.55907759, 0.62773741, 0.32789544, ..., 0.71019219, 0.38645837,\n",
       "           0.51532347],\n",
       "          [0.55007581, 0.62137698, 0.33206441, ..., 0.7519715 , 0.33711958,\n",
       "           0.47640212],\n",
       "          ...,\n",
       "          [0.51578142, 0.66391249, 0.05045577, ..., 0.74687589, 0.80905342,\n",
       "           0.66087925],\n",
       "          [0.53262003, 0.66079005, 0.08066278, ..., 0.77120431, 0.41770964,\n",
       "           0.49262346],\n",
       "          [0.58641178, 0.58836841, 0.16813508, ..., 0.8920783 , 0.45888942,\n",
       "           0.52171813]]),\n",
       "   12: array([[0.4461222 , 1.        , 0.30666069, ..., 0.56848986, 0.56361316,\n",
       "           0.        ],\n",
       "          [0.5996922 , 0.62991692, 0.07861309, ..., 0.41985227, 0.08415513,\n",
       "           0.11194506],\n",
       "          [0.40495647, 0.70472157, 0.07458217, ..., 0.46667984, 0.        ,\n",
       "           0.16950894],\n",
       "          ...,\n",
       "          [0.2992069 , 0.4405152 , 0.20275028, ..., 0.61055872, 0.        ,\n",
       "           0.        ],\n",
       "          [0.42081471, 0.52676995, 0.17170837, ..., 0.55682517, 0.        ,\n",
       "           0.        ],\n",
       "          [0.57812587, 0.5005993 , 0.11980898, ..., 0.53542588, 0.        ,\n",
       "           0.39143458]]),\n",
       "   13: array([[0.        , 0.87606034, 0.32153989, ..., 1.        , 0.4415567 ,\n",
       "           0.4342676 ],\n",
       "          [0.15383288, 1.        , 0.48798234, ..., 0.77275072, 0.27544458,\n",
       "           0.07316539],\n",
       "          [0.13233985, 1.        , 0.54677573, ..., 0.81998164, 0.        ,\n",
       "           0.        ],\n",
       "          ...,\n",
       "          [0.07154757, 1.        , 0.11076646, ..., 0.66291442, 0.15985813,\n",
       "           0.26363209],\n",
       "          [0.22562876, 1.        , 0.32447884, ..., 0.83784049, 0.44920787,\n",
       "           0.35790644],\n",
       "          [0.16502525, 1.        , 0.29066562, ..., 0.84614267, 0.44925558,\n",
       "           0.14971954]]),\n",
       "   14: array([[0.        , 0.        , 0.03838359, ..., 0.65081695, 0.43316935,\n",
       "           0.39131679],\n",
       "          [0.        , 0.        , 0.17479389, ..., 0.73780005, 0.65871729,\n",
       "           0.44230885],\n",
       "          [0.        , 0.        , 0.51908855, ..., 0.79813804, 0.46353585,\n",
       "           0.47844714],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.10026618, ..., 0.94770361, 0.59300258,\n",
       "           0.74224826],\n",
       "          [0.        , 0.        , 0.17478647, ..., 0.82163282, 0.21109813,\n",
       "           0.46569008],\n",
       "          [0.        , 0.        , 0.13972335, ..., 0.64110611, 0.0095391 ,\n",
       "           0.14667576]]),\n",
       "   15: array([[0.76195018, 0.1215869 , 0.60273022, ..., 0.56728933, 0.74270972,\n",
       "           0.77413709],\n",
       "          [0.7924036 , 0.03404307, 0.47815253, ..., 0.82448758, 0.83835258,\n",
       "           0.64418527],\n",
       "          [1.        , 0.        , 0.25660428, ..., 0.65729509, 0.78320888,\n",
       "           0.42482503],\n",
       "          ...,\n",
       "          [1.        , 0.09990985, 0.01710039, ..., 0.49790589, 0.61428333,\n",
       "           0.50157189],\n",
       "          [1.        , 0.        , 0.26044846, ..., 0.20674904, 0.55623337,\n",
       "           0.44818203],\n",
       "          [1.        , 0.        , 0.31139467, ..., 0.23893673, 0.52155326,\n",
       "           0.39145506]]),\n",
       "   16: array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           6.06905164e-01, 0.00000000e+00, 2.97149188e-01],\n",
       "          [0.00000000e+00, 0.00000000e+00, 2.33614063e-01, ...,\n",
       "           2.62377554e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 1.98352402e-01, 3.57063050e-01, ...,\n",
       "           9.38105690e-01, 0.00000000e+00, 1.01738234e-01],\n",
       "          ...,\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           7.48008612e-01, 4.05423232e-01, 8.37415972e-02],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           7.53089035e-01, 4.58638416e-01, 1.84323833e-01],\n",
       "          [0.00000000e+00, 0.00000000e+00, 5.84840235e-04, ...,\n",
       "           7.36243167e-01, 4.80870902e-01, 3.03287116e-01]]),\n",
       "   17: array([[0.        , 0.        , 1.        , ..., 0.34439538, 0.10319555,\n",
       "           0.45999855],\n",
       "          [0.        , 0.        , 1.        , ..., 0.59715799, 0.52174451,\n",
       "           0.39353178],\n",
       "          [0.        , 0.        , 1.        , ..., 0.71779826, 0.54828102,\n",
       "           0.81914289],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.50939253, 0.45759361,\n",
       "           0.74759421],\n",
       "          [0.        , 0.        , 1.        , ..., 0.70030802, 0.18987442,\n",
       "           0.62684304],\n",
       "          [0.        , 0.        , 1.        , ..., 0.78732095, 0.23292291,\n",
       "           0.55667938]]),\n",
       "   18: array([[0.        , 0.75092625, 0.21168504, ..., 0.7068098 , 0.6567967 ,\n",
       "           0.57082542],\n",
       "          [0.01039546, 0.41654179, 0.13325309, ..., 0.86826123, 0.44529955,\n",
       "           0.57023702],\n",
       "          [0.18784405, 0.51654809, 0.07196267, ..., 0.78279812, 0.33042562,\n",
       "           0.47756556],\n",
       "          ...,\n",
       "          [0.20956941, 0.42933194, 0.02353519, ..., 0.70787045, 0.71974244,\n",
       "           0.32428263],\n",
       "          [0.21831642, 0.59343503, 0.        , ..., 0.66391036, 0.5739954 ,\n",
       "           0.05549253],\n",
       "          [0.12459361, 0.5762817 , 0.07187198, ..., 0.51562173, 0.46571985,\n",
       "           0.16878712]])},\n",
       "  'fitted_feature_scaler': MinMaxScaler(clip=True)},\n",
       " 3: {'profile_windows': array([[0.        , 0.        , 0.99594371, ..., 0.81813059, 0.59615348,\n",
       "          0.79216445],\n",
       "         [0.72671297, 0.17170688, 1.        , ..., 0.45239188, 0.38884551,\n",
       "          0.77338549],\n",
       "         [0.72911934, 0.35275146, 0.86714573, ..., 0.79841661, 0.77156696,\n",
       "          0.54112326],\n",
       "         ...,\n",
       "         [0.37707363, 0.54754438, 0.31333674, ..., 0.58508006, 0.45774292,\n",
       "          0.67188304],\n",
       "         [0.89252934, 0.58443744, 0.36036806, ..., 0.8593138 , 0.42898069,\n",
       "          0.69549027],\n",
       "         [0.84638838, 0.58810238, 0.51302813, ..., 0.81382991, 0.72367793,\n",
       "          0.73059248]]),\n",
       "  'unknown_users_dict': {0: array([[0.        , 0.73189819, 1.        , ..., 0.74915863, 0.56685296,\n",
       "           0.94831   ],\n",
       "          [0.        , 0.90534708, 1.        , ..., 0.81032079, 0.56163517,\n",
       "           0.67742384],\n",
       "          [0.        , 1.        , 0.94846731, ..., 0.70886296, 0.61875151,\n",
       "           0.72529945],\n",
       "          ...,\n",
       "          [0.        , 0.68377755, 0.80105634, ..., 0.60361747, 0.        ,\n",
       "           0.20274967],\n",
       "          [0.        , 0.5772432 , 0.89403464, ..., 0.        , 0.        ,\n",
       "           0.06599295],\n",
       "          [0.        , 0.62739105, 0.81076135, ..., 0.        , 0.16772315,\n",
       "           0.48799863]]),\n",
       "   1: array([[1.        , 0.78908351, 0.6643862 , ..., 0.83687196, 0.88775027,\n",
       "           1.        ],\n",
       "          [1.        , 0.83362646, 0.53176129, ..., 0.54234911, 0.86223856,\n",
       "           0.94932425],\n",
       "          [1.        , 0.64514382, 0.6449197 , ..., 0.57698035, 0.97723911,\n",
       "           0.95792348],\n",
       "          ...,\n",
       "          [0.38624672, 0.95254761, 0.15668116, ..., 0.83360886, 0.67914397,\n",
       "           1.        ],\n",
       "          [0.2050239 , 0.89968496, 0.15934797, ..., 0.84195249, 0.5958077 ,\n",
       "           0.86848608],\n",
       "          [0.07305081, 0.61381605, 0.23408592, ..., 0.66380253, 0.1971311 ,\n",
       "           0.2948919 ]]),\n",
       "   2: array([[0.        , 0.        , 1.        , ..., 0.2334911 , 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.65224956, 1.        , ..., 0.54977979, 0.64226957,\n",
       "           0.70262085],\n",
       "          [0.        , 1.        , 1.        , ..., 0.63189841, 0.55616257,\n",
       "           0.45993394],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.49571979, 0.44257035,\n",
       "           0.61187251],\n",
       "          [0.        , 0.99722644, 1.        , ..., 0.85280667, 0.95725439,\n",
       "           0.9240285 ],\n",
       "          [0.        , 1.        , 1.        , ..., 0.93212965, 0.59063018,\n",
       "           0.84627579]]),\n",
       "   3: array([[0.99150172, 0.47697519, 0.84613555, ..., 0.52404263, 0.63052724,\n",
       "           1.        ],\n",
       "          [0.80525247, 0.55729972, 0.77412115, ..., 0.57235226, 0.81186871,\n",
       "           0.7695196 ],\n",
       "          [0.34277363, 0.42908805, 0.90899395, ..., 0.65567142, 0.72420235,\n",
       "           0.81623222],\n",
       "          ...,\n",
       "          [0.68372887, 0.29228931, 0.68331714, ..., 0.29936013, 0.86865212,\n",
       "           0.92077813],\n",
       "          [0.37429698, 0.52520356, 0.56665692, ..., 0.50009149, 0.80431139,\n",
       "           0.79422658],\n",
       "          [0.61165493, 0.27092499, 0.60140577, ..., 0.58888389, 0.67782411,\n",
       "           0.27123552]]),\n",
       "   4: array([[0.        , 0.        , 1.        , ..., 0.89172866, 0.81638446,\n",
       "           0.56763628],\n",
       "          [0.        , 0.        , 1.        , ..., 0.52731935, 0.25045341,\n",
       "           0.57830133],\n",
       "          [0.        , 0.        , 1.        , ..., 0.74928565, 0.39022879,\n",
       "           0.62711722],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.67962047, ..., 0.77554079, 0.48792782,\n",
       "           0.76688753],\n",
       "          [0.        , 0.        , 0.72778294, ..., 0.69022128, 0.34192638,\n",
       "           0.87428143],\n",
       "          [0.        , 0.        , 0.91856934, ..., 0.70046253, 0.01200892,\n",
       "           0.80611189]]),\n",
       "   5: array([[0.        , 0.        , 1.        , ..., 0.28312766, 0.35014249,\n",
       "           0.21957022],\n",
       "          [0.        , 0.        , 1.        , ..., 0.29563789, 0.52321805,\n",
       "           0.42808948],\n",
       "          [0.        , 0.        , 1.        , ..., 0.32689597, 0.4718324 ,\n",
       "           0.33946805],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.        , 0.54963601,\n",
       "           0.91796262],\n",
       "          [0.        , 0.        , 1.        , ..., 0.09658332, 1.        ,\n",
       "           0.79784693],\n",
       "          [0.        , 0.        , 1.        , ..., 0.25202455, 0.65204108,\n",
       "           0.68982594]]),\n",
       "   6: array([[0.        , 0.        , 1.        , ..., 0.        , 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.5224064 , 0.        ,\n",
       "           1.        ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.12366324, 0.02634744,\n",
       "           0.85565048],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.66684605, 0.38586692,\n",
       "           0.52257542],\n",
       "          [0.        , 0.        , 1.        , ..., 0.2731393 , 0.27445892,\n",
       "           0.55679392],\n",
       "          [0.        , 0.        , 1.        , ..., 0.28263815, 0.13179598,\n",
       "           0.57280627]]),\n",
       "   7: array([[0.        , 1.        , 1.        , ..., 0.60109309, 0.40865046,\n",
       "           0.68760334],\n",
       "          [0.        , 1.        , 1.        , ..., 0.74693072, 0.61684152,\n",
       "           0.78142453],\n",
       "          [0.        , 1.        , 1.        , ..., 0.48631458, 0.57129445,\n",
       "           0.93508712],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.35502956, 0.30291817,\n",
       "           0.87975141],\n",
       "          [0.        , 1.        , 1.        , ..., 0.65695914, 0.40912724,\n",
       "           0.97708197],\n",
       "          [0.        , 1.        , 1.        , ..., 0.60082773, 0.1973548 ,\n",
       "           0.77327411]]),\n",
       "   8: array([[0.        , 0.        , 1.        , ..., 0.5949209 , 0.47564711,\n",
       "           0.62064806],\n",
       "          [0.        , 0.        , 1.        , ..., 0.70325894, 0.12547168,\n",
       "           0.35868877],\n",
       "          [0.        , 0.        , 1.        , ..., 0.84984848, 0.33571673,\n",
       "           0.62630554],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.49226243, 0.39630237,\n",
       "           0.65899524],\n",
       "          [0.        , 0.        , 0.92348851, ..., 0.50298184, 0.36930342,\n",
       "           0.74580002],\n",
       "          [0.        , 0.        , 0.89962555, ..., 0.2308175 , 0.10340368,\n",
       "           0.48566096]]),\n",
       "   9: array([[0.        , 1.        , 1.        , ..., 0.43000092, 0.67323892,\n",
       "           0.54839021],\n",
       "          [0.        , 1.        , 1.        , ..., 0.44877111, 0.56578887,\n",
       "           0.53318199],\n",
       "          [0.        , 1.        , 1.        , ..., 0.73482121, 0.55199137,\n",
       "           0.62181942],\n",
       "          ...,\n",
       "          [0.        , 1.        , 0.78896176, ..., 0.        , 0.10785951,\n",
       "           0.01084632],\n",
       "          [0.        , 1.        , 0.78946778, ..., 0.0402283 , 0.56332955,\n",
       "           0.4509289 ],\n",
       "          [0.        , 0.97795472, 0.77388131, ..., 0.1465609 , 0.45831754,\n",
       "           0.4566621 ]]),\n",
       "   10: array([[0.        , 0.84726063, 1.        , ..., 0.        , 0.24319229,\n",
       "           0.35100063],\n",
       "          [0.        , 0.47034051, 1.        , ..., 0.25961148, 0.05146736,\n",
       "           0.26469016],\n",
       "          [0.        , 0.4808804 , 1.        , ..., 0.90739105, 0.44118187,\n",
       "           0.76457571],\n",
       "          ...,\n",
       "          [0.        , 0.35369863, 1.        , ..., 0.3704654 , 0.50390999,\n",
       "           0.39626512],\n",
       "          [0.        , 0.45852337, 1.        , ..., 0.07727325, 0.12640584,\n",
       "           0.07519945],\n",
       "          [0.        , 0.37758521, 1.        , ..., 0.16902604, 0.35902163,\n",
       "           0.29977263]]),\n",
       "   11: array([[0.        , 1.        , 1.        , ..., 0.36891605, 0.1543486 ,\n",
       "           0.55893374],\n",
       "          [0.        , 0.86172549, 1.        , ..., 0.42680021, 0.21735111,\n",
       "           0.61215629],\n",
       "          [0.        , 0.84783218, 1.        , ..., 0.50977952, 0.14771939,\n",
       "           0.56245477],\n",
       "          ...,\n",
       "          [0.        , 0.94074399, 1.        , ..., 0.49965896, 0.81375847,\n",
       "           0.79802714],\n",
       "          [0.        , 0.93392353, 1.        , ..., 0.54797845, 0.26145595,\n",
       "           0.58316898],\n",
       "          [0.        , 0.7757304 , 1.        , ..., 0.78805036, 0.31957287,\n",
       "           0.6203221 ]]),\n",
       "   12: array([[0.        , 1.        , 1.        , ..., 0.14536043, 0.46736923,\n",
       "           0.        ],\n",
       "          [0.        , 0.86648626, 1.        , ..., 0.        , 0.        ,\n",
       "           0.09705282],\n",
       "          [0.        , 1.        , 1.        , ..., 0.        , 0.        ,\n",
       "           0.17056036],\n",
       "          ...,\n",
       "          [0.        , 0.45276946, 1.        , ..., 0.22891481, 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.64117873, 1.        , ..., 0.12219278, 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.58401327, 1.        , ..., 0.07969094, 0.        ,\n",
       "           0.45395347]]),\n",
       "   13: array([[0.        , 1.        , 1.        , ..., 1.        , 0.29511124,\n",
       "           0.5086501 ],\n",
       "          [0.        , 1.        , 1.        , ..., 0.55104983, 0.06067762,\n",
       "           0.04753221],\n",
       "          [0.        , 1.        , 1.        , ..., 0.64485676, 0.        ,\n",
       "           0.        ],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.33290023, 0.        ,\n",
       "           0.29075311],\n",
       "          [0.        , 1.        , 1.        , ..., 0.68032682, 0.30590933,\n",
       "           0.41113894],\n",
       "          [0.        , 1.        , 1.        , ..., 0.69681606, 0.30597666,\n",
       "           0.14528982]]),\n",
       "   14: array([[0.        , 0.        , 1.        , ..., 0.30887304, 0.28327419,\n",
       "           0.45380307],\n",
       "          [0.        , 0.        , 1.        , ..., 0.48163313, 0.60158945,\n",
       "           0.51891857],\n",
       "          [0.        , 0.        , 1.        , ..., 0.60147244, 0.32613036,\n",
       "           0.5650662 ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.89852968, 0.50884645,\n",
       "           0.90193319],\n",
       "          [0.        , 0.        , 1.        , ..., 0.64813622, 0.        ,\n",
       "           0.54877577],\n",
       "          [0.        , 0.        , 1.        , ..., 0.28958601, 0.        ,\n",
       "           0.14140299]]),\n",
       "   15: array([[0.        , 0.        , 1.        , ..., 0.14297602, 0.72012776,\n",
       "           0.94265437],\n",
       "          [0.        , 0.        , 1.        , ..., 0.65380616, 0.85510831,\n",
       "           0.77670937],\n",
       "          [0.        , 0.        , 1.        , ..., 0.32173949, 0.77728413,\n",
       "           0.4965922 ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.0051712 , 0.53887991,\n",
       "           0.59459589],\n",
       "          [0.        , 0.        , 1.        , ..., 0.        , 0.45695415,\n",
       "           0.52641846],\n",
       "          [0.        , 0.        , 1.        , ..., 0.        , 0.4080102 ,\n",
       "           0.45397963]]),\n",
       "   16: array([[0.        , 0.        , 0.        , ..., 0.22165836, 0.        ,\n",
       "           0.33355355],\n",
       "          [0.        , 0.        , 1.        , ..., 0.        , 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.87946692, 0.        ,\n",
       "           0.08401898],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.96674237, ..., 0.50190869, 0.24411617,\n",
       "           0.06103775],\n",
       "          [0.        , 0.        , 1.        , ..., 0.51199909, 0.31921863,\n",
       "           0.18947858],\n",
       "          [0.        , 0.        , 1.        , ..., 0.47854094, 0.35059528,\n",
       "           0.34139152]]),\n",
       "   17: array([[0.00000000e+00, 0.00000000e+00, 1.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 5.41507837e-01],\n",
       "          [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, ...,\n",
       "           2.02299160e-01, 4.08280107e-01, 4.56631544e-01],\n",
       "          [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, ...,\n",
       "           4.41906878e-01, 4.45731013e-01, 1.00000000e+00],\n",
       "          ...,\n",
       "          [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, ...,\n",
       "           2.79852168e-02, 3.17744093e-01, 9.08759829e-01],\n",
       "          [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, ...,\n",
       "           4.07168924e-01, 0.00000000e+00, 7.54563804e-01],\n",
       "          [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, ...,\n",
       "           5.79988250e-01, 6.66888623e-04, 6.64966684e-01]]),\n",
       "   18: array([[0.        , 1.        , 1.        , ..., 0.42008233, 0.59887893,\n",
       "           0.68303079],\n",
       "          [0.        , 0.4004035 , 1.        , ..., 0.74074648, 0.30039351,\n",
       "           0.68227943],\n",
       "          [0.        , 0.61885077, 1.        , ..., 0.5710053 , 0.13827222,\n",
       "           0.56394044],\n",
       "          ...,\n",
       "          [0.        , 0.42834148, 1.        , ..., 0.42218892, 0.68771411,\n",
       "           0.36820223],\n",
       "          [0.        , 0.7867976 , 1.        , ..., 0.33487831, 0.48202166,\n",
       "           0.02496445],\n",
       "          [0.        , 0.74932898, 1.        , ..., 0.04035724, 0.32921264,\n",
       "           0.16963861]])},\n",
       "  'fitted_feature_scaler': MinMaxScaler(clip=True)},\n",
       " 4: {'profile_windows': array([[0.16909187, 0.78485155, 0.90746012, ..., 0.6715282 , 0.58665501,\n",
       "          0.90648541],\n",
       "         [0.34027278, 0.70374711, 0.55482888, ..., 0.65460886, 0.72689611,\n",
       "          0.66655804],\n",
       "         [0.27557935, 0.61235403, 0.74502112, ..., 0.74725666, 0.70491887,\n",
       "          0.71421803],\n",
       "         ...,\n",
       "         [0.29478257, 0.83602342, 0.2606502 , ..., 0.39887736, 0.67149061,\n",
       "          0.83412479],\n",
       "         [0.23341169, 0.80655611, 0.12198288, ..., 0.62831805, 0.7272267 ,\n",
       "          0.63062953],\n",
       "         [0.20317462, 0.87692436, 0.08863834, ..., 0.77534293, 0.85247076,\n",
       "          0.65707615]]),\n",
       "  'unknown_users_dict': {0: array([[0.        , 1.        , 1.        , ..., 0.89538431, 0.73752129,\n",
       "           0.91256731],\n",
       "          [0.        , 1.        , 1.        , ..., 0.93733596, 0.73508599,\n",
       "           0.72023027],\n",
       "          [0.        , 1.        , 0.87574935, ..., 0.86774516, 0.76174396,\n",
       "           0.75422334],\n",
       "          ...,\n",
       "          [0.        , 1.        , 0.53061543, ..., 0.79555637, 0.39589386,\n",
       "           0.38319783],\n",
       "          [0.        , 1.        , 0.74830591, ..., 0.18877273, 0.37015844,\n",
       "           0.2860966 ],\n",
       "          [0.        , 1.        , 0.55333782, ..., 0.25249444, 0.55123499,\n",
       "           0.58573287]]),\n",
       "   1: array([[1.        , 1.        , 0.21062907, ..., 0.95554764, 0.88729405,\n",
       "           0.95708739],\n",
       "          [1.        , 1.        , 0.        , ..., 0.75353188, 0.87538694,\n",
       "           0.91328745],\n",
       "          [1.        , 1.        , 0.16505206, ..., 0.77728574, 0.92906128,\n",
       "           0.91939316],\n",
       "          ...,\n",
       "          [0.7790119 , 1.        , 0.        , ..., 0.95330945, 0.789931  ,\n",
       "           0.98696433],\n",
       "          [0.71862888, 1.        , 0.        , ..., 0.95903242, 0.75103537,\n",
       "           0.85589001],\n",
       "          [0.67465575, 1.        , 0.        , ..., 0.83683782, 0.5649606 ,\n",
       "           0.44862149]]),\n",
       "   2: array([[0.33664432, 1.        , 1.        , ..., 0.5416835 , 0.37710347,\n",
       "           0.1734069 ],\n",
       "          [0.        , 1.        , 1.        , ..., 0.75862865, 0.77272057,\n",
       "           0.73812088],\n",
       "          [0.        , 1.        , 1.        , ..., 0.81495452, 0.73253176,\n",
       "           0.56580613],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.72154842, 0.67951473,\n",
       "           0.67368693],\n",
       "          [0.        , 1.        , 1.        , ..., 0.96647739, 0.91973379,\n",
       "           0.89532674],\n",
       "          [0.        , 1.        , 1.        , ..., 1.        , 0.74861886,\n",
       "           0.84012005]]),\n",
       "   3: array([[0.98068148, 1.        , 0.63615959, ..., 0.74097531, 0.76724006,\n",
       "           0.95456349],\n",
       "          [0.91862366, 1.        , 0.46755196, ..., 0.7741113 , 0.85187777,\n",
       "           0.78562093],\n",
       "          [0.76452677, 1.        , 0.78333022, ..., 0.83126063, 0.81096115,\n",
       "           0.81878825],\n",
       "          ...,\n",
       "          [0.87813227, 1.        , 0.25495216, ..., 0.58686364, 0.87838035,\n",
       "           0.89301888],\n",
       "          [0.77503028, 1.        , 0.        , ..., 0.72454701, 0.84835053,\n",
       "           0.8031636 ],\n",
       "          [0.8541174 , 1.        , 0.06317271, ..., 0.78545048, 0.78931498,\n",
       "           0.43182478]]),\n",
       "   4: array([[0.34071208, 0.81928855, 1.        , ..., 0.99317432, 0.85398541,\n",
       "           0.64227792],\n",
       "          [0.49923222, 1.        , 1.        , ..., 0.74322283, 0.58984778,\n",
       "           0.64985041],\n",
       "          [0.38845402, 0.72277678, 1.        , ..., 0.89547144, 0.6550853 ,\n",
       "           0.68451111],\n",
       "          ...,\n",
       "          [0.26748464, 0.61167675, 0.24629713, ..., 0.91348006, 0.70068447,\n",
       "           0.78375209],\n",
       "          [0.22275293, 0.54836505, 0.35906012, ..., 0.85495867, 0.63254106,\n",
       "           0.86000487],\n",
       "          [0.13249515, 0.63729864, 0.80574912, ..., 0.86198324, 0.47855832,\n",
       "           0.81160252]]),\n",
       "   5: array([[0.        , 0.7438962 , 1.        , ..., 0.57572965, 0.63637578,\n",
       "           0.39514091],\n",
       "          [0.        , 0.62872041, 1.        , ..., 0.58431052, 0.71715552,\n",
       "           0.54319564],\n",
       "          [0.        , 0.60908833, 1.        , ..., 0.60575072, 0.69317224,\n",
       "           0.48027185],\n",
       "          ...,\n",
       "          [0.        , 0.74780513, 1.        , ..., 0.27146107, 0.72948561,\n",
       "           0.89101979],\n",
       "          [0.        , 0.8558038 , 1.        , ..., 0.44777728, 0.94328   ,\n",
       "           0.80573416],\n",
       "          [0.        , 0.73626962, 1.        , ..., 0.55439576, 0.77728124,\n",
       "           0.72903613]]),\n",
       "   6: array([[0.        , 0.        , 1.        , ..., 0.15591435, 0.38364641,\n",
       "           0.11387902],\n",
       "          [0.        , 0.        , 1.        , ..., 0.739853  , 0.33153742,\n",
       "           0.95706422],\n",
       "          [0.        , 0.        , 1.        , ..., 0.46635163, 0.48525055,\n",
       "           0.84677636],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.8389254 , 0.65304948,\n",
       "           0.6102834 ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.56887855, 0.6010519 ,\n",
       "           0.63457953],\n",
       "          [0.        , 0.        , 1.        , ..., 0.57539389, 0.53446666,\n",
       "           0.64594876]]),\n",
       "   7: array([[0.        , 1.        , 1.        , ..., 0.79382488, 0.66368326,\n",
       "           0.72745801],\n",
       "          [0.        , 1.        , 1.        , ..., 0.89385617, 0.76085251,\n",
       "           0.79407378],\n",
       "          [0.        , 1.        , 1.        , ..., 0.71509731, 0.73959428,\n",
       "           0.90317868],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.62504778, 0.61433471,\n",
       "           0.86388872],\n",
       "          [0.        , 1.        , 1.        , ..., 0.83214389, 0.66390579,\n",
       "           0.93299624],\n",
       "          [0.        , 1.        , 1.        , ..., 0.79364287, 0.56506501,\n",
       "           0.78828675]]),\n",
       "   8: array([[0.06737281, 0.56241277, 1.        , ..., 0.78959132, 0.69495269,\n",
       "           0.67991782],\n",
       "          [0.        , 0.70072223, 1.        , ..., 0.86390132, 0.53151492,\n",
       "           0.49391912],\n",
       "          [0.0716429 , 0.63956755, 1.        , ..., 0.96444834, 0.62964282,\n",
       "           0.68393479],\n",
       "          ...,\n",
       "          [0.01344151, 0.5396725 , 1.        , ..., 0.71917699, 0.65792003,\n",
       "           0.70714543],\n",
       "          [0.22280563, 0.61870409, 0.8172664 , ..., 0.72652953, 0.64531878,\n",
       "           0.76877935],\n",
       "          [0.23471406, 0.84293409, 0.76139594, ..., 0.53984966, 0.52121509,\n",
       "           0.58407306]]),\n",
       "   9: array([[0.37431584, 1.        , 1.        , ..., 0.67647129, 0.78717493,\n",
       "           0.62861265],\n",
       "          [0.        , 1.        , 1.        , ..., 0.68934592, 0.73702465,\n",
       "           0.61781437],\n",
       "          [0.        , 1.        , 1.        , ..., 0.88555015, 0.73058493,\n",
       "           0.68074952],\n",
       "          ...,\n",
       "          [0.        , 1.        , 0.50229834, ..., 0.14369544, 0.52329477,\n",
       "           0.24694088],\n",
       "          [0.        , 1.        , 0.50348309, ..., 0.40912289, 0.73587681,\n",
       "           0.55941229],\n",
       "          [0.        , 1.        , 0.46699042, ..., 0.48205733, 0.68686443,\n",
       "           0.56348303]]),\n",
       "   10: array([[0.        , 1.        , 1.        , ..., 0.37482019, 0.58645879,\n",
       "           0.48846032],\n",
       "          [0.        , 1.        , 1.        , ..., 0.5595997 , 0.4969748 ,\n",
       "           0.42717738],\n",
       "          [0.        , 1.        , 1.        , ..., 1.        , 0.67886668,\n",
       "           0.78211063],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.63563536, 0.70814385,\n",
       "           0.52059942],\n",
       "          [0.        , 1.        , 1.        , ..., 0.43453234, 0.53195092,\n",
       "           0.29263348],\n",
       "          [0.        , 1.        , 1.        , ..., 0.49746637, 0.64051994,\n",
       "           0.45208695]]),\n",
       "   11: array([[0.        , 1.        , 1.        , ..., 0.63457265, 0.54499267,\n",
       "           0.63609886],\n",
       "          [0.        , 1.        , 1.        , ..., 0.67427589, 0.57439791,\n",
       "           0.67388841],\n",
       "          [0.        , 1.        , 1.        , ..., 0.73119211, 0.54189862,\n",
       "           0.63859889],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.72425033, 0.85275978,\n",
       "           0.80586211],\n",
       "          [0.        , 1.        , 1.        , ..., 0.75739309, 0.59498301,\n",
       "           0.65330659],\n",
       "          [0.        , 1.        , 1.        , ..., 0.92206048, 0.62210798,\n",
       "           0.67968638]]),\n",
       "   12: array([[0.        , 1.        , 1.        , ..., 0.48123392, 0.69108914,\n",
       "           0.1092495 ],\n",
       "          [0.        , 1.        , 1.        , ..., 0.27874399, 0.37527182,\n",
       "           0.30815   ],\n",
       "          [0.        , 1.        , 1.        , ..., 0.34253748, 0.26937368,\n",
       "           0.36034249],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.53854459, 0.09394201,\n",
       "           0.12116685],\n",
       "          [0.        , 1.        , 1.        , ..., 0.46534303, 0.        ,\n",
       "           0.05161533],\n",
       "          [0.        , 1.        , 1.        , ..., 0.43619065, 0.25257719,\n",
       "           0.56155983]]),\n",
       "   13: array([[0.        , 1.        , 1.        , ..., 1.        , 0.61069098,\n",
       "           0.60039602],\n",
       "          [0.        , 1.        , 1.        , ..., 0.75949978, 0.50127351,\n",
       "           0.27298893],\n",
       "          [0.        , 1.        , 1.        , ..., 0.82384276, 0.27626491,\n",
       "           0.12061622],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.60986909, 0.42513712,\n",
       "           0.44568283],\n",
       "          [0.        , 1.        , 1.        , ..., 0.84817198, 0.61573078,\n",
       "           0.53116026],\n",
       "          [0.        , 1.        , 1.        , ..., 0.85948209, 0.6157622 ,\n",
       "           0.34239967]]),\n",
       "   14: array([[0.        , 1.        , 1.        , ..., 0.59338863, 0.60516626,\n",
       "           0.56145304],\n",
       "          [0.        , 1.        , 1.        , ..., 0.71188626, 0.75373389,\n",
       "           0.60768693],\n",
       "          [0.        , 1.        , 1.        , ..., 0.79408508, 0.62516857,\n",
       "           0.64045308],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.9978392 , 0.71044785,\n",
       "           0.87963843],\n",
       "          [0.        , 0.75755736, 1.        , ..., 0.82609217, 0.45888873,\n",
       "           0.62888641],\n",
       "          [0.        , 0.95922732, 1.        , ..., 0.58015949, 0.3261225 ,\n",
       "           0.33963991]]),\n",
       "   15: array([[0.        , 1.        , 1.        , ..., 0.47959843, 0.80905941,\n",
       "           0.90855165],\n",
       "          [0.        , 1.        , 1.        , ..., 0.82998123, 0.87205903,\n",
       "           0.79072588],\n",
       "          [0.02621261, 1.        , 1.        , ..., 0.60221384, 0.83573607,\n",
       "           0.59183456],\n",
       "          ...,\n",
       "          [0.0281988 , 1.        , 1.        , ..., 0.38507692, 0.7244654 ,\n",
       "           0.66142002],\n",
       "          [0.00871065, 1.        , 1.        , ..., 0.        , 0.6862281 ,\n",
       "           0.61301207],\n",
       "          [0.        , 1.        , 1.        , ..., 0.03228161, 0.66338443,\n",
       "           0.5615784 ]]),\n",
       "   16: array([[0.        , 0.        , 0.        , ..., 0.53356733, 0.1856419 ,\n",
       "           0.47607238],\n",
       "          [0.        , 0.        , 1.        , ..., 0.06421519, 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 1.        , 1.        , ..., 0.98476389, 0.07632965,\n",
       "           0.29889559],\n",
       "          ...,\n",
       "          [0.        , 0.796897  , 0.91853683, ..., 0.72579344, 0.58688999,\n",
       "           0.28257826],\n",
       "          [0.        , 0.7364305 , 1.        , ..., 0.73271453, 0.62194265,\n",
       "           0.37377498],\n",
       "          [0.        , 0.86773837, 1.        , ..., 0.7097653 , 0.63658711,\n",
       "           0.48163757]]),\n",
       "   17: array([[0.        , 0.        , 1.        , ..., 0.17594859, 0.38781368,\n",
       "           0.62372596],\n",
       "          [0.        , 0.        , 1.        , ..., 0.52028869, 0.66351041,\n",
       "           0.56346133],\n",
       "          [0.        , 0.        , 1.        , ..., 0.68463769, 0.68098991,\n",
       "           0.94935788],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.40072525, 0.62125443,\n",
       "           0.88448554],\n",
       "          [0.        , 0.        , 1.        , ..., 0.66081062, 0.44490874,\n",
       "           0.77500188],\n",
       "          [0.        , 0.        , 1.        , ..., 0.77934889, 0.47326464,\n",
       "           0.71138533]]),\n",
       "   18: array([[0.        , 1.        , 1.        , ..., 0.66966804, 0.75246881,\n",
       "           0.72421137],\n",
       "          [0.        , 1.        , 1.        , ..., 0.88961434, 0.61315638,\n",
       "           0.72367788],\n",
       "          [0.        , 1.        , 1.        , ..., 0.7731874 , 0.53748933,\n",
       "           0.63965377],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.67111297, 0.79393095,\n",
       "           0.50067395],\n",
       "          [0.        , 1.        , 1.        , ..., 0.61122587, 0.69792789,\n",
       "           0.25696517],\n",
       "          [0.        , 1.        , 1.        , ..., 0.40921133, 0.62660716,\n",
       "           0.35968802]])},\n",
       "  'fitted_feature_scaler': MinMaxScaler(clip=True)},\n",
       " 5: {'profile_windows': array([[0.74206328, 0.03245703, 0.52539963, ..., 0.75614737, 0.91078049,\n",
       "          0.93704856],\n",
       "         [0.47767967, 0.50904534, 0.58484384, ..., 0.86337912, 0.87379844,\n",
       "          0.86604924],\n",
       "         [0.        , 0.04581641, 0.31224991, ..., 0.88775172, 0.89301841,\n",
       "          0.72583657],\n",
       "         ...,\n",
       "         [0.52185471, 0.44370047, 0.33912256, ..., 0.84463081, 0.79256005,\n",
       "          0.78636351],\n",
       "         [0.50391393, 0.        , 0.        , ..., 0.96185242, 0.71634581,\n",
       "          0.78672664],\n",
       "         [0.6607634 , 0.25985334, 0.30676886, ..., 0.84235318, 0.8158167 ,\n",
       "          0.65651582]]),\n",
       "  'unknown_users_dict': {0: array([[0.        , 1.        , 0.36795866, ..., 1.        , 0.81204505,\n",
       "           0.9453242 ],\n",
       "          [0.        , 1.        , 0.30559361, ..., 1.        , 0.80934167,\n",
       "           0.73042467],\n",
       "          [0.        , 1.        , 0.09999708, ..., 1.        , 0.83893423,\n",
       "           0.76840537],\n",
       "          ...,\n",
       "          [0.        , 1.        , 0.        , ..., 0.96013206, 0.43281019,\n",
       "           0.35385595],\n",
       "          [0.        , 1.        , 0.        , ..., 0.41559878, 0.40424174,\n",
       "           0.24536405],\n",
       "          [0.        , 1.        , 0.        , ..., 0.47278324, 0.60525177,\n",
       "           0.58014978]]),\n",
       "   1: array([[1.        , 1.        , 0.        , ..., 1.        , 0.97830527,\n",
       "           0.9950668 ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.92241889, 0.96508739,\n",
       "           0.94612882],\n",
       "          [1.        , 1.        , 0.        , ..., 0.94373583, 1.        ,\n",
       "           0.95295077],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 1.        , 0.87022419,\n",
       "           1.        ],\n",
       "          [1.        , 1.        , 0.        , ..., 1.        , 0.82704681,\n",
       "           0.88199825],\n",
       "          [1.        , 1.        , 0.        , ..., 0.99717842, 0.62048833,\n",
       "           0.42695426]]),\n",
       "   2: array([[1.        , 1.        , 1.        , ..., 0.73230419, 0.4119513 ,\n",
       "           0.11945506],\n",
       "          [0.30331058, 1.        , 1.        , ..., 0.92699277, 0.85111918,\n",
       "           0.75041398],\n",
       "          [0.09625374, 1.        , 0.94788864, ..., 0.97754013, 0.80650626,\n",
       "           0.55788549],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.89371664, 0.74765295,\n",
       "           0.67842147],\n",
       "          [0.        , 1.        , 1.        , ..., 1.        , 1.        ,\n",
       "           0.92606119],\n",
       "          [0.01934729, 1.        , 1.        , ..., 1.        , 0.82436428,\n",
       "           0.86437837]]),\n",
       "   3: array([[1.        , 1.        , 0.        , ..., 0.91115051, 0.84503536,\n",
       "           0.99224684],\n",
       "          [1.        , 1.        , 0.        , ..., 0.94088705, 0.93899025,\n",
       "           0.80348612],\n",
       "          [1.        , 1.        , 0.00462192, ..., 0.99217339, 0.8935694 ,\n",
       "           0.84054419],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.77284927, 0.96841033,\n",
       "           0.9234826 ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.8964076 , 0.93507473,\n",
       "           0.82308667],\n",
       "          [1.        , 1.        , 0.        , ..., 0.95106294, 0.86954036,\n",
       "           0.40818718]]),\n",
       "   4: array([[1.        , 0.42904313, 0.67287856, ..., 1.        , 0.94132991,\n",
       "           0.64332796],\n",
       "          [1.        , 0.8449281 , 0.33030772, ..., 0.91316746, 0.64811518,\n",
       "           0.65178876],\n",
       "          [1.        , 0.27659729, 0.45591989, ..., 1.        , 0.72053424,\n",
       "           0.6905154 ],\n",
       "          ...,\n",
       "          [1.        , 0.10110845, 0.        , ..., 1.        , 0.77115312,\n",
       "           0.80139805],\n",
       "          [1.        , 0.001104  , 0.        , ..., 1.        , 0.69550827,\n",
       "           0.88659582],\n",
       "          [0.91401311, 0.14157969, 0.02775789, ..., 1.        , 0.52457462,\n",
       "           0.83251553]]),\n",
       "   5: array([[0.28227126, 0.3099566 , 0.82748989, ..., 0.76285752, 0.69976512,\n",
       "           0.36720003],\n",
       "          [0.61542602, 0.12802988, 1.        , ..., 0.77055807, 0.78943736,\n",
       "           0.53262263],\n",
       "          [0.58753095, 0.09701988, 1.        , ..., 0.7897987 , 0.76281392,\n",
       "           0.46231744],\n",
       "          ...,\n",
       "          [0.5105141 , 0.31613098, 0.4932182 , ..., 0.48980407, 0.80312478,\n",
       "           0.921249  ],\n",
       "          [0.56874955, 0.48672104, 0.549552  , ..., 0.64803187, 1.        ,\n",
       "           0.82595877],\n",
       "          [0.15363744, 0.29791   , 0.52479067, ..., 0.74371228, 0.8561819 ,\n",
       "           0.74026352]]),\n",
       "   6: array([[0.        , 0.        , 1.        , ..., 0.38611137, 0.41921451,\n",
       "           0.05294414],\n",
       "          [0.        , 0.        , 1.        , ..., 0.91014333, 0.36136919,\n",
       "           0.99504092],\n",
       "          [0.        , 0.        , 1.        , ..., 0.66470067, 0.53200355,\n",
       "           0.87181551],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.99905183, 0.71827432,\n",
       "           0.60758025],\n",
       "          [0.        , 0.        , 1.        , ..., 0.75670928, 0.66055268,\n",
       "           0.63472649],\n",
       "          [0.        , 0.        , 1.        , ..., 0.76255621, 0.58663753,\n",
       "           0.64742942]]),\n",
       "   7: array([[0.        , 1.        , 1.        , ..., 0.9585782 , 0.7300787 ,\n",
       "           0.73850028],\n",
       "          [0.        , 1.        , 1.        , ..., 1.        , 0.83794465,\n",
       "           0.81293055],\n",
       "          [0.        , 1.        , 1.        , ..., 0.88792735, 0.81434624,\n",
       "           0.93483422],\n",
       "          ...,\n",
       "          [0.        , 1.        , 0.96013375, ..., 0.80711607, 0.67529771,\n",
       "           0.89093527],\n",
       "          [0.        , 1.        , 0.91608944, ..., 0.99296604, 0.73032573,\n",
       "           0.9681496 ],\n",
       "          [0.        , 1.        , 0.68087329, ..., 0.95841486, 0.62060424,\n",
       "           0.80646465]]),\n",
       "   8: array([[0.82650144, 0.02329319, 0.56073154, ..., 0.95477897, 0.76479036,\n",
       "           0.68538328],\n",
       "          [0.72153118, 0.24176087, 0.39831217, ..., 1.        , 0.58336085,\n",
       "           0.47756562],\n",
       "          [0.8322396 , 0.14516356, 0.45959736, ..., 1.        , 0.69229098,\n",
       "           0.68987148],\n",
       "          ...,\n",
       "          [0.75402835, 0.        , 0.24974878, ..., 0.89158849, 0.72368103,\n",
       "           0.71580489],\n",
       "          [1.        , 0.11220853, 0.03964355, ..., 0.89818673, 0.7096926 ,\n",
       "           0.7846689 ],\n",
       "          [1.        , 0.4663926 , 0.        , ..., 0.73065848, 0.57192719,\n",
       "           0.57829526]]),\n",
       "   9: array([[1.        , 1.        , 0.35038599, ..., 0.853264  , 0.86716473,\n",
       "           0.62805966],\n",
       "          [0.46819591, 1.        , 0.56544082, ..., 0.86481781, 0.81149374,\n",
       "           0.61599466],\n",
       "          [0.27251232, 1.        , 0.62055354, ..., 1.        , 0.80434511,\n",
       "           0.68631255],\n",
       "          ...,\n",
       "          [0.37990495, 1.        , 0.        , ..., 0.37514601, 0.5742358 ,\n",
       "           0.2016151 ],\n",
       "          [0.33288134, 1.        , 0.        , ..., 0.61334306, 0.81021954,\n",
       "           0.55074161],\n",
       "          [0.03173737, 1.        , 0.        , ..., 0.67879511, 0.75581173,\n",
       "           0.55528987]]),\n",
       "   10: array([[0.        , 1.        , 0.28796958, ..., 0.58255951, 0.64435311,\n",
       "           0.47146647],\n",
       "          [0.        , 1.        , 1.        , ..., 0.74838235, 0.54501844,\n",
       "           0.40299461],\n",
       "          [0.        , 1.        , 1.        , ..., 1.        , 0.74693356,\n",
       "           0.79956404],\n",
       "          ...,\n",
       "          [0.        , 1.        , 0.61877409, ..., 0.81661746, 0.77943365,\n",
       "           0.50737572],\n",
       "          [0.        , 1.        , 0.54502004, ..., 0.63614574, 0.58384484,\n",
       "           0.25266776],\n",
       "          [0.        , 1.        , 0.54131383, ..., 0.69262332, 0.70436549,\n",
       "           0.43082625]]),\n",
       "   11: array([[0.        , 1.        , 1.        , ..., 0.81566377, 0.59832228,\n",
       "           0.63642406],\n",
       "          [0.        , 1.        , 1.        , ..., 0.85129383, 0.63096453,\n",
       "           0.67864659],\n",
       "          [0.        , 1.        , 1.        , ..., 0.90237098, 0.59488762,\n",
       "           0.63921736],\n",
       "          ...,\n",
       "          [0.        , 1.        , 0.38314462, ..., 0.89614136, 0.93996936,\n",
       "           0.82610174],\n",
       "          [0.        , 1.        , 0.4510546 , ..., 0.92588397, 0.65381571,\n",
       "           0.65565037],\n",
       "          [0.        , 1.        , 0.6477057 , ..., 1.        , 0.68392668,\n",
       "           0.68512469]]),\n",
       "   12: array([[0.        , 1.        , 0.95913245, ..., 0.67805617, 0.76050151,\n",
       "           0.04777154],\n",
       "          [0.        , 1.        , 0.44644659, ..., 0.49633983, 0.40991802,\n",
       "           0.27000448],\n",
       "          [0.        , 1.        , 0.43738447, ..., 0.5535887 , 0.29236227,\n",
       "           0.32831951],\n",
       "          ...,\n",
       "          [0.        , 1.        , 0.72552597, ..., 0.7294873 , 0.09761854,\n",
       "           0.06108689],\n",
       "          [0.        , 1.        , 0.655739  , ..., 0.66379554, 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 1.        , 0.53906124, ..., 0.63763393, 0.27371678,\n",
       "           0.55314106]]),\n",
       "   13: array([[0.        , 1.        , 0.99258315, ..., 1.        , 0.67125286,\n",
       "           0.59653301],\n",
       "          [0.        , 1.        , 1.        , ..., 0.92777454, 0.54979037,\n",
       "           0.23071877],\n",
       "          [0.        , 1.        , 1.        , ..., 0.98551652, 0.30001211,\n",
       "           0.06047166],\n",
       "          ...,\n",
       "          [0.        , 1.        , 0.51873228, ..., 0.79349457, 0.46527265,\n",
       "           0.42367088],\n",
       "          [0.        , 1.        , 0.99919037, ..., 1.        , 0.67684746,\n",
       "           0.51917542],\n",
       "          [0.        , 1.        , 0.92317308, ..., 1.        , 0.67688234,\n",
       "           0.30827188]]),\n",
       "   14: array([[0.        , 0.98930906, 0.35600452, ..., 0.77870485, 0.66511996,\n",
       "           0.55302175],\n",
       "          [0.        , 0.72542588, 0.66267573, ..., 0.88504572, 0.83004239,\n",
       "           0.6046792 ],\n",
       "          [0.        , 0.92216688, 1.        , ..., 0.95881171, 0.68732419,\n",
       "           0.64128906],\n",
       "          ...,\n",
       "          [0.        , 0.72069332, 0.49512605, ..., 1.        , 0.78199129,\n",
       "           0.90853253],\n",
       "          [0.        , 0.33153519, 0.66265904, ..., 0.98753517, 0.50273974,\n",
       "           0.62836553],\n",
       "          [0.        , 0.65008439, 0.58383178, ..., 0.76683289, 0.35535819,\n",
       "           0.30518838]]),\n",
       "   15: array([[0.14091372, 1.        , 1.        , ..., 0.67658847, 0.89145832,\n",
       "           0.94083747],\n",
       "          [0.19777363, 1.        , 1.        , ..., 0.99102524, 0.96139314,\n",
       "           0.80918991],\n",
       "          [0.7711902 , 0.99583431, 0.84659799, ..., 0.78662467, 0.92107162,\n",
       "           0.58696723],\n",
       "          ...,\n",
       "          [0.77385924, 1.        , 0.30815664, ..., 0.59176399, 0.79755193,\n",
       "           0.66471557],\n",
       "          [0.747671  , 0.78959582, 0.85524028, ..., 0.23581126, 0.75510535,\n",
       "           0.61062902],\n",
       "          [0.65006344, 0.79112802, 0.96977516, ..., 0.27516221, 0.72974698,\n",
       "           0.55316182]]),\n",
       "   16: array([[0.        , 0.        , 0.        , ..., 0.72502066, 0.19941303,\n",
       "           0.45762534],\n",
       "          [0.        , 0.        , 0.79491246, ..., 0.30381969, 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 1.        , 1.        , ..., 1.        , 0.07806735,\n",
       "           0.25966446],\n",
       "          ...,\n",
       "          [0.        , 0.3936744 , 0.14415311, ..., 0.89752616, 0.64483179,\n",
       "           0.24143299],\n",
       "          [0.        , 0.29816411, 0.25267217, ..., 0.90373721, 0.68374315,\n",
       "           0.34332772],\n",
       "          [0.        , 0.50557238, 0.27102716, ..., 0.88314236, 0.69999972,\n",
       "           0.46384336]]),\n",
       "   17: array([[0.        , 0.        , 1.        , ..., 0.40409029, 0.42384052,\n",
       "           0.62259973],\n",
       "          [0.        , 0.        , 1.        , ..., 0.71310428, 0.72988682,\n",
       "           0.55526563],\n",
       "          [0.        , 0.        , 1.        , ..., 0.8605926 , 0.74929052,\n",
       "           0.98643056],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.60580695, 0.68297917,\n",
       "           0.91394824],\n",
       "          [0.        , 0.        , 1.        , ..., 0.83920997, 0.48722079,\n",
       "           0.79162138],\n",
       "          [0.        , 0.        , 1.        , ..., 0.94558731, 0.51869819,\n",
       "           0.72054216]]),\n",
       "   18: array([[0.        , 1.        , 0.74561268, ..., 0.8471587 , 0.82863805,\n",
       "           0.73487278],\n",
       "          [0.        , 1.        , 0.56928565, ..., 1.        , 0.67398966,\n",
       "           0.73427671],\n",
       "          [0.        , 1.        , 0.43149543, ..., 0.94005794, 0.58999294,\n",
       "           0.64039597],\n",
       "          ...,\n",
       "          [0.        , 1.        , 0.32262306, ..., 0.84845539, 0.87466447,\n",
       "           0.48511285],\n",
       "          [0.        , 1.        , 0.24995179, ..., 0.79471215, 0.76809308,\n",
       "           0.2128153 ],\n",
       "          [0.        , 1.        , 0.43129155, ..., 0.61342243, 0.68892115,\n",
       "           0.32758827]])},\n",
       "  'fitted_feature_scaler': MinMaxScaler(clip=True)},\n",
       " 6: {'profile_windows': array([[0.67583131, 0.4571011 , 0.        , ..., 0.46801503, 0.95252999,\n",
       "          0.7876091 ],\n",
       "         [0.95934374, 0.34537278, 0.18741339, ..., 0.68335923, 1.        ,\n",
       "          0.70934989],\n",
       "         [0.96767495, 0.27793162, 0.37315291, ..., 0.43262556, 0.94752209,\n",
       "          0.65672686],\n",
       "         ...,\n",
       "         [0.85390343, 0.32463058, 0.3294878 , ..., 0.06904596, 0.65677768,\n",
       "          0.42752903],\n",
       "         [0.73469092, 0.38008314, 0.33652584, ..., 0.43188242, 0.76750946,\n",
       "          0.39904347],\n",
       "         [0.65798157, 0.44158231, 0.33975011, ..., 0.42652305, 0.65545683,\n",
       "          0.46392192]]),\n",
       "  'unknown_users_dict': {0: array([[1.        , 1.        , 0.        , ..., 0.8860658 , 0.84763962,\n",
       "           0.91700391],\n",
       "          [1.        , 1.        , 0.        , ..., 0.93339752, 0.84557214,\n",
       "           0.77858073],\n",
       "          [1.        , 1.        , 0.        , ..., 0.85488208, 0.86820381,\n",
       "           0.80304523],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.77343546, 0.55761   ,\n",
       "           0.53602161],\n",
       "          [1.        , 1.        , 0.        , ..., 0.08883506, 0.53576154,\n",
       "           0.46613875],\n",
       "          [1.        , 1.        , 0.        , ..., 0.16072875, 0.68948913,\n",
       "           0.6817842 ]]),\n",
       "   1: array([[1.        , 1.        , 0.        , ..., 0.95394475, 0.97479141,\n",
       "           0.9490446 ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.72602155, 0.96468269,\n",
       "           0.9175222 ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.75282172, 1.        ,\n",
       "           0.92191642],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.95141953, 0.89213361,\n",
       "           0.97054676],\n",
       "          [1.        , 1.        , 0.        , ..., 0.95787643, 0.8591126 ,\n",
       "           0.87621379],\n",
       "          [1.        , 1.        , 0.        , ..., 0.82001104, 0.70114169,\n",
       "           0.58310641]]),\n",
       "   2: array([[1.        , 1.        , 0.        , ..., 0.48700476, 0.54165763,\n",
       "           0.38503702],\n",
       "          [1.        , 1.        , 0.        , ..., 0.73177195, 0.87752257,\n",
       "           0.79145644],\n",
       "          [1.        , 1.        , 0.        , ..., 0.79532131, 0.84340368,\n",
       "           0.66744311],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.68993639, 0.7983941 ,\n",
       "           0.74508392],\n",
       "          [1.        , 1.        , 0.        , ..., 0.96627619, 1.        ,\n",
       "           0.90459604],\n",
       "          [1.        , 1.        , 0.        , ..., 1.        , 0.85706107,\n",
       "           0.8648643 ]]),\n",
       "   3: array([[1.        , 1.        , 0.        , ..., 0.71185467, 0.87286981,\n",
       "           0.94722818],\n",
       "          [1.        , 1.        , 0.        , ..., 0.74924017, 0.94472423,\n",
       "           0.82564179],\n",
       "          [1.        , 1.        , 0.        , ..., 0.81371859, 0.90998747,\n",
       "           0.84951199],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.53797901, 0.96722399,\n",
       "           0.90293509],\n",
       "          [1.        , 1.        , 0.        , ..., 0.69331953, 0.94172973,\n",
       "           0.83826708],\n",
       "          [1.        , 1.        , 0.        , ..., 0.76203355, 0.89161064,\n",
       "           0.57101797]]),\n",
       "   4: array([[1.        , 1.        , 0.        , ..., 0.99639685, 0.94651355,\n",
       "           0.72247915],\n",
       "          [1.        , 1.        , 0.        , ..., 0.71439043, 0.72227003,\n",
       "           0.727929  ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.8861641 , 0.77765437,\n",
       "           0.75287398],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.90648223, 0.81636647,\n",
       "           0.8242968 ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.84045579, 0.75851511,\n",
       "           0.87917522],\n",
       "          [1.        , 1.        , 0.        , ..., 0.84838122, 0.62778921,\n",
       "           0.84434049]]),\n",
       "   5: array([[1.        , 1.        , 0.        , ..., 0.52541714, 0.76177065,\n",
       "           0.54461693],\n",
       "          [1.        , 1.        , 0.        , ..., 0.53509847, 0.83034981,\n",
       "           0.65117055],\n",
       "          [1.        , 1.        , 0.        , ..., 0.55928825, 0.80998885,\n",
       "           0.60588488],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.18212775, 0.84081761,\n",
       "           0.90149636],\n",
       "          [1.        , 1.        , 0.        , ..., 0.38105556, 1.        ,\n",
       "           0.84011709],\n",
       "          [1.        , 1.        , 0.        , ..., 0.50134729, 0.88139442,\n",
       "           0.78491822]]),\n",
       "   6: array([[1.        , 0.39037364, 0.42814637, ..., 0.05176278, 0.54721235,\n",
       "           0.34219536],\n",
       "          [1.        , 0.30225232, 0.55608726, ..., 0.71058843, 0.50297366,\n",
       "           0.94902793],\n",
       "          [1.        , 0.32291802, 0.48981065, ..., 0.40201197, 0.63347068,\n",
       "           0.86965478],\n",
       "          ...,\n",
       "          [0.94206646, 0.37183576, 0.55738339, ..., 0.82236634, 0.77592604,\n",
       "           0.69945299],\n",
       "          [0.87398561, 0.38905122, 0.52667047, ..., 0.51768742, 0.73178193,\n",
       "           0.71693869],\n",
       "          [0.90494566, 0.34491724, 0.52616515, ..., 0.52503832, 0.67525342,\n",
       "           0.72512102]]),\n",
       "   7: array([[1.        , 1.        , 0.        , ..., 0.77148191, 0.78495375,\n",
       "           0.78378247],\n",
       "          [1.        , 1.        , 0.        , ..., 0.88434168, 0.867447  ,\n",
       "           0.83172522],\n",
       "          [1.        , 1.        , 0.        , ..., 0.68265796, 0.84939951,\n",
       "           0.910247  ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.58106006, 0.74305857,\n",
       "           0.88197038],\n",
       "          [1.        , 1.        , 0.        , ..., 0.81471513, 0.78514266,\n",
       "           0.93170643],\n",
       "          [1.        , 1.        , 0.        , ..., 0.77127656, 0.70123033,\n",
       "           0.82756035]]),\n",
       "   8: array([[1.        , 1.        , 0.        , ..., 0.76670542, 0.81150038,\n",
       "           0.74956823],\n",
       "          [1.        , 1.        , 0.        , ..., 0.85054528, 0.67274749,\n",
       "           0.61570669],\n",
       "          [1.        , 1.        , 0.        , ..., 0.96398693, 0.75605462,\n",
       "           0.75245921],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.68726084, 0.78006096,\n",
       "           0.7691637 ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.6955563 , 0.76936295,\n",
       "           0.81352105],\n",
       "          [1.        , 1.        , 0.        , ..., 0.48493573, 0.6640033 ,\n",
       "           0.68058965]]),\n",
       "   9: array([[1.        , 1.        , 0.        , ..., 0.63907835, 0.88979381,\n",
       "           0.71264438],\n",
       "          [1.        , 1.        , 0.        , ..., 0.65360409, 0.84721799,\n",
       "           0.70487296],\n",
       "          [1.        , 1.        , 0.        , ..., 0.87497046, 0.84175089,\n",
       "           0.7501668 ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.03797686, 0.66576887,\n",
       "           0.43795875],\n",
       "          [1.        , 1.        , 0.        , ..., 0.33744395, 0.84624352,\n",
       "           0.66284152],\n",
       "          [1.        , 1.        , 0.        , ..., 0.41973185, 0.80463374,\n",
       "           0.6657712 ]]),\n",
       "   10: array([[1.        , 1.        , 0.        , ..., 0.29874212, 0.71939289,\n",
       "           0.61177804],\n",
       "          [1.        , 1.        , 0.        , ..., 0.50721861, 0.64342415,\n",
       "           0.56767328],\n",
       "          [1.        , 1.        , 0.        , ..., 1.        , 0.79784393,\n",
       "           0.82311545],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.59300544, 0.82269921,\n",
       "           0.63490826],\n",
       "          [1.        , 1.        , 0.        , ..., 0.36611203, 0.67311764,\n",
       "           0.47084328],\n",
       "          [1.        , 1.        , 0.        , ..., 0.43711701, 0.76528891,\n",
       "           0.58560047]]),\n",
       "   11: array([[1.        , 1.        , 0.        , ..., 0.59180644, 0.68418963,\n",
       "           0.71803214],\n",
       "          [1.        , 1.        , 0.        , ..., 0.63660141, 0.70915363,\n",
       "           0.74522893],\n",
       "          [1.        , 1.        , 0.        , ..., 0.70081683, 0.68156288,\n",
       "           0.71983139],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.69298481, 0.94547303,\n",
       "           0.84020918],\n",
       "          [1.        , 1.        , 0.        , ..., 0.73037794, 0.72662966,\n",
       "           0.73041638],\n",
       "          [1.        , 1.        , 0.        , ..., 0.91616305, 0.7496578 ,\n",
       "           0.74940167]]),\n",
       "   12: array([[1.        , 1.        , 0.        , ..., 0.41880284, 0.80822037,\n",
       "           0.33886353],\n",
       "          [1.        , 1.        , 0.        , ..., 0.19034466, 0.54010262,\n",
       "           0.48201038],\n",
       "          [1.        , 1.        , 0.        , ..., 0.26231933, 0.45019884,\n",
       "           0.51957283],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.48346329, 0.30126355,\n",
       "           0.34744034],\n",
       "          [1.        , 1.        , 0.        , ..., 0.40087403, 0.20359837,\n",
       "           0.29738475],\n",
       "          [1.        , 1.        , 0.        , ..., 0.36798301, 0.43593921,\n",
       "           0.66438708]]),\n",
       "   13: array([[1.        , 1.        , 0.        , ..., 1.        , 0.73996517,\n",
       "           0.69233713],\n",
       "          [1.        , 1.        , 0.        , ..., 0.7327548 , 0.6470736 ,\n",
       "           0.45670529],\n",
       "          [1.        , 1.        , 0.        , ..., 0.80534942, 0.45604925,\n",
       "           0.34704406],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.56393478, 0.5824365 ,\n",
       "           0.58099148],\n",
       "          [1.        , 1.        , 0.        , ..., 0.83279874, 0.74424378,\n",
       "           0.6425088 ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.84555931, 0.74427046,\n",
       "           0.50665955]]),\n",
       "   14: array([[1.        , 1.        , 0.        , ..., 0.54534079, 0.73527488,\n",
       "           0.66431023],\n",
       "          [1.        , 1.        , 0.        , ..., 0.67903511, 0.86140355,\n",
       "           0.69758433],\n",
       "          [1.        , 1.        , 0.        , ..., 0.77177548, 0.75225613,\n",
       "           0.72116583],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 1.        , 0.82465523,\n",
       "           0.89330531],\n",
       "          [1.        , 1.        , 0.        , ..., 0.80788731, 0.61109042,\n",
       "           0.7128414 ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.53041508, 0.49837659,\n",
       "           0.50467338]]),\n",
       "   15: array([[1.        , 1.        , 0.        , ..., 0.41695761, 0.90837296,\n",
       "           0.91411388],\n",
       "          [1.        , 1.        , 0.        , ..., 0.81227512, 0.96185741,\n",
       "           0.82931577],\n",
       "          [1.        , 1.        , 0.        , ..., 0.55529778, 0.9310205 ,\n",
       "           0.68617553],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.31031422, 0.83655563,\n",
       "           0.73625554],\n",
       "          [1.        , 1.        , 0.        , ..., 0.        , 0.80409352,\n",
       "           0.70141679],\n",
       "          [1.        , 1.        , 0.        , ..., 0.        , 0.78470005,\n",
       "           0.66440045]]),\n",
       "   16: array([[1.        , 0.89584863, 0.        , ..., 0.47784772, 0.37911351,\n",
       "           0.60286257],\n",
       "          [1.        , 1.        , 0.        , ..., 0.        , 0.        ,\n",
       "           0.21908476],\n",
       "          [1.        , 1.        , 0.        , ..., 0.98690783, 0.28631128,\n",
       "           0.47535007],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.69472581, 0.71975897,\n",
       "           0.46360664],\n",
       "          [1.        , 1.        , 0.        , ..., 0.70253449, 0.74951744,\n",
       "           0.52924007],\n",
       "          [1.        , 1.        , 0.        , ..., 0.67664215, 0.76195007,\n",
       "           0.60686778]]),\n",
       "   17: array([[1.        , 0.10304738, 0.28948857, ..., 0.0743663 , 0.55075021,\n",
       "           0.70912748],\n",
       "          [1.        , 0.32749551, 0.17121403, ..., 0.46286617, 0.784807  ,\n",
       "           0.66575558],\n",
       "          [1.        , 0.40381506, 0.08428421, ..., 0.64829205, 0.79964648,\n",
       "           0.94348175],\n",
       "          ...,\n",
       "          [0.8756239 , 0.39926637, 0.32222554, ..., 0.32796936, 0.74893317,\n",
       "           0.89679373],\n",
       "          [0.8354967 , 0.37854288, 0.30113363, ..., 0.62140929, 0.59922191,\n",
       "           0.81799936],\n",
       "          [0.81760742, 0.38896157, 0.30008828, ..., 0.75514946, 0.62329506,\n",
       "           0.77221511]]),\n",
       "   18: array([[1.        , 1.        , 0.        , ..., 0.63140263, 0.86032954,\n",
       "           0.78144589],\n",
       "          [1.        , 1.        , 0.        , ..., 0.87955586, 0.74205821,\n",
       "           0.78106194],\n",
       "          [1.        , 1.        , 0.        , ..., 0.74819779, 0.67781955,\n",
       "           0.72059057],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.63303285, 0.89552943,\n",
       "           0.62056809],\n",
       "          [1.        , 1.        , 0.        , ..., 0.56546556, 0.81402622,\n",
       "           0.44517313],\n",
       "          [1.        , 1.        , 0.        , ..., 0.33754374, 0.75347745,\n",
       "           0.51910182]])},\n",
       "  'fitted_feature_scaler': MinMaxScaler(clip=True)},\n",
       " 7: {'profile_windows': array([[0.33708868, 0.89575257, 0.75451328, ..., 0.73340218, 0.59597294,\n",
       "          0.67266561],\n",
       "         [0.57917708, 0.43857378, 0.55070783, ..., 0.94730984, 0.8627573 ,\n",
       "          0.41485609],\n",
       "         [1.        , 0.11954921, 0.40275421, ..., 0.63248816, 0.5098333 ,\n",
       "          0.6998673 ],\n",
       "         ...,\n",
       "         [0.46627062, 0.5593339 , 0.29623561, ..., 0.95547949, 0.64810325,\n",
       "          0.38198335],\n",
       "         [0.73787728, 0.47702024, 0.09887646, ..., 0.97671874, 0.39997339,\n",
       "          0.50519831],\n",
       "         [0.68766582, 0.87141827, 0.        , ..., 0.64400085, 0.67529879,\n",
       "          0.29898939]]),\n",
       "  'unknown_users_dict': {0: array([[0.19499731, 0.12646063, 0.38259314, ..., 0.86462278, 0.61292808,\n",
       "           0.74422786],\n",
       "          [0.1968178 , 0.27081408, 0.34559249, ..., 0.92223886, 0.60797459,\n",
       "           0.46934889],\n",
       "          [0.35887332, 0.40694653, 0.22361384, ..., 0.82666338, 0.66219792,\n",
       "           0.51793018],\n",
       "          ...,\n",
       "          [0.        , 0.08641206, 0.01229945, ..., 0.72751984, 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 0.14558433, ..., 0.        , 0.        ,\n",
       "           0.        ],\n",
       "          [0.04529801, 0.0394842 , 0.02621164, ..., 0.        , 0.23401465,\n",
       "           0.2771316 ]]),\n",
       "   1: array([[1.        , 0.17405331, 0.        , ..., 0.94725065, 0.91757158,\n",
       "           0.80785385],\n",
       "          [1.        , 0.21112434, 0.        , ..., 0.66980371, 0.89335207,\n",
       "           0.74525706],\n",
       "          [1.        , 0.05425901, 0.        , ..., 0.70242709, 1.        ,\n",
       "           0.75398305],\n",
       "          ...,\n",
       "          [1.        , 0.31009688, 0.        , ..., 0.94417673, 0.71953143,\n",
       "           0.85055256],\n",
       "          [1.        , 0.26610176, 0.        , ..., 0.95203661, 0.64041624,\n",
       "           0.66322736],\n",
       "          [1.        , 0.02818636, 0.        , ..., 0.78421547, 0.26193306,\n",
       "           0.08117851]]),\n",
       "   2: array([[1.        , 0.        , 1.        , ..., 0.37885274, 0.        ,\n",
       "           0.        ],\n",
       "          [1.        , 0.06017279, 1.        , ..., 0.67680358, 0.68452476,\n",
       "           0.49491731],\n",
       "          [1.        , 0.54445377, 0.72666059, ..., 0.7541611 , 0.60277918,\n",
       "           0.24865324],\n",
       "          ...,\n",
       "          [0.12267734, 0.4216881 , 0.84510872, ..., 0.62587787, 0.49494053,\n",
       "           0.40283136],\n",
       "          [0.73114889, 0.34728099, 0.81192274, ..., 0.96226149, 0.98355524,\n",
       "           0.71958846],\n",
       "          [1.        , 0.48952159, 0.87720307, ..., 1.        , 0.63550096,\n",
       "           0.64068969]]),\n",
       "   3: array([[1.        , 0.        , 0.07692074, ..., 0.65255861, 0.67337719,\n",
       "           0.80424682],\n",
       "          [1.        , 0.        , 0.        , ..., 0.69806733, 0.84553351,\n",
       "           0.56280213],\n",
       "          [1.        , 0.        , 0.16702858, ..., 0.77655579, 0.76230755,\n",
       "           0.61020328],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.        , ..., 0.44090281, 0.89944078,\n",
       "           0.71629018],\n",
       "          [1.        , 0.        , 0.        , ..., 0.62999611, 0.83835898,\n",
       "           0.58787328],\n",
       "          [1.        , 0.        , 0.        , ..., 0.71364048, 0.71827842,\n",
       "           0.05717345]]),\n",
       "   4: array([[1.        , 0.        , 0.56349948, ..., 0.99892685, 0.84982054,\n",
       "           0.3579431 ],\n",
       "          [1.        , 0.        , 0.36025516, ..., 0.65564535, 0.31255454,\n",
       "           0.36876535],\n",
       "          [1.        , 0.        , 0.43477978, ..., 0.86474244, 0.44525013,\n",
       "           0.41830077],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.        , ..., 0.88947535, 0.53800059,\n",
       "           0.56013126],\n",
       "          [1.        , 0.        , 0.        , ..., 0.80910251, 0.39939429,\n",
       "           0.66910813],\n",
       "          [1.        , 0.        , 0.18075496, ..., 0.81875   , 0.08618753,\n",
       "           0.59993378]]),\n",
       "   5: array([[1.        , 0.        , 0.65522905, ..., 0.42561147, 0.40719425,\n",
       "           0.0047466 ],\n",
       "          [1.        , 0.        , 0.88005362, ..., 0.43739638, 0.57150334,\n",
       "           0.2163394 ],\n",
       "          [1.        , 0.        , 0.86464516, ..., 0.46684217, 0.52272043,\n",
       "           0.12641171],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.45690854, ..., 0.00773128, 0.5965832 ,\n",
       "           0.71343317],\n",
       "          [1.        , 0.        , 0.4903309 , ..., 0.24988264, 1.        ,\n",
       "           0.591547  ],\n",
       "          [1.        , 0.        , 0.47564022, ..., 0.39631166, 0.69380133,\n",
       "           0.48193381]]),\n",
       "   6: array([[0.        , 0.        , 1.        , ..., 0.        , 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.65101724, 0.        ,\n",
       "           0.80782075],\n",
       "          [0.        , 0.        , 1.        , ..., 0.27539251, 0.09979979,\n",
       "           0.65020256],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.78708254, 0.4411092 ,\n",
       "           0.31221804],\n",
       "          [0.        , 0.        , 1.        , ..., 0.41620221, 0.33534414,\n",
       "           0.34694092],\n",
       "          [0.        , 0.        , 1.        , ..., 0.42515034, 0.19990724,\n",
       "           0.36318929]]),\n",
       "   7: array([[0.        , 0.37263385, 0.98003949, ..., 0.72514182, 0.46273873,\n",
       "           0.47967844],\n",
       "          [0.51437249, 0.83840322, 0.99775934, ..., 0.86252404, 0.66038467,\n",
       "           0.57488254],\n",
       "          [0.74602999, 0.65688927, 1.        , ..., 0.61701797, 0.61714461,\n",
       "           0.73081008],\n",
       "          ...,\n",
       "          [0.54410758, 0.93208346, 0.73392551, ..., 0.49334462, 0.36236189,\n",
       "           0.67465873],\n",
       "          [0.57287936, 0.85012385, 0.7077944 , ..., 0.77776886, 0.46319135,\n",
       "           0.77342393],\n",
       "          [0.24899349, 0.83981691, 0.56824269, ..., 0.72489185, 0.26214543,\n",
       "           0.56661198]]),\n",
       "   8: array([[1.        , 0.        , 0.49696362, ..., 0.71932748, 0.52634192,\n",
       "           0.41173625],\n",
       "          [1.        , 0.        , 0.40060161, ..., 0.82138427, 0.19390327,\n",
       "           0.14591573],\n",
       "          [1.        , 0.        , 0.43696159, ..., 0.95947481, 0.39349917,\n",
       "           0.41747712],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.31246024, ..., 0.62262097, 0.45101608,\n",
       "           0.45064867],\n",
       "          [1.        , 0.        , 0.18780661, ..., 0.63271889, 0.42538466,\n",
       "           0.53873293],\n",
       "          [1.        , 0.        , 0.15359893, ..., 0.37633416, 0.17295304,\n",
       "           0.27475947]]),\n",
       "   9: array([[1.        , 1.        , 0.37216742, ..., 0.56396928, 0.71392548,\n",
       "           0.33841334],\n",
       "          [1.        , 1.        , 0.4997576 , ..., 0.5816512 , 0.61191789,\n",
       "           0.32298095],\n",
       "          [1.        , 0.5750359 , 0.5324555 , ..., 0.85111662, 0.59881925,\n",
       "           0.41292488],\n",
       "          ...,\n",
       "          [1.        , 0.69695978, 0.        , ..., 0.        , 0.17718317,\n",
       "           0.        ],\n",
       "          [1.        , 0.63165813, 0.        , ..., 0.19679498, 0.60958315,\n",
       "           0.23951547],\n",
       "          [1.        , 0.33124203, 0.        , ..., 0.29696261, 0.50989011,\n",
       "           0.24533318]]),\n",
       "   10: array([[1.        , 0.22247143, 0.3351363 , ..., 0.14968392, 0.3056612 ,\n",
       "           0.13811427],\n",
       "          [0.743392  , 0.        , 0.81217186, ..., 0.40345871, 0.12364736,\n",
       "           0.05053161],\n",
       "          [0.81523619, 0.        , 1.        , ..., 1.        , 0.49362238,\n",
       "           0.55778537],\n",
       "          ...,\n",
       "          [0.17684684, 0.        , 0.53139977, ..., 0.50788552, 0.55317325,\n",
       "           0.18404596],\n",
       "          [0.13558073, 0.        , 0.48764213, ..., 0.23169214, 0.19479011,\n",
       "           0.        ],\n",
       "          [0.17904647, 0.        , 0.48544326, ..., 0.31812526, 0.41562365,\n",
       "           0.08613119]]),\n",
       "   11: array([[1.        , 0.95854852, 0.80150671, ..., 0.506426  , 0.22131754,\n",
       "           0.34911228],\n",
       "          [1.        , 0.23450985, 0.76165456, ..., 0.56095413, 0.2811289 ,\n",
       "           0.40311931],\n",
       "          [0.98168973, 0.2229471 , 0.76721518, ..., 0.63912245, 0.21502411,\n",
       "           0.3526852 ],\n",
       "          ...,\n",
       "          [0.89585255, 0.30027328, 0.39160284, ..., 0.62958866, 0.84732755,\n",
       "           0.59172986],\n",
       "          [0.93799876, 0.29459693, 0.43189324, ..., 0.67510667, 0.32299979,\n",
       "           0.37370474],\n",
       "          [1.        , 0.16294014, 0.54856464, ..., 0.90125964, 0.37817302,\n",
       "           0.41140549]]),\n",
       "   12: array([[0.72149893, 1.        , 0.73333145, ..., 0.29583174, 0.51848332,\n",
       "           0.        ],\n",
       "          [1.        , 0.23847202, 0.42915935, ..., 0.01773359, 0.        ,\n",
       "           0.        ],\n",
       "          [0.61846311, 0.37446094, 0.42378287, ..., 0.10534709, 0.        ,\n",
       "           0.        ],\n",
       "          ...,\n",
       "          [0.35377712, 0.        , 0.59473474, ..., 0.37454178, 0.        ,\n",
       "           0.        ],\n",
       "          [0.65815552, 0.05095905, 0.55333073, ..., 0.27400731, 0.        ,\n",
       "           0.        ],\n",
       "          [1.        , 0.00338289, 0.48410682, ..., 0.23396965, 0.        ,\n",
       "           0.24258462]]),\n",
       "   13: array([[0.        , 0.68594118, 0.75317746, ..., 1.        , 0.3549504 ,\n",
       "           0.29808746],\n",
       "          [0.        , 1.        , 0.97518001, ..., 0.67799998, 0.1323911 ,\n",
       "           0.        ],\n",
       "          [0.        , 1.        , 1.        , ..., 0.76636815, 0.        ,\n",
       "           0.        ],\n",
       "          ...,\n",
       "          [0.        , 1.        , 0.47204583, ..., 0.47249831, 0.        ,\n",
       "           0.07697872],\n",
       "          [0.16961461, 1.        , 0.75709746, ..., 0.79978172, 0.36520155,\n",
       "           0.19913901],\n",
       "          [0.01792699, 1.        , 0.71199707, ..., 0.81531494, 0.36526547,\n",
       "           0.        ]]),\n",
       "   14: array([[0.        , 0.        , 0.37550085, ..., 0.44986418, 0.34371291,\n",
       "           0.242432  ],\n",
       "          [0.        , 0.        , 0.55744623, ..., 0.61260793, 0.64590514,\n",
       "           0.30850729],\n",
       "          [0.        , 0.        , 1.        , ..., 0.72549918, 0.38439836,\n",
       "           0.35533513],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.45804045, ..., 1.        , 0.55785968,\n",
       "           0.69716747],\n",
       "          [0.        , 0.        , 0.55743633, ..., 0.76945747, 0.04617885,\n",
       "           0.33880458],\n",
       "          [0.        , 0.        , 0.5106688 , ..., 0.43169537, 0.        ,\n",
       "           0.        ]]),\n",
       "   15: array([[1.        , 0.        , 1.        , ..., 0.29358557, 0.75843935,\n",
       "           0.73848887],\n",
       "          [1.        , 0.        , 0.96206892, ..., 0.77479868, 0.88658297,\n",
       "           0.57009787],\n",
       "          [1.        , 0.        , 0.66656572, ..., 0.46198465, 0.81270068,\n",
       "           0.28585183],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.34711311, ..., 0.16377044, 0.58637191,\n",
       "           0.38530008],\n",
       "          [1.        , 0.        , 0.67169312, ..., 0.        , 0.50859578,\n",
       "           0.31611773],\n",
       "          [1.        , 0.        , 0.73964568, ..., 0.        , 0.46213089,\n",
       "           0.24261117]]),\n",
       "   16: array([[0.        , 0.        , 0.        , ..., 0.36770605, 0.        ,\n",
       "           0.12041003],\n",
       "          [0.        , 0.        , 0.63590114, ..., 0.        , 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 0.8005586 , ..., 0.98737603, 0.        ,\n",
       "           0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.24981123, ..., 0.63170795, 0.30653828,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 0.31419466, ..., 0.64121333, 0.37783672,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 0.32508451, ..., 0.60969503, 0.40762411,\n",
       "           0.12836353]]),\n",
       "   17: array([[0.        , 0.        , 1.        , ..., 0.        , 0.        ,\n",
       "           0.33142952],\n",
       "          [0.        , 0.        , 1.        , ..., 0.34946926, 0.46238713,\n",
       "           0.24530217],\n",
       "          [0.        , 0.        , 1.        , ..., 0.57518495, 0.49794111,\n",
       "           0.7968072 ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.18526174, 0.37643687,\n",
       "           0.70409473],\n",
       "          [0.        , 0.        , 1.        , ..., 0.54246104, 0.01774302,\n",
       "           0.54762589],\n",
       "          [0.        , 0.        , 1.        , ..., 0.7052606 , 0.07541999,\n",
       "           0.45670812]]),\n",
       "   18: array([[0.        , 0.45845737, 0.60665201, ..., 0.55462575, 0.64333191,\n",
       "           0.4750385 ],\n",
       "          [0.        , 0.        , 0.5020387 , ..., 0.85669835, 0.35996512,\n",
       "           0.47427605],\n",
       "          [0.07504135, 0.03237652, 0.42028896, ..., 0.69679846, 0.20605545,\n",
       "           0.35419278],\n",
       "          ...,\n",
       "          [0.12941887, 0.        , 0.35569592, ..., 0.5566102 , 0.72766748,\n",
       "           0.15556942],\n",
       "          [0.15131219, 0.17215085, 0.31258068, ..., 0.4743617 , 0.53239359,\n",
       "           0.        ],\n",
       "          [0.        , 0.14096747, 0.42016799, ..., 0.19691645, 0.38732452,\n",
       "           0.        ]])},\n",
       "  'fitted_feature_scaler': MinMaxScaler(clip=True)},\n",
       " 8: {'profile_windows': array([[0.        , 0.04585637, 0.27003949, ..., 0.71243701, 0.55092883,\n",
       "          0.75337417],\n",
       "         [0.78937427, 0.44536352, 0.54139192, ..., 0.65278499, 0.69291555,\n",
       "          0.4339067 ],\n",
       "         [0.91118916, 0.57050144, 0.78668645, ..., 0.58280418, 1.        ,\n",
       "          0.62495889],\n",
       "         ...,\n",
       "         [0.45781572, 0.08771178, 0.52122241, ..., 0.5237752 , 0.47495631,\n",
       "          0.5851162 ],\n",
       "         [0.56886329, 0.16079434, 0.55246649, ..., 0.67685883, 0.71116808,\n",
       "          0.38677383],\n",
       "         [0.60649307, 0.11818348, 0.46266051, ..., 0.8450916 , 0.95083614,\n",
       "          0.72588857]]),\n",
       "  'unknown_users_dict': {0: array([[0.        , 1.        , 1.        , ..., 0.90384389, 0.91857909,\n",
       "           1.        ],\n",
       "          [0.        , 1.        , 0.96777297, ..., 0.95582425, 0.91535125,\n",
       "           0.88068847],\n",
       "          [0.        , 1.        , 0.76652281, ..., 0.8695975 , 0.95068468,\n",
       "           0.92583267],\n",
       "          ...,\n",
       "          [0.        , 1.        , 0.41787939, ..., 0.7801517 , 0.46577389,\n",
       "           0.43309561],\n",
       "          [0.        , 1.        , 0.63778346, ..., 0.02831404, 0.43166325,\n",
       "           0.3041412 ],\n",
       "          [0.        , 1.        , 0.44083283, ..., 0.10726868, 0.67166857,\n",
       "           0.70207043]]),\n",
       "   1: array([[1.        , 1.        , 0.09463925, ..., 0.9783895 , 1.        ,\n",
       "           1.        ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.72808108, 1.        ,\n",
       "           1.        ],\n",
       "          [1.        , 1.        , 0.04859879, ..., 0.7575134 , 1.        ,\n",
       "           1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.97561626, 0.98804478,\n",
       "           1.        ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.98270733, 0.93649113,\n",
       "           1.        ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.83130164, 0.68986098,\n",
       "           0.51998091]]),\n",
       "   2: array([[1.        , 1.        , 1.        , ..., 0.46558952, 0.44086844,\n",
       "           0.15448468],\n",
       "          [0.26048325, 1.        , 1.        , ..., 0.73439625, 0.96523347,\n",
       "           0.90444794],\n",
       "          [0.05412279, 1.        , 1.        , ..., 0.80418704, 0.91196578,\n",
       "           0.6756069 ],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.68845185, 0.84169512,\n",
       "           0.81887701],\n",
       "          [0.        , 1.        , 1.        , ..., 0.99193206, 1.        ,\n",
       "           1.        ],\n",
       "          [0.        , 1.        , 1.        , ..., 1.        , 0.9332882 ,\n",
       "           1.        ]]),\n",
       "   3: array([[1.        , 1.        , 0.52449678, ..., 0.71252281, 0.95796941,\n",
       "           1.        ],\n",
       "          [1.        , 1.        , 0.35417467, ..., 0.75358009, 1.        ,\n",
       "           0.96752995],\n",
       "          [1.        , 1.        , 0.67316392, ..., 0.82439119, 1.        ,\n",
       "           1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.13941304, ..., 0.52157015, 1.        ,\n",
       "           1.        ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.69216727, 1.        ,\n",
       "           0.99082734],\n",
       "          [1.        , 1.        , 0.        , ..., 0.76762995, 0.98722829,\n",
       "           0.49767419]]),\n",
       "   4: array([[1.        , 0.56311333, 1.        , ..., 1.        , 1.        ,\n",
       "           0.77716457],\n",
       "          [1.        , 0.82401374, 0.99196462, ..., 0.71530762, 0.72284734,\n",
       "           0.78722115],\n",
       "          [1.        , 0.46747827, 1.        , ..., 0.90395185, 0.80931547,\n",
       "           0.83325197],\n",
       "          ...,\n",
       "          [1.        , 0.35738747, 0.13067   , ..., 0.9262655 , 0.86975425,\n",
       "           0.96504805],\n",
       "          [0.99001275, 0.29465089, 0.24457962, ..., 0.85375435, 0.77943454,\n",
       "           1.        ],\n",
       "          [0.86913189, 0.38277662, 0.69581079, ..., 0.86245816, 0.57534032,\n",
       "           1.        ]]),\n",
       "   5: array([[0.2395147 , 0.48840584, 1.        , ..., 0.50777454, 0.78451721,\n",
       "           0.44895651],\n",
       "          [0.571549  , 0.37427631, 1.        , ..., 0.5184067 , 0.89158557,\n",
       "           0.64557924],\n",
       "          [0.54374774, 0.35482256, 1.        , ..., 0.54497225, 0.85979727,\n",
       "           0.56201388],\n",
       "          ...,\n",
       "          [0.46698992, 0.49227926, 1.        , ..., 0.13076936, 0.90792831,\n",
       "           1.        ],\n",
       "          [0.52502951, 0.59929687, 1.        , ..., 0.34923465, 1.        ,\n",
       "           0.99424114],\n",
       "          [0.1113135 , 0.48084854, 1.        , ..., 0.48134069, 0.97127834,\n",
       "           0.89238301]]),\n",
       "   6: array([[0.        , 0.        , 1.        , ..., 0.        , 0.44954069,\n",
       "           0.07542922],\n",
       "          [0.        , 0.        , 1.        , ..., 0.71113221, 0.38047357,\n",
       "           1.        ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.37224926, 0.58421044,\n",
       "           1.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.83388826, 0.80661713,\n",
       "           0.73467453],\n",
       "          [0.        , 0.        , 1.        , ..., 0.49928565, 0.73769768,\n",
       "           0.76694079],\n",
       "          [0.        , 0.        , 1.        , ..., 0.50735851, 0.64944324,\n",
       "           0.78203959]]),\n",
       "   7: array([[0.        , 1.        , 1.        , ..., 0.77800628, 0.82071153,\n",
       "           0.89028721],\n",
       "          [0.        , 1.        , 1.        , ..., 0.90195044, 0.94950311,\n",
       "           0.97875568],\n",
       "          [0.        , 1.        , 1.        , ..., 0.68045858, 0.92132669,\n",
       "           1.        ],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.56888236, 0.7553032 ,\n",
       "           1.        ],\n",
       "          [0.        , 1.        , 1.        , ..., 0.8254856 , 0.82100648,\n",
       "           1.        ],\n",
       "          [0.        , 1.        , 1.        , ..., 0.77778076, 0.68999937,\n",
       "           0.97107026]]),\n",
       "   8: array([[0.78191454, 0.30857101, 1.        , ..., 0.77276067, 0.86215714,\n",
       "           0.82715189],\n",
       "          [0.67729731, 0.44562406, 1.        , ..., 0.86483477, 0.64553088,\n",
       "           0.58013801],\n",
       "          [0.7876334 , 0.38502491, 1.        , ..., 0.98941796, 0.77559311,\n",
       "           0.8324866 ],\n",
       "          ...,\n",
       "          [0.70968519, 0.2860373 , 0.91310872, ..., 0.68551352, 0.81307273,\n",
       "           0.86331128],\n",
       "          [0.99008333, 0.36435097, 0.70744518, ..., 0.69462371, 0.79637058,\n",
       "           0.94516363],\n",
       "          [1.        , 0.58654407, 0.6510066 , ..., 0.46331729, 0.63187913,\n",
       "           0.69986613]]),\n",
       "   9: array([[1.        , 1.        , 1.        , ..., 0.63259885, 0.98439179,\n",
       "           0.75901653],\n",
       "          [0.42481405, 1.        , 1.        , ..., 0.64855122, 0.91792082,\n",
       "           0.74467597],\n",
       "          [0.22978858, 1.        , 1.        , ..., 0.89165884, 0.90938538,\n",
       "           0.82825642],\n",
       "          ...,\n",
       "          [0.33682003, 1.        , 0.38927436, ..., 0.        , 0.6346356 ,\n",
       "           0.25214081],\n",
       "          [0.28995456, 1.        , 0.39047116, ..., 0.30133977, 0.91639943,\n",
       "           0.66711563],\n",
       "          [0.        , 1.        , 0.35360741, ..., 0.39170948, 0.85143668,\n",
       "           0.67252173]]),\n",
       "   10: array([[0.        , 1.        , 0.95052152, ..., 0.25883688, 0.71835545,\n",
       "           0.5728885 ],\n",
       "          [0.        , 1.        , 1.        , ..., 0.48778865, 0.59975018,\n",
       "           0.49150225],\n",
       "          [0.        , 1.        , 1.        , ..., 1.        , 0.84083618,\n",
       "           0.96286813],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.58200094, 0.87964118,\n",
       "           0.61557055],\n",
       "          [0.        , 1.        , 1.        , ..., 0.33282345, 0.64610877,\n",
       "           0.31282245],\n",
       "          [0.        , 1.        , 1.        , ..., 0.4108021 , 0.79001004,\n",
       "           0.52458318]]),\n",
       "   11: array([[0.        , 1.        , 1.        , ..., 0.58068418, 0.66339478,\n",
       "           0.76895853],\n",
       "          [0.        , 1.        , 1.        , ..., 0.62987864, 0.70236953,\n",
       "           0.8191446 ],\n",
       "          [0.        , 1.        , 1.        , ..., 0.70040091, 0.65929381,\n",
       "           0.77227867],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.69179967, 1.        ,\n",
       "           0.99441107],\n",
       "          [0.        , 1.        , 1.        , ..., 0.73286533, 0.72965376,\n",
       "           0.79181109],\n",
       "          [0.        , 1.        , 1.        , ..., 0.93689711, 0.76560617,\n",
       "           0.82684453]]),\n",
       "   12: array([[0.        , 1.        , 1.        , ..., 0.39068923, 0.85703626,\n",
       "           0.06928102],\n",
       "          [0.        , 1.        , 1.        , ..., 0.13979329, 0.43844071,\n",
       "           0.33342903],\n",
       "          [0.        , 1.        , 1.        , ..., 0.21883686, 0.29807953,\n",
       "           0.40274278],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.46170024, 0.06555614,\n",
       "           0.08510776],\n",
       "          [0.        , 1.        , 1.        , ..., 0.37099956, 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 1.        , 1.        , ..., 0.33487819, 0.27581687,\n",
       "           0.66996764]]),\n",
       "   13: array([[0.        , 1.        , 1.        , ..., 1.        , 0.75047366,\n",
       "           0.72154369],\n",
       "          [0.        , 1.        , 1.        , ..., 0.73547563, 0.60544785,\n",
       "           0.28673369],\n",
       "          [0.        , 1.        , 1.        , ..., 0.81520005, 0.30721341,\n",
       "           0.08437649],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.55007513, 0.50453395,\n",
       "           0.51607824],\n",
       "          [0.        , 1.        , 1.        , ..., 0.84534527, 0.7571536 ,\n",
       "           0.62959577],\n",
       "          [0.        , 1.        , 1.        , ..., 0.85935911, 0.75719525,\n",
       "           0.37891399]]),\n",
       "   14: array([[0.        , 0.91458938, 1.        , ..., 0.52965496, 0.743151  ,\n",
       "           0.66982581],\n",
       "          [0.        , 0.74904546, 1.        , ..., 0.67647992, 0.94006785,\n",
       "           0.73122631],\n",
       "          [0.        , 0.87246855, 1.        , ..., 0.77832868, 0.76966277,\n",
       "           0.77474111],\n",
       "          ...,\n",
       "          [0.        , 0.74607655, 1.        , ..., 1.        , 0.88269498,\n",
       "           1.        ],\n",
       "          [0.        , 0.5019429 , 1.        , ..., 0.81798719, 0.54926954,\n",
       "           0.75938009],\n",
       "          [0.        , 0.70178089, 1.        , ..., 0.51326334, 0.37329645,\n",
       "           0.37524892]]),\n",
       "   15: array([[0.09863257, 1.        , 1.        , ..., 0.38866277, 1.        ,\n",
       "           1.        ],\n",
       "          [0.15530124, 1.        , 1.        , ..., 0.82280595, 1.        ,\n",
       "           0.97430953],\n",
       "          [0.72678932, 0.91868292, 1.        , ..., 0.54058987, 1.        ,\n",
       "           0.71017372],\n",
       "          ...,\n",
       "          [0.72944939, 1.        , 0.97028181, ..., 0.27154552, 0.90127435,\n",
       "           0.80258606],\n",
       "          [0.70334922, 0.7893017 , 1.        , ..., 0.        , 0.85059326,\n",
       "           0.73829832],\n",
       "          [0.60606993, 0.7902629 , 1.        , ..., 0.        , 0.82031545,\n",
       "           0.6699923 ]]),\n",
       "   16: array([[0.        , 0.        , 0.        , ..., 0.45553314, 0.18709844,\n",
       "           0.55643682],\n",
       "          [0.        , 0.        , 1.        , ..., 0.        , 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 1.        , 1.        , ..., 1.        , 0.04221208,\n",
       "           0.32113879],\n",
       "          ...,\n",
       "          [0.        , 0.54092518, 0.80974537, ..., 0.69371166, 0.71892698,\n",
       "           0.2994687 ],\n",
       "          [0.        , 0.48100796, 0.91597031, ..., 0.70228727, 0.76538702,\n",
       "           0.42058168],\n",
       "          [0.        , 0.61112303, 0.93393726, ..., 0.67385193, 0.78479731,\n",
       "           0.56382761]]),\n",
       "   17: array([[0.        , 0.        , 1.        , ..., 0.01242425, 0.45506413,\n",
       "           0.75252681],\n",
       "          [0.        , 0.        , 1.        , ..., 0.43908019, 0.82048242,\n",
       "           0.67249292],\n",
       "          [0.        , 0.        , 1.        , ..., 0.64271746, 0.84365038,\n",
       "           1.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.29093464, 0.76447484,\n",
       "           1.        ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.61319445, 0.53073997,\n",
       "           0.95342742],\n",
       "          [0.        , 0.        , 1.        , ..., 0.76006975, 0.56832388,\n",
       "           0.86894203]]),\n",
       "   18: array([[0.        , 1.        , 1.        , ..., 0.62416926, 0.93839106,\n",
       "           0.88597554],\n",
       "          [0.        , 1.        , 1.        , ..., 0.89669459, 0.75374139,\n",
       "           0.88526704],\n",
       "          [0.        , 1.        , 1.        , ..., 0.75243534, 0.65344958,\n",
       "           0.77367958],\n",
       "          ...,\n",
       "          [0.        , 1.        , 0.98444241, ..., 0.6259596 , 0.99334646,\n",
       "           0.58910871],\n",
       "          [0.        , 1.        , 0.91330744, ..., 0.55175626, 0.86610058,\n",
       "           0.26545347],\n",
       "          [0.        , 1.        , 1.        , ..., 0.30144935, 0.77156954,\n",
       "           0.40187362]])},\n",
       "  'fitted_feature_scaler': MinMaxScaler(clip=True)},\n",
       " 9: {'profile_windows': array([[1.        , 1.        , 0.52311615, ..., 0.55346908, 0.64560099,\n",
       "          0.70157795],\n",
       "         [0.8252919 , 0.61133094, 0.83465926, ..., 0.65355779, 0.77792403,\n",
       "          0.75647951],\n",
       "         [0.78917631, 0.35315626, 1.        , ..., 0.98551528, 0.87709471,\n",
       "          0.82349858],\n",
       "         ...,\n",
       "         [0.51241043, 0.33371571, 0.5045975 , ..., 0.87895264, 0.95856102,\n",
       "          0.83961445],\n",
       "         [0.39783189, 0.20500808, 0.39971529, ..., 0.89782317, 0.90564637,\n",
       "          0.88660385],\n",
       "         [0.44628933, 0.39946782, 0.7095848 , ..., 0.56215821, 0.63002729,\n",
       "          0.76307136]]),\n",
       "  'unknown_users_dict': {0: array([[0.        , 0.11712719, 0.61118695, ..., 0.96231264, 0.83278049,\n",
       "           1.        ],\n",
       "          [0.        , 0.29482861, 0.54950584, ..., 0.99445172, 0.83010943,\n",
       "           0.8201093 ],\n",
       "          [0.        , 0.46240986, 0.34616401, ..., 0.94113835, 0.8593481 ,\n",
       "           0.85843629],\n",
       "          ...,\n",
       "          [0.        , 0.06782676, 0.        , ..., 0.88583467, 0.45808082,\n",
       "           0.44010718],\n",
       "          [0.        , 0.        , 0.21608662, ..., 0.42097897, 0.42985401,\n",
       "           0.33062611],\n",
       "          [0.        , 0.01005781, 0.01708901, ..., 0.46979604, 0.6284602 ,\n",
       "           0.66846427]]),\n",
       "   1: array([[1.        , 0.17571455, 0.        , ..., 1.        , 0.99705243,\n",
       "           1.        ],\n",
       "          [1.        , 0.22134958, 0.        , ..., 0.85363979, 0.98399262,\n",
       "           1.        ],\n",
       "          [1.        , 0.02824583, 0.        , ..., 0.87183758, 1.        ,\n",
       "           1.        ],\n",
       "          ...,\n",
       "          [1.        , 0.34318638, 0.        , ..., 1.        , 0.89026387,\n",
       "           1.        ],\n",
       "          [1.        , 0.28902767, 0.        , ..., 1.        , 0.84760284,\n",
       "           0.97306487],\n",
       "          [1.        , 0.        , 0.        , ..., 0.9174603 , 0.64351456,\n",
       "           0.51387198]]),\n",
       "   2: array([[1.        , 0.        , 1.        , ..., 0.69134319, 0.43747138,\n",
       "           0.20356913],\n",
       "          [0.40283684, 0.03552578, 1.        , ..., 0.85754441, 0.87138733,\n",
       "           0.84028086],\n",
       "          [0.02454549, 0.63168349, 1.        , ..., 0.90069555, 0.82730792,\n",
       "           0.64599698],\n",
       "          ...,\n",
       "          [0.        , 0.48055697, 1.        , ..., 0.82913733, 0.76915843,\n",
       "           0.76763195],\n",
       "          [0.        , 0.38896062, 1.        , ..., 1.        , 1.        ,\n",
       "           1.        ],\n",
       "          [0.        , 0.56406109, 1.        , ..., 1.        , 0.84495239,\n",
       "           0.95528434]]),\n",
       "   3: array([[1.        , 0.        , 0.10162251, ..., 0.84402023, 0.86537627,\n",
       "           1.        ],\n",
       "          [1.        , 0.        , 0.        , ..., 0.86940564, 0.95820757,\n",
       "           0.89383689],\n",
       "          [1.        , 0.        , 0.2518348 , ..., 0.91318762, 0.9133299 ,\n",
       "           0.93123285],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.        , ..., 0.7259556 , 0.98727582,\n",
       "           1.        ],\n",
       "          [1.        , 0.        , 0.        , ..., 0.83143454, 0.95433887,\n",
       "           0.91361615],\n",
       "          [1.        , 0.        , 0.        , ..., 0.87809257, 0.88958821,\n",
       "           0.49493379]]),\n",
       "   4: array([[1.        , 0.        , 0.9127629 , ..., 1.        , 0.96051925,\n",
       "           0.73221848],\n",
       "          [1.        , 0.        , 0.57394892, ..., 0.84574205, 0.67081101,\n",
       "           0.74075642],\n",
       "          [1.        , 0.        , 0.69818354, ..., 0.96237939, 0.74236403,\n",
       "           0.77983616],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.        , ..., 0.97617576, 0.79237757,\n",
       "           0.89172978],\n",
       "          [1.        , 0.        , 0.        , ..., 0.93134265, 0.71763734,\n",
       "           0.97770436],\n",
       "          [1.        , 0.        , 0.27471705, ..., 0.93672415, 0.54874785,\n",
       "           0.92313098]]),\n",
       "   5: array([[0.36439817, 0.        , 1.        , ..., 0.71742587, 0.72184329,\n",
       "           0.45357294],\n",
       "          [0.97306952, 0.        , 1.        , ..., 0.72399966, 0.81044315,\n",
       "           0.62050378],\n",
       "          [0.92210543, 0.        , 1.        , ..., 0.74042495, 0.7841381 ,\n",
       "           0.54955758],\n",
       "          ...,\n",
       "          [0.7813962 , 0.        , 0.73507282, ..., 0.48432634, 0.82396688,\n",
       "           1.        ],\n",
       "          [0.88779194, 0.        , 0.79078882, ..., 0.61940182, 1.        ,\n",
       "           0.91651444],\n",
       "          [0.1293851 , 0.        , 0.76629904, ..., 0.70108202, 0.87638951,\n",
       "           0.83003785]]),\n",
       "   6: array([[0.        , 0.        , 1.        , ..., 0.39580624, 0.44464773,\n",
       "           0.13645179],\n",
       "          [0.        , 0.        , 1.        , ..., 0.84316043, 0.38749417,\n",
       "           1.        ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.63363159, 0.55608795,\n",
       "           0.96278928],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.91905959, 0.74013114,\n",
       "           0.69614484],\n",
       "          [0.        , 0.        , 1.        , ..., 0.71217725, 0.68309978,\n",
       "           0.72353859],\n",
       "          [0.        , 0.        , 1.        , ..., 0.71716865, 0.61006857,\n",
       "           0.73635733]]),\n",
       "   7: array([[0.        , 0.4201704 , 1.        , ..., 0.88450817, 0.75179435,\n",
       "           0.82825854],\n",
       "          [0.        , 0.99354001, 1.        , ..., 0.96114194, 0.85837035,\n",
       "           0.90336743],\n",
       "          [0.        , 0.77009341, 1.        , ..., 0.82419515, 0.83505415,\n",
       "           1.        ],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.75520839, 0.69766848,\n",
       "           0.98208337],\n",
       "          [0.        , 1.        , 1.        , ..., 0.91386429, 0.75203842,\n",
       "           1.        ],\n",
       "          [0.        , 0.99528029, 0.92066996, ..., 0.88436873, 0.64362907,\n",
       "           0.89684259]]),\n",
       "   8: array([[1.        , 0.        , 0.80184576, ..., 0.88126485, 0.7860909 ,\n",
       "           0.77465724],\n",
       "          [1.        , 0.        , 0.64120758, ..., 0.93819359, 0.60683107,\n",
       "           0.56494479],\n",
       "          [1.        , 0.        , 0.70182068, ..., 1.        , 0.71445853,\n",
       "           0.77918636],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.49427344, ..., 0.82732058, 0.74547319,\n",
       "           0.80535622],\n",
       "          [1.        , 0.        , 0.28647236, ..., 0.83295335, 0.73165204,\n",
       "           0.8748481 ],\n",
       "          [1.        , 0.        , 0.2294472 , ..., 0.68993828, 0.59553414,\n",
       "           0.66659284]]),\n",
       "   9: array([[1.        , 1.        , 0.59380699, ..., 0.79460383, 0.88724099,\n",
       "           0.71681097],\n",
       "          [0.70408118, 1.        , 0.80650339, ..., 0.80446706, 0.83223576,\n",
       "           0.70463597],\n",
       "          [0.34656865, 0.66933059, 0.86101171, ..., 0.95477871, 0.82517263,\n",
       "           0.77559498],\n",
       "          ...,\n",
       "          [0.54277423, 0.81942085, 0.        , ..., 0.38644536, 0.59781514,\n",
       "           0.28647827],\n",
       "          [0.45686242, 0.73903347, 0.        , ..., 0.58978877, 0.83097681,\n",
       "           0.63878797],\n",
       "          [0.        , 0.3692164 , 0.        , ..., 0.6456637 , 0.77721964,\n",
       "           0.6433777 ]]),\n",
       "   10: array([[0.        , 0.23531803, 0.53207508, ..., 0.56350955, 0.66709394,\n",
       "           0.55879003],\n",
       "          [0.        , 0.        , 1.        , ..., 0.70506875, 0.56894719,\n",
       "           0.48969387],\n",
       "          [0.        , 0.        , 1.        , ..., 1.        , 0.76844765,\n",
       "           0.88987905],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.85925178, ..., 0.76331951, 0.80055908,\n",
       "           0.59502668],\n",
       "          [0.        , 0.        , 0.78630656, ..., 0.6092549 , 0.60730928,\n",
       "           0.3379964 ],\n",
       "          [0.        , 0.        , 0.78264099, ..., 0.65746853, 0.72638864,\n",
       "           0.51777927]]),\n",
       "   11: array([[0.        , 1.        , 1.        , ..., 0.76250537, 0.62161358,\n",
       "           0.72525163],\n",
       "          [0.        , 0.25013753, 1.        , ..., 0.79292194, 0.65386547,\n",
       "           0.76785913],\n",
       "          [0.        , 0.2359036 , 1.        , ..., 0.83652534, 0.61821999,\n",
       "           0.7280704 ],\n",
       "          ...,\n",
       "          [0.        , 0.33109337, 0.62620638, ..., 0.83120726, 0.95917496,\n",
       "           0.91665871],\n",
       "          [0.        , 0.32410568, 0.69337161, ..., 0.85659785, 0.67644337,\n",
       "           0.74465324],\n",
       "          [0.        , 0.16203406, 0.88786611, ..., 0.9827492 , 0.70619426,\n",
       "           0.7743963 ]]),\n",
       "   12: array([[0.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "           6.45032886e-01, 7.81853338e-01, 1.31232033e-01],\n",
       "          [0.00000000e+00, 2.55015021e-01, 6.88814130e-01, ...,\n",
       "           4.89905770e-01, 4.35462408e-01, 3.55491197e-01],\n",
       "          [0.00000000e+00, 4.22419572e-01, 6.79851394e-01, ...,\n",
       "           5.38777833e-01, 3.19312490e-01, 4.14337919e-01],\n",
       "          ...,\n",
       "          [0.00000000e+00, 0.00000000e+00, 9.64832942e-01, ...,\n",
       "           6.88938471e-01, 1.26897655e-01, 1.44668781e-01],\n",
       "          [0.00000000e+00, 2.41835208e-02, 8.95811309e-01, ...,\n",
       "           6.32858908e-01, 7.20512117e-04, 6.62498387e-02],\n",
       "          [0.00000000e+00, 0.00000000e+00, 7.80413110e-01, ...,\n",
       "           6.10525332e-01, 3.00889971e-01, 6.41209298e-01]]),\n",
       "   13: array([[0.        , 0.80585678, 1.        , ..., 1.        , 0.693672  ,\n",
       "           0.68499688],\n",
       "          [0.        , 1.        , 1.        , ..., 0.85821178, 0.57366206,\n",
       "           0.31584729],\n",
       "          [0.        , 1.        , 1.        , ..., 0.90750481, 0.32687085,\n",
       "           0.14404794],\n",
       "          ...,\n",
       "          [0.        , 1.        , 0.76030709, ..., 0.74358002, 0.49015506,\n",
       "           0.51055866],\n",
       "          [0.        , 1.        , 1.        , ..., 0.92614338, 0.69919969,\n",
       "           0.60693397],\n",
       "          [0.        , 1.        , 1.        , ..., 0.93480803, 0.69923416,\n",
       "           0.3941075 ]]),\n",
       "   14: array([[0.        , 0.        , 0.59936391, ..., 0.73095438, 0.68761244,\n",
       "           0.64108889],\n",
       "          [0.        , 0.        , 0.90267196, ..., 0.82173517, 0.8505626 ,\n",
       "           0.69321734],\n",
       "          [0.        , 0.        , 1.        , ..., 0.88470751, 0.70955113,\n",
       "           0.73016099],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.73695974, ..., 1.        , 0.80308612,\n",
       "           0.99984107],\n",
       "          [0.        , 0.        , 0.90265545, ..., 0.90922808, 0.52717409,\n",
       "           0.71711963],\n",
       "          [0.        , 0.        , 0.82469267, ..., 0.72081956, 0.38155505,\n",
       "           0.39099589]]),\n",
       "   15: array([[0.10613896, 0.        , 1.        , ..., 0.64377994, 0.91124406,\n",
       "           1.        ],\n",
       "          [0.21002159, 0.        , 1.        , ..., 0.91220748, 0.98034254,\n",
       "           0.89959269],\n",
       "          [1.        , 0.        , 1.        , ..., 0.73771535, 0.94050323,\n",
       "           0.67534388],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.55204076, ..., 0.57136721, 0.81846068,\n",
       "           0.75380109],\n",
       "          [1.        , 0.        , 1.        , ..., 0.26749844, 0.77652171,\n",
       "           0.6992214 ],\n",
       "          [1.        , 0.        , 1.        , ..., 0.30109145, 0.75146659,\n",
       "           0.64123024]]),\n",
       "   16: array([[0.        , 0.        , 0.        , ..., 0.6851254 , 0.22747481,\n",
       "           0.54482271],\n",
       "          [0.        , 0.        , 1.        , ..., 0.3255557 , 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 1.        , ..., 1.        , 0.10758028,\n",
       "           0.3450569 ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.3898358 , ..., 0.83238943, 0.66756689,\n",
       "           0.3266592 ],\n",
       "          [0.        , 0.        , 0.49716477, ..., 0.83769166, 0.70601292,\n",
       "           0.42948297],\n",
       "          [0.        , 0.        , 0.51531846, ..., 0.82011031, 0.72207508,\n",
       "           0.55109742]]),\n",
       "   17: array([[0.        , 0.        , 1.        , ..., 0.41115443, 0.44921842,\n",
       "           0.71130126],\n",
       "          [0.        , 0.        , 1.        , ..., 0.67495266, 0.75160477,\n",
       "           0.64335324],\n",
       "          [0.        , 0.        , 1.        , ..., 0.80086009, 0.77077642,\n",
       "           1.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.58335536, 0.70525808,\n",
       "           1.        ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.78260623, 0.51184074,\n",
       "           0.88186398],\n",
       "          [0.        , 0.        , 1.        , ..., 0.87341815, 0.5429417 ,\n",
       "           0.81013668]]),\n",
       "   18: array([[0.        , 0.52582053, 0.98469937, ..., 0.78939188, 0.84917504,\n",
       "           0.82459797],\n",
       "          [0.        , 0.        , 0.81030606, ..., 0.95789228, 0.69637607,\n",
       "           0.82399646],\n",
       "          [0.        , 0.00130813, 0.67402694, ..., 0.86869785, 0.61338385,\n",
       "           0.72925976],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.56634853, ..., 0.79049883, 0.89465104,\n",
       "           0.57256083],\n",
       "          [0.        , 0.17337259, 0.49447422, ..., 0.74461945, 0.78935413,\n",
       "           0.29778059],\n",
       "          [0.        , 0.13498534, 0.67382529, ..., 0.58985653, 0.71112899,\n",
       "           0.41360001]])},\n",
       "  'fitted_feature_scaler': MinMaxScaler(clip=True)},\n",
       " 10: {'profile_windows': array([[0.87371218, 1.        , 0.69487806, ..., 0.08526438, 0.50828542,\n",
       "          0.31566425],\n",
       "         [0.21379753, 0.58728019, 0.74532413, ..., 0.29411527, 0.86825091,\n",
       "          0.50525542],\n",
       "         [0.47491425, 0.27405817, 1.        , ..., 0.45775794, 0.74085811,\n",
       "          0.73687564],\n",
       "         ...,\n",
       "         [0.57242451, 0.5180629 , 0.43970892, ..., 0.78527718, 0.977465  ,\n",
       "          0.69419918],\n",
       "         [0.25167594, 0.51447308, 0.2166236 , ..., 1.        , 0.7977428 ,\n",
       "          0.78623105],\n",
       "         [0.26702197, 0.31251743, 0.30910152, ..., 0.76640307, 0.61609766,\n",
       "          0.9274373 ]]),\n",
       "  'unknown_users_dict': {0: array([[0.62628358, 0.60020154, 0.36164005, ..., 0.83658058, 0.7279704 ,\n",
       "           1.        ],\n",
       "          [0.62860463, 0.84253917, 0.23814261, ..., 0.89294011, 0.72396956,\n",
       "           0.81328919],\n",
       "          [0.8352188 , 1.        , 0.        , ..., 0.79944905, 0.76776464,\n",
       "           0.85390845],\n",
       "          ...,\n",
       "          [0.18603113, 0.53296881, 0.        , ..., 0.70246776, 0.16672739,\n",
       "           0.41055987],\n",
       "          [0.35847529, 0.38412218, 0.        , ..., 0.        , 0.12444793,\n",
       "           0.29453093],\n",
       "          [0.4354231 , 0.45418728, 0.        , ..., 0.        , 0.42192974,\n",
       "           0.65257456]]),\n",
       "   1: array([[1.        , 0.68009916, 0.        , ..., 0.91740641, 0.97402474,\n",
       "           1.        ],\n",
       "          [1.        , 0.74233325, 0.        , ..., 0.64601037, 0.95446314,\n",
       "           1.        ],\n",
       "          [1.        , 0.47899092, 0.        , ..., 0.67792226, 1.        ,\n",
       "           1.        ],\n",
       "          ...,\n",
       "          [1.        , 0.90848634, 0.        , ..., 0.91439953, 0.81407174,\n",
       "           1.        ],\n",
       "          [1.        , 0.83462822, 0.        , ..., 0.92208799, 0.75017202,\n",
       "           0.97539275],\n",
       "          [1.        , 0.43522068, 0.        , ..., 0.7579269 , 0.44447887,\n",
       "           0.4887364 ]]),\n",
       "   2: array([[1.        , 0.        , 1.        , ..., 0.36140481, 0.13585758,\n",
       "           0.15987492],\n",
       "          [1.        , 0.48891885, 1.        , ..., 0.65285757, 0.78579757,\n",
       "           0.83466718],\n",
       "          [1.        , 1.        , 1.        , ..., 0.72852799, 0.71977333,\n",
       "           0.62876354],\n",
       "          ...,\n",
       "          [0.53407859, 1.        , 1.        , ..., 0.60304252, 0.63267426,\n",
       "           0.75767328],\n",
       "          [1.        , 0.97091027, 1.        , ..., 0.93208987, 1.        ,\n",
       "           1.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        , 0.74620205,\n",
       "           0.9565488 ]]),\n",
       "   3: array([[1.        , 0.2440307 , 0.        , ..., 0.62914137, 0.77679391,\n",
       "           1.        ],\n",
       "          [1.        , 0.35625776, 0.        , ..., 0.67365758, 0.91584106,\n",
       "           0.8914263 ],\n",
       "          [1.        , 0.17712423, 0.        , ..., 0.75043427, 0.84862115,\n",
       "           0.93105883],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.        , ..., 0.42210161, 0.95938086,\n",
       "           1.        ],\n",
       "          [1.        , 0.31141396, 0.        , ..., 0.60707094, 0.91004634,\n",
       "           0.91238852],\n",
       "          [1.        , 0.        , 0.        , ..., 0.6888911 , 0.81305972,\n",
       "           0.46866554]]),\n",
       "   4: array([[1.        , 0.        , 0.96545302, ..., 0.96795559, 0.9193036 ,\n",
       "           0.72014179],\n",
       "          [1.        , 0.        , 0.28708235, ..., 0.63216079, 0.4853648 ,\n",
       "           0.72919037],\n",
       "          [1.        , 0.        , 0.53582393, ..., 0.83669764, 0.59254033,\n",
       "           0.77060739],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.        , ..., 0.86089114, 0.66745299,\n",
       "           0.88919316],\n",
       "          [1.        , 0.        , 0.        , ..., 0.78227117, 0.5555035 ,\n",
       "           0.98030972],\n",
       "          [1.        , 0.        , 0.        , ..., 0.79170825, 0.30253279,\n",
       "           0.92247241]]),\n",
       "   5: array([[1.        , 0.        , 1.        , ..., 0.40714377, 0.56180337,\n",
       "           0.42483098],\n",
       "          [1.        , 0.        , 1.        , ..., 0.41867165, 0.69451247,\n",
       "           0.60174565],\n",
       "          [1.        , 0.        , 1.        , ..., 0.44747526, 0.65511151,\n",
       "           0.52655629],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.60968334, ..., 0.        , 0.71476896,\n",
       "           1.        ],\n",
       "          [1.        , 0.        , 0.72123749, ..., 0.23524745, 1.        ,\n",
       "           0.91546015],\n",
       "          [1.        , 0.        , 0.67220425, ..., 0.37848296, 0.79329007,\n",
       "           0.82381156]]),\n",
       "   6: array([[0.        , 0.        , 1.        , ..., 0.        , 0.14660666,\n",
       "           0.08874341],\n",
       "          [0.        , 0.        , 1.        , ..., 0.62763362, 0.06099934,\n",
       "           1.        ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.26020097, 0.31352713,\n",
       "           0.9645026 ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.76073144, 0.58919578,\n",
       "           0.68191065],\n",
       "          [0.        , 0.        , 1.        , ..., 0.39793972, 0.5037715 ,\n",
       "           0.71094277],\n",
       "          [0.        , 0.        , 1.        , ..., 0.4066927 , 0.39438188,\n",
       "           0.72452818]]),\n",
       "   7: array([[0.        , 1.        , 1.        , ..., 0.70014159, 0.60666551,\n",
       "           0.82192582],\n",
       "          [1.        , 1.        , 1.        , ..., 0.83452762, 0.76630011,\n",
       "           0.90152684],\n",
       "          [1.        , 1.        , 1.        , ..., 0.59437584, 0.731376  ,\n",
       "           1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        , ..., 0.47339971, 0.52559319,\n",
       "           0.98495062],\n",
       "          [1.        , 1.        , 1.        , ..., 0.75162088, 0.60703109,\n",
       "           1.        ],\n",
       "          [0.69512651, 1.        , 0.98128448, ..., 0.69989707, 0.4446504 ,\n",
       "           0.89461175]]),\n",
       "   8: array([[1.        , 0.        , 0.7433756 , ..., 0.69445406, 0.65803652,\n",
       "           0.76511874],\n",
       "          [1.        , 0.        , 0.42174712, ..., 0.79428508, 0.38953259,\n",
       "           0.54286378],\n",
       "          [1.        , 0.        , 0.54310618, ..., 0.92936396, 0.55074214,\n",
       "           0.76991873],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.12755674, ..., 0.59985665, 0.59719737,\n",
       "           0.79765376],\n",
       "          [1.        , 0.        , 0.        , ..., 0.60973434, 0.57649539,\n",
       "           0.87130182],\n",
       "          [1.        , 0.        , 0.        , ..., 0.35894116, 0.37261152,\n",
       "           0.6505912 ]]),\n",
       "   9: array([[1.        , 1.        , 0.32684202, ..., 0.5424841 , 0.80954393,\n",
       "           0.70381278],\n",
       "          [1.        , 1.        , 0.75270108, ..., 0.55978039, 0.72715448,\n",
       "           0.69090962],\n",
       "          [1.        , 1.        , 0.86183721, ..., 0.82336898, 0.71657498,\n",
       "           0.76611256],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.        , 0.37602811,\n",
       "           0.2477427 ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.18331759, 0.72526876,\n",
       "           0.62112337],\n",
       "          [1.        , 0.94398439, 0.        , ..., 0.28130064, 0.64474872,\n",
       "           0.62598761]]),\n",
       "   10: array([[1.        , 0.76138251, 0.20324288, ..., 0.13723398, 0.47979719,\n",
       "           0.53634091],\n",
       "          [1.        , 0.23476093, 1.        , ..., 0.38547414, 0.33278832,\n",
       "           0.46311224],\n",
       "          [1.        , 0.24948695, 1.        , ..., 0.98238812, 0.63160961,\n",
       "           0.88723174],\n",
       "          ...,\n",
       "          [0.60314247, 0.07179237, 0.85831347, ..., 0.48762349, 0.67970764,\n",
       "           0.57474481],\n",
       "          [0.55052987, 0.21825037, 0.71226315, ..., 0.21745367, 0.39024887,\n",
       "           0.30234203],\n",
       "          [0.60594692, 0.10516599, 0.70492397, ..., 0.30200175, 0.56861162,\n",
       "           0.49287737]]),\n",
       "   11: array([[1.        , 1.        , 1.        , ..., 0.4861958 , 0.41167454,\n",
       "           0.71275826],\n",
       "          [1.        , 0.78159237, 1.        , ..., 0.53953471, 0.45998296,\n",
       "           0.75791404],\n",
       "          [1.        , 0.76218106, 1.        , ..., 0.61599824, 0.40659147,\n",
       "           0.71574562],\n",
       "          ...,\n",
       "          [1.        , 0.89199468, 0.39171182, ..., 0.60667238, 0.91729007,\n",
       "           0.91561304],\n",
       "          [1.        , 0.88246533, 0.52618951, ..., 0.65119767, 0.49380122,\n",
       "           0.73332025],\n",
       "          [1.        , 0.6614426 , 0.91560486, ..., 0.87241842, 0.53836351,\n",
       "           0.76484218]]),\n",
       "   12: array([[1.        , 1.        , 1.        , ..., 0.28019443, 0.65168929,\n",
       "           0.08321147],\n",
       "          [1.        , 0.78824398, 0.51706457, ..., 0.00816139, 0.13284845,\n",
       "           0.32088316],\n",
       "          [1.        , 1.        , 0.49911945, ..., 0.09386411, 0.        ,\n",
       "           0.38324939],\n",
       "          ...,\n",
       "          [0.82872135, 0.21021118, 1.        , ..., 0.35718787, 0.        ,\n",
       "           0.09745185],\n",
       "          [1.        , 0.47345101, 0.93151268, ..., 0.25884598, 0.        ,\n",
       "           0.01434281],\n",
       "          [1.        , 0.39358112, 0.70046332, ..., 0.21968151, 0.        ,\n",
       "           0.62368952]]),\n",
       "   13: array([[0.        , 1.        , 1.        , ..., 1.        , 0.51960708,\n",
       "           0.67009595],\n",
       "          [0.36480952, 1.        , 1.        , ..., 0.65402788, 0.33985047,\n",
       "           0.27886823],\n",
       "          [0.29622182, 1.        , 1.        , ..., 0.74046881, 0.        ,\n",
       "           0.09679388],\n",
       "          ...,\n",
       "          [0.10222391, 1.        , 0.66020721, ..., 0.45300805, 0.21476971,\n",
       "           0.48522491],\n",
       "          [0.59392168, 1.        , 1.        , ..., 0.77315366, 0.52788672,\n",
       "           0.58736424],\n",
       "          [0.40052617, 1.        , 1.        , ..., 0.78834811, 0.52793834,\n",
       "           0.36180903]]),\n",
       "   14: array([[0.        , 0.        , 0.33796804, ..., 0.43086755, 0.51053078,\n",
       "           0.62356191],\n",
       "          [0.        , 0.        , 0.94524902, ..., 0.59006199, 0.75460529,\n",
       "           0.67880806],\n",
       "          [0.        , 0.        , 1.        , ..., 0.70049116, 0.5433916 ,\n",
       "           0.71796124],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.61346133, ..., 0.97422257, 0.68349277,\n",
       "           1.        ],\n",
       "          [0.        , 0.        , 0.94521597, ..., 0.74349076, 0.27021858,\n",
       "           0.7041399 ],\n",
       "          [0.        , 0.        , 0.7891195 , ..., 0.41309498, 0.05210344,\n",
       "           0.35851131]]),\n",
       "   15: array([[1.        , 0.        , 1.        , ..., 0.27799725, 0.84549688,\n",
       "           1.        ],\n",
       "          [1.        , 0.        , 1.        , ..., 0.74871548, 0.94899588,\n",
       "           0.89752634],\n",
       "          [1.        , 0.        , 1.        , ..., 0.44272368, 0.88932265,\n",
       "           0.65986563],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.243218  , ..., 0.15101329, 0.70652151,\n",
       "           0.74301521],\n",
       "          [1.        , 0.        , 1.        , ..., 0.        , 0.64370332,\n",
       "           0.68517122],\n",
       "          [1.        , 0.        , 1.        , ..., 0.        , 0.60617457,\n",
       "           0.62371171]]),\n",
       "   16: array([[0.        , 0.        , 0.        , ..., 0.35050122, 0.        ,\n",
       "           0.52153823],\n",
       "          [0.        , 0.        , 1.        , ..., 0.        , 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.95665668, 0.        ,\n",
       "           0.30982481],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.60874545, 0.4805056 ,\n",
       "           0.29032677],\n",
       "          [0.        , 0.        , 0.13334573, ..., 0.61804352, 0.53809189,\n",
       "           0.39930024],\n",
       "          [0.        , 0.        , 0.16969291, ..., 0.58721261, 0.56215056,\n",
       "           0.52818822]]),\n",
       "   17: array([[0.        , 0.        , 1.        , ..., 0.        , 0.15345285,\n",
       "           0.69797355],\n",
       "          [0.        , 0.        , 1.        , ..., 0.33266216, 0.60638154,\n",
       "           0.62596168],\n",
       "          [0.        , 0.        , 1.        , ..., 0.55345516, 0.63509776,\n",
       "           1.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.17203587, 0.53696126,\n",
       "           1.        ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.52144494, 0.24725155,\n",
       "           0.8787373 ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.68069397, 0.29383606,\n",
       "           0.80272013]]),\n",
       "   18: array([[0.        , 1.        , 1.        , ..., 0.53334435, 0.75252695,\n",
       "           0.81804632],\n",
       "          [0.        , 0.13704702, 0.76031474, ..., 0.82882898, 0.52365736,\n",
       "           0.81740884],\n",
       "          [0.47334464, 0.44225504, 0.48745777, ..., 0.67241639, 0.39934766,\n",
       "           0.71700612],\n",
       "          ...,\n",
       "          [0.54267375, 0.17608113, 0.27186492, ..., 0.53528552, 0.82064307,\n",
       "           0.55093531],\n",
       "          [0.57058684, 0.67690535, 0.12795874, ..., 0.4548308 , 0.66292433,\n",
       "           0.25972098],\n",
       "          [0.27150232, 0.62455532, 0.48705402, ..., 0.18343641, 0.545755  ,\n",
       "           0.38246735]])},\n",
       "  'fitted_feature_scaler': MinMaxScaler(clip=True)},\n",
       " 11: {'profile_windows': array([[0.4334256 , 0.31861938, 0.5778467 , ..., 0.5409243 , 0.76923175,\n",
       "          0.79898261],\n",
       "         [0.47474101, 0.6881512 , 0.46671518, ..., 0.338194  , 0.9126204 ,\n",
       "          0.74787985],\n",
       "         [0.78889967, 0.72262131, 0.77851064, ..., 0.51944332, 0.9280681 ,\n",
       "          0.84915073],\n",
       "         ...,\n",
       "         [0.79469266, 0.64801475, 0.47716662, ..., 0.61202113, 0.83402925,\n",
       "          0.80236862],\n",
       "         [0.75141922, 0.72855312, 0.40163658, ..., 0.82698811, 0.88991767,\n",
       "          0.86346251],\n",
       "         [0.67368744, 0.69168841, 0.18461889, ..., 0.73410775, 0.92944677,\n",
       "          0.89266526]]),\n",
       "  'unknown_users_dict': {0: array([[0.2006278 , 0.5046489 , 0.25283965, ..., 0.88437361, 0.76675102,\n",
       "           0.98470065],\n",
       "          [0.20152659, 0.64063624, 0.15241675, ..., 0.93905284, 0.76408108,\n",
       "           0.80870139],\n",
       "          [0.28153442, 0.76887904, 0.        , ..., 0.84834911, 0.79330754,\n",
       "           0.83980698],\n",
       "          ...,\n",
       "          [0.0301475 , 0.46692137, 0.        , ..., 0.7542592 , 0.3922078 ,\n",
       "           0.50029768],\n",
       "          [0.09692358, 0.38339635, 0.        , ..., 0.        , 0.36399278,\n",
       "           0.41144457],\n",
       "          [0.12672032, 0.42271325, 0.        , ..., 0.04643946, 0.56251604,\n",
       "           0.68562868]]),\n",
       "   1: array([[1.        , 0.54948331, 0.        , ..., 0.9627897 , 0.93095438,\n",
       "           1.        ],\n",
       "          [1.        , 0.58440587, 0.        , ..., 0.69948503, 0.91790002,\n",
       "           0.98535962],\n",
       "          [1.        , 0.43663177, 0.        , ..., 0.7304455 , 0.97674588,\n",
       "           0.99094669],\n",
       "          ...,\n",
       "          [1.        , 0.67764238, 0.        , ..., 0.95987247, 0.8242104 ,\n",
       "           1.        ],\n",
       "          [1.        , 0.63619703, 0.        , ..., 0.96733171, 0.78156718,\n",
       "           0.93283772],\n",
       "          [1.        , 0.41207018, 0.        , ..., 0.8080649 , 0.57756411,\n",
       "           0.56016402]]),\n",
       "   2: array([[1.        , 0.        , 1.        , ..., 0.42336466, 0.37160696,\n",
       "           0.30832713],\n",
       "          [0.95832102, 0.44220281, 1.        , ..., 0.70612809, 0.80534175,\n",
       "           0.82507231],\n",
       "          [0.82128316, 0.89841696, 1.        , ..., 0.77954248, 0.76128075,\n",
       "           0.66739458],\n",
       "          ...,\n",
       "          [0.16492298, 0.78276626, 1.        , ..., 0.65779822, 0.70315553,\n",
       "           0.76611161],\n",
       "          [0.46532921, 0.71267147, 1.        , ..., 0.9770354 , 0.96651968,\n",
       "           0.96892455],\n",
       "          [0.77038363, 0.84666841, 1.        , ..., 1.        , 0.77891784,\n",
       "           0.91840732]]),\n",
       "   3: array([[1.        , 0.30478425, 0.        , ..., 0.68311896, 0.79933319,\n",
       "           1.        ],\n",
       "          [1.        , 0.36776027, 0.        , ..., 0.72630797, 0.89212574,\n",
       "           0.86853754],\n",
       "          [1.        , 0.2672398 , 0.        , ..., 0.80079565, 0.84726681,\n",
       "           0.89888751],\n",
       "          ...,\n",
       "          [1.        , 0.15998689, 0.        , ..., 0.48225186, 0.92118185,\n",
       "           0.96681273],\n",
       "          [1.        , 0.34259625, 0.        , ..., 0.66170653, 0.88825865,\n",
       "           0.88459008],\n",
       "          [1.        , 0.14323685, 0.        , ..., 0.74108731, 0.82353503,\n",
       "           0.54479408]]),\n",
       "   4: array([[1.        , 0.        , 0.74383487, ..., 1.        , 0.89443645,\n",
       "           0.73737059],\n",
       "          [1.        , 0.        , 0.19221248, ..., 0.68604836, 0.60484918,\n",
       "           0.74429985],\n",
       "          [1.        , 0.        , 0.39447864, ..., 0.88448717, 0.67637232,\n",
       "           0.77601635],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.        , ..., 0.90795937, 0.72636498,\n",
       "           0.86682745],\n",
       "          [1.        , 0.        , 0.        , ..., 0.83168337, 0.65165595,\n",
       "           0.93660305],\n",
       "          [1.        , 0.        , 0.        , ..., 0.8408391 , 0.48283698,\n",
       "           0.89231216]]),\n",
       "   5: array([[0.94439643, 0.        , 0.99279674, ..., 0.46773996, 0.65586014,\n",
       "           0.51122627],\n",
       "          [1.        , 0.        , 1.        , ..., 0.47892416, 0.74442301,\n",
       "           0.64670471],\n",
       "          [1.        , 0.        , 1.        , ..., 0.50686902, 0.71812894,\n",
       "           0.58912589],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.45453799, ..., 0.07116032, 0.7579411 ,\n",
       "           0.96498345],\n",
       "          [1.        , 0.        , 0.54524911, ..., 0.30096853, 0.99233457,\n",
       "           0.88694228],\n",
       "          [0.85926181, 0.        , 0.50537735, ..., 0.43993364, 0.81034184,\n",
       "           0.81675925]]),\n",
       "   6: array([[0.        , 0.        , 1.        , ..., 0.        , 0.37878032,\n",
       "           0.25385576],\n",
       "          [0.        , 0.        , 1.        , ..., 0.68165616, 0.32165062,\n",
       "           1.        ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.32517809, 0.49017401,\n",
       "           0.92449821],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.81078582, 0.67414035,\n",
       "           0.7080938 ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.45881032, 0.61713281,\n",
       "           0.73032613],\n",
       "          [0.        , 0.        , 1.        , ..., 0.46730234, 0.54413209,\n",
       "           0.74072962]]),\n",
       "   7: array([[0.        , 0.73655498, 1.        , ..., 0.75200239, 0.6857987 ,\n",
       "           0.81531519],\n",
       "          [0.35830534, 1.        , 1.        , ..., 0.88238185, 0.7923302 ,\n",
       "           0.87627238],\n",
       "          [0.47267611, 1.        , 1.        , ..., 0.64938992, 0.76902374,\n",
       "           0.97610952],\n",
       "          ...,\n",
       "          [0.37298574, 1.        , 1.        , ..., 0.53202056, 0.63169542,\n",
       "           0.94015699],\n",
       "          [0.38719055, 1.        , 1.        , ..., 0.80194688, 0.68604267,\n",
       "           1.        ],\n",
       "          [0.22728606, 1.        , 0.75670834, ..., 0.75176516, 0.57767858,\n",
       "           0.87097692]]),\n",
       "   8: array([[1.        , 0.        , 0.56325089, ..., 0.74648443, 0.72008093,\n",
       "           0.77181322],\n",
       "          [1.        , 0.        , 0.30171618, ..., 0.8433391 , 0.54089594,\n",
       "           0.60161391],\n",
       "          [1.        , 0.        , 0.40040025, ..., 0.97439076, 0.64847847,\n",
       "           0.77548898],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.06249299, ..., 0.65470733, 0.67948018,\n",
       "           0.79672803],\n",
       "          [1.        , 0.        , 0.        , ..., 0.66429052, 0.6656648 ,\n",
       "           0.85312654],\n",
       "          [1.        , 0.        , 0.        , ..., 0.42097445, 0.52960373,\n",
       "           0.68410986]]),\n",
       "   9: array([[1.        , 1.        , 0.22454337, ..., 0.59904527, 0.82118879,\n",
       "           0.7248661 ],\n",
       "          [1.        , 1.        , 0.57083397, ..., 0.6158259 , 0.76620653,\n",
       "           0.71498506],\n",
       "          [0.9379376 , 0.92722668, 0.65957886, ..., 0.87155589, 0.75914634,\n",
       "           0.77257428],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.        , 0.53188378,\n",
       "           0.37561488],\n",
       "          [0.97789204, 0.98056734, 0.        , ..., 0.2505869 , 0.76494809,\n",
       "           0.66154386],\n",
       "          [0.77858385, 0.69756206, 0.        , ..., 0.3456487 , 0.71121338,\n",
       "           0.66526882]]),\n",
       "   10: array([[0.67733843, 0.59509532, 0.12403777, ..., 0.20587723, 0.60113365,\n",
       "           0.5966188 ],\n",
       "          [0.47137372, 0.29958253, 1.        , ..., 0.44671639, 0.50302789,\n",
       "           0.54054145],\n",
       "          [0.50684365, 0.30784602, 1.        , ..., 1.        , 0.70244505,\n",
       "           0.86532542],\n",
       "          ...,\n",
       "          [0.1916668 , 0.20813301, 0.65671351, ..., 0.54582027, 0.73454307,\n",
       "           0.62602789],\n",
       "          [0.17129346, 0.29031766, 0.53795155, ..., 0.28370525, 0.54137395,\n",
       "           0.41742618],\n",
       "          [0.19275277, 0.22686056, 0.53198364, ..., 0.36573263, 0.6604036 ,\n",
       "           0.56333512]]),\n",
       "   11: array([[0.66519005, 1.        , 1.        , ..., 0.54443514, 0.55567228,\n",
       "           0.73171641],\n",
       "          [0.60014651, 0.60643605, 1.        , ..., 0.59618382, 0.58791071,\n",
       "           0.76629598],\n",
       "          [0.58902281, 0.59554343, 1.        , ..., 0.67036768, 0.55228011,\n",
       "           0.73400407],\n",
       "          ...,\n",
       "          [0.54664445, 0.66838812, 0.27729275, ..., 0.66131985, 0.89309273,\n",
       "           0.88705937],\n",
       "          [0.5674523 , 0.66304074, 0.38664433, ..., 0.70451768, 0.61047918,\n",
       "           0.74746245],\n",
       "          [0.63392399, 0.53901422, 0.70330045, ..., 0.91914298, 0.64021765,\n",
       "           0.77160144]]),\n",
       "   12: array([[0.46056497, 1.        , 1.        , ..., 0.34457547, 0.71584514,\n",
       "           0.24961948],\n",
       "          [0.6503349 , 0.61016859, 0.37922431, ..., 0.08065278, 0.36959883,\n",
       "           0.43162468],\n",
       "          [0.40969554, 0.73827618, 0.3646321 , ..., 0.16380037, 0.25349741,\n",
       "           0.47938375],\n",
       "          ...,\n",
       "          [0.27901839, 0.28580649, 0.82861001, ..., 0.41927344, 0.06116291,\n",
       "           0.26052454],\n",
       "          [0.42929193, 0.43352306, 0.71623602, ..., 0.3238635 , 0.        ,\n",
       "           0.19688096],\n",
       "          [0.62368491, 0.38870421, 0.52835643, ..., 0.28586667, 0.23508258,\n",
       "           0.66350898]]),\n",
       "   13: array([[0.        , 1.        , 1.        , ..., 1.        , 0.62770062,\n",
       "           0.69904629],\n",
       "          [0.0993764 , 1.        , 1.        , ..., 0.70726351, 0.50774078,\n",
       "           0.39945031],\n",
       "          [0.07281698, 1.        , 1.        , ..., 0.7911273 , 0.26105261,\n",
       "           0.26002067],\n",
       "          ...,\n",
       "          [0.        , 1.        , 0.49562186, ..., 0.51223685, 0.42426865,\n",
       "           0.55747498],\n",
       "          [0.1880962 , 1.        , 1.        , ..., 0.82283769, 0.633226  ,\n",
       "           0.63569166],\n",
       "          [0.11320707, 1.        , 1.        , ..., 0.83757913, 0.63326045,\n",
       "           0.46296506]]),\n",
       "   14: array([[0.        , 0.        , 0.23359057, ..., 0.49075644, 0.62164359,\n",
       "           0.66341126],\n",
       "          [0.        , 0.        , 0.72740583, ..., 0.64520468, 0.78452571,\n",
       "           0.70571788],\n",
       "          [0.        , 0.        , 1.        , ..., 0.75234153, 0.64357312,\n",
       "           0.73570076],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.45761009, ..., 1.        , 0.73706906,\n",
       "           0.95456886],\n",
       "          [0.        , 0.        , 0.72737895, ..., 0.79405915, 0.46127223,\n",
       "           0.7251166 ],\n",
       "          [0.        , 0.        , 0.60044789, ..., 0.47351375, 0.31571398,\n",
       "           0.46043972]]),\n",
       "   15: array([[0.85084079, 0.        , 1.        , ..., 0.3424438 , 0.84518184,\n",
       "           0.98102609],\n",
       "          [0.88847277, 0.        , 1.        , ..., 0.7991281 , 0.91425147,\n",
       "           0.87320886],\n",
       "          [1.        , 0.        , 1.        , ..., 0.5022591 , 0.87442879,\n",
       "           0.69121207],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.15654384, ..., 0.21924571, 0.7524372 ,\n",
       "           0.7548867 ],\n",
       "          [1.        , 0.        , 1.        , ..., 0.        , 0.71051574,\n",
       "           0.71059068],\n",
       "          [1.        , 0.        , 1.        , ..., 0.        , 0.68547108,\n",
       "           0.66352597]]),\n",
       "   16: array([[0.        , 0.        , 0.        , ..., 0.41278614, 0.16169807,\n",
       "           0.58528314],\n",
       "          [0.        , 0.        , 0.94033914, ..., 0.        , 0.        ,\n",
       "           0.09732581],\n",
       "          [0.        , 0.        , 1.        , ..., 1.        , 0.0418536 ,\n",
       "           0.42315637],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.66333112, 0.6016064 ,\n",
       "           0.40822509],\n",
       "          [0.        , 0.        , 0.06720036, ..., 0.67235198, 0.64003638,\n",
       "           0.49167523],\n",
       "          [0.        , 0.        , 0.09675635, ..., 0.64244025, 0.65609183,\n",
       "           0.5903756 ]]),\n",
       "   17: array([[0.        , 0.        , 1.        , ..., 0.        , 0.3833491 ,\n",
       "           0.72039451],\n",
       "          [0.        , 0.        , 1.        , ..., 0.39547894, 0.68560919,\n",
       "           0.66524896],\n",
       "          [0.        , 0.        , 1.        , ..., 0.60968925, 0.70477285,\n",
       "           1.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.23964154, 0.63928186,\n",
       "           0.95900424],\n",
       "          [0.        , 0.        , 1.        , ..., 0.57863337, 0.44594527,\n",
       "           0.85882052],\n",
       "          [0.        , 0.        , 1.        , ..., 0.73313458, 0.47703325,\n",
       "           0.80060777]]),\n",
       "   18: array([[0.        , 0.81740454, 0.8609545 , ..., 0.59017801, 0.78313873,\n",
       "           0.81234433],\n",
       "          [0.        , 0.24475054, 0.57702508, ..., 0.87685311, 0.63040356,\n",
       "           0.81185615],\n",
       "          [0.14140479, 0.4160175 , 0.35514931, ..., 0.72510378, 0.54744599,\n",
       "           0.73496934],\n",
       "          ...,\n",
       "          [0.16825131, 0.26665447, 0.1798383 , ..., 0.59206131, 0.82859575,\n",
       "           0.60779496],\n",
       "          [0.17906018, 0.54769112, 0.06281988, ..., 0.51400526, 0.7233428 ,\n",
       "           0.38478766],\n",
       "          [0.06324477, 0.51831499, 0.354821  , ..., 0.25070218, 0.64515032,\n",
       "           0.47878487]])},\n",
       "  'fitted_feature_scaler': MinMaxScaler(clip=True)},\n",
       " 12: {'profile_windows': array([[1.        , 0.66823593, 0.9836247 , ..., 0.58493188, 0.80204945,\n",
       "          0.43579572],\n",
       "         [0.21804334, 0.23699849, 0.7684741 , ..., 0.781787  , 0.90404722,\n",
       "          0.80872376],\n",
       "         [0.24592448, 0.26135029, 0.58262199, ..., 0.81017143, 0.74931986,\n",
       "          0.63645273],\n",
       "         ...,\n",
       "         [0.47861566, 0.45916607, 0.43860162, ..., 0.22432589, 0.78947073,\n",
       "          0.87346783],\n",
       "         [0.62617541, 0.38581784, 0.39347039, ..., 0.58594972, 0.83625987,\n",
       "          0.60972188],\n",
       "         [0.65993411, 0.3332819 , 0.48647395, ..., 0.41271787, 0.60521678,\n",
       "          0.22383096]]),\n",
       "  'unknown_users_dict': {0: array([[0.20783221, 0.37316623, 0.40682035, ..., 0.90655532, 0.92402946,\n",
       "           1.        ],\n",
       "          [0.20895436, 0.46660848, 0.35201196, ..., 0.96415774, 0.92123224,\n",
       "           0.7908274 ],\n",
       "          [0.30884561, 0.55472915, 0.17132717, ..., 0.86860493, 0.95185197,\n",
       "           0.83384398],\n",
       "          ...,\n",
       "          [0.        , 0.34724216, 0.        , ..., 0.76948493, 0.53163129,\n",
       "           0.36432932],\n",
       "          [0.0783556 , 0.28984883, 0.05574345, ..., 0.        , 0.50207122,\n",
       "           0.24145245],\n",
       "          [0.11555737, 0.31686502, 0.        , ..., 0.02382475, 0.71005834,\n",
       "           0.62062752]]),\n",
       "   1: array([[1.        , 0.40397372, 0.        , ..., 0.98916358, 1.        ,\n",
       "           1.        ],\n",
       "          [1.        , 0.42797037, 0.        , ..., 0.71178249, 1.        ,\n",
       "           1.        ],\n",
       "          [1.        , 0.32642898, 0.        , ..., 0.74439813, 1.        ,\n",
       "           1.        ],\n",
       "          ...,\n",
       "          [1.        , 0.49203685, 0.        , ..., 0.9860904 , 0.98422801,\n",
       "           1.        ],\n",
       "          [1.        , 0.46355812, 0.        , ..., 0.99394841, 0.93955193,\n",
       "           0.96249818],\n",
       "          [1.        , 0.30955175, 0.        , ..., 0.8261671 , 0.72582377,\n",
       "           0.44711976]]),\n",
       "   2: array([[1.        , 0.        , 1.        , ..., 0.42090058, 0.51004838,\n",
       "           0.09884914],\n",
       "          [1.        , 0.33025707, 1.        , ..., 0.7187807 , 0.96445986,\n",
       "           0.8134671 ],\n",
       "          [0.98273174, 0.64373974, 0.91648131, ..., 0.79611987, 0.91829841,\n",
       "           0.5954112 ],\n",
       "          ...,\n",
       "          [0.16325409, 0.5642716 , 1.        , ..., 0.66786708, 0.8574023 ,\n",
       "           0.73192908],\n",
       "          [0.53831675, 0.51610671, 1.        , ..., 1.        , 1.        ,\n",
       "           1.        ],\n",
       "          [0.91918275, 0.60818128, 1.        , ..., 1.        , 0.93677629,\n",
       "           0.94254207]]),\n",
       "   3: array([[1.        , 0.23583137, 0.        , ..., 0.69454149, 0.95816487,\n",
       "           1.        ],\n",
       "          [1.        , 0.27910467, 0.        , ..., 0.74003941, 1.        ,\n",
       "           0.87357609],\n",
       "          [1.        , 0.2100331 , 0.08750844, ..., 0.81850924, 1.        ,\n",
       "           0.91554771],\n",
       "          ...,\n",
       "          [1.        , 0.13633541, 0.        , ..., 0.48293592, 1.        ,\n",
       "           1.        ],\n",
       "          [1.        , 0.26181348, 0.        , ..., 0.67198434, 1.        ,\n",
       "           0.89577549],\n",
       "          [1.        , 0.1248258 , 0.        , ..., 0.75560886, 0.98352044,\n",
       "           0.42586434]]),\n",
       "   4: array([[1.        , 0.        , 0.67479368, ..., 1.        , 1.        ,\n",
       "           0.69218251],\n",
       "          [1.        , 0.        , 0.37373151, ..., 0.69762749, 0.75440955,\n",
       "           0.70176514],\n",
       "          [1.        , 0.        , 0.48412349, ..., 0.90667496, 0.82934229,\n",
       "           0.74562655],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.        , ..., 0.93140199, 0.88171816,\n",
       "           0.87121116],\n",
       "          [1.        , 0.        , 0.        , ..., 0.85104824, 0.80344766,\n",
       "           0.96770534],\n",
       "          [1.        , 0.        , 0.10784107, ..., 0.86069343, 0.62658087,\n",
       "           0.90645452]]),\n",
       "   5: array([[1.        , 0.        , 0.81067105, ..., 0.46764821, 0.80785227,\n",
       "           0.3794427 ],\n",
       "          [1.        , 0.        , 1.        , ..., 0.47943031, 0.90063705,\n",
       "           0.56679873],\n",
       "          [1.        , 0.        , 1.        , ..., 0.50886912, 0.87308951,\n",
       "           0.48717175],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.51690243, ..., 0.0498672 , 0.91479956,\n",
       "           1.        ],\n",
       "          [1.        , 0.        , 0.56641037, ..., 0.29196108, 1.        ,\n",
       "           0.8990284 ],\n",
       "          [1.        , 0.        , 0.54464933, ..., 0.43835535, 0.96969831,\n",
       "           0.80197078]]),\n",
       "   6: array([[0.        , 0.        , 1.        , ..., 0.        , 0.5175637 ,\n",
       "           0.02351952],\n",
       "          [0.        , 0.        , 1.        , ..., 0.69300048, 0.45771056,\n",
       "           1.        ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.3174649 , 0.63426767,\n",
       "           0.9509653 ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.82903349, 0.82700393,\n",
       "           0.65169501],\n",
       "          [0.        , 0.        , 1.        , ..., 0.45824118, 0.76727876,\n",
       "           0.68244057],\n",
       "          [0.        , 0.        , 1.        , ..., 0.46718719, 0.690798  ,\n",
       "           0.69682778]]),\n",
       "   7: array([[0.        , 0.53251802, 1.        , ..., 0.76710747, 0.83921804,\n",
       "           0.79997376],\n",
       "          [0.40469549, 0.83401783, 1.        , ..., 0.90445709, 0.95082803,\n",
       "           0.88427276],\n",
       "          [0.54748948, 0.71652101, 1.        , ..., 0.65900928, 0.92641052,\n",
       "           1.        ],\n",
       "          ...,\n",
       "          [0.42302425, 0.89465852, 0.9272427 , ..., 0.53536528, 0.78253559,\n",
       "           0.97262015],\n",
       "          [0.44075921, 0.84160478, 0.88853516, ..., 0.81972202, 0.83947364,\n",
       "           1.        ],\n",
       "          [0.24111553, 0.83493294, 0.68181971, ..., 0.76685755, 0.7259437 ,\n",
       "           0.87694955]]),\n",
       "   8: array([[1.        , 0.        , 0.57623531, ..., 0.76129451, 0.87513455,\n",
       "           0.73981396],\n",
       "          [1.        , 0.        , 0.43349598, ..., 0.86332708, 0.68740758,\n",
       "           0.5044417 ],\n",
       "          [1.        , 0.        , 0.48735536, ..., 1.        , 0.80011871,\n",
       "           0.74489725],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.30293374, ..., 0.66461095, 0.83259831,\n",
       "           0.77426918],\n",
       "          [1.        , 0.        , 0.11828656, ..., 0.67470647, 0.81812433,\n",
       "           0.85226389],\n",
       "          [1.        , 0.        , 0.06761534, ..., 0.41838259, 0.67557706,\n",
       "           0.61852711]]),\n",
       "   9: array([[1.        , 1.        , 0.39137692, ..., 0.60597318, 0.98106235,\n",
       "           0.67488978],\n",
       "          [1.        , 0.9756423 , 0.58037397, ..., 0.6236509 , 0.92345901,\n",
       "           0.66122508],\n",
       "          [1.        , 0.66353603, 0.62880879, ..., 0.89305237, 0.91606225,\n",
       "           0.74086644],\n",
       "          ...,\n",
       "          [1.        , 0.74245927, 0.        , ..., 0.        , 0.6779658 ,\n",
       "           0.19190281],\n",
       "          [1.        , 0.7001885 , 0.        , ..., 0.23888603, 0.92214059,\n",
       "           0.58732011],\n",
       "          [0.92942087, 0.50572444, 0.        , ..., 0.33902988, 0.86584427,\n",
       "           0.59247143]]),\n",
       "   10: array([[0.80301412, 0.43531552, 0.33652339, ..., 0.19178614, 0.7505169 ,\n",
       "           0.49753385],\n",
       "          [0.54586343, 0.23225707, 1.        , ..., 0.4455007 , 0.64773431,\n",
       "           0.41998329],\n",
       "          [0.59014828, 0.23793523, 1.        , ..., 1.        , 0.85665794,\n",
       "           0.86913398],\n",
       "          ...,\n",
       "          [0.19664424, 0.1694185 , 0.62724495, ..., 0.54990273, 0.89028611,\n",
       "           0.53820432],\n",
       "          [0.17120775, 0.22589081, 0.56242754, ..., 0.2737749 , 0.68790838,\n",
       "           0.24972455],\n",
       "          [0.1980001 , 0.18228694, 0.5591704 , ..., 0.36018751, 0.81261232,\n",
       "           0.45150513]]),\n",
       "   11: array([[0.78784665, 0.91178977, 1.        , ..., 0.54844356, 0.70288832,\n",
       "           0.68436322],\n",
       "          [0.7066386 , 0.44310818, 0.96831725, ..., 0.60295875, 0.7366636 ,\n",
       "           0.73218405],\n",
       "          [0.69275046, 0.43562343, 0.97655409, ..., 0.68110851, 0.69933445,\n",
       "           0.68752688],\n",
       "          ...,\n",
       "          [0.63984031, 0.48567788, 0.42016626, ..., 0.67157699, 1.        ,\n",
       "           0.89919032],\n",
       "          [0.66581929, 0.48200349, 0.47984771, ..., 0.71708419, 0.76030794,\n",
       "           0.70613875],\n",
       "          [0.7488104 , 0.39677999, 0.65267097, ..., 0.94318349, 0.79146408,\n",
       "           0.73952109]]),\n",
       "   12: array([[0.5323685 , 1.        , 0.92636273, ..., 0.33789928, 0.87069683,\n",
       "           0.01766108],\n",
       "          [0.76929973, 0.44567296, 0.47579805, ..., 0.05986713, 0.50794452,\n",
       "           0.26935996],\n",
       "          [0.46885709, 0.53370072, 0.46783397, ..., 0.14745984, 0.38630839,\n",
       "           0.335407  ],\n",
       "          ...,\n",
       "          [0.3057043 , 0.222791  , 0.7210619 , ..., 0.41659064, 0.18480506,\n",
       "           0.03274191],\n",
       "          [0.49332354, 0.32429287, 0.65973089, ..., 0.31608003, 0.05266808,\n",
       "           0.        ],\n",
       "          [0.73602672, 0.29349608, 0.55719076, ..., 0.27605187, 0.36701571,\n",
       "           0.59003771]]),\n",
       "   13: array([[0.        , 0.73532676, 0.95576027, ..., 1.        , 0.77835034,\n",
       "           0.63918301],\n",
       "          [0.081418  , 1.        , 1.        , ..., 0.71997682, 0.65267187,\n",
       "           0.22486534],\n",
       "          [0.04825807, 1.        , 1.        , ..., 0.80832401, 0.39422376,\n",
       "           0.03204511],\n",
       "          ...,\n",
       "          [0.        , 1.        , 0.53932502, ..., 0.51452392, 0.56522052,\n",
       "           0.44340103],\n",
       "          [0.19218629, 1.        , 0.96156691, ..., 0.84172965, 0.78413913,\n",
       "           0.55156854],\n",
       "          [0.09868584, 1.        , 0.89476049, ..., 0.85725919, 0.78417523,\n",
       "           0.31270124]]),\n",
       "   14: array([[0.        , 0.        , 0.39631467, ..., 0.49189516, 0.77200457,\n",
       "           0.58990257],\n",
       "          [0.        , 0.        , 0.66582711, ..., 0.65460029, 0.94265149,\n",
       "           0.6484093 ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.76746474, 0.79497951,\n",
       "           0.68987327],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.51857911, ..., 1.        , 0.89293252,\n",
       "           0.99255064],\n",
       "          [0.        , 0.        , 0.66581244, ..., 0.8114126 , 0.6039881 ,\n",
       "           0.67523621],\n",
       "          [0.        , 0.        , 0.59653653, ..., 0.47373066, 0.45149091,\n",
       "           0.3092089 ]]),\n",
       "   15: array([[1.        , 0.        , 1.        , ..., 0.33565364, 1.        ,\n",
       "           1.        ],\n",
       "          [1.        , 0.        , 1.        , ..., 0.81675254, 1.        ,\n",
       "           0.88003615],\n",
       "          [1.        , 0.        , 0.82746386, ..., 0.50401275, 1.        ,\n",
       "           0.6283489 ],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.35426443, ..., 0.20586932, 0.90903328,\n",
       "           0.71640591],\n",
       "          [1.        , 0.        , 0.83505898, ..., 0.        , 0.86511337,\n",
       "           0.65514801],\n",
       "          [1.        , 0.        , 0.93571589, ..., 0.        , 0.8388748 ,\n",
       "           0.59006121]]),\n",
       "   16: array([[0.        , 0.        , 0.        , ..., 0.40975653, 0.29013286,\n",
       "           0.48185753],\n",
       "          [0.        , 0.        , 0.78204097, ..., 0.        , 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 1.        , ..., 1.        , 0.16457525,\n",
       "           0.25764895],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.2101329 , ..., 0.67369578, 0.75101219,\n",
       "           0.23700016],\n",
       "          [0.        , 0.        , 0.30550291, ..., 0.6831989 , 0.79127417,\n",
       "           0.35240515],\n",
       "          [0.        , 0.        , 0.32163385, ..., 0.65168808, 0.80809501,\n",
       "           0.4889    ]]),\n",
       "   17: array([[0.        , 0.        , 1.        , ..., 0.        , 0.52235027,\n",
       "           0.66870592],\n",
       "          [0.        , 0.        , 1.        , ..., 0.39152407, 0.8390195 ,\n",
       "           0.59244397],\n",
       "          [0.        , 0.        , 1.        , ..., 0.61718618, 0.85909671,\n",
       "           1.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.22735551, 0.79048368,\n",
       "           0.99868442],\n",
       "          [0.        , 0.        , 1.        , ..., 0.58447004, 0.58793049,\n",
       "           0.86013821],\n",
       "          [0.        , 0.        , 1.        , ..., 0.74723097, 0.62050047,\n",
       "           0.77963456]]),\n",
       "   18: array([[0.        , 0.58807292, 0.73871475, ..., 0.59663187, 0.9411984 ,\n",
       "           0.79586529],\n",
       "          [0.        , 0.19457985, 0.58375293, ..., 0.89863277, 0.78118214,\n",
       "           0.79519018],\n",
       "          [0.1338912 , 0.31226411, 0.46265849, ..., 0.73877084, 0.69426987,\n",
       "           0.68886177],\n",
       "          ...,\n",
       "          [0.16740958, 0.2096309 , 0.36697798, ..., 0.59861584, 0.98882241,\n",
       "           0.51298961],\n",
       "          [0.18090465, 0.40274223, 0.30311215, ..., 0.51638687, 0.87855191,\n",
       "           0.20458804],\n",
       "          [0.036307  , 0.38255673, 0.46247931, ..., 0.23900746, 0.7966319 ,\n",
       "           0.3345788 ]])},\n",
       "  'fitted_feature_scaler': MinMaxScaler(clip=True)},\n",
       " 13: {'profile_windows': array([[0.76754084, 1.        , 1.        , ..., 0.54790014, 0.80621442,\n",
       "          0.86640488],\n",
       "         [0.39718641, 0.35330735, 0.32579537, ..., 0.88002686, 0.77011387,\n",
       "          0.68460535],\n",
       "         [0.42281046, 0.17304328, 0.38110145, ..., 0.62211697, 0.6785938 ,\n",
       "          0.7582744 ],\n",
       "         ...,\n",
       "         [0.62132729, 0.11413235, 0.23587869, ..., 0.1012717 , 0.24554738,\n",
       "          0.26300049],\n",
       "         [0.21011528, 0.1710642 , 0.17307811, ..., 0.34488023, 0.70749827,\n",
       "          0.33459261],\n",
       "         [0.11008693, 0.12831564, 0.16555165, ..., 0.57830356, 0.78500784,\n",
       "          0.52542704]]),\n",
       "  'unknown_users_dict': {0: array([[0.89998411, 0.        , 0.        , ..., 0.91510599, 0.97627213,\n",
       "           1.        ],\n",
       "          [0.90262916, 0.        , 0.        , ..., 0.96952961, 0.97343687,\n",
       "           0.83213046],\n",
       "          [1.        , 0.        , 0.        , ..., 0.8792499 , 1.        ,\n",
       "           0.86673748],\n",
       "          ...,\n",
       "          [0.39827506, 0.        , 0.        , ..., 0.78559985, 0.578538  ,\n",
       "           0.48901097],\n",
       "          [0.59479139, 0.        , 0.        , ..., 0.        , 0.54857596,\n",
       "           0.39015601],\n",
       "          [0.68248065, 0.        , 0.        , ..., 0.08108903, 0.75939136,\n",
       "           0.69520395]]),\n",
       "   1: array([[1.        , 0.        , 0.        , ..., 0.99315551, 1.        ,\n",
       "           1.        ],\n",
       "          [1.        , 0.        , 0.        , ..., 0.73108173, 1.        ,\n",
       "           1.        ],\n",
       "          [1.        , 0.        , 0.        , ..., 0.76189747, 1.        ,\n",
       "           1.        ],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.        , ..., 0.99025192, 1.        ,\n",
       "           1.        ],\n",
       "          [1.        , 0.        , 0.        , ..., 0.99767629, 0.99200567,\n",
       "           0.97024032],\n",
       "          [1.        , 0.        , 0.        , ..., 0.83915401, 0.77537117,\n",
       "           0.55561623]]),\n",
       "   2: array([[1.        , 0.        , 1.        , ..., 0.45625217, 0.5566616 ,\n",
       "           0.27543106],\n",
       "          [1.        , 0.        , 1.        , ..., 0.73769374, 1.        ,\n",
       "           0.85034419],\n",
       "          [1.        , 0.        , 0.29456966, ..., 0.81076493, 0.97046314,\n",
       "           0.67491731],\n",
       "          ...,\n",
       "          [0.79490785, 0.        , 0.44511517, ..., 0.6895898 , 0.90873894,\n",
       "           0.78474651],\n",
       "          [1.        , 0.        , 0.40293636, ..., 1.        , 1.        ,\n",
       "           1.        ],\n",
       "          [1.        , 0.        , 0.48590653, ..., 1.        , 0.98919229,\n",
       "           0.95418555]]),\n",
       "   3: array([[1.        , 0.        , 0.        , ..., 0.71479217, 1.        ,\n",
       "           1.        ],\n",
       "          [1.        , 0.        , 0.        , ..., 0.75777928, 1.        ,\n",
       "           0.89870213],\n",
       "          [1.        , 0.        , 0.        , ..., 0.83191875, 1.        ,\n",
       "           0.93246847],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.        , ..., 0.51486408, 1.        ,\n",
       "           1.        ],\n",
       "          [1.        , 0.        , 0.        , ..., 0.69347985, 1.        ,\n",
       "           0.91656164],\n",
       "          [1.        , 0.        , 0.        , ..., 0.77248954, 1.        ,\n",
       "           0.53851615]]),\n",
       "   4: array([[1.        , 0.        , 0.08719472, ..., 1.        , 1.        ,\n",
       "           0.75277024],\n",
       "          [1.        , 0.        , 0.        , ..., 0.71770788, 0.80434567,\n",
       "           0.7604795 ],\n",
       "          [1.        , 0.        , 0.        , ..., 0.91521903, 0.88029737,\n",
       "           0.79576619],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.        , ..., 0.9385815 , 0.93338546,\n",
       "           0.89679953],\n",
       "          [1.        , 0.        , 0.        , ..., 0.86266208, 0.85405061,\n",
       "           0.9744295 ],\n",
       "          [1.        , 0.        , 0.        , ..., 0.871775  , 0.67477874,\n",
       "           0.92515296]]),\n",
       "   5: array([[1.        , 0.        , 0.20378141, ..., 0.50042003, 0.85851512,\n",
       "           0.50116975],\n",
       "          [1.        , 0.        , 0.48952952, ..., 0.51155194, 0.95256161,\n",
       "           0.65189845],\n",
       "          [1.        , 0.        , 0.46994564, ..., 0.53936616, 0.92463947,\n",
       "           0.58783822],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.        , ..., 0.10569432, 0.96691671,\n",
       "           1.        ],\n",
       "          [1.        , 0.        , 0.        , ..., 0.33442822, 1.        ,\n",
       "           0.91917862],\n",
       "          [1.        , 0.        , 0.        , ..., 0.4727437 , 1.        ,\n",
       "           0.84109536]]),\n",
       "   6: array([[0.        , 0.        , 1.        , ..., 0.        , 0.56427911,\n",
       "           0.21482806],\n",
       "          [0.        , 0.        , 1.        , ..., 0.71333621, 0.50361207,\n",
       "           1.        ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.35852461, 0.68257006,\n",
       "           0.96096207],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.84186221, 0.87792721,\n",
       "           0.72019787],\n",
       "          [0.        , 0.        , 1.        , ..., 0.49153213, 0.81738987,\n",
       "           0.74493281],\n",
       "          [0.        , 0.        , 1.        , ..., 0.49998445, 0.73986911,\n",
       "           0.75650738]]),\n",
       "   7: array([[0.        , 0.        , 0.61660982, ..., 0.78335359, 0.89030742,\n",
       "           0.83948874],\n",
       "          [1.        , 0.        , 0.63913145, ..., 0.91312355, 1.        ,\n",
       "           0.90730764],\n",
       "          [1.        , 0.        , 0.77117541, ..., 0.68122081, 0.97868556,\n",
       "           1.        ],\n",
       "          ...,\n",
       "          [1.        , 0.01946415, 0.30380324, ..., 0.56440013, 0.83285418,\n",
       "           0.97838349],\n",
       "          [1.        , 0.        , 0.27059106, ..., 0.8330646 , 0.89056649,\n",
       "           1.        ],\n",
       "          [0.97843709, 0.        , 0.09322326, ..., 0.78311747, 0.77549273,\n",
       "           0.90141609]]),\n",
       "   8: array([[1.        , 0.        , 0.00262881, ..., 0.77786142, 0.92671233,\n",
       "           0.79108994],\n",
       "          [1.        , 0.        , 0.        , ..., 0.87426331, 0.73643259,\n",
       "           0.60173196],\n",
       "          [1.        , 0.        , 0.        , ..., 1.        , 0.85067639,\n",
       "           0.79517946],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.        , ..., 0.68651336, 0.88359766,\n",
       "           0.8188093 ],\n",
       "          [1.        , 0.        , 0.        , ..., 0.69605176, 0.86892686,\n",
       "           0.88155637],\n",
       "          [1.        , 0.        , 0.        , ..., 0.45387314, 0.72444118,\n",
       "           0.69351415]]),\n",
       "   9: array([[1.        , 0.45498487, 0.        , ..., 0.63111152, 1.        ,\n",
       "           0.73885816],\n",
       "          [1.        , 0.09589424, 0.0061799 , ..., 0.6478137 , 0.97569392,\n",
       "           0.72786485],\n",
       "          [1.        , 0.        , 0.04773836, ..., 0.9023482 , 0.96819658,\n",
       "           0.79193666],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.        , ..., 0.        , 0.72686241,\n",
       "           0.35029312],\n",
       "          [1.        , 0.        , 0.        , ..., 0.28428212, 0.97435757,\n",
       "           0.66840799],\n",
       "          [1.        , 0.        , 0.        , ..., 0.37889952, 0.91729571,\n",
       "           0.67255225]]),\n",
       "   10: array([[1.        , 0.        , 0.        , ..., 0.23978145, 0.80040009,\n",
       "           0.59617457],\n",
       "          [1.        , 0.        , 0.40325299, ..., 0.47949474, 0.69621982,\n",
       "           0.53378482],\n",
       "          [1.        , 0.        , 0.7996777 , ..., 1.        , 0.90798446,\n",
       "           0.89512843],\n",
       "          ...,\n",
       "          [0.87361264, 0.        , 0.04639655, ..., 0.57813532, 0.94206992,\n",
       "           0.62889413],\n",
       "          [0.81365563, 0.        , 0.        , ..., 0.31724565, 0.73694019,\n",
       "           0.39681096],\n",
       "          [0.87680857, 0.        , 0.        , ..., 0.39888956, 0.8633399 ,\n",
       "           0.55914428]]),\n",
       "   11: array([[1.        , 0.03563212, 0.38969777, ..., 0.57675668, 0.75212384,\n",
       "           0.74647958],\n",
       "          [1.        , 0.        , 0.33904638, ..., 0.62826344, 0.7863584 ,\n",
       "           0.78495164],\n",
       "          [1.        , 0.        , 0.34611383, ..., 0.7021005 , 0.74852164,\n",
       "           0.74902476],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.        , ..., 0.69309497, 1.        ,\n",
       "           0.91930888],\n",
       "          [1.        , 0.        , 0.        , ..., 0.73609086, 0.81032427,\n",
       "           0.76399809],\n",
       "          [1.        , 0.        , 0.0682128 , ..., 0.94971283, 0.84190408,\n",
       "           0.79085432]]),\n",
       "   12: array([[1.        , 0.64007436, 0.30304819, ..., 0.37783131, 0.92221426,\n",
       "           0.21011492],\n",
       "          [1.        , 0.        , 0.        , ..., 0.11514241, 0.55452913,\n",
       "           0.41260772],\n",
       "          [1.        , 0.        , 0.        , ..., 0.1979013 , 0.43123895,\n",
       "           0.46574284],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.12689419, ..., 0.45218008, 0.22699552,\n",
       "           0.22224752],\n",
       "          [1.        , 0.        , 0.07427042, ..., 0.35721616, 0.0930617 ,\n",
       "           0.15143984],\n",
       "          [1.        , 0.        , 0.        , ..., 0.31939696, 0.41168392,\n",
       "           0.67059431]]),\n",
       "   13: array([[0.        , 0.        , 0.32827213, ..., 1.        , 0.82861202,\n",
       "           0.71013191],\n",
       "          [0.60200983, 0.169664  , 0.61043352, ..., 0.73882385, 0.70122453,\n",
       "           0.37681162],\n",
       "          [0.5238477 , 0.21367165, 0.71010295, ..., 0.82229559, 0.43926196,\n",
       "           0.22168694],\n",
       "          ...,\n",
       "          [0.30276886, 0.12419281, 0.        , ..., 0.5447089 , 0.61258398,\n",
       "           0.55262449],\n",
       "          [0.86310468, 0.13896971, 0.33325439, ..., 0.85385774, 0.83447952,\n",
       "           0.63964571],\n",
       "          [0.64271231, 0.22046285, 0.27593257, ..., 0.86853028, 0.83451611,\n",
       "           0.44747597]]),\n",
       "   14: array([[0.        , 0.        , 0.        , ..., 0.52332891, 0.82217995,\n",
       "           0.67048559],\n",
       "          [0.        , 0.        , 0.07950115, ..., 0.67705514, 0.99514738,\n",
       "           0.7175545 ],\n",
       "          [0.        , 0.        , 0.66316628, ..., 0.78369114, 0.84546731,\n",
       "           0.75091244],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 1.        , 0.94475232,\n",
       "           0.99441765],\n",
       "          [0.        , 0.        , 0.07948856, ..., 0.82521374, 0.65187874,\n",
       "           0.73913686],\n",
       "          [0.        , 0.        , 0.02004784, ..., 0.50616682, 0.49730784,\n",
       "           0.44466636]]),\n",
       "   15: array([[1.        , 0.        , 0.80495973, ..., 0.3757096 , 1.        ,\n",
       "           1.        ],\n",
       "          [1.        , 0.        , 0.59376955, ..., 0.83025899, 1.        ,\n",
       "           0.90389928],\n",
       "          [1.        , 0.        , 0.21819012, ..., 0.53477779, 1.        ,\n",
       "           0.70141583],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.        , ..., 0.25308745, 0.96107202,\n",
       "           0.77225806],\n",
       "          [1.        , 0.        , 0.22470695, ..., 0.        , 0.91655487,\n",
       "           0.72297582],\n",
       "          [1.        , 0.        , 0.31107347, ..., 0.        , 0.88995951,\n",
       "           0.67061322]]),\n",
       "   16: array([[0.        , 0.        , 0.        , ..., 0.44572311, 0.3337556 ,\n",
       "           0.58356291],\n",
       "          [0.        , 0.        , 0.17921598, ..., 0.        , 0.        ,\n",
       "           0.04067816],\n",
       "          [0.        , 0.        , 0.38849274, ..., 1.        , 0.20649061,\n",
       "           0.40318617],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.69509684, 0.80090211,\n",
       "           0.38657413],\n",
       "          [0.        , 0.        , 0.        , ..., 0.70407552, 0.84171159,\n",
       "           0.47941793],\n",
       "          [0.        , 0.        , 0.        , ..., 0.67430363, 0.85876116,\n",
       "           0.5892286 ]]),\n",
       "   17: array([[0.        , 0.        , 1.        , ..., 0.        , 0.56913078,\n",
       "           0.73388322],\n",
       "          [0.        , 0.        , 1.        , ..., 0.42849681, 0.89010617,\n",
       "           0.67253016],\n",
       "          [0.        , 0.        , 1.        , ..., 0.64170573, 0.9104564 ,\n",
       "           1.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.27338792, 0.84091035,\n",
       "           0.9993523 ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.61079503, 0.63560277,\n",
       "           0.88789129],\n",
       "          [0.        , 0.        , 1.        , ..., 0.76457398, 0.66861565,\n",
       "           0.82312578]]),\n",
       "   18: array([[0.        , 0.        , 0.14204083, ..., 0.62228571, 0.99367453,\n",
       "           0.83618347],\n",
       "          [0.08038143, 0.        , 0.00907914, ..., 0.90762065, 0.83148232,\n",
       "           0.83564034],\n",
       "          [0.7256958 , 0.        , 0.        , ..., 0.75658072, 0.74338819,\n",
       "           0.75009869],\n",
       "          ...,\n",
       "          [0.80470284, 0.        , 0.        , ..., 0.6241602 , 1.        ,\n",
       "           0.6086088 ],\n",
       "          [0.83651244, 0.        , 0.        , ..., 0.54646904, 0.93017616,\n",
       "           0.36049845],\n",
       "          [0.49567751, 0.        , 0.        , ..., 0.28439686, 0.84714217,\n",
       "           0.46507655]])},\n",
       "  'fitted_feature_scaler': MinMaxScaler(clip=True)},\n",
       " 14: {'profile_windows': array([[0.50888926, 0.75855932, 0.8910829 , ..., 0.8031362 , 0.42827863,\n",
       "          0.39669127],\n",
       "         [0.70080256, 0.72043243, 0.71672937, ..., 0.89643454, 0.57409853,\n",
       "          0.43506088],\n",
       "         [0.61866408, 0.7138506 , 0.57067486, ..., 0.75634422, 0.45373755,\n",
       "          0.26122619],\n",
       "         ...,\n",
       "         [0.68709664, 0.67938429, 0.44969723, ..., 0.60970813, 0.79083861,\n",
       "          0.57767152],\n",
       "         [0.55254669, 0.63168078, 0.46437702, ..., 0.46231702, 0.62871947,\n",
       "          0.41527607],\n",
       "         [0.56889199, 0.69961636, 0.70822328, ..., 0.49557881, 0.35424744,\n",
       "          0.26060494]]),\n",
       "  'unknown_users_dict': {0: array([[1.        , 0.95630069, 0.74632378, ..., 0.81198376, 0.7220761 ,\n",
       "           0.78589637],\n",
       "          [1.        , 1.        , 0.7159694 , ..., 0.86488602, 0.71852017,\n",
       "           0.56364092],\n",
       "          [1.        , 1.        , 0.61590125, ..., 0.77712998, 0.75744505,\n",
       "           0.60292169],\n",
       "          ...,\n",
       "          [1.        , 0.94328945, 0.44254435, ..., 0.68609781, 0.22324566,\n",
       "           0.17418241],\n",
       "          [1.        , 0.91448386, 0.55188784, ..., 0.        , 0.18566785,\n",
       "           0.06197689],\n",
       "          [1.        , 0.92804323, 0.45395755, ..., 0.0012808 , 0.45006844,\n",
       "           0.40822219]]),\n",
       "   1: array([[1.        , 0.9717629 , 0.28181879, ..., 0.88785148, 0.94076818,\n",
       "           0.83734164],\n",
       "          [1.        , 0.98380677, 0.12585046, ..., 0.63310368, 0.9233819 ,\n",
       "           0.78672854],\n",
       "          [1.        , 0.93284336, 0.25892597, ..., 0.66305801, 1.        ,\n",
       "           0.793784  ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.88502906, 0.79860262,\n",
       "           0.871866  ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.89224589, 0.74180881,\n",
       "           0.72040281],\n",
       "          [1.        , 0.92437271, 0.        , ..., 0.73815492, 0.47011001,\n",
       "           0.24978286]]),\n",
       "   2: array([[1.        , 0.71880095, 1.        , ..., 0.36595668, 0.1958087 ,\n",
       "           0.        ],\n",
       "          [1.        , 0.93476467, 1.        , ..., 0.63953086, 0.77347265,\n",
       "           0.58431445],\n",
       "          [1.        , 1.        , 1.        , ..., 0.71055943, 0.71479058,\n",
       "           0.3851958 ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        , ..., 0.59277161, 0.63737729,\n",
       "           0.50985767],\n",
       "          [1.        , 1.        , 1.        , ..., 0.90163422, 0.98813536,\n",
       "           0.765974  ],\n",
       "          [1.        , 1.        , 1.        , ..., 0.9702447 , 0.73828032,\n",
       "           0.7021798 ]]),\n",
       "   3: array([[1.        , 0.88737261, 0.495558  , ..., 0.61726948, 0.76547024,\n",
       "           0.83442515],\n",
       "          [1.        , 0.90909139, 0.41086828, ..., 0.65905494, 0.88905476,\n",
       "           0.63920324],\n",
       "          [1.        , 0.87442451, 0.56948016, ..., 0.73112191, 0.82930999,\n",
       "           0.67752979],\n",
       "          ...,\n",
       "          [1.        , 0.83743579, 0.30408179, ..., 0.42293016, 0.92775276,\n",
       "           0.76330715],\n",
       "          [1.        , 0.90041297, 0.1668881 , ..., 0.59655292, 0.88390444,\n",
       "           0.65947471],\n",
       "          [1.        , 0.83165914, 0.20775312, ..., 0.67335398, 0.79770313,\n",
       "           0.23037338]]),\n",
       "   4: array([[1.        , 0.47678552, 0.8947347 , ..., 0.93529983, 0.89213226,\n",
       "           0.47356293],\n",
       "          [1.        , 0.56982378, 0.72799828, ..., 0.62010368, 0.50644926,\n",
       "           0.48231334],\n",
       "          [1.        , 0.44268163, 0.78913636, ..., 0.81209363, 0.60170642,\n",
       "           0.52236558],\n",
       "          ...,\n",
       "          [1.        , 0.40342275, 0.29973446, ..., 0.83480303, 0.66828848,\n",
       "           0.63704369],\n",
       "          [1.        , 0.3810506 , 0.35637405, ..., 0.76100585, 0.56878824,\n",
       "           0.72515775],\n",
       "          [1.        , 0.41247663, 0.58074092, ..., 0.76986403, 0.34394893,\n",
       "           0.66922631]]),\n",
       "   5: array([[1.        , 0.45014449, 0.96998728, ..., 0.40888987, 0.57438754,\n",
       "           0.18798326],\n",
       "          [1.        , 0.40944539, 1.        , ..., 0.4197106 , 0.69233884,\n",
       "           0.3590682 ],\n",
       "          [1.        , 0.40250809, 1.        , ..., 0.44674731, 0.65731942,\n",
       "           0.2863565 ],\n",
       "          ...,\n",
       "          [1.        , 0.45152577, 0.80729023, ..., 0.02519828, 0.71034272,\n",
       "           0.76099709],\n",
       "          [1.        , 0.48968873, 0.83470907, ..., 0.24753818, 1.        ,\n",
       "           0.66244512],\n",
       "          [1.        , 0.44744953, 0.82265721, ..., 0.3819872 , 0.78013196,\n",
       "           0.57381655]]),\n",
       "   6: array([[0.        , 0.        , 1.        , ..., 0.        , 0.20536244,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.61585422, 0.12927501,\n",
       "           0.83731488],\n",
       "          [0.        , 0.        , 1.        , ..., 0.27096098, 0.35372065,\n",
       "           0.70987151],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.74078742, 0.5987338 ,\n",
       "           0.43659159],\n",
       "          [0.        , 0.        , 1.        , ..., 0.40025043, 0.52280906,\n",
       "           0.46466704],\n",
       "          [0.        , 0.        , 1.        , ..., 0.40846647, 0.42558402,\n",
       "           0.47780478]]),\n",
       "   7: array([[0.84474503, 1.        , 1.        , ..., 0.68391434, 0.61426082,\n",
       "           0.57199296],\n",
       "          [1.        , 1.        , 1.        , ..., 0.81005673, 0.75614339,\n",
       "           0.64897094],\n",
       "          [1.        , 1.        , 1.        , ..., 0.58463657, 0.72510299,\n",
       "           0.77504732],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        , ..., 0.47108148, 0.54220408,\n",
       "           0.72964573],\n",
       "          [1.        , 1.        , 1.        , ..., 0.73223573, 0.61458575,\n",
       "           0.80950307],\n",
       "          [1.        , 1.        , 0.8986259 , ..., 0.68368482, 0.47026247,\n",
       "           0.64228372]]),\n",
       "   8: array([[1.        , 0.38601458, 0.84015039, ..., 0.6785757 , 0.65991916,\n",
       "           0.51705779],\n",
       "          [1.        , 0.43488832, 0.76109747, ..., 0.77228278, 0.42127399,\n",
       "           0.30212662],\n",
       "          [1.        , 0.41327839, 0.79092626, ..., 0.89907553, 0.56455636,\n",
       "           0.52169961],\n",
       "          ...,\n",
       "          [1.        , 0.37797896, 0.68878855, ..., 0.58978117, 0.60584558,\n",
       "           0.54852071],\n",
       "          [1.        , 0.40590597, 0.58652592, ..., 0.59905293, 0.58744575,\n",
       "           0.61974191],\n",
       "          [1.        , 0.48514103, 0.55846281, ..., 0.36364415, 0.40623461,\n",
       "           0.40630419]]),\n",
       "   9: array([[1.        , 1.        , 0.73777079, ..., 0.53592802, 0.79457832,\n",
       "           0.457772  ],\n",
       "          [1.        , 1.        , 0.84244249, ..., 0.55216331, 0.72135092,\n",
       "           0.44529402],\n",
       "          [1.        , 1.        , 0.86926701, ..., 0.79958259, 0.7119479 ,\n",
       "           0.51801886],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.428321  , ..., 0.        , 0.40927126,\n",
       "           0.01673043],\n",
       "          [1.        , 1.        , 0.42891608, ..., 0.19879385, 0.71967489,\n",
       "           0.37780739],\n",
       "          [1.        , 1.        , 0.41058622, ..., 0.29076633, 0.64810902,\n",
       "           0.38251134]]),\n",
       "   10: array([[1.        , 0.98749329, 0.70739141, ..., 0.15553715, 0.50150079,\n",
       "           0.29581869],\n",
       "          [1.        , 0.88557867, 1.        , ..., 0.38854952, 0.37083992,\n",
       "           0.22500307],\n",
       "          [1.        , 0.88842853, 1.        , ..., 0.94884703, 0.63643103,\n",
       "           0.6351469 ],\n",
       "          ...,\n",
       "          [1.        , 0.85404013, 0.86840092, ..., 0.48443272, 0.67918036,\n",
       "           0.3329571 ],\n",
       "          [1.        , 0.88238346, 0.83250327, ..., 0.23083592, 0.42191062,\n",
       "           0.0695306 ],\n",
       "          [1.        , 0.86049877, 0.83069938, ..., 0.31019757, 0.58043869,\n",
       "           0.25378738]]),\n",
       "   11: array([[1.        , 1.        , 1.        , ..., 0.48309261, 0.44095366,\n",
       "           0.46642271],\n",
       "          [1.        , 0.99140441, 1.        , ..., 0.53315956, 0.48388998,\n",
       "           0.5100905 ],\n",
       "          [1.        , 0.98764783, 1.        , ..., 0.60493259, 0.43643585,\n",
       "           0.46931162],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.75371511, ..., 0.5961788 , 0.89034264,\n",
       "           0.66259297],\n",
       "          [1.        , 1.        , 0.78676832, ..., 0.63797279, 0.51394751,\n",
       "           0.48630713],\n",
       "          [1.        , 0.96815238, 0.88248254, ..., 0.84562319, 0.55355429,\n",
       "           0.51679035]]),\n",
       "   12: array([[1.        , 1.        , 1.        , ..., 0.28972798, 0.65427777,\n",
       "           0.        ],\n",
       "          [1.        , 0.99269166, 0.7845255 , ..., 0.03438225, 0.1931342 ,\n",
       "           0.08746075],\n",
       "          [1.        , 1.        , 0.78011478, ..., 0.11482771, 0.03850605,\n",
       "           0.14777188],\n",
       "          ...,\n",
       "          [1.        , 0.88082767, 0.9203593 , ..., 0.36199842, 0.        ,\n",
       "           0.        ],\n",
       "          [1.        , 0.93177125, 0.88639252, ..., 0.2696891 , 0.        ,\n",
       "           0.        ],\n",
       "          [1.        , 0.9163144 , 0.829603  , ..., 0.2329271 , 0.0139805 ,\n",
       "           0.38028898]]),\n",
       "   13: array([[0.74316665, 1.        , 1.        , ..., 1.        , 0.53688365,\n",
       "           0.42516621],\n",
       "          [1.        , 1.        , 1.        , ..., 0.64062938, 0.37711673,\n",
       "           0.0468303 ],\n",
       "          [1.        , 1.        , 1.        , ..., 0.72176776, 0.04856834,\n",
       "           0.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.81970847, ..., 0.4519407 , 0.26594547,\n",
       "           0.24638708],\n",
       "          [1.        , 1.        , 1.        , ..., 0.75244763, 0.54424256,\n",
       "           0.3451607 ],\n",
       "          [1.        , 1.        , 1.        , ..., 0.76671001, 0.54428845,\n",
       "           0.12703802]]),\n",
       "   14: array([[0.52459591, 0.60212346, 0.74050545, ..., 0.43115836, 0.52881668,\n",
       "           0.38016557],\n",
       "          [0.72357121, 0.54308976, 0.88976877, ..., 0.58058734, 0.74574908,\n",
       "           0.43359124],\n",
       "          [0.84045771, 0.58710298, 1.        , ..., 0.68424246, 0.55802323,\n",
       "           0.47145424],\n",
       "          ...,\n",
       "          [0.62030875, 0.54203103, 0.80821882, ..., 0.94118238, 0.68254457,\n",
       "           0.74784534],\n",
       "          [0.62844088, 0.45497187, 0.88976064, ..., 0.72460434, 0.3152282 ,\n",
       "           0.45808834],\n",
       "          [0.65116737, 0.526235  , 0.85139376, ..., 0.41447602, 0.12136837,\n",
       "           0.12384898]]),\n",
       "   15: array([[1.        , 0.692463  , 1.        , ..., 0.28766558, 0.82653315,\n",
       "           0.78125607],\n",
       "          [1.        , 0.64075807, 1.        , ..., 0.72950856, 0.91852262,\n",
       "           0.64510227],\n",
       "          [1.        , 0.60358324, 0.9792876 , ..., 0.4422872 , 0.86548531,\n",
       "           0.415273  ],\n",
       "          ...,\n",
       "          [1.        , 0.67966015, 0.71721688, ..., 0.16847119, 0.70301242,\n",
       "           0.49568263],\n",
       "          [1.        , 0.55744531, 0.98349398, ..., 0.        , 0.64717988,\n",
       "           0.43974472],\n",
       "          [1.        , 0.55778808, 1.        , ..., 0.        , 0.61382448,\n",
       "           0.38031044]]),\n",
       "   16: array([[0.        , 0.        , 0.        , ..., 0.35572194, 0.        ,\n",
       "           0.28150379],\n",
       "          [0.62601045, 0.15267846, 0.95413116, ..., 0.        , 0.        ,\n",
       "           0.        ],\n",
       "          [0.6686415 , 0.73780206, 1.        , ..., 0.92469403, 0.        ,\n",
       "           0.0767668 ],\n",
       "          ...,\n",
       "          [0.50306319, 0.46887313, 0.63739292, ..., 0.59812471, 0.50213042,\n",
       "           0.05791127],\n",
       "          [0.48731381, 0.44750637, 0.69021142, ..., 0.60685241, 0.55331288,\n",
       "           0.16329382],\n",
       "          [0.49272999, 0.49390599, 0.69914518, ..., 0.57791275, 0.57469612,\n",
       "           0.28793465]]),\n",
       "   17: array([[0.        , 0.        , 1.        , ..., 0.        , 0.2114473 ,\n",
       "           0.45212518],\n",
       "          [0.        , 0.        , 1.        , ..., 0.33897719, 0.61400843,\n",
       "           0.38248627],\n",
       "          [0.        , 0.        , 1.        , ..., 0.54622609, 0.63953129,\n",
       "           0.82840979],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.18820419, 0.55230798,\n",
       "           0.75344643],\n",
       "          [0.        , 0.        , 1.        , ..., 0.51617947, 0.29481519,\n",
       "           0.62693237],\n",
       "          [0.        , 0.        , 1.        , ..., 0.66565969, 0.33621931,\n",
       "           0.55342014]]),\n",
       "   18: array([[1.        , 1.        , 0.93013592, ..., 0.52734893, 0.74390186,\n",
       "           0.56824129],\n",
       "          [1.        , 0.86666855, 0.84431385, ..., 0.80470766, 0.54048353,\n",
       "           0.56762482],\n",
       "          [1.        , 0.92573404, 0.77724846, ..., 0.65788988, 0.42999758,\n",
       "           0.47053058],\n",
       "          ...,\n",
       "          [1.        , 0.87422264, 0.72425799, ..., 0.52917102, 0.80444318,\n",
       "           0.30993218],\n",
       "          [1.        , 0.97114482, 0.68888736, ..., 0.45365164, 0.66426343,\n",
       "           0.02831401],\n",
       "          [1.        , 0.96101376, 0.77714922, ..., 0.19890538, 0.56012381,\n",
       "           0.14701561]])},\n",
       "  'fitted_feature_scaler': MinMaxScaler(clip=True)},\n",
       " 15: {'profile_windows': array([[0.40400152, 0.71485761, 0.88035014, ..., 0.73786018, 0.34477457,\n",
       "          0.61037357],\n",
       "         [0.13384266, 0.67405725, 0.79805725, ..., 0.70267011, 0.46873248,\n",
       "          0.7018822 ],\n",
       "         [0.6268984 , 0.61085555, 0.41341264, ..., 0.63117422, 0.61174254,\n",
       "          0.56742606],\n",
       "         ...,\n",
       "         [0.65498855, 0.64082285, 0.31345732, ..., 0.57860252, 0.71531085,\n",
       "          1.        ],\n",
       "         [0.35450874, 0.84877036, 0.68407898, ..., 0.57386499, 0.64515625,\n",
       "          0.76780481],\n",
       "         [0.45751459, 0.77586071, 0.52937268, ..., 0.4298979 , 0.82760454,\n",
       "          0.47129006]]),\n",
       "  'unknown_users_dict': {0: array([[0.        , 1.        , 0.81790878, ..., 1.        , 0.49479771,\n",
       "           0.84047848],\n",
       "          [0.        , 1.        , 0.77527761, ..., 1.        , 0.48944818,\n",
       "           0.50578298],\n",
       "          [0.        , 1.        , 0.63473702, ..., 0.98376757, 0.54800668,\n",
       "           0.56493606],\n",
       "          ...,\n",
       "          [0.        , 1.        , 0.39126613, ..., 0.90985113, 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 1.        , 0.54483346, ..., 0.28854561, 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 1.        , 0.40729539, ..., 0.35379235, 0.08559008,\n",
       "           0.27173719]]),\n",
       "   1: array([[1.        , 1.        , 0.16553531, ..., 1.        , 0.82379752,\n",
       "           0.91795015],\n",
       "          [1.        , 1.        , 0.        , ..., 0.86682088, 0.79764165,\n",
       "           0.84173165],\n",
       "          [1.        , 1.        , 0.13338352, ..., 0.89114323, 0.91554591,\n",
       "           0.8523565 ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 1.        , 0.60992401,\n",
       "           0.96994054],\n",
       "          [1.        , 1.        , 0.        , ..., 1.        , 0.52448355,\n",
       "           0.74185142],\n",
       "          [1.        , 1.        , 0.        , ..., 0.95212055, 0.11574057,\n",
       "           0.03314267]]),\n",
       "   2: array([[0.86559823, 1.        , 1.        , ..., 0.64990244, 0.        ,\n",
       "           0.        ],\n",
       "          [0.0618396 , 1.        , 1.        , ..., 0.87203962, 0.57211856,\n",
       "           0.53691534],\n",
       "          [0.        , 1.        , 1.        , ..., 0.92971352, 0.4838374 ,\n",
       "           0.23706164],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.83407198, 0.36737704,\n",
       "           0.42479053],\n",
       "          [0.        , 1.        , 1.        , ..., 1.        , 0.89505658,\n",
       "           0.81047729],\n",
       "          [0.        , 1.        , 1.        , ..., 1.        , 0.5191753 ,\n",
       "           0.71440932]]),\n",
       "   3: array([[1.        , 1.        , 0.46572108, ..., 0.8539638 , 0.56007974,\n",
       "           0.91355819],\n",
       "          [1.        , 1.        , 0.34677871, ..., 0.88789282, 0.74599999,\n",
       "           0.61957261],\n",
       "          [1.        , 1.        , 0.56954097, ..., 0.94640987, 0.6561201 ,\n",
       "           0.67728875],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.19680255, ..., 0.69616385, 0.80421715,\n",
       "           0.80646126],\n",
       "          [1.        , 1.        , 0.00412104, ..., 0.83714233, 0.73825185,\n",
       "           0.65009951],\n",
       "          [1.        , 1.        , 0.06151387, ..., 0.89950337, 0.60857083,\n",
       "           0.00391385]]),\n",
       "   4: array([[0.87056248, 0.76929546, 1.        , ..., 1.        , 0.75062976,\n",
       "           0.37013412],\n",
       "          [1.        , 1.        , 0.79217155, ..., 0.85626511, 0.17040925,\n",
       "           0.3833114 ],\n",
       "          [0.92882634, 0.67899952, 0.87803686, ..., 1.        , 0.31371388,\n",
       "           0.44362625],\n",
       "          ...,\n",
       "          [0.78119633, 0.57505488, 0.19069695, ..., 1.        , 0.41387976,\n",
       "           0.61632054],\n",
       "          [0.72660614, 0.51582075, 0.27024435, ..., 0.97067507, 0.26419185,\n",
       "           0.74901191],\n",
       "          [0.61645631, 0.59902659, 0.58535613, ..., 0.97786775, 0.        ,\n",
       "           0.6647845 ]]),\n",
       "   5: array([[0.0427325 , 0.69875874, 1.        , ..., 0.6847634 , 0.27261542,\n",
       "           0.        ],\n",
       "          [0.34529092, 0.59100084, 1.        , ..., 0.69354963, 0.45006105,\n",
       "           0.19771598],\n",
       "          [0.31995768, 0.57263316, 1.        , ..., 0.71550294, 0.39737794,\n",
       "           0.08821908],\n",
       "          ...,\n",
       "          [0.25001392, 0.70241592, 0.90353304, ..., 0.3732129 , 0.47714605,\n",
       "           0.80298254],\n",
       "          [0.30290113, 0.80345894, 0.94204141, ..., 0.5537488 , 0.94677959,\n",
       "           0.65457267],\n",
       "          [0.        , 0.69162336, 0.92511519, ..., 0.66291893, 0.58213679,\n",
       "           0.5211065 ]]),\n",
       "   6: array([[0.        , 0.        , 1.        , ..., 0.25490085, 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.85281463, 0.        ,\n",
       "           0.91790984],\n",
       "          [0.        , 0.        , 1.        , ..., 0.57276768, 0.        ,\n",
       "           0.7259923 ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.95425809, 0.30924188,\n",
       "           0.31445881],\n",
       "          [0.        , 0.        , 1.        , ..., 0.67774833, 0.19502088,\n",
       "           0.35673775],\n",
       "          [0.        , 0.        , 1.        , ..., 0.6844196 , 0.04875578,\n",
       "           0.37652193]]),\n",
       "   7: array([[0.        , 1.        , 1.        , ..., 0.90807819, 0.33260069,\n",
       "           0.51836034],\n",
       "          [0.        , 1.        , 1.        , ..., 1.        , 0.54604846,\n",
       "           0.63428185],\n",
       "          [0.        , 1.        , 1.        , ..., 0.82746648, 0.49935136,\n",
       "           0.82414085],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.73526183, 0.22419871,\n",
       "           0.75577038],\n",
       "          [0.        , 1.        , 1.        , ..., 0.94731427, 0.33308951,\n",
       "           0.87602792],\n",
       "          [0.        , 1.        , 1.        , ..., 0.90789182, 0.11596992,\n",
       "           0.62421154]]),\n",
       "   8: array([[0.53698156, 0.52896374, 0.94968345, ..., 0.90374331, 0.40128897,\n",
       "           0.43563322],\n",
       "          [0.44165158, 0.65836539, 0.83865767, ..., 0.97983174, 0.04227179,\n",
       "           0.11196739],\n",
       "          [0.54219273, 0.60114937, 0.88055068, ..., 1.        , 0.25782542,\n",
       "           0.44262337],\n",
       "          ...,\n",
       "          [0.47116426, 0.50768805, 0.7371035 , ..., 0.83164379, 0.31994082,\n",
       "           0.48301339],\n",
       "          [0.72667045, 0.58162962, 0.59348087, ..., 0.8391723 , 0.29226016,\n",
       "           0.59026571],\n",
       "          [0.7412034 , 0.79141811, 0.55406768, ..., 0.64802471, 0.01964658,\n",
       "           0.26884887]]),\n",
       "   9: array([[0.91157223, 1.        , 0.80589655, ..., 0.78791603, 0.60386987,\n",
       "           0.34635448],\n",
       "          [0.21158216, 1.        , 0.9529026 , ..., 0.80109879, 0.49370676,\n",
       "           0.32756384],\n",
       "          [0.0338698 , 1.        , 0.99057626, ..., 1.        , 0.47956088,\n",
       "           0.43708051],\n",
       "          ...,\n",
       "          [0.13139969, 1.        , 0.37129016, ..., 0.24238952, 0.02421492,\n",
       "           0.        ],\n",
       "          [0.08869463, 1.        , 0.37212593, ..., 0.51416931, 0.49118535,\n",
       "           0.22593541],\n",
       "          [0.        , 1.        , 0.34638257, ..., 0.58884926, 0.38352183,\n",
       "           0.2330191 ]]),\n",
       "   10: array([[0.        , 1.        , 0.76323027, ..., 0.47904566, 0.16296478,\n",
       "           0.10246825],\n",
       "          [0.        , 1.        , 1.        , ..., 0.66824741, 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 1.        , 1.        , ..., 1.        , 0.3659535 ,\n",
       "           0.61346416],\n",
       "          ...,\n",
       "          [0.        , 1.        , 0.98935988, ..., 0.7461028 , 0.43026547,\n",
       "           0.15839515],\n",
       "          [0.        , 1.        , 0.93894347, ..., 0.54018687, 0.04322953,\n",
       "           0.        ],\n",
       "          [0.        , 1.        , 0.93641   , ..., 0.60462707, 0.28171875,\n",
       "           0.0391731 ]]),\n",
       "   11: array([[0.        , 1.        , 1.        , ..., 0.74501465, 0.07187783,\n",
       "           0.35938162],\n",
       "          [0.        , 1.        , 1.        , ..., 0.78566809, 0.13647113,\n",
       "           0.42514115],\n",
       "          [0.        , 1.        , 1.        , ..., 0.84394647, 0.06508125,\n",
       "           0.36373205],\n",
       "          ...,\n",
       "          [0.        , 1.        , 0.82828952, ..., 0.83683855, 0.74793746,\n",
       "           0.65479533],\n",
       "          [0.        , 1.        , 0.87471107, ..., 0.8707745 , 0.18168959,\n",
       "           0.38932566],\n",
       "          [0.        , 1.        , 1.        , ..., 1.        , 0.24127394,\n",
       "           0.43523048]]),\n",
       "   12: array([[0.        , 1.        , 1.        , ..., 0.58800614, 0.39280208,\n",
       "           0.        ],\n",
       "          [0.        , 1.        , 0.87156115, ..., 0.38067012, 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 1.        , 0.86536651, ..., 0.44599035, 0.        ,\n",
       "           0.        ],\n",
       "          ...,\n",
       "          [0.        , 1.        , 1.        , ..., 0.6466884 , 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 1.        , 1.        , ..., 0.57173494, 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 1.        , 0.93487019, ..., 0.54188488, 0.        ,\n",
       "           0.22967244]]),\n",
       "   13: array([[0.        , 1.        , 1.        , ..., 1.        , 0.21619467,\n",
       "           0.29725328],\n",
       "          [0.        , 1.        , 1.        , ..., 0.8729316 , 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 1.        , 1.        , ..., 0.93881447, 0.        ,\n",
       "           0.        ],\n",
       "          ...,\n",
       "          [0.        , 1.        , 0.92097382, ..., 0.71971987, 0.        ,\n",
       "           0.02802895],\n",
       "          [0.        , 1.        , 1.        , ..., 0.96372596, 0.2272654 ,\n",
       "           0.1767726 ],\n",
       "          [0.        , 1.        , 1.        , ..., 0.97530675, 0.22733442,\n",
       "           0.        ]]),\n",
       "   14: array([[0.        , 1.        , 0.80973723, ..., 0.702845  , 0.20405874,\n",
       "           0.2294866 ],\n",
       "          [0.        , 0.94484737, 1.        , ..., 0.82417858, 0.53041129,\n",
       "           0.30994056],\n",
       "          [0.        , 1.        , 1.        , ..., 0.90834462, 0.247997  ,\n",
       "           0.36695863],\n",
       "          ...,\n",
       "          [0.        , 0.94204421, 0.9048372 , ..., 1.        , 0.43532658,\n",
       "           0.78317726],\n",
       "          [0.        , 0.71154006, 1.        , ..., 0.94111772, 0.        ,\n",
       "           0.34683086],\n",
       "          [0.        , 0.90022149, 0.9654742 , ..., 0.68929925, 0.        ,\n",
       "           0.        ]]),\n",
       "   15: array([[0.        , 1.        , 1.        , ..., 0.58633151, 0.65194263,\n",
       "           0.83349063],\n",
       "          [0.        , 1.        , 1.        , ..., 0.94509985, 0.79033136,\n",
       "           0.62845599],\n",
       "          [0.48675   , 1.        , 1.        , ..., 0.71188141, 0.71054216,\n",
       "           0.28235504],\n",
       "          ...,\n",
       "          [0.48917392, 1.        , 0.77702963, ..., 0.48954786, 0.46611837,\n",
       "           0.40344427],\n",
       "          [0.46539076, 0.98285618, 1.        , ..., 0.08341035, 0.38212402,\n",
       "           0.31920711],\n",
       "          [0.3767473 , 0.98376372, 1.        , ..., 0.12830927, 0.33194426,\n",
       "           0.22970476]]),\n",
       "   16: array([[0.        , 0.        , 0.        , ..., 0.64159202, 0.        ,\n",
       "           0.08091137],\n",
       "          [0.        , 0.        , 1.        , ..., 0.1610071 , 0.        ,\n",
       "           0.        ],\n",
       "          [0.        , 1.        , 1.        , ..., 1.        , 0.        ,\n",
       "           0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.74834604, 0.66492096, ..., 0.83841859, 0.16391199,\n",
       "           0.        ],\n",
       "          [0.        , 0.69177387, 0.73910185, ..., 0.84550532, 0.24091075,\n",
       "           0.        ],\n",
       "          [0.        , 0.81462488, 0.75164886, ..., 0.82200686, 0.27307965,\n",
       "           0.09059564]]),\n",
       "   17: array([[0.        , 0.        , 1.        , ..., 0.27541457, 0.        ,\n",
       "           0.33785091],\n",
       "          [0.        , 0.        , 1.        , ..., 0.62799559, 0.33222098,\n",
       "           0.23298135],\n",
       "          [0.        , 0.        , 1.        , ..., 0.79627787, 0.37061751,\n",
       "           0.90449963],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.5055707 , 0.23939898,\n",
       "           0.79161196],\n",
       "          [0.        , 0.        , 1.        , ..., 0.77188057, 0.        ,\n",
       "           0.60109387],\n",
       "          [0.        , 0.        , 1.        , ..., 0.89325576, 0.        ,\n",
       "           0.49039145]]),\n",
       "   18: array([[0.        , 1.        , 1.        , ..., 0.78094997, 0.52763233,\n",
       "           0.5127107 ],\n",
       "          [0.        , 1.        , 0.95553083, ..., 1.        , 0.22161031,\n",
       "           0.51178234],\n",
       "          [0.        , 1.        , 0.86134092, ..., 0.88694681, 0.05539553,\n",
       "           0.36556769],\n",
       "          ...,\n",
       "          [0.        , 1.        , 0.78691851, ..., 0.78242947, 0.61871055,\n",
       "           0.12372182],\n",
       "          [0.        , 1.        , 0.73724227, ..., 0.72110912, 0.40782448,\n",
       "           0.        ],\n",
       "          [0.        , 1.        , 0.86120154, ..., 0.51425987, 0.25115711,\n",
       "           0.        ]])},\n",
       "  'fitted_feature_scaler': MinMaxScaler(clip=True)},\n",
       " 16: {'profile_windows': array([[0.32955842, 0.        , 0.32129585, ..., 0.62025073, 0.75866064,\n",
       "          0.63395083],\n",
       "         [0.21549152, 0.99176559, 0.6455232 , ..., 0.70571315, 0.69439537,\n",
       "          0.62528044],\n",
       "         [0.38448438, 0.96026066, 0.29475791, ..., 0.69705642, 0.45119884,\n",
       "          0.46184971],\n",
       "         ...,\n",
       "         [0.20573757, 0.74523009, 0.44758271, ..., 0.9799458 , 0.72641082,\n",
       "          0.81707262],\n",
       "         [0.0941839 , 0.74022408, 0.51849055, ..., 0.951708  , 0.80144561,\n",
       "          1.        ],\n",
       "         [0.23313327, 0.78361735, 0.37453976, ..., 0.58104298, 0.60491855,\n",
       "          0.68866248]]),\n",
       "  'unknown_users_dict': {0: array([[1.        , 1.        , 1.        , ..., 0.83308082, 0.74485675,\n",
       "           0.91782255],\n",
       "          [1.        , 1.        , 1.        , ..., 0.87693584, 0.74101137,\n",
       "           0.69883541],\n",
       "          [1.        , 1.        , 1.        , ..., 0.80418767, 0.78310475,\n",
       "           0.73753854],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.68465375, ..., 0.72872364, 0.20542139,\n",
       "           0.31510395],\n",
       "          [1.        , 1.        , 1.        , ..., 0.0944099 , 0.16478474,\n",
       "           0.20454842],\n",
       "          [1.        , 1.        , 0.73462106, ..., 0.1610227 , 0.4507076 ,\n",
       "           0.54570213]]),\n",
       "   1: array([[1.        , 1.        , 0.        , ..., 0.8959738 , 0.98135041,\n",
       "           0.96851131],\n",
       "          [1.        , 1.        , 0.        , ..., 0.68479247, 0.96254889,\n",
       "           0.91864248],\n",
       "          [1.        , 1.        , 0.        , ..., 0.70962406, 1.        ,\n",
       "           0.92559419],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.89363406, 0.82761255,\n",
       "           1.        ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.89961668, 0.76619571,\n",
       "           0.85329208],\n",
       "          [1.        , 1.        , 0.        , ..., 0.77187805, 0.47238056,\n",
       "           0.38959267]]),\n",
       "   2: array([[1.        , 1.        , 1.        , ..., 0.46333243, 0.17575106,\n",
       "           0.07624451],\n",
       "          [1.        , 1.        , 1.        , ..., 0.69012048, 0.800437  ,\n",
       "           0.71920493],\n",
       "          [1.        , 1.        , 1.        , ..., 0.74900189, 0.73697819,\n",
       "           0.52301435],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        , ..., 0.65135791, 0.65326343,\n",
       "           0.64584305],\n",
       "          [1.        , 1.        , 1.        , ..., 0.90739944, 1.        ,\n",
       "           0.89819314],\n",
       "          [1.        , 1.        , 1.        , ..., 0.96427629, 0.76238   ,\n",
       "           0.83533705]]),\n",
       "   3: array([[1.        , 1.        , 0.91674898, ..., 0.6716662 , 0.79178318,\n",
       "           0.96563771],\n",
       "          [1.        , 1.        , 0.54597503, ..., 0.70630559, 0.92542752,\n",
       "           0.77328656],\n",
       "          [1.        , 1.        , 1.        , ..., 0.76604781, 0.8608195 ,\n",
       "           0.81104952],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.07846076, ..., 0.51056241, 0.96727554,\n",
       "           0.8955655 ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.65449255, 0.91985795,\n",
       "           0.79325994],\n",
       "          [1.        , 1.        , 0.        , ..., 0.71815924, 0.82663984,\n",
       "           0.37046862]]),\n",
       "   4: array([[1.        , 0.49410075, 1.        , ..., 0.93530762, 0.92875552,\n",
       "           0.61008203],\n",
       "          [1.        , 0.79039862, 1.        , ..., 0.6740157 , 0.51167784,\n",
       "           0.61870376],\n",
       "          [1.        , 0.38549049, 1.        , ..., 0.8331719 , 0.61468895,\n",
       "           0.65816702],\n",
       "          ...,\n",
       "          [1.        , 0.2604632 , 0.05942804, ..., 0.85199759, 0.6866908 ,\n",
       "           0.77115877],\n",
       "          [1.        , 0.18921489, 0.30739774, ..., 0.79082106, 0.57909123,\n",
       "           0.8579771 ],\n",
       "          [1.        , 0.289297  , 1.        , ..., 0.79816433, 0.33594994,\n",
       "           0.80286814]]),\n",
       "   5: array([[1.        , 0.40925738, 1.        , ..., 0.49892327, 0.58514631,\n",
       "           0.32870185],\n",
       "          [1.        , 0.27964342, 1.        , ..., 0.50789346, 0.71269887,\n",
       "           0.49727096],\n",
       "          [1.        , 0.2575503 , 1.        , ..., 0.5303064 , 0.67482887,\n",
       "           0.42562849],\n",
       "          ...,\n",
       "          [1.        , 0.41365633, 1.        , ..., 0.18084986, 0.73216827,\n",
       "           0.89328942],\n",
       "          [1.        , 0.53519347, 1.        , ..., 0.36516563, 1.        ,\n",
       "           0.79618667],\n",
       "          [1.        , 0.40067476, 1.        , ..., 0.47662144, 0.80763837,\n",
       "           0.7088614 ]]),\n",
       "   6: array([[0.        , 0.        , 1.        , ..., 0.06006072, 0.18608247,\n",
       "           0.00846848],\n",
       "          [0.        , 0.        , 1.        , ..., 0.67049297, 0.10380151,\n",
       "           0.96848494],\n",
       "          [0.        , 0.        , 1.        , ..., 0.3845827 , 0.34651708,\n",
       "           0.84291565],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.77406035, 0.61147436,\n",
       "           0.57365436],\n",
       "          [0.        , 0.        , 1.        , ..., 0.49176132, 0.52936932,\n",
       "           0.60131695],\n",
       "          [0.        , 0.        , 1.        , ..., 0.49857227, 0.42423014,\n",
       "           0.6142615 ]]),\n",
       "   7: array([[1.        , 1.        , 1.        , ..., 0.72691359, 0.62826529,\n",
       "           0.70706462],\n",
       "          [1.        , 1.        , 1.        , ..., 0.83148335, 0.78169713,\n",
       "           0.78291063],\n",
       "          [1.        , 1.        , 1.        , ..., 0.64461411, 0.74813003,\n",
       "           0.90713304],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        , ..., 0.55047899, 0.55034311,\n",
       "           0.86239909],\n",
       "          [1.        , 1.        , 1.        , ..., 0.76697115, 0.62861666,\n",
       "           0.94108211],\n",
       "          [1.        , 1.        , 1.        , ..., 0.72672332, 0.47254543,\n",
       "           0.77632175]]),\n",
       "   8: array([[1.        , 0.20502361, 1.        , ..., 0.72248795, 0.67764022,\n",
       "           0.65293728],\n",
       "          [1.        , 0.36067123, 1.        , ..., 0.80016943, 0.41956928,\n",
       "           0.44116671],\n",
       "          [1.        , 0.29185033, 1.        , ..., 0.90527833, 0.57451487,\n",
       "           0.65751085],\n",
       "          ...,\n",
       "          [1.        , 0.17943266, 1.        , ..., 0.64887889, 0.61916504,\n",
       "           0.68393754],\n",
       "          [1.        , 0.26837147, 1.        , ..., 0.65656501, 0.59926746,\n",
       "           0.75411142],\n",
       "          [1.        , 0.52071044, 1.        , ..., 0.46141538, 0.40330569,\n",
       "           0.54381234]]),\n",
       "   9: array([[1.        , 1.        , 1.        , ..., 0.6042356 , 0.82326067,\n",
       "           0.5945233 ],\n",
       "          [1.        , 1.        , 1.        , ..., 0.61769436, 0.74407254,\n",
       "           0.58222882],\n",
       "          [1.        , 1.        , 1.        , ..., 0.82280048, 0.73390411,\n",
       "           0.65388423],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.62238351, ..., 0.04728744, 0.40658953,\n",
       "           0.15996732],\n",
       "          [1.        , 1.        , 0.62498882, ..., 0.32475746, 0.74226009,\n",
       "           0.51573459],\n",
       "          [1.        , 1.        , 0.54474017, ..., 0.40100098, 0.66486874,\n",
       "           0.52036936]]),\n",
       "   10: array([[1.        , 1.        , 1.        , ..., 0.28889844, 0.50632656,\n",
       "           0.43495154],\n",
       "          [1.        , 1.        , 1.        , ..., 0.48206149, 0.36502986,\n",
       "           0.36517728],\n",
       "          [1.        , 1.        , 1.        , ..., 0.946538  , 0.65224016,\n",
       "           0.76928988],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        , ..., 0.56154693, 0.69846929,\n",
       "           0.47154383],\n",
       "          [1.        , 1.        , 1.        , ..., 0.35131976, 0.42025773,\n",
       "           0.21199105],\n",
       "          [1.        , 1.        , 1.        , ..., 0.41710913, 0.59169002,\n",
       "           0.39353831]]),\n",
       "   11: array([[1.        , 1.        , 1.        , ..., 0.560436  , 0.44085088,\n",
       "           0.6030468 ],\n",
       "          [1.        , 1.        , 1.        , ..., 0.6019406 , 0.48728223,\n",
       "           0.64607245],\n",
       "          [1.        , 1.        , 1.        , ..., 0.66143914, 0.43596531,\n",
       "           0.60589323],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        , ..., 0.65418241, 0.92682022,\n",
       "           0.79633235],\n",
       "          [1.        , 1.        , 1.        , ..., 0.68882887, 0.51978644,\n",
       "           0.62263882],\n",
       "          [1.        , 1.        , 1.        , ..., 0.86096731, 0.56261723,\n",
       "           0.65267378]]),\n",
       "   12: array([[1.        , 1.        , 1.        , ..., 0.40014021, 0.67153962,\n",
       "           0.0031975 ],\n",
       "          [1.        , 1.        , 1.        , ..., 0.18846321, 0.17285885,\n",
       "           0.22965754],\n",
       "          [1.        , 1.        , 1.        , ..., 0.25515105, 0.00564394,\n",
       "           0.28908178],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        , ..., 0.4600511 , 0.        ,\n",
       "           0.01676611],\n",
       "          [1.        , 1.        , 1.        , ..., 0.38352834, 0.        ,\n",
       "           0.        ],\n",
       "          [1.        , 1.        , 1.        , ..., 0.35305331, 0.        ,\n",
       "           0.51817968]]),\n",
       "   13: array([[0.89274559, 1.        , 1.        , ..., 1.        , 0.54458959,\n",
       "           0.56239699],\n",
       "          [1.        , 1.        , 1.        , ..., 0.69103114, 0.3718176 ,\n",
       "           0.18962457],\n",
       "          [1.        , 1.        , 1.        , ..., 0.75829339, 0.0165253 ,\n",
       "           0.01613918],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        , ..., 0.53461162, 0.25159698,\n",
       "           0.38624683],\n",
       "          [1.        , 1.        , 1.        , ..., 0.78372645, 0.55254752,\n",
       "           0.48356797],\n",
       "          [1.        , 1.        , 1.        , ..., 0.7955497 , 0.55259714,\n",
       "           0.26865282]]),\n",
       "   14: array([[0.        , 0.89326305, 1.        , ..., 0.51738344, 0.53586597,\n",
       "           0.51805809],\n",
       "          [0.78120885, 0.70525908, 1.        , ..., 0.64125737, 0.77045672,\n",
       "           0.57069812],\n",
       "          [1.        , 0.8454275 , 1.        , ..., 0.72718559, 0.56744994,\n",
       "           0.60800434],\n",
       "          ...,\n",
       "          [0.19344129, 0.70188737, 1.        , ..., 0.94018415, 0.70210734,\n",
       "           0.88033107],\n",
       "          [0.23972922, 0.42463108, 1.        , ..., 0.76064486, 0.30489133,\n",
       "           0.59483499],\n",
       "          [0.36908786, 0.65158193, 1.        , ..., 0.50355408, 0.09525127,\n",
       "           0.26551067]]),\n",
       "   15: array([[1.        , 1.        , 1.        , ..., 0.39843052, 0.85781663,\n",
       "           0.91325049],\n",
       "          [1.        , 1.        , 1.        , ..., 0.76471037, 0.95729407,\n",
       "           0.77909885],\n",
       "          [1.        , 0.89791197, 1.        , ..., 0.52660905, 0.8999395 ,\n",
       "           0.55264926],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        , ..., 0.29962052, 0.72424129,\n",
       "           0.63187645],\n",
       "          [1.        , 0.75097706, 1.        , ..., 0.        , 0.66386396,\n",
       "           0.57676111],\n",
       "          [1.        , 0.75206867, 1.        , ..., 0.        , 0.62779343,\n",
       "           0.51820083]]),\n",
       "   16: array([[0.        , 0.        , 0.        , ..., 0.45484801, 0.        ,\n",
       "           0.42084715],\n",
       "          [0.22589525, 0.        , 1.        , ..., 0.        , 0.        ,\n",
       "           0.        ],\n",
       "          [0.46855022, 1.        , 1.        , ..., 0.9265156 , 0.        ,\n",
       "           0.21912084],\n",
       "          ...,\n",
       "          [0.        , 0.46890225, 1.        , ..., 0.65579553, 0.50700744,\n",
       "           0.20054259],\n",
       "          [0.        , 0.4008558 , 1.        , ..., 0.66303063, 0.56235616,\n",
       "           0.30437547],\n",
       "          [0.        , 0.54862413, 1.        , ..., 0.63904018, 0.58548   ,\n",
       "           0.42718344]]),\n",
       "   17: array([[0.        , 0.        , 1.        , ..., 0.08100393, 0.19266265,\n",
       "           0.58895952],\n",
       "          [0.        , 0.        , 1.        , ..., 0.44096691, 0.62799234,\n",
       "           0.52034466],\n",
       "          [0.        , 0.        , 1.        , ..., 0.61277251, 0.65559277,\n",
       "           0.9597108 ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.31597882, 0.56126946,\n",
       "           0.88584979],\n",
       "          [0.        , 0.        , 1.        , ..., 0.58786441, 0.2828167 ,\n",
       "           0.76119615],\n",
       "          [0.        , 0.        , 1.        , ..., 0.71178081, 0.32759113,\n",
       "           0.68876492]]),\n",
       "   18: array([[1.        , 1.        , 1.        , ..., 0.59712369, 0.76845913,\n",
       "           0.70336813],\n",
       "          [1.        , 1.        , 1.        , ..., 0.82704907, 0.5484825 ,\n",
       "           0.70276072],\n",
       "          [1.        , 1.        , 1.        , ..., 0.70533978, 0.42900297,\n",
       "           0.60709427],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        , ..., 0.59863417, 0.83392853,\n",
       "           0.44885749],\n",
       "          [1.        , 1.        , 1.        , ..., 0.53602996, 0.68233811,\n",
       "           0.17138056],\n",
       "          [1.        , 1.        , 1.        , ..., 0.32484992, 0.56972151,\n",
       "           0.28833663]])},\n",
       "  'fitted_feature_scaler': MinMaxScaler(clip=True)},\n",
       " 17: {'profile_windows': array([[0.30089426, 0.11405109, 0.83123442, ..., 1.        , 0.80826146,\n",
       "          0.90214643],\n",
       "         [0.50299938, 0.19771961, 0.7582579 , ..., 0.91768994, 0.71064764,\n",
       "          0.81697777],\n",
       "         [0.48708899, 0.21865932, 0.76242761, ..., 0.86706951, 0.79508352,\n",
       "          0.77779394],\n",
       "         ...,\n",
       "         [1.        , 0.        , 0.1673388 , ..., 0.69933288, 0.74749929,\n",
       "          0.72099383],\n",
       "         [0.41149396, 0.2751223 , 0.32159158, ..., 0.71785547, 0.96778384,\n",
       "          0.86667007],\n",
       "         [0.2044758 , 0.38739457, 0.46408641, ..., 0.74307219, 0.81365087,\n",
       "          0.78175801]]),\n",
       "  'unknown_users_dict': {0: array([[1.        , 1.        , 0.        , ..., 0.83130691, 0.85255465,\n",
       "           0.96485185],\n",
       "          [1.        , 1.        , 0.        , ..., 0.87365694, 0.84964151,\n",
       "           0.79709161],\n",
       "          [1.        , 1.        , 0.        , ..., 0.8034053 , 0.88153017,\n",
       "           0.82674105],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.730531  , 0.44389492,\n",
       "           0.50312513],\n",
       "          [1.        , 1.        , 0.        , ..., 0.11798526, 0.41310984,\n",
       "           0.41843148],\n",
       "          [1.        , 1.        , 0.        , ..., 0.18231209, 0.62971626,\n",
       "           0.67978028]]),\n",
       "   1: array([[1.        , 1.        , 0.        , ..., 0.89204157, 1.        ,\n",
       "           1.        ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.68810743, 1.        ,\n",
       "           0.96547997],\n",
       "          [1.        , 1.        , 0.        , ..., 0.71208687, 1.        ,\n",
       "           0.97080549],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.88978212, 0.91524792,\n",
       "           1.        ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.89555943, 0.8687204 ,\n",
       "           0.91541677],\n",
       "          [1.        , 1.        , 0.        , ..., 0.77220446, 0.64613503,\n",
       "           0.56018897]]),\n",
       "   2: array([[1.        , 1.        , 0.1367965 , ..., 0.47424732, 0.42141758,\n",
       "           0.32014126],\n",
       "          [1.        , 1.        , 0.07094405, ..., 0.6932526 , 0.89466055,\n",
       "           0.81269616],\n",
       "          [1.        , 1.        , 0.        , ..., 0.75011335, 0.8465861 ,\n",
       "           0.66239977],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.65582026, 0.78316636,\n",
       "           0.75649557],\n",
       "          [1.        , 1.        , 0.        , ..., 0.90307511, 1.        ,\n",
       "           0.94981428],\n",
       "          [1.        , 1.        , 0.        , ..., 0.95800009, 0.86582973,\n",
       "           0.90166189]]),\n",
       "   3: array([[1.        , 1.        , 0.        , ..., 0.67543163, 0.88810468,\n",
       "           1.        ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.70888228, 0.98934955,\n",
       "           0.85412666],\n",
       "          [1.        , 1.        , 0.        , ..., 0.7665743 , 0.94040449,\n",
       "           0.88305586],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.5198565 , 1.        ,\n",
       "           0.94780131],\n",
       "          [1.        , 1.        , 0.        , ..., 0.65884732, 0.98513021,\n",
       "           0.86942774],\n",
       "          [1.        , 1.        , 0.        , ..., 0.72032914, 0.91451102,\n",
       "           0.54553853]]),\n",
       "   4: array([[1.        , 1.        , 0.        , ..., 0.93002555, 0.99187074,\n",
       "           0.7291    ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.6777005 , 0.67590544,\n",
       "           0.73570489],\n",
       "          [1.        , 1.        , 0.        , ..., 0.83139487, 0.75394351,\n",
       "           0.76593664],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.8495745 , 0.80848991,\n",
       "           0.85249662],\n",
       "          [1.        , 1.        , 0.        , ..., 0.79049739, 0.72697576,\n",
       "           0.91900584],\n",
       "          [1.        , 1.        , 0.        , ..., 0.79758866, 0.54277936,\n",
       "           0.87678833]]),\n",
       "   5: array([[1.        , 1.        , 0.        , ..., 0.50861678, 0.73156291,\n",
       "           0.51354213],\n",
       "          [1.        , 1.        , 0.        , ..., 0.51727914, 0.82819283,\n",
       "           0.64267844],\n",
       "          [1.        , 1.        , 0.        , ..., 0.53892292, 0.79950367,\n",
       "           0.58779505],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.20145883, 0.84294225,\n",
       "           0.94605767],\n",
       "          [1.        , 1.        , 0.        , ..., 0.37944935, 1.        ,\n",
       "           0.87166983],\n",
       "          [1.        , 1.        , 0.        , ..., 0.48708029, 0.90011609,\n",
       "           0.80477226]]),\n",
       "   6: array([[0.73873602, 0.17528202, 0.87455907, ..., 0.08481486, 0.42924435,\n",
       "           0.26821984],\n",
       "          [0.78803954, 0.05108325, 0.97139524, ..., 0.67429866, 0.3669108 ,\n",
       "           1.        ],\n",
       "          [0.6282079 , 0.08020963, 0.92123166, ..., 0.39820009, 0.5507847 ,\n",
       "           0.90746766],\n",
       "          ...,\n",
       "          [0.51050392, 0.14915461, 0.97237626, ..., 0.77431187, 0.75150824,\n",
       "           0.70119373],\n",
       "          [0.45859634, 0.1734182 , 0.94913021, ..., 0.50170061, 0.68930797,\n",
       "           0.72238531],\n",
       "          [0.48220153, 0.11121547, 0.94874774, ..., 0.50827783, 0.60965774,\n",
       "           0.73230178]]),\n",
       "   7: array([[1.        , 1.        , 0.        , ..., 0.72878306, 0.76422853,\n",
       "           0.80339579],\n",
       "          [1.        , 1.        , 0.        , ..., 0.82976426, 0.8804638 ,\n",
       "           0.86149941],\n",
       "          [1.        , 1.        , 0.        , ..., 0.64930789, 0.85503439,\n",
       "           0.9566629 ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.55840324, 0.70519707,\n",
       "           0.9223934 ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.76746595, 0.76449472,\n",
       "           0.98267038],\n",
       "          [1.        , 1.        , 0.        , ..., 0.72859932, 0.64625992,\n",
       "           0.85645184]]),\n",
       "   8: array([[1.        , 1.        , 0.        , ..., 0.7245093 , 0.80163347,\n",
       "           0.76193028],\n",
       "          [1.        , 1.        , 0.        , ..., 0.79952496, 0.60612681,\n",
       "           0.59969847],\n",
       "          [1.        , 1.        , 0.        , ..., 0.90102679, 0.72350885,\n",
       "           0.76543396],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.65342631, 0.75733446,\n",
       "           0.78567875],\n",
       "          [1.        , 1.        , 0.        , ..., 0.66084867, 0.74226066,\n",
       "           0.83943709],\n",
       "          [1.        , 1.        , 0.        , ..., 0.47239607, 0.59380601,\n",
       "           0.67833255]]),\n",
       "   9: array([[1.        , 1.        , 0.        , ..., 0.61031507, 0.91195107,\n",
       "           0.71718088],\n",
       "          [1.        , 1.        , 0.        , ..., 0.62331196, 0.85196056,\n",
       "           0.7077624 ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.82137937, 0.84425727,\n",
       "           0.76265571],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.07247992, 0.59629375,\n",
       "           0.38427908],\n",
       "          [1.        , 1.        , 0.        , ..., 0.34042789, 0.8505875 ,\n",
       "           0.65682293],\n",
       "          [1.        , 1.        , 0.        , ..., 0.41405493, 0.79195818,\n",
       "           0.66037351]]),\n",
       "   10: array([[1.        , 1.        , 0.        , ..., 0.30579946, 0.67185148,\n",
       "           0.59493719],\n",
       "          [1.        , 1.        , 0.        , ..., 0.49233365, 0.56480942,\n",
       "           0.54148498],\n",
       "          [1.        , 1.        , 0.        , ..., 0.94087054, 0.78239116,\n",
       "           0.85106491],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.56909136, 0.81741293,\n",
       "           0.62296957],\n",
       "          [1.        , 1.        , 0.        , ..., 0.36607864, 0.60664836,\n",
       "           0.42413308],\n",
       "          [1.        , 1.        , 0.        , ..., 0.42961029, 0.73652022,\n",
       "           0.56321161]]),\n",
       "   11: array([[1.        , 1.        , 0.        , ..., 0.56801856, 0.6222491 ,\n",
       "           0.7237105 ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.60809882, 0.65742407,\n",
       "           0.75667131],\n",
       "          [1.        , 1.        , 0.        , ..., 0.66555553, 0.61854795,\n",
       "           0.72589108],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.65854783, 0.99040462,\n",
       "           0.87178143],\n",
       "          [1.        , 1.        , 0.        , ..., 0.69200531, 0.68204827,\n",
       "           0.73871943],\n",
       "          [1.        , 1.        , 0.        , ..., 0.85823641, 0.71449557,\n",
       "           0.76172841]]),\n",
       "   12: array([[1.        , 1.        , 0.        , ..., 0.4132237 , 0.79701184,\n",
       "           0.26418188],\n",
       "          [1.        , 1.        , 0.        , ..., 0.20881091, 0.41922654,\n",
       "           0.43766691],\n",
       "          [1.        , 1.        , 0.        , ..., 0.27321019, 0.29254963,\n",
       "           0.48319024],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.4710786 , 0.0826957 ,\n",
       "           0.27457644],\n",
       "          [1.        , 1.        , 0.        , ..., 0.39718191, 0.        ,\n",
       "           0.2139122 ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.3677527 , 0.27245743,\n",
       "           0.65869606]]),\n",
       "   13: array([[1.        , 1.        , 0.        , ..., 1.        , 0.70083838,\n",
       "           0.69256976],\n",
       "          [1.        , 1.        , 0.        , ..., 0.69413201, 0.5699516 ,\n",
       "           0.40699871],\n",
       "          [1.        , 1.        , 0.        , ..., 0.75908599, 0.30079302,\n",
       "           0.27409616],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.5430804 , 0.47887614,\n",
       "           0.5576258 ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.78364625, 0.70686706,\n",
       "           0.63218094],\n",
       "          [1.        , 1.        , 0.        , ..., 0.79506376, 0.70690465,\n",
       "           0.46754016]]),\n",
       "   14: array([[1.        , 1.        , 0.        , ..., 0.52644345, 0.69422963,\n",
       "           0.65860291],\n",
       "          [1.        , 1.        , 0.        , ..., 0.64606635, 0.87194841,\n",
       "           0.69892904],\n",
       "          [1.        , 1.        , 0.        , ..., 0.72904573, 0.71815668,\n",
       "           0.72750834],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.93473473, 0.82016901,\n",
       "           0.93613062],\n",
       "          [1.        , 1.        , 0.        , ..., 0.76135677, 0.51925031,\n",
       "           0.71741965],\n",
       "          [1.        , 1.        , 0.        , ..., 0.51308868, 0.36043341,\n",
       "           0.46513304]]),\n",
       "   15: array([[1.        , 1.        , 0.        , ..., 0.41157268, 0.9381296 ,\n",
       "           0.96134931],\n",
       "          [1.        , 1.        , 0.        , ..., 0.76528276, 1.        ,\n",
       "           0.8585793 ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.53535246, 0.97004061,\n",
       "           0.68510229],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.31615359, 0.83693701,\n",
       "           0.74579613],\n",
       "          [1.        , 1.        , 0.        , ..., 0.        , 0.79119699,\n",
       "           0.70357374],\n",
       "          [1.        , 1.        , 0.        , ..., 0.        , 0.76387106,\n",
       "           0.65871226]]),\n",
       "   16: array([[1.        , 0.8877018 , 0.        , ..., 0.46605407, 0.19238845,\n",
       "           0.58413219],\n",
       "          [1.        , 1.        , 0.        , ..., 0.        , 0.        ,\n",
       "           0.1190175 ],\n",
       "          [1.        , 1.        , 0.        , ..., 0.92153526, 0.06162754,\n",
       "           0.42959502],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.66010559, 0.67236729,\n",
       "           0.41536272],\n",
       "          [1.        , 1.        , 0.        , ..., 0.66709241, 0.71429779,\n",
       "           0.49490633],\n",
       "          [1.        , 1.        , 0.        , ..., 0.64392525, 0.73181571,\n",
       "           0.58898626]]),\n",
       "   17: array([[0.90224035, 0.        , 0.76961147, ..., 0.10503935, 0.43422929,\n",
       "           0.71291861],\n",
       "          [0.62914837, 0.08666117, 0.68009158, ..., 0.45264934, 0.76402175,\n",
       "           0.66035459],\n",
       "          [0.58392581, 0.19422645, 0.61429595, ..., 0.61855902, 0.78493099,\n",
       "           0.99694132],\n",
       "          ...,\n",
       "          [0.45984544, 0.1878155 , 0.79438949, ..., 0.33195051, 0.71347454,\n",
       "           0.94035836],\n",
       "          [0.42925085, 0.15860766, 0.7784254 , ..., 0.59450569, 0.50252724,\n",
       "           0.84486452],\n",
       "          [0.41561135, 0.17329183, 0.7776342 , ..., 0.71416961, 0.53644698,\n",
       "           0.78937688]]),\n",
       "   18: array([[1.        , 1.        , 0.        , ..., 0.60344722, 0.8704351 ,\n",
       "           0.80056401],\n",
       "          [1.        , 1.        , 0.        , ..., 0.82548215, 0.70378753,\n",
       "           0.80009869],\n",
       "          [1.        , 1.        , 0.        , ..., 0.70794961, 0.61327349,\n",
       "           0.72681116],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.60490586, 0.92003272,\n",
       "           0.60559017],\n",
       "          [1.        , 1.        , 0.        , ..., 0.54445007, 0.80519245,\n",
       "           0.39302246],\n",
       "          [1.        , 1.        , 0.        , ..., 0.34051718, 0.71987755,\n",
       "           0.4826194 ]])},\n",
       "  'fitted_feature_scaler': MinMaxScaler(clip=True)},\n",
       " 18: {'profile_windows': array([[0.83996089, 0.84727957, 0.7421009 , ..., 0.88598756, 0.26918608,\n",
       "          0.83737649],\n",
       "         [0.6483491 , 0.50666094, 0.65609177, ..., 0.7842805 , 0.1636529 ,\n",
       "          0.77747493],\n",
       "         [0.53542478, 0.28660138, 0.72294336, ..., 0.63775817, 0.38607289,\n",
       "          0.589968  ],\n",
       "         ...,\n",
       "         [0.04134424, 0.        , 0.51229156, ..., 0.1787412 , 0.68993719,\n",
       "          0.67879148],\n",
       "         [0.57686319, 0.23764519, 0.15588563, ..., 0.71529027, 0.78556019,\n",
       "          0.82898015],\n",
       "         [0.14687035, 0.37775589, 0.5003772 , ..., 0.70079553, 0.89602061,\n",
       "          0.79590628]]),\n",
       "  'unknown_users_dict': {0: array([[5.24610147e-01, 6.43783900e-01, 5.52869912e-01, ...,\n",
       "           9.50046474e-01, 7.46695177e-01, 9.50061240e-01],\n",
       "          [5.27112430e-01, 8.39793736e-01, 4.97679518e-01, ...,\n",
       "           1.00000000e+00, 7.42853637e-01, 7.71245010e-01],\n",
       "          [7.49860023e-01, 1.00000000e+00, 3.15735398e-01, ...,\n",
       "           9.14603584e-01, 7.84904953e-01, 8.02848458e-01],\n",
       "          ...,\n",
       "          [4.99806792e-02, 5.89404082e-01, 5.37532627e-04, ...,\n",
       "           8.22032738e-01, 2.07798856e-01, 4.57905101e-01],\n",
       "          [2.35890103e-01, 4.69012529e-01, 1.99346085e-01, ...,\n",
       "           4.39274534e-02, 1.67202811e-01, 3.67629838e-01],\n",
       "          [3.18846367e-01, 5.25683248e-01, 2.12890391e-02, ...,\n",
       "           1.25640605e-01, 4.52839963e-01, 6.46202444e-01]]),\n",
       "   1: array([[1.        , 0.70840746, 0.        , ..., 1.        , 0.98295252,\n",
       "           0.99145167],\n",
       "          [1.        , 0.75874423, 0.        , ..., 0.76814288, 0.96416979,\n",
       "           0.95073076],\n",
       "          [1.        , 0.54574516, 0.        , ..., 0.79860351, 1.        ,\n",
       "           0.95640725],\n",
       "          ...,\n",
       "          [1.        , 0.89313376, 0.        , ..., 1.        , 0.82936828,\n",
       "           1.        ],\n",
       "          [1.        , 0.83339513, 0.        , ..., 1.        , 0.76801281,\n",
       "           0.89736821],\n",
       "          [1.        , 0.5103425 , 0.        , ..., 0.87496975, 0.47449127,\n",
       "           0.51872964]]),\n",
       "   2: array([[1.        , 0.        , 1.        , ..., 0.49648042, 0.17815817,\n",
       "           0.26286194],\n",
       "          [1.        , 0.55377517, 1.        , ..., 0.77467869, 0.80221988,\n",
       "           0.78787796],\n",
       "          [1.        , 1.        , 1.        , ..., 0.84690783, 0.73882449,\n",
       "           0.6276765 ],\n",
       "          ...,\n",
       "          [0.42520534, 1.        , 1.        , ..., 0.72712909, 0.65519339,\n",
       "           0.72797355],\n",
       "          [1.        , 0.94362408, 1.        , ..., 1.        , 1.        ,\n",
       "           0.93403264],\n",
       "          [1.        , 1.        , 1.        , ..., 1.        , 0.76420091,\n",
       "           0.88270685]]),\n",
       "   3: array([[1.        , 0.3557024 , 0.09692705, ..., 0.75204104, 0.79357471,\n",
       "           0.9891052 ],\n",
       "          [1.        , 0.44647496, 0.        , ..., 0.79453277, 0.92708551,\n",
       "           0.83203888],\n",
       "          [1.        , 0.30158646, 0.23133247, ..., 0.86781786, 0.86254205,\n",
       "           0.86287461],\n",
       "          ...,\n",
       "          [1.        , 0.14699395, 0.        , ..., 0.5544169 , 0.96889171,\n",
       "           0.93188702],\n",
       "          [1.        , 0.41020397, 0.        , ..., 0.73097431, 0.92152151,\n",
       "           0.84834834],\n",
       "          [1.        , 0.12285072, 0.        , ..., 0.80907351, 0.82839655,\n",
       "           0.5031137 ]]),\n",
       "   4: array([[1.        , 0.        , 0.82271095, ..., 1.        , 0.93041018,\n",
       "           0.69877252],\n",
       "          [1.        , 0.        , 0.51955044, ..., 0.75492314, 0.51374927,\n",
       "           0.70581268],\n",
       "          [1.        , 0.        , 0.63071183, ..., 0.9501582 , 0.61665745,\n",
       "           0.73803682],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.        , ..., 0.97325145, 0.68858736,\n",
       "           0.83030141],\n",
       "          [1.        , 0.        , 0.        , ..., 0.89820691, 0.5810953 ,\n",
       "           0.90119381],\n",
       "          [1.        , 0.        , 0.25180682, ..., 0.90721482, 0.33819697,\n",
       "           0.85619402]]),\n",
       "   5: array([[1.        , 0.        , 0.95953536, ..., 0.54013929, 0.58714433,\n",
       "           0.46900861],\n",
       "          [1.        , 0.        , 1.        , ..., 0.55114292, 0.71456943,\n",
       "           0.60665547],\n",
       "          [1.        , 0.        , 1.        , ..., 0.57863662, 0.67673727,\n",
       "           0.54815507],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.66371924, ..., 0.14996235, 0.73401938,\n",
       "           0.93002846],\n",
       "          [1.        , 0.        , 0.71357224, ..., 0.37606035, 1.        ,\n",
       "           0.8507382 ],\n",
       "          [1.        , 0.        , 0.69165952, ..., 0.5127819 , 0.80941406,\n",
       "           0.77943184]]),\n",
       "   6: array([[0.        , 0.        , 1.        , ..., 0.00179172, 0.18847926,\n",
       "           0.20751872],\n",
       "          [0.        , 0.        , 1.        , ..., 0.75060185, 0.10628052,\n",
       "           0.99143014],\n",
       "          [0.        , 0.        , 1.        , ..., 0.39987905, 0.34875356,\n",
       "           0.88889523],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.87764674, 0.61344607,\n",
       "           0.66902712],\n",
       "          [0.        , 0.        , 1.        , ..., 0.53135382, 0.53142308,\n",
       "           0.6916153 ],\n",
       "          [0.        , 0.        , 1.        , ..., 0.53970873, 0.42638896,\n",
       "           0.70218531]]),\n",
       "   7: array([[0.        , 0.97804938, 1.        , ..., 0.81981236, 0.63022022,\n",
       "           0.77796466],\n",
       "          [0.9635958 , 1.        , 1.        , ..., 0.94808687, 0.78349874,\n",
       "           0.83989751],\n",
       "          [1.        , 1.        , 1.        , ..., 0.71885655, 0.74996518,\n",
       "           0.94133261],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        , ..., 0.60338209, 0.55237591,\n",
       "           0.90480463],\n",
       "          [1.        , 1.        , 1.        , ..., 0.86895051, 0.63057124,\n",
       "           0.96905408],\n",
       "          [0.59882866, 1.        , 0.82978595, ..., 0.81957896, 0.47465597,\n",
       "           0.83451729]]),\n",
       "   8: array([[1.        , 0.        , 0.72346565, ..., 0.81438349, 0.67954582,\n",
       "           0.73376642],\n",
       "          [1.        , 0.        , 0.57973146, ..., 0.90967446, 0.42173276,\n",
       "           0.56084296],\n",
       "          [1.        , 0.        , 0.63396623, ..., 1.        , 0.57652351,\n",
       "           0.73750101],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.44825924, ..., 0.72408811, 0.62112906,\n",
       "           0.75908001],\n",
       "          [1.        , 0.        , 0.26232511, ..., 0.73351659, 0.60125137,\n",
       "           0.81638121],\n",
       "          [1.        , 0.        , 0.21130072, ..., 0.49412881, 0.40548542,\n",
       "           0.64465931]]),\n",
       "   9: array([[1.        , 1.        , 0.53731885, ..., 0.66932471, 0.82502075,\n",
       "           0.68606788],\n",
       "          [1.        , 1.        , 0.72763316, ..., 0.68583441, 0.74591175,\n",
       "           0.67602869],\n",
       "          [1.        , 1.        , 0.77640555, ..., 0.9374357 , 0.73575349,\n",
       "           0.73453966],\n",
       "          ...,\n",
       "          [1.        , 1.        , 0.        , ..., 0.        , 0.40876597,\n",
       "           0.33122667],\n",
       "          [1.        , 1.        , 0.        , ..., 0.32649213, 0.74410111,\n",
       "           0.62173213],\n",
       "          [1.        , 0.92184563, 0.        , ..., 0.42001917, 0.66678709,\n",
       "           0.62551671]]),\n",
       "   10: array([[1.        , 0.77415183, 0.482083  , ..., 0.28250428, 0.50840334,\n",
       "           0.5557679 ],\n",
       "          [1.        , 0.34820473, 1.        , ..., 0.51945514, 0.36724784,\n",
       "           0.498793  ],\n",
       "          [1.        , 0.36011557, 1.        , ..., 1.        , 0.65417113,\n",
       "           0.82877534],\n",
       "          ...,\n",
       "          [0.49966207, 0.21639094, 0.77483082, ..., 0.61695901, 0.70035407,\n",
       "           0.58564771],\n",
       "          [0.44294122, 0.3348505 , 0.70956165, ..., 0.35907578, 0.42242052,\n",
       "           0.37370719],\n",
       "          [0.50268549, 0.24338451, 0.7062818 , ..., 0.43977885, 0.5936815 ,\n",
       "           0.52195149]]),\n",
       "   11: array([[1.        , 1.        , 1.        , ..., 0.61559625, 0.44299309,\n",
       "           0.69302783],\n",
       "          [1.        , 0.79049817, 1.        , ..., 0.66650945, 0.48937804,\n",
       "           0.72816087],\n",
       "          [1.        , 0.77479773, 1.        , ..., 0.73949562, 0.4381124 ,\n",
       "           0.69535211],\n",
       "          ...,\n",
       "          [1.        , 0.87979481, 0.56630884, ..., 0.73059387, 0.92847682,\n",
       "           0.85085715],\n",
       "          [1.        , 0.8720872 , 0.62640626, ..., 0.77309428, 0.52184978,\n",
       "           0.70902589],\n",
       "          [1.        , 0.69331748, 0.80043405, ..., 0.9842545 , 0.56463776,\n",
       "           0.73355125]]),\n",
       "   12: array([[1.        , 1.        , 1.        , ..., 0.41896327, 0.67345131,\n",
       "           0.20321464],\n",
       "          [1.        , 0.79587818, 0.62232836, ..., 0.15930156, 0.17526886,\n",
       "           0.38813295],\n",
       "          [1.        , 0.98053028, 0.61430878, ..., 0.24110675, 0.00822103,\n",
       "           0.43665643],\n",
       "          ...,\n",
       "          [0.74285522, 0.32834817, 0.86930165, ..., 0.49245525, 0.        ,\n",
       "           0.21429424],\n",
       "          [1.        , 0.54126432, 0.80754318, ..., 0.39858568, 0.        ,\n",
       "           0.14963201],\n",
       "          [1.        , 0.4766632 , 0.70428836, ..., 0.36120231, 0.        ,\n",
       "           0.6237287 ]]),\n",
       "   13: array([[0.        , 1.        , 1.        , ..., 1.        , 0.54662814,\n",
       "           0.6598348 ],\n",
       "          [0.24271894, 1.        , 1.        , ..., 0.77579578, 0.37402879,\n",
       "           0.35544361],\n",
       "          [0.16877558, 1.        , 1.        , ..., 0.8583056 , 0.01909153,\n",
       "           0.21378231],\n",
       "          ...,\n",
       "          [0.        , 1.        , 0.6862981 , ..., 0.58391778, 0.25392831,\n",
       "           0.51599756],\n",
       "          [0.48972128, 1.        , 1.        , ..., 0.88950404, 0.55457812,\n",
       "           0.59546615],\n",
       "          [0.28122451, 1.        , 1.        , ..., 0.90400749, 0.55462769,\n",
       "           0.41997495]]),\n",
       "   14: array([[0.        , 0.        , 0.54229101, ..., 0.56278418, 0.53791323,\n",
       "           0.62362942],\n",
       "          [0.        , 0.        , 0.81368188, ..., 0.71473888, 0.77226957,\n",
       "           0.66661318],\n",
       "          [0.        , 0.        , 1.        , ..., 0.82014603, 0.56946564,\n",
       "           0.69707596],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.6654076 , ..., 1.        , 0.70398849,\n",
       "           0.91944718],\n",
       "          [0.        , 0.        , 0.81366711, ..., 0.86119013, 0.3071694 ,\n",
       "           0.68632239],\n",
       "          [0.        , 0.        , 0.74390837, ..., 0.54581986, 0.09773882,\n",
       "           0.4174092 ]]),\n",
       "   15: array([[1.        , 0.        , 1.        , ..., 0.41686601, 0.85954218,\n",
       "           0.94632787],\n",
       "          [1.        , 0.        , 1.        , ..., 0.86617724, 0.95892021,\n",
       "           0.83678496],\n",
       "          [1.        , 0.        , 0.9764452 , ..., 0.57410112, 0.90162296,\n",
       "           0.65187519],\n",
       "          ...,\n",
       "          [1.        , 0.        , 0.49994768, ..., 0.29565694, 0.72610031,\n",
       "           0.71656898],\n",
       "          [1.        , 0.        , 0.98409326, ..., 0.        , 0.66578332,\n",
       "           0.67156398],\n",
       "          [1.        , 0.        , 1.        , ..., 0.        , 0.62974883,\n",
       "           0.62374597]]),\n",
       "   16: array([[0.        , 0.        , 0.        , ..., 0.48607269, 0.        ,\n",
       "           0.54425081],\n",
       "          [0.        , 0.        , 0.93070572, ..., 0.        , 0.        ,\n",
       "           0.04848341],\n",
       "          [0.        , 0.        , 1.        , ..., 1.        , 0.        ,\n",
       "           0.3795291 ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.35481159, ..., 0.73257267, 0.50908354,\n",
       "           0.36435883],\n",
       "          [0.        , 0.        , 0.45084631, ..., 0.74144789, 0.56437695,\n",
       "           0.44914465],\n",
       "          [0.        , 0.        , 0.46708968, ..., 0.71201908, 0.58747769,\n",
       "           0.54942478]]),\n",
       "   17: array([[0.        , 0.        , 1.        , ..., 0.02748251, 0.19505287,\n",
       "           0.68152472],\n",
       "          [0.        , 0.        , 1.        , ..., 0.46904491, 0.62994755,\n",
       "           0.62549653],\n",
       "          [0.        , 0.        , 1.        , ..., 0.67979684, 0.65752039,\n",
       "           0.98426553],\n",
       "          ...,\n",
       "          [0.        , 0.        , 1.        , ..., 0.31572347, 0.56329134,\n",
       "           0.92395355],\n",
       "          [0.        , 0.        , 1.        , ..., 0.64924235, 0.28511683,\n",
       "           0.82216632],\n",
       "          [0.        , 0.        , 1.        , ..., 0.80124917, 0.32984651,\n",
       "           0.76302185]]),\n",
       "   18: array([[0.        , 1.        , 0.88707753, ..., 0.66060061, 0.77027397,\n",
       "           0.77494625],\n",
       "          [0.        , 0.26917083, 0.73103567, ..., 0.94264739, 0.55051716,\n",
       "           0.77445026],\n",
       "          [0.359729  , 0.5160321 , 0.60909723, ..., 0.79334802, 0.43115702,\n",
       "           0.69633283],\n",
       "          ...,\n",
       "          [0.43447166, 0.30074278, 0.51274985, ..., 0.6624535 , 0.83567796,\n",
       "           0.56712295],\n",
       "          [0.46456434, 0.70582421, 0.44843889, ..., 0.58565764, 0.68423901,\n",
       "           0.34054627],\n",
       "          [0.14212587, 0.66348196, 0.6089168 , ..., 0.32660554, 0.57173494,\n",
       "           0.43604797]])},\n",
       "  'fitted_feature_scaler': MinMaxScaler(clip=True)}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print('Time: ', stop - start)  \n",
    "X_exp_train_dic\n",
    "# X_exp_train_dic[9]\n",
    "# IF_train_train_WACA_features_dic = MakeWACAXExpDic(X_exp2_train_dic)\n",
    "# IF_train_valid_WACA_features_dic = MakeWACAXExpDic(X_exp1_train_dic, fitted_scaler_dic=fitted_scaler_IF_exp2_train_dic)\n",
    "# IF_test_train_WACA_features_dic = MakeWACAXExpDic(X_exp2_test_dic)\n",
    "# IF_test_test_WACA_features_dic = MakeWACAXExpDic(X_exp1_test_dic, fitted_scaler_dic=fitted_scaler_IF_exp2_test_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for owner in X_exp_train_dic:\n",
    "#     if (X_exp_train_dic[owner][\"profile_windows\"] != temp[owner][\"profile_windows\"]).any():\n",
    "#         print(\"found\")\n",
    "#     for adv in X_exp_train_dic[owner][\"unknown_users_dict\"]:\n",
    "#         if (X_exp_train_dic[owner][\"profile_windows\"][adv] != temp[owner][\"profile_windows\"][adv]).any():\n",
    "#             print(\"found\")\n",
    "            \n",
    "# print(\"not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_exp_train_dic, X_exp_test_dic\n",
    "# np.ptp(X_exp_train_dic[0]['unknown_users_dict'][0], axis=0)\n",
    "# X_exp_train_dic[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(IF_train_train_WACA_features_dic[0].shape)\n",
    "# print(IF_train_train_WACA_features_dic[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>VALID-ROBUST-IF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frequency</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_subjects</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_test_subjects</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_ids</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_sample_points_per_exp</th>\n",
       "      <td>21000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp_begin_cutoff_idx</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp_end_cutoff_idx</th>\n",
       "      <td>-500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seconds_per_subject_train</th>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seconds_per_subject_test</th>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>window_size</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IF_step_width</th>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler</th>\n",
       "      <td>RobustScaler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler_scope</th>\n",
       "      <td>subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler_global</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IF_kernel</th>\n",
       "      <td>rbf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IF_nu</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IF_gamma</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_cols</th>\n",
       "      <td>[EMA_x_a, EMA_y_a, EMA_z_a, EMA_x_g, EMA_y_g, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exclude_subjects</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       Value\n",
       "name                                                         VALID-ROBUST-IF\n",
       "frequency                                                                100\n",
       "max_subjects                                                              29\n",
       "max_test_subjects                                                         10\n",
       "user_ids                   [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 2...\n",
       "num_sample_points_per_exp                                              21000\n",
       "exp_begin_cutoff_idx                                                     500\n",
       "exp_end_cutoff_idx                                                      -500\n",
       "seconds_per_subject_train                                                210\n",
       "seconds_per_subject_test                                                 210\n",
       "window_size                                                              500\n",
       "IF_step_width                                                            250\n",
       "scaler                                                          RobustScaler\n",
       "scaler_scope                                                         subject\n",
       "scaler_global                                                          False\n",
       "IF_kernel                                                                rbf\n",
       "IF_nu                                                                   None\n",
       "IF_gamma                                                                None\n",
       "feature_cols               [EMA_x_a, EMA_y_a, EMA_z_a, EMA_x_g, EMA_y_g, ...\n",
       "exclude_subjects                                                          []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils_ppp(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21000/250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# len(np.linspace(0.0001, 0.3))*len(np.logspace(-3, 3)) * 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # source: https://github.com/dynobo/ContinAuth/blob/master/notebooks/utils.ipynb\n",
    "# def utils_eer(y_true, y_pred, return_threshold=False):\n",
    "#     \"\"\"Calculate the Equal Error Rate.\n",
    "\n",
    "#     Based on https://stackoverflow.com/a/49555212, https://yangcha.github.io/EER-ROC/\n",
    "#     and https://scikit-learn.org/stable/modules/model_evaluation.html#implementing-your-own-scoring-object\n",
    "\n",
    "#     Arguments:\n",
    "#         y_true {np.array}  -- Actual labels\n",
    "#         y_pred {np.array}  -- Predicted labels or probability\n",
    "        \n",
    "#     Returns:\n",
    "#         float              -- Equal Error Rate        \n",
    "#     \"\"\"\n",
    "#     # print(y_true)\n",
    "#     # print(y_pred)\n",
    "#     # print('---')\n",
    "#     fpr, tpr, thresholds = roc_curve(y_true, y_pred, pos_label=1)\n",
    "#     eer = brentq(lambda x: 1.0 - x - interp1d(fpr, tpr)(x), 0.0, 1.0)\n",
    "#     thresh = interp1d(fpr, thresholds)(eer)  # Calculated threshold, not needed for score\n",
    "#     if return_threshold:\n",
    "#         return eer, thresh\n",
    "#     else:\n",
    "#         return eer\n",
    "    \n",
    "# utils_eer_scorer = make_scorer(utils_eer, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_owner_IF_train_valid(owner_idx, X_exp_train_dic, SEED, run, param_dist, CORES):\n",
    "    run_seed = SEED + run\n",
    "    train_dic, valid_test_dic = { owner_idx: X_exp_train_dic[owner_idx][\"profile_windows\"]}, X_exp_train_dic[owner_idx][\"unknown_users_dict\"] \n",
    "                                #IF_train_train_WACA_features_dic, IF_train_valid_WACA_features_dic\n",
    "    X_vals_owner_idx = utils_create_cv_splits(owner_idx, train_dic, valid_test_dic, seed=run_seed)\n",
    "\n",
    "    X_test_regular = X_vals_owner_idx['X_test_regular'].copy()\n",
    "    X_test_anomalous = X_vals_owner_idx['X_test_anomalous'].copy()\n",
    "\n",
    "#         print(X_vals_owner_idx['cv_splits'])\n",
    "    cv_splits = X_vals_owner_idx['cv_splits']\n",
    "\n",
    "#         pca = PCA(n_components = run)\n",
    "#         X_train = pca.fit_transform(X_train)\n",
    "#         X_test_regular = pca.transform(X_test_regular)\n",
    "#         X_test_anomalous = pca.transform(X_test_anomalous)\n",
    "\n",
    "#         pca_fs.add_user_pca(owner_idx, pca)\n",
    "\n",
    "    clf = IsolationForest(random_state=run_seed, n_jobs=CORES, verbose=0)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "#                             ('scaler', StandardScaler()), \n",
    "                        # ('scaler', get_new_scaler_dict[P.scaler]()), \n",
    "#                             ('scaler', Normalizer()),\n",
    "#                              ('pca', pca), \n",
    "#                              ('selector', VarianceThreshold()), \n",
    "                         ('model', clf)\n",
    "                        ])\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        cv=cv_splits,\n",
    "        n_iter=120,\n",
    "        #n_iter=80,\n",
    "#             n_iter=480,\n",
    "        n_jobs=CORES,\n",
    "        refit=False,\n",
    "        scoring={\"eer\": utils_eer_scorer, \"accuracy\": \"accuracy\"},\n",
    "        verbose=0,\n",
    "        return_train_score=False,\n",
    "        # iid=False, #invalid arg in sklearn 1.0.2\n",
    "        error_score=\"raise\",\n",
    "        random_state=run_seed\n",
    "    )\n",
    "#         search = GridSearchCV(\n",
    "#             pipeline,\n",
    "#             param_grid=param_dist,\n",
    "#             cv=cv_splits,\n",
    "#             n_jobs=CORES,\n",
    "#             refit=False,\n",
    "#             scoring={\"eer\": utils_eer_scorer, \"accuracy\": \"accuracy\"},\n",
    "#             verbose=1,\n",
    "#             return_train_score=False,\n",
    "#             iid=False,\n",
    "#             error_score=\"raise\",\n",
    "#         )\n",
    "#         print(X_train.shape)\n",
    "#         print(X_test_anomalous.shape)\n",
    "\n",
    "\n",
    "    search.fit(X_vals_owner_idx['X_train'], X_vals_owner_idx['y_train'])\n",
    "    impostors = [idx for idx in range(len(train_set)) if idx != owner_idx]\n",
    "\n",
    "    df_report = utils_cv_report(search, owner_idx, impostors)\n",
    "    df_report[\"run\"] = run\n",
    "    return df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.001 , 0.1008, 0.2006, 0.3004, 0.4002, 0.5   ])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)\n",
    "np.logspace(-3, 3, 6)\n",
    "# np.arange(0, 10, 10)\n",
    "np.logspace(1, 1, num=10, dtype=np.int64)\n",
    "np.linspace(0.001, 0.5, num=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  10,   16,   27,   46,   77,  129,  215,  359,  599, 1000])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(1, 3, num=10, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(1, 3, num=10, dtype=np.int64)\n",
    "np.arange(0.1, 1.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 15.7min\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  10 | elapsed: 15.7min remaining: 62.8min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed: 15.7min remaining: 36.7min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  10 | elapsed: 15.8min remaining: 23.6min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed: 15.8min remaining: 15.8min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed: 15.8min remaining: 10.5min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed: 15.8min remaining:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed: 15.8min remaining:  4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 1/3 [15:54<31:48, 954.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 15.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 15.9min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 12.1min\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  10 | elapsed: 12.1min remaining: 48.3min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed: 12.1min remaining: 28.3min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  10 | elapsed: 12.1min remaining: 18.2min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed: 12.2min remaining: 12.2min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed: 12.2min remaining:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed: 12.2min remaining:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed: 12.2min remaining:  3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|   | 2/3 [28:11<13:46, 826.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 12.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 12.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 12.5min\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  10 | elapsed: 12.5min remaining: 50.2min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed: 12.5min remaining: 29.3min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  10 | elapsed: 12.6min remaining: 18.8min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed: 12.6min remaining: 12.6min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed: 12.6min remaining:  8.4min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed: 12.6min remaining:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed: 12.6min remaining:  3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3/3 [40:53<00:00, 817.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 12.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 12.7min finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_model__n_estimators</th>\n",
       "      <th>param_model__max_samples</th>\n",
       "      <th>param_model__max_features</th>\n",
       "      <th>param_model__contamination</th>\n",
       "      <th>1_test_eer</th>\n",
       "      <th>2_test_eer</th>\n",
       "      <th>...</th>\n",
       "      <th>16_test_accuracy</th>\n",
       "      <th>17_test_accuracy</th>\n",
       "      <th>18_test_accuracy</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>rank_test_accuracy</th>\n",
       "      <th>owner</th>\n",
       "      <th>run</th>\n",
       "      <th>0_test_eer</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.453447</td>\n",
       "      <td>0.007488</td>\n",
       "      <td>0.121797</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>359</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.947122</td>\n",
       "      <td>0.015101</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.442412</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.116743</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>359</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>...</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.215314</td>\n",
       "      <td>0.002032</td>\n",
       "      <td>0.318605</td>\n",
       "      <td>0.002730</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.274336</td>\n",
       "      <td>0.467532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.751339</td>\n",
       "      <td>0.129791</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.035520</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.009939</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.935074</td>\n",
       "      <td>0.055885</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.712912</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.179290</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>599</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.347107</td>\n",
       "      <td>0.392308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.737952</td>\n",
       "      <td>0.119545</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.449281</td>\n",
       "      <td>0.001977</td>\n",
       "      <td>0.121763</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>359</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.484472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777108</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.584337</td>\n",
       "      <td>0.692102</td>\n",
       "      <td>0.159419</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>0.873494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.268562</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>0.072634</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>215</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.490798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.765060</td>\n",
       "      <td>0.560241</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.165414</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>0.873494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.036122</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.010363</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>27</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.471338</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.692771</td>\n",
       "      <td>0.548193</td>\n",
       "      <td>0.506024</td>\n",
       "      <td>0.634538</td>\n",
       "      <td>0.151190</td>\n",
       "      <td>46</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.194175</td>\n",
       "      <td>0.879518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.162019</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.042704</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>129</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.487654</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.578313</td>\n",
       "      <td>0.530120</td>\n",
       "      <td>0.628514</td>\n",
       "      <td>0.142641</td>\n",
       "      <td>49</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.209524</td>\n",
       "      <td>0.867470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.021974</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.006438</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>16</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.490798</td>\n",
       "      <td>0.375940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.584337</td>\n",
       "      <td>0.548193</td>\n",
       "      <td>0.512048</td>\n",
       "      <td>0.576640</td>\n",
       "      <td>0.077662</td>\n",
       "      <td>81</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.341270</td>\n",
       "      <td>0.740964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3600 rows  54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0         0.453447      0.007488         0.121797        0.001021   \n",
       "1         0.442412      0.001450         0.116743        0.000640   \n",
       "2         1.215314      0.002032         0.318605        0.002730   \n",
       "3         0.035520      0.000208         0.009939        0.000220   \n",
       "4         0.712912      0.001332         0.179290        0.000904   \n",
       "..             ...           ...              ...             ...   \n",
       "115       0.449281      0.001977         0.121763        0.000682   \n",
       "116       0.268562      0.002384         0.072634        0.000552   \n",
       "117       0.036122      0.000219         0.010363        0.000126   \n",
       "118       0.162019      0.001252         0.042704        0.000459   \n",
       "119       0.021974      0.000110         0.006438        0.000065   \n",
       "\n",
       "    param_model__n_estimators param_model__max_samples  \\\n",
       "0                         359                      0.6   \n",
       "1                         359                      0.8   \n",
       "2                        1000                      0.5   \n",
       "3                          27                      1.0   \n",
       "4                         599                      0.1   \n",
       "..                        ...                      ...   \n",
       "115                       359                      0.7   \n",
       "116                       215                      0.7   \n",
       "117                        27                      0.3   \n",
       "118                       129                      0.9   \n",
       "119                        16                      0.1   \n",
       "\n",
       "    param_model__max_features param_model__contamination  1_test_eer  \\\n",
       "0                         0.7                        0.1    0.087912   \n",
       "1                         0.3                       0.08    0.107527   \n",
       "2                         0.3                       0.01    0.274336   \n",
       "3                         0.2                       0.04    0.056818   \n",
       "4                         0.2                       0.03    0.347107   \n",
       "..                        ...                        ...         ...   \n",
       "115                       1.0                       0.07    0.484472   \n",
       "116                       0.4                       0.06    0.490798   \n",
       "117                       1.0                        0.1    0.471338   \n",
       "118                       0.1                       0.07    0.487654   \n",
       "119                       0.4                       0.05    0.490798   \n",
       "\n",
       "     2_test_eer  ...  16_test_accuracy  17_test_accuracy  18_test_accuracy  \\\n",
       "0      0.087912  ...          0.951807          0.951807          0.951807   \n",
       "1      0.107527  ...          0.939759          0.939759          0.939759   \n",
       "2      0.467532  ...          0.963855          0.909639          0.867470   \n",
       "3      0.204082  ...          0.969880          0.963855          0.969880   \n",
       "4      0.392308  ...          0.903614          0.921687          0.819277   \n",
       "..          ...  ...               ...               ...               ...   \n",
       "115    0.000000  ...          0.777108          0.753012          0.584337   \n",
       "116    0.000000  ...          0.753012          0.765060          0.560241   \n",
       "117    0.087912  ...          0.692771          0.548193          0.506024   \n",
       "118    0.097826  ...          0.686747          0.578313          0.530120   \n",
       "119    0.375940  ...          0.584337          0.548193          0.512048   \n",
       "\n",
       "     mean_test_accuracy  std_test_accuracy  rank_test_accuracy  owner  run  \\\n",
       "0              0.947122           0.015101                   6      0    0   \n",
       "1              0.939759           0.000000                  17      0    0   \n",
       "2              0.751339           0.129791                  93      0    0   \n",
       "3              0.935074           0.055885                  30      0    0   \n",
       "4              0.737952           0.119545                  96      0    0   \n",
       "..                  ...                ...                 ...    ...  ...   \n",
       "115            0.692102           0.159419                  19      9    2   \n",
       "116            0.694444           0.165414                  18      9    2   \n",
       "117            0.634538           0.151190                  46      9    2   \n",
       "118            0.628514           0.142641                  49      9    2   \n",
       "119            0.576640           0.077662                  81      9    2   \n",
       "\n",
       "     0_test_eer  0_test_accuracy  \n",
       "0           NaN              NaN  \n",
       "1           NaN              NaN  \n",
       "2           NaN              NaN  \n",
       "3           NaN              NaN  \n",
       "4           NaN              NaN  \n",
       "..          ...              ...  \n",
       "115    0.201923         0.873494  \n",
       "116    0.201923         0.873494  \n",
       "117    0.194175         0.879518  \n",
       "118    0.209524         0.867470  \n",
       "119    0.341270         0.740964  \n",
       "\n",
       "[3600 rows x 54 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# param_dist = {\n",
    "#     # \"model__gamma\": np.logspace(-3, 3), \n",
    "#     # \"model__nu\": np.linspace(0.0001, 0.3),\n",
    "#     \"model__n_estimators\": np.logspace(1, 2, num=10, dtype=np.int64), \n",
    "#     \"model__max_samples\": [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0], \n",
    "#     \"model__contamination\": np.linspace(0.001, 0.5, num=6), \n",
    "#     \"model__max_features\": [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0], \n",
    "#     \"model__bootstrap\": [False], \n",
    "#     \"model__warm_start\": [False],\n",
    "# #     'scaler': [StandardScaler(), MinMaxScaler(),\n",
    "# #         Normalizer(), MaxAbsScaler()],\n",
    "# #     \"selector__threshold\": np.linspace(0, 2, num=1000),\n",
    "\n",
    "# param_dist\n",
    "param_dist = {\"model__n_estimators\": np.logspace(1, 3, num=10, dtype=np.int64), \n",
    "              'model__max_samples':  [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0], \n",
    "              'model__contamination':[0.001, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1], #[0.001, 0.01, 0.1, .15, 0.2, .25, 0.3, .35, 0.4],#[0.1, .15, 0.2, .25, 0.3, .35, 0.4, .45, .5], \n",
    "              'model__max_features': np.arange(0.1, 1.1, 0.1)#[1.0]#[int(.1*i*84) for i in range(1, 11)]#[12, 24, 36, 48, 60, 72, 84]#[5,10,15], \n",
    "              # 'model__bootstrap': [True, False], \n",
    "              }\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "df_results = None  # Will be filled with randomsearch scores\n",
    "\n",
    "\n",
    "# Don't need to loop over to find the best PCA.\n",
    "for run in tqdm(range(3)):\n",
    "#     df_results[run] = {}\n",
    "    \n",
    "    # for owner_idx in tqdm(\n",
    "    #     range(len(train_set)),\n",
    "    #     desc=\"Owner\",\n",
    "    # ):\n",
    "    #     df_report = evaluate_owner_IF_train_valid(owner_idx, X_exp_train_dic, SEED, run, param_dist, CORES=-1)\n",
    "    #     df_results = pd.concat([df_results, df_report], sort=False)\n",
    "    df_reports = Parallel(n_jobs=-1, verbose=100)(delayed(evaluate_owner_IF_train_valid)(owner_idx, X_exp_train_dic, SEED, run, param_dist, CORES=1) for owner_idx in range(len(test_set)))\n",
    "    # df_reports = Parallel(n_jobs=-1)(delayed(evaluate_owner_IF_train_valid)(owner_idx, X_exp_train_dic, SEED, run, param_dist, CORES=1) for owner_idx in range(len([0])))\n",
    "    df_results = pd.concat([df_results] + df_reports, sort=False)\n",
    "        \n",
    "\n",
    "\n",
    "df_results\n",
    "#2000 win size\n",
    "#100%|| 3/3 [01:55<00:00, 38.45s/it] parallel CORES=-1\n",
    "#100%|| 3/3 [01:53<00:00, 37.92s/it] CORES=-1\n",
    "#100%|| 3/3 [00:49<00:00, 16.40s/it] parallel CORES=1\n",
    "\n",
    "#250 win size\n",
    "#100%|| 3/3 [00:52<00:00, 17.46s/it] CORES=1\n",
    "#100%|| 3/3 [02:00<00:00, 40.21s/it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 12, 14, 9, 18, 23, 2, 15, 10, 4]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 1\n",
      "idx: 1, impostor: 2\n",
      "idx: 2, impostor: 3\n",
      "idx: 3, impostor: 4\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 1\n",
      "idx: 1, impostor: 2\n",
      "idx: 2, impostor: 3\n",
      "idx: 3, impostor: 4\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 1\n",
      "idx: 1, impostor: 2\n",
      "idx: 2, impostor: 3\n",
      "idx: 3, impostor: 4\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 2\n",
      "idx: 2, impostor: 3\n",
      "idx: 3, impostor: 4\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 3\n",
      "idx: 3, impostor: 4\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 2\n",
      "idx: 2, impostor: 3\n",
      "idx: 3, impostor: 4\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 5\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 4\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 5\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 5\n",
      "idx: 6, impostor: 6\n",
      "idx: 7, impostor: 7\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 4\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 2\n",
      "idx: 2, impostor: 3\n",
      "idx: 3, impostor: 4\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 3\n",
      "idx: 3, impostor: 4\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 3\n",
      "idx: 3, impostor: 4\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 5\n",
      "idx: 6, impostor: 6\n",
      "idx: 7, impostor: 7\n",
      "idx: 8, impostor: 8\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 5\n",
      "idx: 6, impostor: 6\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 5\n",
      "idx: 6, impostor: 6\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 5\n",
      "idx: 6, impostor: 6\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 5\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 5\n",
      "idx: 6, impostor: 6\n",
      "idx: 7, impostor: 7\n",
      "idx: 8, impostor: 8\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 5\n",
      "idx: 6, impostor: 6\n",
      "idx: 7, impostor: 7\n",
      "idx: 8, impostor: 8\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 4\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 5\n",
      "idx: 6, impostor: 6\n",
      "idx: 7, impostor: 7\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 5\n",
      "idx: 6, impostor: 6\n",
      "idx: 7, impostor: 7\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n"
     ]
    }
   ],
   "source": [
    "# len(df_reports)\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__n_estimators': array([  10,   16,   27,   46,   77,  129,  215,  359,  599, 1000]),\n",
       " 'model__max_samples': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
       " 'model__contamination': [0.001, 0.01, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4],\n",
       " 'model__max_features': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 5\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 1\n",
      "idx: 1, impostor: 2\n",
      "idx: 2, impostor: 3\n",
      "idx: 3, impostor: 4\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 2\n",
      "idx: 2, impostor: 3\n",
      "idx: 3, impostor: 4\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 3\n",
      "idx: 3, impostor: 4\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 1\n",
      "idx: 1, impostor: 2\n",
      "idx: 2, impostor: 3\n",
      "idx: 3, impostor: 4\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 2\n",
      "idx: 2, impostor: 3\n",
      "idx: 3, impostor: 4\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 2\n",
      "idx: 2, impostor: 3\n",
      "idx: 3, impostor: 4\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 4\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 5\n",
      "idx: 6, impostor: 6\n",
      "idx: 7, impostor: 7\n",
      "idx: 8, impostor: 8\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 3\n",
      "idx: 3, impostor: 4\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 5\n",
      "idx: 6, impostor: 6\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 5\n",
      "idx: 6, impostor: 6\n",
      "idx: 7, impostor: 7\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 5\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 4\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 5\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 5\n",
      "idx: 6, impostor: 6\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 4\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 5\n",
      "idx: 6, impostor: 6\n",
      "idx: 7, impostor: 7\n",
      "idx: 8, impostor: 8\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 3\n",
      "idx: 3, impostor: 4\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 5\n",
      "idx: 6, impostor: 6\n",
      "idx: 7, impostor: 7\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 5\n",
      "idx: 6, impostor: 6\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 1\n",
      "idx: 1, impostor: 2\n",
      "idx: 2, impostor: 3\n",
      "idx: 3, impostor: 4\n",
      "idx: 4, impostor: 5\n",
      "idx: 5, impostor: 6\n",
      "idx: 6, impostor: 7\n",
      "idx: 7, impostor: 8\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 5\n",
      "idx: 6, impostor: 6\n",
      "idx: 7, impostor: 7\n",
      "idx: 8, impostor: 8\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "imposter: 10 valid_idx range: 913, 995\n",
      "imposter: 11 valid_idx range: 996, 1078\n",
      "imposter: 12 valid_idx range: 1079, 1161\n",
      "imposter: 13 valid_idx range: 1162, 1244\n",
      "imposter: 14 valid_idx range: 1245, 1327\n",
      "imposter: 15 valid_idx range: 1328, 1410\n",
      "imposter: 16 valid_idx range: 1411, 1493\n",
      "imposter: 17 valid_idx range: 1494, 1576\n",
      "imposter: 18 valid_idx range: 1577, 1659\n",
      "idx: 0, impostor: 0\n",
      "idx: 1, impostor: 1\n",
      "idx: 2, impostor: 2\n",
      "idx: 3, impostor: 3\n",
      "idx: 4, impostor: 4\n",
      "idx: 5, impostor: 5\n",
      "idx: 6, impostor: 6\n",
      "idx: 7, impostor: 7\n",
      "idx: 8, impostor: 9\n",
      "idx: 9, impostor: 10\n",
      "idx: 10, impostor: 11\n",
      "idx: 11, impostor: 12\n",
      "idx: 12, impostor: 13\n",
      "idx: 13, impostor: 14\n",
      "idx: 14, impostor: 15\n",
      "idx: 15, impostor: 16\n",
      "idx: 16, impostor: 17\n",
      "idx: 17, impostor: 18\n"
     ]
    }
   ],
   "source": [
    "param_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results = Parallel(n_jobs=8)(delayed(evaluate_owner_IF)(owner_idx, SEED, run, param_dist) for owner_idx in tqdm(range(len(train_set)),desc=\"Owner\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 1.0.2.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "# df_results[df_results['run']==1 ].groupby('owner').head(40)[6*20:8*20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(df_results[(df_results.rank_test_eer == 1) & (df_results.rank_test_accuracy == 1)])\n",
    "len(df_results[(df_results.rank_test_eer == 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_model__n_estimators</th>\n",
       "      <th>param_model__max_samples</th>\n",
       "      <th>param_model__max_features</th>\n",
       "      <th>param_model__contamination</th>\n",
       "      <th>1_test_eer</th>\n",
       "      <th>2_test_eer</th>\n",
       "      <th>3_test_eer</th>\n",
       "      <th>4_test_eer</th>\n",
       "      <th>5_test_eer</th>\n",
       "      <th>6_test_eer</th>\n",
       "      <th>7_test_eer</th>\n",
       "      <th>8_test_eer</th>\n",
       "      <th>9_test_eer</th>\n",
       "      <th>10_test_eer</th>\n",
       "      <th>11_test_eer</th>\n",
       "      <th>12_test_eer</th>\n",
       "      <th>13_test_eer</th>\n",
       "      <th>14_test_eer</th>\n",
       "      <th>15_test_eer</th>\n",
       "      <th>16_test_eer</th>\n",
       "      <th>17_test_eer</th>\n",
       "      <th>18_test_eer</th>\n",
       "      <th>mean_test_eer</th>\n",
       "      <th>std_test_eer</th>\n",
       "      <th>rank_test_eer</th>\n",
       "      <th>1_test_accuracy</th>\n",
       "      <th>2_test_accuracy</th>\n",
       "      <th>3_test_accuracy</th>\n",
       "      <th>4_test_accuracy</th>\n",
       "      <th>5_test_accuracy</th>\n",
       "      <th>6_test_accuracy</th>\n",
       "      <th>7_test_accuracy</th>\n",
       "      <th>8_test_accuracy</th>\n",
       "      <th>9_test_accuracy</th>\n",
       "      <th>10_test_accuracy</th>\n",
       "      <th>11_test_accuracy</th>\n",
       "      <th>12_test_accuracy</th>\n",
       "      <th>13_test_accuracy</th>\n",
       "      <th>14_test_accuracy</th>\n",
       "      <th>15_test_accuracy</th>\n",
       "      <th>16_test_accuracy</th>\n",
       "      <th>17_test_accuracy</th>\n",
       "      <th>18_test_accuracy</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>rank_test_accuracy</th>\n",
       "      <th>owner</th>\n",
       "      <th>run</th>\n",
       "      <th>0_test_eer</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.035941</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.010368</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.067982</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.959505</td>\n",
       "      <td>0.007727</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.035906</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>0.010295</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>0.360825</td>\n",
       "      <td>0.221053</td>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.221053</td>\n",
       "      <td>0.216495</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.504000</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>0.203883</td>\n",
       "      <td>0.216495</td>\n",
       "      <td>0.347368</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.294926</td>\n",
       "      <td>0.107018</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.475904</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.728916</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.644578</td>\n",
       "      <td>0.584337</td>\n",
       "      <td>0.493976</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.644578</td>\n",
       "      <td>0.744311</td>\n",
       "      <td>0.130556</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>0.873494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.227387</td>\n",
       "      <td>0.003038</td>\n",
       "      <td>0.330109</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>48</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.275510</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.520270</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.144578</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.310680</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.236559</td>\n",
       "      <td>0.360360</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.197394</td>\n",
       "      <td>0.107036</td>\n",
       "      <td>1</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.765060</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.463855</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.836011</td>\n",
       "      <td>0.115257</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.260417</td>\n",
       "      <td>0.777108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.218555</td>\n",
       "      <td>0.002707</td>\n",
       "      <td>0.315039</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>1000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.309735</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.058140</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.297297</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.344538</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>0.167091</td>\n",
       "      <td>0.130762</td>\n",
       "      <td>1</td>\n",
       "      <td>0.813253</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.692771</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.692771</td>\n",
       "      <td>0.875167</td>\n",
       "      <td>0.111835</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.969880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.096751</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.025847</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>77</td>\n",
       "      <td>0.9</td>\n",
       "      <td>12</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.267442</td>\n",
       "      <td>0.194175</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>0.206186</td>\n",
       "      <td>0.491935</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.483607</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.194175</td>\n",
       "      <td>0.198020</td>\n",
       "      <td>0.267442</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.308986</td>\n",
       "      <td>0.124522</td>\n",
       "      <td>1</td>\n",
       "      <td>0.740964</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.590361</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.512048</td>\n",
       "      <td>0.542169</td>\n",
       "      <td>0.524096</td>\n",
       "      <td>0.554217</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.740964</td>\n",
       "      <td>0.704819</td>\n",
       "      <td>0.457831</td>\n",
       "      <td>0.723896</td>\n",
       "      <td>0.148245</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.194175</td>\n",
       "      <td>0.879518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.214301</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>0.314889</td>\n",
       "      <td>0.003020</td>\n",
       "      <td>1000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.101124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.362069</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.098901</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.098901</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.120468</td>\n",
       "      <td>0.065775</td>\n",
       "      <td>1</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.692771</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.813253</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.920683</td>\n",
       "      <td>0.062837</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.945783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.095958</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.025472</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>77</td>\n",
       "      <td>0.9</td>\n",
       "      <td>12</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.122222</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.288288</td>\n",
       "      <td>0.254717</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.487013</td>\n",
       "      <td>0.254717</td>\n",
       "      <td>0.111519</td>\n",
       "      <td>0.122217</td>\n",
       "      <td>1</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.813253</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.524096</td>\n",
       "      <td>0.813253</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.114427</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.975904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.035292</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.010231</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>27</td>\n",
       "      <td>0.1</td>\n",
       "      <td>60</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.178947</td>\n",
       "      <td>0.123596</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.152174</td>\n",
       "      <td>0.417910</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.178947</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.058140</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.157192</td>\n",
       "      <td>0.102726</td>\n",
       "      <td>1</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.632530</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.879183</td>\n",
       "      <td>0.089023</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.753012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.249944</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>0.338129</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>60</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.209524</td>\n",
       "      <td>0.209524</td>\n",
       "      <td>0.217822</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>0.247191</td>\n",
       "      <td>0.209524</td>\n",
       "      <td>0.209524</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.213592</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.209524</td>\n",
       "      <td>0.209524</td>\n",
       "      <td>0.209524</td>\n",
       "      <td>0.209524</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.209524</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.217549</td>\n",
       "      <td>0.014983</td>\n",
       "      <td>1</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.777108</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.861446</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.861446</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.846720</td>\n",
       "      <td>0.037525</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.209524</td>\n",
       "      <td>0.867470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.436656</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.113234</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.084337</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.289720</td>\n",
       "      <td>0.164835</td>\n",
       "      <td>0.164835</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.377049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.289720</td>\n",
       "      <td>0.105882</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.078652</td>\n",
       "      <td>0.080460</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.146067</td>\n",
       "      <td>0.134816</td>\n",
       "      <td>0.088766</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.680723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.895917</td>\n",
       "      <td>0.077584</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.078652</td>\n",
       "      <td>0.951807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.036352</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.010720</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>27</td>\n",
       "      <td>0.6</td>\n",
       "      <td>72</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.208791</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.208791</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.123596</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.191011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.424000</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.152941</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.120879</td>\n",
       "      <td>0.242105</td>\n",
       "      <td>0.458647</td>\n",
       "      <td>0.181443</td>\n",
       "      <td>0.099967</td>\n",
       "      <td>1</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.566265</td>\n",
       "      <td>0.852744</td>\n",
       "      <td>0.103841</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.933735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.231900</td>\n",
       "      <td>0.004356</td>\n",
       "      <td>0.334627</td>\n",
       "      <td>0.001562</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>60</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.146067</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.146067</td>\n",
       "      <td>0.156627</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.136842</td>\n",
       "      <td>0.278351</td>\n",
       "      <td>0.326923</td>\n",
       "      <td>0.385965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.146067</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.146067</td>\n",
       "      <td>0.144444</td>\n",
       "      <td>0.406780</td>\n",
       "      <td>0.195468</td>\n",
       "      <td>0.091949</td>\n",
       "      <td>1</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.716867</td>\n",
       "      <td>0.656627</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.632530</td>\n",
       "      <td>0.845047</td>\n",
       "      <td>0.094159</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.921687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.096054</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.025393</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>77</td>\n",
       "      <td>0.9</td>\n",
       "      <td>12</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.184783</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.191011</td>\n",
       "      <td>0.177083</td>\n",
       "      <td>0.223529</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.319588</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.182796</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.171717</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.175258</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.173469</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.208037</td>\n",
       "      <td>0.050318</td>\n",
       "      <td>1</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.704819</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.765060</td>\n",
       "      <td>0.833668</td>\n",
       "      <td>0.065381</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.180851</td>\n",
       "      <td>0.861446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.036164</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.010744</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>27</td>\n",
       "      <td>0.6</td>\n",
       "      <td>72</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.159574</td>\n",
       "      <td>0.154639</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.284211</td>\n",
       "      <td>0.174419</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.268817</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.252747</td>\n",
       "      <td>0.184193</td>\n",
       "      <td>0.043210</td>\n",
       "      <td>1</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.789157</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.857430</td>\n",
       "      <td>0.058681</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.909639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.035833</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.010454</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.327273</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.318584</td>\n",
       "      <td>0.313043</td>\n",
       "      <td>0.302521</td>\n",
       "      <td>0.336449</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.302521</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.302521</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.302521</td>\n",
       "      <td>0.302521</td>\n",
       "      <td>0.310467</td>\n",
       "      <td>0.009357</td>\n",
       "      <td>1</td>\n",
       "      <td>0.777108</td>\n",
       "      <td>0.728916</td>\n",
       "      <td>0.765060</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.777108</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.765060</td>\n",
       "      <td>0.777108</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.765395</td>\n",
       "      <td>0.020228</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.740964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.036608</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.010847</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>27</td>\n",
       "      <td>0.6</td>\n",
       "      <td>72</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.154762</td>\n",
       "      <td>0.154762</td>\n",
       "      <td>0.154762</td>\n",
       "      <td>0.193182</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.303922</td>\n",
       "      <td>0.164706</td>\n",
       "      <td>0.183908</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.154762</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.153103</td>\n",
       "      <td>0.041704</td>\n",
       "      <td>1</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.740964</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.877510</td>\n",
       "      <td>0.049554</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.915663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.035123</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.010161</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>27</td>\n",
       "      <td>0.1</td>\n",
       "      <td>60</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>0.252252</td>\n",
       "      <td>0.271845</td>\n",
       "      <td>0.274510</td>\n",
       "      <td>0.271845</td>\n",
       "      <td>0.466019</td>\n",
       "      <td>0.256881</td>\n",
       "      <td>0.329412</td>\n",
       "      <td>0.288660</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.337349</td>\n",
       "      <td>0.261682</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.252252</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.325009</td>\n",
       "      <td>0.084687</td>\n",
       "      <td>1</td>\n",
       "      <td>0.680723</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.777108</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.542169</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.560241</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.668675</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.469880</td>\n",
       "      <td>0.560241</td>\n",
       "      <td>0.714190</td>\n",
       "      <td>0.113489</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.252252</td>\n",
       "      <td>0.831325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.059389</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.016520</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>46</td>\n",
       "      <td>0.4</td>\n",
       "      <td>48</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.485915</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.112360</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.188889</td>\n",
       "      <td>0.109890</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.130952</td>\n",
       "      <td>0.138372</td>\n",
       "      <td>0.086710</td>\n",
       "      <td>1</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.524096</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.861446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.895248</td>\n",
       "      <td>0.095168</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.939759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.035989</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.010433</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.147727</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.136842</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.357798</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.141304</td>\n",
       "      <td>0.147727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.173555</td>\n",
       "      <td>0.075628</td>\n",
       "      <td>1</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.813253</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.866466</td>\n",
       "      <td>0.080177</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.921687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.239470</td>\n",
       "      <td>0.008075</td>\n",
       "      <td>0.331064</td>\n",
       "      <td>0.002431</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>84</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.233236</td>\n",
       "      <td>0.007086</td>\n",
       "      <td>0.330453</td>\n",
       "      <td>0.002223</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>84</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.347368</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.318681</td>\n",
       "      <td>0.431193</td>\n",
       "      <td>0.446429</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>0.203883</td>\n",
       "      <td>0.221053</td>\n",
       "      <td>0.253012</td>\n",
       "      <td>0.491803</td>\n",
       "      <td>0.302781</td>\n",
       "      <td>0.116004</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.487952</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.590361</td>\n",
       "      <td>0.572289</td>\n",
       "      <td>0.487952</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.861446</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.512048</td>\n",
       "      <td>0.731593</td>\n",
       "      <td>0.139731</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>0.873494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.263917</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.070670</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>215</td>\n",
       "      <td>0.5</td>\n",
       "      <td>36</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.129412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.482517</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.098901</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.105882</td>\n",
       "      <td>0.321101</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.221053</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.102273</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.162892</td>\n",
       "      <td>0.106994</td>\n",
       "      <td>1</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.789157</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.530120</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.868474</td>\n",
       "      <td>0.105403</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.897590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.272066</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>0.076630</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>215</td>\n",
       "      <td>0.9</td>\n",
       "      <td>72</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.193878</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.288288</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.288288</td>\n",
       "      <td>0.268519</td>\n",
       "      <td>0.318966</td>\n",
       "      <td>0.318966</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.131868</td>\n",
       "      <td>0.268519</td>\n",
       "      <td>0.141128</td>\n",
       "      <td>0.113483</td>\n",
       "      <td>1</td>\n",
       "      <td>0.861446</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.801205</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.801205</td>\n",
       "      <td>0.897925</td>\n",
       "      <td>0.089453</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.975904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.014207</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.004296</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.407080</td>\n",
       "      <td>0.161616</td>\n",
       "      <td>0.507353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.168421</td>\n",
       "      <td>0.202381</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.422414</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.255556</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.164948</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.161616</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.168421</td>\n",
       "      <td>0.222654</td>\n",
       "      <td>0.103598</td>\n",
       "      <td>1</td>\n",
       "      <td>0.626506</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.487952</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.801205</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.608434</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.765060</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.814592</td>\n",
       "      <td>0.116614</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.885542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.272306</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.076335</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>215</td>\n",
       "      <td>0.9</td>\n",
       "      <td>72</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.395161</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.096386</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.299065</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.157303</td>\n",
       "      <td>0.128223</td>\n",
       "      <td>0.081662</td>\n",
       "      <td>1</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.656627</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.902945</td>\n",
       "      <td>0.076540</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.951807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.206139</td>\n",
       "      <td>0.010410</td>\n",
       "      <td>0.311011</td>\n",
       "      <td>0.003016</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>12</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.359375</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.474359</td>\n",
       "      <td>0.344000</td>\n",
       "      <td>0.106638</td>\n",
       "      <td>0.146222</td>\n",
       "      <td>1</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.716867</td>\n",
       "      <td>0.777108</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.548193</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.126637</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.993976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.265523</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.070246</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>215</td>\n",
       "      <td>0.5</td>\n",
       "      <td>36</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.203883</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.207921</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>0.207921</td>\n",
       "      <td>0.207764</td>\n",
       "      <td>0.010992</td>\n",
       "      <td>1</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.861446</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.857430</td>\n",
       "      <td>0.029306</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>0.873494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.022140</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>16</td>\n",
       "      <td>0.3</td>\n",
       "      <td>84</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.339450</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.130952</td>\n",
       "      <td>0.242105</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.130952</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.234043</td>\n",
       "      <td>0.167973</td>\n",
       "      <td>0.074385</td>\n",
       "      <td>1</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.765060</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.740964</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.801205</td>\n",
       "      <td>0.872490</td>\n",
       "      <td>0.075006</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.933735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.743676</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.206942</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>599</td>\n",
       "      <td>0.7</td>\n",
       "      <td>72</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.120879</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.234043</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.120879</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.130952</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.120879</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.151482</td>\n",
       "      <td>0.067235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.813253</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.801205</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.068715</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.119565</td>\n",
       "      <td>0.921687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.021811</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.006560</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>16</td>\n",
       "      <td>0.2</td>\n",
       "      <td>48</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>0.209524</td>\n",
       "      <td>0.231579</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>0.209524</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.236559</td>\n",
       "      <td>0.290698</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.217822</td>\n",
       "      <td>0.209524</td>\n",
       "      <td>0.209524</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.364583</td>\n",
       "      <td>0.357895</td>\n",
       "      <td>0.243970</td>\n",
       "      <td>0.052397</td>\n",
       "      <td>1</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.861446</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.716867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.680723</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.861446</td>\n",
       "      <td>0.656627</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.808902</td>\n",
       "      <td>0.073226</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.209524</td>\n",
       "      <td>0.867470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.021937</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.006466</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>16</td>\n",
       "      <td>0.2</td>\n",
       "      <td>48</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.168539</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.381818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.154639</td>\n",
       "      <td>0.376147</td>\n",
       "      <td>0.181813</td>\n",
       "      <td>0.070278</td>\n",
       "      <td>1</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.813253</td>\n",
       "      <td>0.656627</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.870817</td>\n",
       "      <td>0.078448</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.909639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.021951</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.006435</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>16</td>\n",
       "      <td>0.9</td>\n",
       "      <td>24</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>0.409449</td>\n",
       "      <td>0.395161</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.147727</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.271845</td>\n",
       "      <td>0.385246</td>\n",
       "      <td>0.278846</td>\n",
       "      <td>0.385246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.091954</td>\n",
       "      <td>0.226804</td>\n",
       "      <td>0.089888</td>\n",
       "      <td>0.094118</td>\n",
       "      <td>0.271845</td>\n",
       "      <td>0.275644</td>\n",
       "      <td>0.131876</td>\n",
       "      <td>1</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>0.656627</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.469880</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.668675</td>\n",
       "      <td>0.777108</td>\n",
       "      <td>0.668675</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.762048</td>\n",
       "      <td>0.130649</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.475524</td>\n",
       "      <td>0.542169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.036106</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.010537</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>27</td>\n",
       "      <td>0.6</td>\n",
       "      <td>72</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.193878</td>\n",
       "      <td>0.186275</td>\n",
       "      <td>0.186275</td>\n",
       "      <td>0.186275</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.195876</td>\n",
       "      <td>0.186275</td>\n",
       "      <td>0.191919</td>\n",
       "      <td>0.264368</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.186275</td>\n",
       "      <td>0.188119</td>\n",
       "      <td>0.378641</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.216606</td>\n",
       "      <td>0.051694</td>\n",
       "      <td>1</td>\n",
       "      <td>0.861446</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.716867</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.835007</td>\n",
       "      <td>0.071169</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.186275</td>\n",
       "      <td>0.885542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.014343</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.004342</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.175258</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.171717</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.175258</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.175258</td>\n",
       "      <td>0.175258</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.178947</td>\n",
       "      <td>0.177083</td>\n",
       "      <td>0.178947</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.274725</td>\n",
       "      <td>0.173469</td>\n",
       "      <td>0.171717</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.195854</td>\n",
       "      <td>0.066990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.566265</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.856760</td>\n",
       "      <td>0.078738</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.193182</td>\n",
       "      <td>0.825301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.022138</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.006563</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>16</td>\n",
       "      <td>0.4</td>\n",
       "      <td>84</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.144578</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.174419</td>\n",
       "      <td>0.236559</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.330189</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.134831</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.151411</td>\n",
       "      <td>0.050484</td>\n",
       "      <td>1</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.716867</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.883199</td>\n",
       "      <td>0.054814</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.897590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.021840</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.006431</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>16</td>\n",
       "      <td>0.2</td>\n",
       "      <td>36</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.330645</td>\n",
       "      <td>0.344538</td>\n",
       "      <td>0.336066</td>\n",
       "      <td>0.344538</td>\n",
       "      <td>0.422680</td>\n",
       "      <td>0.330645</td>\n",
       "      <td>0.347458</td>\n",
       "      <td>0.353448</td>\n",
       "      <td>0.414141</td>\n",
       "      <td>0.376147</td>\n",
       "      <td>0.336066</td>\n",
       "      <td>0.394231</td>\n",
       "      <td>0.330645</td>\n",
       "      <td>0.330645</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.465909</td>\n",
       "      <td>0.440860</td>\n",
       "      <td>0.367163</td>\n",
       "      <td>0.041927</td>\n",
       "      <td>1</td>\n",
       "      <td>0.656627</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.740964</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.590361</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.716867</td>\n",
       "      <td>0.704819</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.740964</td>\n",
       "      <td>0.632530</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.536145</td>\n",
       "      <td>0.566265</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.070596</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.330645</td>\n",
       "      <td>0.753012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.746362</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.203167</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>599</td>\n",
       "      <td>0.6</td>\n",
       "      <td>60</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.134831</td>\n",
       "      <td>0.485507</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0.149980</td>\n",
       "      <td>0.081555</td>\n",
       "      <td>1</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.524096</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.889893</td>\n",
       "      <td>0.091378</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.927711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.267838</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.072216</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>215</td>\n",
       "      <td>0.7</td>\n",
       "      <td>84</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.131868</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.202247</td>\n",
       "      <td>0.202247</td>\n",
       "      <td>0.462121</td>\n",
       "      <td>0.275510</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.131868</td>\n",
       "      <td>0.154762</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.164703</td>\n",
       "      <td>0.082016</td>\n",
       "      <td>1</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.560241</td>\n",
       "      <td>0.765060</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.875837</td>\n",
       "      <td>0.090320</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.927711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.094760</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.024955</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>77</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.036145</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.051838</td>\n",
       "      <td>0.050040</td>\n",
       "      <td>1</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.036422</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.445781</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.123622</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>359</td>\n",
       "      <td>0.9</td>\n",
       "      <td>60</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.238532</td>\n",
       "      <td>0.508621</td>\n",
       "      <td>0.313253</td>\n",
       "      <td>0.252427</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.238532</td>\n",
       "      <td>0.240741</td>\n",
       "      <td>0.238532</td>\n",
       "      <td>0.238532</td>\n",
       "      <td>0.240741</td>\n",
       "      <td>0.279570</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.309994</td>\n",
       "      <td>0.094905</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.487952</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.777108</td>\n",
       "      <td>0.740964</td>\n",
       "      <td>0.632530</td>\n",
       "      <td>0.590361</td>\n",
       "      <td>0.481928</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.572289</td>\n",
       "      <td>0.735609</td>\n",
       "      <td>0.124787</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.238532</td>\n",
       "      <td>0.843373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.237257</td>\n",
       "      <td>0.004765</td>\n",
       "      <td>0.338551</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>60</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.139785</td>\n",
       "      <td>0.255319</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.503546</td>\n",
       "      <td>0.139785</td>\n",
       "      <td>0.138298</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.138298</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.141304</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.320388</td>\n",
       "      <td>0.138298</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.138298</td>\n",
       "      <td>0.192688</td>\n",
       "      <td>0.096805</td>\n",
       "      <td>1</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.777108</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.493976</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.728916</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.848059</td>\n",
       "      <td>0.108258</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.825301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1.227985</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>0.330484</td>\n",
       "      <td>0.002589</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>48</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.286957</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.405797</td>\n",
       "      <td>0.364341</td>\n",
       "      <td>0.410072</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>0.157320</td>\n",
       "      <td>0.167088</td>\n",
       "      <td>1</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.765060</td>\n",
       "      <td>0.656627</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.668675</td>\n",
       "      <td>0.877510</td>\n",
       "      <td>0.136546</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.993976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.060105</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.017424</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>46</td>\n",
       "      <td>1.0</td>\n",
       "      <td>72</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.261682</td>\n",
       "      <td>0.252252</td>\n",
       "      <td>0.382022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.256881</td>\n",
       "      <td>0.252252</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.402174</td>\n",
       "      <td>0.321839</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.311111</td>\n",
       "      <td>0.252252</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.252252</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.261682</td>\n",
       "      <td>0.321839</td>\n",
       "      <td>0.288198</td>\n",
       "      <td>0.051692</td>\n",
       "      <td>1</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.626506</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.608434</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.620482</td>\n",
       "      <td>0.704819</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.768072</td>\n",
       "      <td>0.082775</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.252252</td>\n",
       "      <td>0.831325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.266792</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.074419</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>215</td>\n",
       "      <td>0.7</td>\n",
       "      <td>60</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.144444</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.425373</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.252427</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.180851</td>\n",
       "      <td>0.114414</td>\n",
       "      <td>0.091029</td>\n",
       "      <td>1</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.620482</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.861446</td>\n",
       "      <td>0.914659</td>\n",
       "      <td>0.083599</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.963855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.094750</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.025022</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>77</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.184783</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.475524</td>\n",
       "      <td>0.091954</td>\n",
       "      <td>0.122194</td>\n",
       "      <td>0.092357</td>\n",
       "      <td>1</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.542169</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.913320</td>\n",
       "      <td>0.096368</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.951807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.439226</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.116401</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>359</td>\n",
       "      <td>0.3</td>\n",
       "      <td>84</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.164706</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.252632</td>\n",
       "      <td>0.211111</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.144578</td>\n",
       "      <td>0.317308</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.153395</td>\n",
       "      <td>0.051918</td>\n",
       "      <td>1</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.813253</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.728916</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.886881</td>\n",
       "      <td>0.058114</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.927711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.095225</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.024702</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>77</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.170455</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.174419</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.163043</td>\n",
       "      <td>0.180970</td>\n",
       "      <td>0.054697</td>\n",
       "      <td>1</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.740964</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.704819</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.868474</td>\n",
       "      <td>0.064640</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.909639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.262946</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.069063</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>215</td>\n",
       "      <td>0.7</td>\n",
       "      <td>24</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.376000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.058140</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.058140</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.111799</td>\n",
       "      <td>0.094085</td>\n",
       "      <td>1</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.728916</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.079868</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.963855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.098419</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.027962</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>77</td>\n",
       "      <td>0.5</td>\n",
       "      <td>72</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.186275</td>\n",
       "      <td>0.197917</td>\n",
       "      <td>0.188119</td>\n",
       "      <td>0.186275</td>\n",
       "      <td>0.188119</td>\n",
       "      <td>0.186275</td>\n",
       "      <td>0.195876</td>\n",
       "      <td>0.202128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.423423</td>\n",
       "      <td>0.186275</td>\n",
       "      <td>0.195876</td>\n",
       "      <td>0.186275</td>\n",
       "      <td>0.186275</td>\n",
       "      <td>0.186275</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.471074</td>\n",
       "      <td>0.223081</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.740964</td>\n",
       "      <td>0.542169</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.098762</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.186275</td>\n",
       "      <td>0.885542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.094857</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.025006</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>77</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.189474</td>\n",
       "      <td>0.178218</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.197802</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.178218</td>\n",
       "      <td>0.183673</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.202247</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.178218</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.178218</td>\n",
       "      <td>0.178218</td>\n",
       "      <td>0.178218</td>\n",
       "      <td>0.178218</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.185867</td>\n",
       "      <td>0.008812</td>\n",
       "      <td>1</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.867805</td>\n",
       "      <td>0.026976</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0.178218</td>\n",
       "      <td>0.891566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.095385</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.025019</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>77</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.244681</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.317308</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.260417</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.398305</td>\n",
       "      <td>0.408333</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.202247</td>\n",
       "      <td>0.330189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.174419</td>\n",
       "      <td>0.221440</td>\n",
       "      <td>0.095443</td>\n",
       "      <td>1</td>\n",
       "      <td>0.789157</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.728916</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.777108</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.644578</td>\n",
       "      <td>0.632530</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.716867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.816600</td>\n",
       "      <td>0.094809</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0.164706</td>\n",
       "      <td>0.843373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1.222765</td>\n",
       "      <td>0.002936</td>\n",
       "      <td>0.330975</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>48</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.164706</td>\n",
       "      <td>0.144330</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.144330</td>\n",
       "      <td>0.224719</td>\n",
       "      <td>0.152174</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.150538</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.415254</td>\n",
       "      <td>0.241758</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.144330</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.389381</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.420168</td>\n",
       "      <td>0.202516</td>\n",
       "      <td>0.095906</td>\n",
       "      <td>1</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.620482</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.837684</td>\n",
       "      <td>0.101151</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0.144330</td>\n",
       "      <td>0.915663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.014468</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.004515</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>24</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.171717</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.188889</td>\n",
       "      <td>0.171717</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.204819</td>\n",
       "      <td>0.180851</td>\n",
       "      <td>0.177083</td>\n",
       "      <td>0.195402</td>\n",
       "      <td>0.202381</td>\n",
       "      <td>0.186813</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.178947</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.175258</td>\n",
       "      <td>0.230139</td>\n",
       "      <td>0.094090</td>\n",
       "      <td>1</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.765060</td>\n",
       "      <td>0.765060</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.692771</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.861446</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.801205</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.656627</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.801539</td>\n",
       "      <td>0.110978</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0.182796</td>\n",
       "      <td>0.855422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.014561</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.004490</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9</td>\n",
       "      <td>36</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.168421</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.161616</td>\n",
       "      <td>0.229885</td>\n",
       "      <td>0.172043</td>\n",
       "      <td>0.179775</td>\n",
       "      <td>0.161616</td>\n",
       "      <td>0.161616</td>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.161616</td>\n",
       "      <td>0.172043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.161616</td>\n",
       "      <td>0.161616</td>\n",
       "      <td>0.189522</td>\n",
       "      <td>0.043970</td>\n",
       "      <td>1</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.740964</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.852410</td>\n",
       "      <td>0.058138</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.316327</td>\n",
       "      <td>0.716867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.095868</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.025051</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>77</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.338843</td>\n",
       "      <td>0.330645</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.330645</td>\n",
       "      <td>0.341667</td>\n",
       "      <td>0.341667</td>\n",
       "      <td>0.330645</td>\n",
       "      <td>0.350427</td>\n",
       "      <td>0.341667</td>\n",
       "      <td>0.460674</td>\n",
       "      <td>0.347458</td>\n",
       "      <td>0.330645</td>\n",
       "      <td>0.350427</td>\n",
       "      <td>0.330645</td>\n",
       "      <td>0.330645</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.344538</td>\n",
       "      <td>0.476744</td>\n",
       "      <td>0.352331</td>\n",
       "      <td>0.041823</td>\n",
       "      <td>1</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.728916</td>\n",
       "      <td>0.728916</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.728916</td>\n",
       "      <td>0.542169</td>\n",
       "      <td>0.716867</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.524096</td>\n",
       "      <td>0.714859</td>\n",
       "      <td>0.066052</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.330645</td>\n",
       "      <td>0.753012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.759016</td>\n",
       "      <td>0.002997</td>\n",
       "      <td>0.205988</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>599</td>\n",
       "      <td>0.9</td>\n",
       "      <td>60</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.486667</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.094118</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.134831</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.163043</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.252427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.306306</td>\n",
       "      <td>0.139664</td>\n",
       "      <td>0.109477</td>\n",
       "      <td>1</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.524096</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.890897</td>\n",
       "      <td>0.106840</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.963855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.098878</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.027996</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>77</td>\n",
       "      <td>0.5</td>\n",
       "      <td>72</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.108434</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.105882</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.108434</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.139867</td>\n",
       "      <td>0.093360</td>\n",
       "      <td>1</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.548193</td>\n",
       "      <td>0.789157</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.801205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.891232</td>\n",
       "      <td>0.095745</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.945783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "12       0.035941      0.000147         0.010368        0.000508   \n",
       "12       0.035906      0.000639         0.010295        0.000187   \n",
       "6        1.227387      0.003038         0.330109        0.003258   \n",
       "53       1.218555      0.002707         0.315039        0.002055   \n",
       "69       0.096751      0.000492         0.025847        0.000521   \n",
       "53       1.214301      0.002859         0.314889        0.003020   \n",
       "69       0.095958      0.000631         0.025472        0.000344   \n",
       "61       0.035292      0.000141         0.010231        0.000162   \n",
       "36       1.249944      0.004029         0.338129        0.001830   \n",
       "23       0.436656      0.000914         0.113234        0.000744   \n",
       "73       0.036352      0.000466         0.010720        0.000221   \n",
       "36       1.231900      0.004356         0.334627        0.001562   \n",
       "69       0.096054      0.000182         0.025393        0.000184   \n",
       "73       0.036164      0.000217         0.010744        0.000195   \n",
       "12       0.035833      0.000195         0.010454        0.000202   \n",
       "73       0.036608      0.000108         0.010847        0.000158   \n",
       "61       0.035123      0.000255         0.010161        0.000189   \n",
       "2        0.059389      0.000410         0.016520        0.000247   \n",
       "12       0.035989      0.000154         0.010433        0.000248   \n",
       "53       1.239470      0.008075         0.331064        0.002431   \n",
       "53       1.233236      0.007086         0.330453        0.002223   \n",
       "1        0.263917      0.000757         0.070670        0.000473   \n",
       "73       0.272066      0.002379         0.076630        0.000582   \n",
       "47       0.014207      0.000167         0.004296        0.000137   \n",
       "73       0.272306      0.001412         0.076335        0.000651   \n",
       "12       1.206139      0.010410         0.311011        0.003016   \n",
       "1        0.265523      0.001419         0.070246        0.000619   \n",
       "20       0.022140      0.000159         0.006494        0.000180   \n",
       "28       0.743676      0.001352         0.206942        0.001150   \n",
       "41       0.021811      0.000211         0.006560        0.000164   \n",
       "41       0.021937      0.000598         0.006466        0.000199   \n",
       "74       0.021951      0.000135         0.006435        0.000123   \n",
       "59       0.036106      0.000157         0.010537        0.000172   \n",
       "19       0.014343      0.000089         0.004342        0.000173   \n",
       "27       0.022138      0.000112         0.006563        0.000167   \n",
       "3        0.021840      0.000316         0.006431        0.000164   \n",
       "77       0.746362      0.001559         0.203167        0.001314   \n",
       "72       0.267838      0.000817         0.072216        0.000532   \n",
       "57       0.094760      0.000295         0.024955        0.000272   \n",
       "13       0.445781      0.000614         0.123622        0.001065   \n",
       "51       1.237257      0.004765         0.338551        0.002576   \n",
       "62       1.227985      0.002379         0.330484        0.002589   \n",
       "74       0.060105      0.000230         0.017424        0.000258   \n",
       "68       0.266792      0.001050         0.074419        0.001034   \n",
       "57       0.094750      0.000874         0.025022        0.000358   \n",
       "48       0.439226      0.001717         0.116401        0.001011   \n",
       "57       0.095225      0.000456         0.024702        0.000280   \n",
       "1        0.262946      0.000798         0.069063        0.000439   \n",
       "63       0.098419      0.000702         0.027962        0.000302   \n",
       "57       0.094857      0.000884         0.025006        0.000263   \n",
       "57       0.095385      0.000402         0.025019        0.000154   \n",
       "71       1.222765      0.002936         0.330975        0.001634   \n",
       "52       0.014468      0.000089         0.004515        0.000106   \n",
       "65       0.014561      0.000048         0.004490        0.000180   \n",
       "57       0.095868      0.000230         0.025051        0.000531   \n",
       "53       0.759016      0.002997         0.205988        0.001313   \n",
       "63       0.098878      0.000977         0.027996        0.000313   \n",
       "\n",
       "   param_model__n_estimators param_model__max_samples  \\\n",
       "12                        27                      1.0   \n",
       "12                        27                      1.0   \n",
       "6                       1000                      0.6   \n",
       "53                      1000                      1.0   \n",
       "69                        77                      0.9   \n",
       "53                      1000                      1.0   \n",
       "69                        77                      0.9   \n",
       "61                        27                      0.1   \n",
       "36                      1000                      0.6   \n",
       "23                       359                      1.0   \n",
       "73                        27                      0.6   \n",
       "36                      1000                      0.6   \n",
       "69                        77                      0.9   \n",
       "73                        27                      0.6   \n",
       "12                        27                      1.0   \n",
       "73                        27                      0.6   \n",
       "61                        27                      0.1   \n",
       "2                         46                      0.4   \n",
       "12                        27                      1.0   \n",
       "53                      1000                      0.9   \n",
       "53                      1000                      0.9   \n",
       "1                        215                      0.5   \n",
       "73                       215                      0.9   \n",
       "47                        10                      0.1   \n",
       "73                       215                      0.9   \n",
       "12                      1000                      0.7   \n",
       "1                        215                      0.5   \n",
       "20                        16                      0.3   \n",
       "28                       599                      0.7   \n",
       "41                        16                      0.2   \n",
       "41                        16                      0.2   \n",
       "74                        16                      0.9   \n",
       "59                        27                      0.6   \n",
       "19                        10                      0.2   \n",
       "27                        16                      0.4   \n",
       "3                         16                      0.2   \n",
       "77                       599                      0.6   \n",
       "72                       215                      0.7   \n",
       "57                        77                      0.4   \n",
       "13                       359                      0.9   \n",
       "51                      1000                      0.8   \n",
       "62                      1000                      0.5   \n",
       "74                        46                      1.0   \n",
       "68                       215                      0.7   \n",
       "57                        77                      0.4   \n",
       "48                       359                      0.3   \n",
       "57                        77                      0.4   \n",
       "1                        215                      0.7   \n",
       "63                        77                      0.5   \n",
       "57                        77                      0.4   \n",
       "57                        77                      0.4   \n",
       "71                      1000                      0.6   \n",
       "52                        10                      0.5   \n",
       "65                        10                      0.9   \n",
       "57                        77                      0.4   \n",
       "53                       599                      0.9   \n",
       "63                        77                      0.5   \n",
       "\n",
       "   param_model__max_features param_model__contamination  1_test_eer  \\\n",
       "12                        84                        0.1    0.067416   \n",
       "12                        84                        0.1         NaN   \n",
       "6                         48                        0.1    0.133333   \n",
       "53                        12                        0.1    0.250000   \n",
       "69                        12                        0.1    0.267442   \n",
       "53                        12                        0.1    0.100000   \n",
       "69                        12                        0.1    0.046512   \n",
       "61                        60                        0.1    0.333333   \n",
       "36                        60                        0.1    0.209524   \n",
       "23                        12                        0.2    0.084337   \n",
       "73                        72                        0.1    0.208791   \n",
       "36                        60                        0.1    0.146067   \n",
       "69                        12                        0.1    0.184783   \n",
       "73                        72                        0.1    0.161290   \n",
       "12                        84                        0.1    0.305085   \n",
       "73                        72                        0.1    0.133333   \n",
       "61                        60                        0.1    0.325581   \n",
       "2                         48                        0.2    0.107527   \n",
       "12                        84                        0.1    0.147727   \n",
       "53                        84                        0.1    0.097826   \n",
       "53                        84                        0.1         NaN   \n",
       "1                         36                        0.1    0.129412   \n",
       "73                        72                        0.1    0.193878   \n",
       "47                        12                        0.3    0.407080   \n",
       "73                        72                        0.1    0.107143   \n",
       "12                        12                        0.1    0.046512   \n",
       "1                         36                        0.1    0.203883   \n",
       "20                        84                        0.4    0.118280   \n",
       "28                        72                        0.2    0.120879   \n",
       "41                        48                        0.4    0.224490   \n",
       "41                        48                        0.4    0.156250   \n",
       "74                        24                        0.1    0.324324   \n",
       "59                        72                        0.1    0.193878   \n",
       "19                        12                        0.1    0.175258   \n",
       "27                        84                        0.1    0.136364   \n",
       "3                         36                        0.1    0.379630   \n",
       "77                        60                        0.2    0.130435   \n",
       "72                        84                        0.1    0.131868   \n",
       "57                        12                        0.1    0.034884   \n",
       "13                        60                        0.1         NaN   \n",
       "51                        60                        0.1    0.142857   \n",
       "62                        48                        0.1    0.286957   \n",
       "74                        72                        0.2    0.261682   \n",
       "68                        60                        0.1    0.144444   \n",
       "57                        12                        0.1    0.087912   \n",
       "48                        84                        0.1    0.164706   \n",
       "57                        12                        0.1    0.153061   \n",
       "1                         24                        0.2    0.059524   \n",
       "63                        72                        0.1    0.190000   \n",
       "57                        12                        0.1    0.189474   \n",
       "57                        12                        0.1    0.244681   \n",
       "71                        48                        0.1    0.164706   \n",
       "52                        24                        0.1    0.171717   \n",
       "65                        36                        0.1    0.168421   \n",
       "57                        12                        0.1    0.338843   \n",
       "53                        60                        0.1    0.214286   \n",
       "63                        72                        0.1    0.100000   \n",
       "\n",
       "    2_test_eer  3_test_eer  4_test_eer  5_test_eer  6_test_eer  7_test_eer  \\\n",
       "12    0.067416    0.067416    0.067416    0.067416    0.067416    0.068966   \n",
       "12    0.201923    0.515625    0.360825    0.221053    0.279070    0.221053   \n",
       "6          NaN    0.127660    0.130435    0.275510    0.126316    0.520270   \n",
       "53    0.056818         NaN    0.309735    0.056818    0.058140    0.056818   \n",
       "69    0.194175    0.432432         NaN    0.250000    0.204082    0.206186   \n",
       "53    0.097826    0.097826    0.101124         NaN    0.097826    0.097826   \n",
       "69    0.045977    0.047059    0.046512    0.045977         NaN    0.045977   \n",
       "61    0.082353    0.178947    0.123596    0.142857    0.113636         NaN   \n",
       "36    0.209524    0.217822    0.244444    0.247191    0.209524    0.209524   \n",
       "23    0.077778    0.289720    0.164835    0.164835    0.077778    0.077778   \n",
       "73    0.117021    0.208791    0.125000    0.123596    0.125000    0.118280   \n",
       "36    0.135417    0.285714    0.146067    0.156627    0.135417    0.136842   \n",
       "69    0.170000    0.191011    0.177083    0.223529    0.170000    0.319588   \n",
       "73    0.153061    0.159574    0.154639    0.200000    0.227273    0.153061   \n",
       "12    0.327273    0.310345    0.318584    0.313043    0.302521    0.336449   \n",
       "73    0.154762    0.154762    0.154762    0.193182    0.126316    0.303922   \n",
       "61    0.252252    0.271845    0.274510    0.271845    0.466019    0.256881   \n",
       "2     0.107527    0.108696    0.107527    0.117647    0.485915    0.107527   \n",
       "12    0.135417    0.166667    0.142857    0.136842    0.142857    0.135417   \n",
       "53    0.097826    0.097826    0.097826    0.097826    0.097826    0.097826   \n",
       "53    0.201923    0.507937    0.347368    0.247059    0.218750    0.241379   \n",
       "1          NaN    0.097826    0.100000    0.260000    0.097826    0.482517   \n",
       "73    0.045977         NaN    0.288288    0.045977    0.047059    0.047059   \n",
       "47    0.161616    0.507353         NaN    0.168421    0.202381    0.163265   \n",
       "73    0.087912    0.087912    0.093023         NaN    0.087912    0.087912   \n",
       "12    0.011905    0.057471    0.023810    0.023810         NaN    0.011905   \n",
       "1     0.201923    0.241379    0.225806    0.201923    0.201923         NaN   \n",
       "20    0.117021    0.181818    0.280000    0.339450    0.118280    0.117021   \n",
       "28    0.117021    0.217391    0.127907    0.234043    0.117021    0.117021   \n",
       "41    0.209524    0.231579    0.224490    0.209524    0.220000    0.211538   \n",
       "41    0.153061    0.168539    0.153061    0.153061    0.153061    0.157895   \n",
       "74    0.409449    0.395161    0.285714    0.218750    0.147727    0.516129   \n",
       "59    0.186275    0.186275    0.186275    0.190000    0.247059    0.195876   \n",
       "19    0.454545    0.171717    0.170000    0.175258    0.170000    0.175258   \n",
       "27    0.144578    0.126316    0.174419    0.236559    0.126316    0.330189   \n",
       "3     0.330645    0.344538    0.336066    0.344538    0.422680    0.330645   \n",
       "77    0.126316    0.129032    0.126316    0.134831    0.485507    0.126316   \n",
       "72    0.126316    0.139535    0.126316    0.126316    0.127660    0.126316   \n",
       "57    0.047619    0.035714    0.035294    0.035714    0.035714    0.245283   \n",
       "13    0.238532    0.508621    0.313253    0.252427    0.245283    0.265306   \n",
       "51         NaN    0.135417    0.139785    0.255319    0.135417    0.503546   \n",
       "62    0.011905         NaN    0.327869    0.011905    0.023810    0.023810   \n",
       "74    0.252252    0.382022         NaN    0.256881    0.252252    0.254545   \n",
       "68    0.067416    0.067416    0.153846         NaN    0.067416    0.068182   \n",
       "57    0.087912    0.087912    0.088889    0.087912         NaN    0.088889   \n",
       "48    0.126316    0.252632    0.211111    0.127660    0.126316         NaN   \n",
       "57    0.153061    0.170455    0.276596    0.291667    0.153061    0.153061   \n",
       "1     0.056818    0.170213    0.133333    0.082353    0.056818    0.056818   \n",
       "63    0.186275    0.197917    0.188119    0.186275    0.188119    0.186275   \n",
       "57    0.178218    0.195652    0.197802    0.181818    0.178218    0.183673   \n",
       "57    0.127660    0.317308    0.290000    0.260417    0.127660    0.398305   \n",
       "71    0.144330    0.153846    0.144330    0.224719    0.152174    0.148936   \n",
       "52    0.538462    0.250000    0.250000    0.188889    0.171717    0.340000   \n",
       "65    0.211765    0.166667    0.186047    0.188235    0.161616    0.229885   \n",
       "57    0.330645    0.333333    0.330645    0.341667    0.341667    0.330645   \n",
       "53    0.067416    0.069767    0.067416    0.125000    0.486667    0.067416   \n",
       "63    0.097826    0.119048    0.097826    0.097826    0.108434    0.097826   \n",
       "\n",
       "    8_test_eer  9_test_eer  10_test_eer  11_test_eer  12_test_eer  \\\n",
       "12    0.068182    0.068966     0.067416     0.067416     0.068966   \n",
       "12    0.216495    0.380000     0.436364     0.504000     0.201923   \n",
       "6     0.136364    0.144578     0.127660     0.133333     0.310680   \n",
       "53    0.297297    0.333333     0.344538     0.370968     0.056818   \n",
       "69    0.491935    0.470588     0.483607     0.461538     0.196078   \n",
       "53    0.362069    0.100000     0.097826     0.098901     0.097826   \n",
       "69    0.122222    0.045977     0.288288     0.254717     0.045977   \n",
       "61    0.152174    0.417910     0.056818     0.220000     0.178947   \n",
       "36         NaN    0.213592     0.211538     0.258824     0.209524   \n",
       "23    0.377049         NaN     0.095238     0.289720     0.105882   \n",
       "73    0.181818    0.191011          NaN     0.424000     0.117021   \n",
       "36    0.278351    0.326923     0.385965          NaN     0.135417   \n",
       "69    0.326531    0.182796     0.266667     0.241379          NaN   \n",
       "73    0.156250    0.157895     0.284211     0.174419     0.153061   \n",
       "12    0.305085    0.307692     0.302521     0.310345     0.305085   \n",
       "73    0.164706    0.183908     0.126316     0.129032     0.154762   \n",
       "61    0.329412    0.288660     0.450000     0.337349     0.261682   \n",
       "2     0.112360    0.108696     0.188889     0.109890     0.107527   \n",
       "12    0.186047    0.204545     0.401709     0.357798     0.135417   \n",
       "53    0.097826    0.097826     0.097826     0.097826     0.097826   \n",
       "53    0.318681    0.431193     0.446429     0.507937     0.201923   \n",
       "1     0.103448    0.098901     0.097826     0.105882     0.321101   \n",
       "73    0.288288    0.268519     0.318966     0.318966     0.045977   \n",
       "47    0.422414    0.181818     0.255556     0.186047     0.181818   \n",
       "73    0.395161    0.090909     0.088889     0.137931     0.096386   \n",
       "12    0.088889    0.088889     0.359375     0.305085     0.011905   \n",
       "1     0.207921    0.225806     0.201923     0.205882     0.201923   \n",
       "20         NaN    0.130952     0.242105     0.307692     0.117021   \n",
       "28    0.384615         NaN     0.120879     0.200000     0.130952   \n",
       "41    0.236559    0.290698          NaN     0.336957     0.215686   \n",
       "41    0.153061    0.190476     0.381818          NaN     0.156250   \n",
       "74    0.271845    0.385246     0.278846     0.385246          NaN   \n",
       "59    0.186275    0.191919     0.264368     0.190000     0.190000   \n",
       "19    0.175258    0.170000     0.170000     0.178947     0.177083   \n",
       "27    0.139535    0.141176     0.126316     0.126316     0.141176   \n",
       "3     0.347458    0.353448     0.414141     0.376147     0.336066   \n",
       "77    0.126316    0.126316     0.130435     0.129032     0.126316   \n",
       "72    0.202247    0.202247     0.462121     0.275510     0.126316   \n",
       "57    0.034884    0.036145     0.034884     0.035714     0.035294   \n",
       "13    0.282609    0.380435     0.424242     0.512821     0.238532   \n",
       "51    0.139785    0.138298     0.135417     0.138298     0.313725   \n",
       "62    0.316667    0.405797     0.364341     0.410072     0.011905   \n",
       "74    0.402174    0.321839     0.388889     0.311111     0.252252   \n",
       "68    0.425373    0.069767     0.070588     0.083333     0.067416   \n",
       "57    0.090909    0.088889     0.210526     0.184783     0.087912   \n",
       "48    0.144578    0.317308     0.126316     0.139535     0.126316   \n",
       "57         NaN    0.157895     0.156250     0.333333     0.153061   \n",
       "1     0.376000         NaN     0.093023     0.338983     0.082353   \n",
       "63    0.195876    0.202128          NaN     0.423423     0.186275   \n",
       "57    0.195652    0.200000     0.202247          NaN     0.178218   \n",
       "57    0.408333    0.290000     0.202247     0.330189          NaN   \n",
       "71    0.150538    0.162791     0.415254     0.241758     0.148936   \n",
       "52    0.204819    0.180851     0.177083     0.195402     0.202381   \n",
       "65    0.172043    0.179775     0.161616     0.161616     0.287234   \n",
       "57    0.350427    0.341667     0.460674     0.347458     0.330645   \n",
       "53    0.094118    0.070588     0.134831     0.125000     0.067416   \n",
       "63    0.105882    0.119048     0.471429     0.260000     0.097826   \n",
       "\n",
       "    13_test_eer  14_test_eer  15_test_eer  16_test_eer  17_test_eer  \\\n",
       "12     0.067416     0.071429     0.067416     0.067416     0.068182   \n",
       "12     0.218750     0.201923     0.203883     0.216495     0.347368   \n",
       "6      0.139535     0.236559     0.360360     0.130435     0.126316   \n",
       "53     0.056818     0.056818     0.056818     0.056818     0.161290   \n",
       "69     0.208333     0.194175     0.198020     0.267442     0.315217   \n",
       "53     0.098901     0.097826     0.097826     0.229167     0.097826   \n",
       "69     0.045977     0.045977     0.045977     0.046512     0.487013   \n",
       "61     0.057471     0.058140     0.113636     0.056818     0.170213   \n",
       "36     0.209524     0.209524     0.209524     0.211538     0.209524   \n",
       "23     0.082353     0.077778     0.078652     0.080460     0.077778   \n",
       "73     0.152941     0.117021     0.117021     0.120879     0.242105   \n",
       "36     0.146067     0.135417     0.135417     0.146067     0.144444   \n",
       "69     0.171717     0.170000     0.175258     0.170000     0.173469   \n",
       "73          NaN     0.153061     0.153061     0.268817     0.200000   \n",
       "12     0.302521          NaN     0.307692     0.307692     0.302521   \n",
       "73     0.129032     0.126316          NaN     0.136364     0.126316   \n",
       "61     0.333333     0.252252     0.254545          NaN     0.521739   \n",
       "2      0.116279     0.107527     0.107527     0.151163          NaN   \n",
       "12     0.135417     0.135417     0.135417     0.141304     0.147727   \n",
       "53     0.097826     0.097826     0.097826     0.097826     0.097826   \n",
       "53     0.205882     0.201923     0.203883     0.221053     0.253012   \n",
       "1      0.107143     0.221053     0.301887     0.102273     0.097826   \n",
       "73     0.047059     0.045977     0.045977     0.045977     0.131868   \n",
       "47     0.164948     0.163265     0.161616     0.163265     0.181818   \n",
       "73     0.137931     0.087912     0.088889     0.299065     0.087912   \n",
       "12     0.012048     0.011905     0.011905     0.023810     0.474359   \n",
       "1      0.201923     0.201923     0.201923     0.201923     0.201923   \n",
       "20     0.118280     0.117021     0.118280     0.130952     0.118280   \n",
       "28     0.118280     0.117021     0.118280     0.120879     0.117021   \n",
       "41     0.217822     0.209524     0.209524     0.211538     0.364583   \n",
       "41     0.153061     0.153061     0.153061     0.153061     0.154639   \n",
       "74     0.093023     0.091954     0.226804     0.089888     0.094118   \n",
       "59          NaN     0.186275     0.188119     0.378641     0.304348   \n",
       "19     0.178947          NaN     0.274725     0.173469     0.171717   \n",
       "27     0.126316     0.134831          NaN     0.129032     0.126316   \n",
       "3      0.394231     0.330645     0.330645          NaN     0.465909   \n",
       "77     0.141176     0.126316     0.126316     0.141176          NaN   \n",
       "72     0.126316     0.126316     0.126316     0.131868     0.154762   \n",
       "57     0.034884     0.111111     0.034884     0.034884     0.035294   \n",
       "13     0.240741     0.238532     0.238532     0.240741     0.279570   \n",
       "51     0.141304     0.230769     0.320388     0.138298     0.135417   \n",
       "62     0.012048     0.011905     0.011905     0.011905     0.180000   \n",
       "74     0.254545     0.252252     0.254545     0.254545     0.261682   \n",
       "68     0.069767     0.067416     0.068966     0.252427     0.067416   \n",
       "57     0.087912     0.087912     0.087912     0.087912     0.475524   \n",
       "48     0.126316     0.126316     0.136364     0.126316     0.127660   \n",
       "57     0.156250     0.153061     0.153061     0.174419     0.153061   \n",
       "1      0.058140     0.056818     0.057471     0.058140     0.056818   \n",
       "63     0.195876     0.186275     0.186275     0.186275     0.272727   \n",
       "57     0.180000     0.178218     0.178218     0.178218     0.178218   \n",
       "57     0.126316     0.126316     0.136364     0.127660     0.133333   \n",
       "71          NaN     0.144330     0.145833     0.389381     0.148936   \n",
       "52     0.186813          NaN     0.377358     0.178947     0.170000   \n",
       "65     0.161616     0.172043          NaN     0.163265     0.161616   \n",
       "57     0.350427     0.330645     0.330645          NaN     0.344538   \n",
       "53     0.163043     0.067416     0.067416     0.252427          NaN   \n",
       "63     0.097826     0.097826     0.097826     0.108434     0.244898   \n",
       "\n",
       "    18_test_eer  mean_test_eer  std_test_eer  rank_test_eer  1_test_accuracy  \\\n",
       "12     0.067416       0.067982      0.001019              1         0.963855   \n",
       "12     0.380000       0.294926      0.107018              1              NaN   \n",
       "6      0.133333       0.197394      0.107036              1         0.897590   \n",
       "53     0.370968       0.167091      0.130762              1         0.813253   \n",
       "69     0.526316       0.308986      0.124522              1         0.740964   \n",
       "53     0.100000       0.120468      0.065775              1         0.933735   \n",
       "69     0.254717       0.111519      0.122217              1         0.969880   \n",
       "61     0.056818       0.157192      0.102726              1         0.734940   \n",
       "36     0.215686       0.217549      0.014983              1         0.867470   \n",
       "23     0.146067       0.134816      0.088766              1         0.915663   \n",
       "73     0.458647       0.181443      0.099967              1         0.819277   \n",
       "36     0.406780       0.195468      0.091949              1         0.879518   \n",
       "69     0.250000       0.208037      0.050318              1         0.849398   \n",
       "73     0.252747       0.184193      0.043210              1         0.879518   \n",
       "12     0.302521       0.310467      0.009357              1         0.777108   \n",
       "73     0.129032       0.153103      0.041704              1         0.897590   \n",
       "61     0.450000       0.325009      0.084687              1         0.680723   \n",
       "2      0.130952       0.138372      0.086710              1         0.939759   \n",
       "12          NaN       0.173555      0.075628              1         0.873494   \n",
       "53     0.097826       0.097826      0.000000              1         0.945783   \n",
       "53     0.491803       0.302781      0.116004              1              NaN   \n",
       "1      0.100000       0.162892      0.106994              1         0.879518   \n",
       "73     0.268519       0.141128      0.113483              1         0.861446   \n",
       "47     0.168421       0.222654      0.103598              1         0.626506   \n",
       "73     0.157303       0.128223      0.081662              1         0.897590   \n",
       "12     0.344000       0.106638      0.146222              1         0.969880   \n",
       "1      0.207921       0.207764      0.010992              1         0.867470   \n",
       "20     0.234043       0.167973      0.074385              1         0.927711   \n",
       "28     0.127907       0.151482      0.067235              1         0.915663   \n",
       "41     0.357895       0.243970      0.052397              1         0.825301   \n",
       "41     0.376147       0.181813      0.070278              1         0.897590   \n",
       "74     0.271845       0.275644      0.131876              1         0.734940   \n",
       "59     0.247059       0.216606      0.051694              1         0.861446   \n",
       "19     0.170000       0.195854      0.066990              1         0.879518   \n",
       "27     0.126316       0.151411      0.050484              1         0.885542   \n",
       "3      0.440860       0.367163      0.041927              1         0.656627   \n",
       "77     0.141176       0.149980      0.081555              1         0.909639   \n",
       "72          NaN       0.164703      0.082016              1         0.903614   \n",
       "57     0.034884       0.051838      0.050040              1         0.981928   \n",
       "13     0.441176       0.309994      0.094905              1              NaN   \n",
       "51     0.138298       0.192688      0.096805              1         0.891566   \n",
       "62     0.397059       0.157320      0.167088              1         0.795181   \n",
       "74     0.321839       0.288198      0.051692              1         0.807229   \n",
       "68     0.180851       0.114414      0.091029              1         0.885542   \n",
       "57     0.091954       0.122194      0.092357              1         0.951807   \n",
       "48     0.129032       0.153395      0.051918              1         0.843373   \n",
       "57     0.163043       0.180970      0.054697              1         0.909639   \n",
       "1      0.161290       0.111799      0.094085              1         0.945783   \n",
       "63     0.471074       0.223081      0.082000              1         0.873494   \n",
       "57     0.193548       0.185867      0.008812              1         0.855422   \n",
       "57     0.174419       0.221440      0.095443              1         0.789157   \n",
       "71     0.420168       0.202516      0.095906              1         0.843373   \n",
       "52     0.175258       0.230139      0.094090              1         0.891566   \n",
       "65     0.161616       0.189522      0.043970              1         0.879518   \n",
       "57     0.476744       0.352331      0.041823              1         0.734940   \n",
       "53     0.306306       0.139664      0.109477              1         0.837349   \n",
       "63          NaN       0.139867      0.093360              1         0.933735   \n",
       "\n",
       "    2_test_accuracy  3_test_accuracy  4_test_accuracy  5_test_accuracy  \\\n",
       "12         0.963855         0.963855         0.963855         0.963855   \n",
       "12         0.873494         0.475904         0.662651         0.819277   \n",
       "6               NaN         0.921687         0.909639         0.765060   \n",
       "53         0.969880              NaN         0.759036         0.969880   \n",
       "69         0.879518         0.590361              NaN         0.753012   \n",
       "53         0.945783         0.945783         0.927711              NaN   \n",
       "69         0.975904         0.963855         0.969880         0.975904   \n",
       "61         0.927711         0.867470         0.903614         0.891566   \n",
       "36         0.867470         0.843373         0.777108         0.771084   \n",
       "23         0.957831         0.771084         0.867470         0.867470   \n",
       "73         0.933735         0.819277         0.897590         0.903614   \n",
       "36         0.921687         0.753012         0.879518         0.843373   \n",
       "69         0.897590         0.831325         0.873494         0.783133   \n",
       "73         0.909639         0.885542         0.903614         0.807229   \n",
       "12         0.728916         0.765060         0.746988         0.759036   \n",
       "73         0.849398         0.849398         0.849398         0.825301   \n",
       "61         0.831325         0.783133         0.777108         0.783133   \n",
       "2          0.939759         0.933735         0.939759         0.891566   \n",
       "12         0.921687         0.837349         0.891566         0.915663   \n",
       "53         0.945783         0.945783         0.945783         0.945783   \n",
       "53         0.873494         0.487952         0.674699         0.759036   \n",
       "1               NaN         0.945783         0.933735         0.789157   \n",
       "73         0.975904              NaN         0.783133         0.975904   \n",
       "47         0.903614         0.487952              NaN         0.879518   \n",
       "73         0.951807         0.951807         0.921687              NaN   \n",
       "12         0.993976         0.963855         0.981928         0.981928   \n",
       "1          0.873494         0.771084         0.807229         0.873494   \n",
       "20         0.933735         0.837349         0.765060         0.710843   \n",
       "28         0.933735         0.813253         0.885542         0.801205   \n",
       "41         0.867470         0.807229         0.825301         0.867470   \n",
       "41         0.909639         0.855422         0.909639         0.909639   \n",
       "74         0.638554         0.656627         0.771084         0.825301   \n",
       "59         0.885542         0.885542         0.885542         0.873494   \n",
       "19         0.566265         0.891566         0.897590         0.879518   \n",
       "27         0.855422         0.927711         0.837349         0.795181   \n",
       "3          0.753012         0.722892         0.740964         0.722892   \n",
       "77         0.927711         0.915663         0.927711         0.891566   \n",
       "72         0.927711         0.873494         0.927711         0.927711   \n",
       "57         0.957831         0.969880         0.975904         0.969880   \n",
       "13         0.843373         0.487952         0.686747         0.807229   \n",
       "51              NaN         0.921687         0.903614         0.777108   \n",
       "62         0.993976              NaN         0.753012         0.993976   \n",
       "74         0.831325         0.626506              NaN         0.819277   \n",
       "68         0.963855         0.963855         0.879518              NaN   \n",
       "57         0.951807         0.951807         0.945783         0.951807   \n",
       "48         0.927711         0.783133         0.813253         0.921687   \n",
       "57         0.909639         0.849398         0.753012         0.740964   \n",
       "1          0.969880         0.873494         0.897590         0.927711   \n",
       "63         0.885542         0.849398         0.879518         0.885542   \n",
       "57         0.891566         0.837349         0.831325         0.879518   \n",
       "57         0.921687         0.728916         0.753012         0.777108   \n",
       "71         0.915663         0.879518         0.915663         0.795181   \n",
       "52         0.433735         0.765060         0.765060         0.837349   \n",
       "65         0.795181         0.885542         0.825301         0.819277   \n",
       "57         0.753012         0.746988         0.753012         0.728916   \n",
       "53         0.963855         0.945783         0.963855         0.897590   \n",
       "63         0.945783         0.885542         0.945783         0.945783   \n",
       "\n",
       "    6_test_accuracy  7_test_accuracy  8_test_accuracy  9_test_accuracy  \\\n",
       "12         0.963855         0.951807         0.957831         0.951807   \n",
       "12         0.728916         0.819277         0.831325         0.644578   \n",
       "6          0.927711         0.463855         0.885542         0.855422   \n",
       "53         0.957831         0.969880         0.771084         0.734940   \n",
       "69         0.849398         0.843373         0.512048         0.542169   \n",
       "53         0.945783         0.945783         0.692771         0.933735   \n",
       "69              NaN         0.975904         0.909639         0.975904   \n",
       "61         0.909639              NaN         0.885542         0.632530   \n",
       "36         0.867470         0.867470              NaN         0.855422   \n",
       "23         0.957831         0.957831         0.680723              NaN   \n",
       "73         0.897590         0.927711         0.837349         0.831325   \n",
       "36         0.921687         0.915663         0.759036         0.716867   \n",
       "69         0.897590         0.710843         0.704819         0.855422   \n",
       "73         0.789157         0.909639         0.897590         0.891566   \n",
       "12         0.783133         0.710843         0.777108         0.771084   \n",
       "73         0.927711         0.740964         0.843373         0.831325   \n",
       "61         0.542169         0.819277         0.674699         0.746988   \n",
       "2          0.524096         0.939759         0.915663         0.933735   \n",
       "12         0.891566         0.921687         0.825301         0.813253   \n",
       "53         0.945783         0.945783         0.945783         0.945783   \n",
       "53         0.825301         0.771084         0.698795         0.590361   \n",
       "1          0.945783         0.530120         0.915663         0.939759   \n",
       "73         0.963855         0.963855         0.783133         0.801205   \n",
       "47         0.801205         0.897590         0.608434         0.837349   \n",
       "73         0.951807         0.951807         0.656627         0.933735   \n",
       "12              NaN         0.993976         0.945783         0.945783   \n",
       "1          0.873494              NaN         0.855422         0.807229   \n",
       "20         0.927711         0.933735              NaN         0.873494   \n",
       "28         0.933735         0.933735         0.662651              NaN   \n",
       "41         0.837349         0.861446         0.795181         0.716867   \n",
       "41         0.909639         0.891566         0.909639         0.813253   \n",
       "74         0.873494         0.469880         0.783133         0.668675   \n",
       "59         0.759036         0.855422         0.885542         0.867470   \n",
       "19         0.897590         0.879518         0.879518         0.897590   \n",
       "27         0.927711         0.716867         0.873494         0.867470   \n",
       "3          0.590361         0.753012         0.716867         0.704819   \n",
       "77         0.524096         0.927711         0.927711         0.927711   \n",
       "72         0.921687         0.927711         0.819277         0.819277   \n",
       "57         0.969880         0.825301         0.981928         0.963855   \n",
       "13         0.825301         0.777108         0.740964         0.632530   \n",
       "51         0.921687         0.493976         0.903614         0.909639   \n",
       "62         0.981928         0.981928         0.765060         0.656627   \n",
       "74         0.831325         0.825301         0.608434         0.686747   \n",
       "68         0.963855         0.957831         0.620482         0.945783   \n",
       "57              NaN         0.945783         0.933735         0.945783   \n",
       "48         0.927711              NaN         0.855422         0.728916   \n",
       "57         0.909639         0.909639              NaN         0.891566   \n",
       "1          0.969880         0.969880         0.686747              NaN   \n",
       "63         0.879518         0.885542         0.855422         0.837349   \n",
       "57         0.891566         0.873494         0.837349         0.825301   \n",
       "57         0.921687         0.644578         0.632530         0.753012   \n",
       "71         0.885542         0.897590         0.891566         0.849398   \n",
       "52         0.891566         0.692771         0.795181         0.861446   \n",
       "65         0.903614         0.783133         0.867470         0.843373   \n",
       "57         0.728916         0.753012         0.710843         0.728916   \n",
       "53         0.524096         0.963855         0.915663         0.939759   \n",
       "63         0.891566         0.945783         0.903614         0.885542   \n",
       "\n",
       "    10_test_accuracy  11_test_accuracy  12_test_accuracy  13_test_accuracy  \\\n",
       "12          0.963855          0.963855          0.951807          0.963855   \n",
       "12          0.584337          0.493976          0.873494          0.825301   \n",
       "6           0.921687          0.897590          0.734940          0.873494   \n",
       "53          0.722892          0.692771          0.969880          0.969880   \n",
       "69          0.524096          0.554217          0.873494          0.837349   \n",
       "53          0.945783          0.939759          0.945783          0.939759   \n",
       "69          0.783133          0.813253          0.975904          0.975904   \n",
       "61          0.969880          0.837349          0.867470          0.963855   \n",
       "36          0.861446          0.746988          0.867470          0.867470   \n",
       "23          0.909639          0.771084          0.903614          0.927711   \n",
       "73               NaN          0.614458          0.933735          0.855422   \n",
       "36          0.656627               NaN          0.921687          0.879518   \n",
       "69          0.753012          0.771084               NaN          0.891566   \n",
       "73          0.746988          0.837349          0.909639               NaN   \n",
       "12          0.783133          0.765060          0.777108          0.783133   \n",
       "73          0.927711          0.915663          0.849398          0.915663   \n",
       "61          0.560241          0.662651          0.807229          0.668675   \n",
       "2           0.837349          0.927711          0.939759          0.897590   \n",
       "12          0.638554          0.686747          0.921687          0.921687   \n",
       "53          0.945783          0.945783          0.945783          0.945783   \n",
       "53          0.572289          0.487952          0.873494          0.861446   \n",
       "1           0.945783          0.903614          0.734940          0.897590   \n",
       "73          0.753012          0.753012          0.975904          0.963855   \n",
       "47          0.765060          0.825301          0.837349          0.891566   \n",
       "73          0.945783          0.879518          0.903614          0.879518   \n",
       "12          0.716867          0.777108          0.993976          0.987952   \n",
       "1           0.873494          0.861446          0.873494          0.873494   \n",
       "20          0.795181          0.740964          0.933735          0.927711   \n",
       "28          0.915663          0.825301          0.873494          0.927711   \n",
       "41               NaN          0.680723          0.849398          0.843373   \n",
       "41          0.656627               NaN          0.897590          0.909639   \n",
       "74          0.777108          0.668675               NaN          0.921687   \n",
       "59          0.746988          0.873494          0.873494               NaN   \n",
       "19          0.897590          0.867470          0.873494          0.867470   \n",
       "27          0.927711          0.927711          0.867470          0.927711   \n",
       "3           0.602410          0.662651          0.740964          0.632530   \n",
       "77          0.909639          0.915663          0.927711          0.867470   \n",
       "72          0.560241          0.765060          0.927711          0.927711   \n",
       "57          0.981928          0.969880          0.975904          0.981928   \n",
       "13          0.590361          0.481928          0.843373          0.837349   \n",
       "51          0.921687          0.909639          0.728916          0.897590   \n",
       "62          0.710843          0.650602          0.993976          0.987952   \n",
       "74          0.620482          0.704819          0.831325          0.825301   \n",
       "68          0.939759          0.921687          0.963855          0.945783   \n",
       "57          0.831325          0.849398          0.951807          0.951807   \n",
       "48          0.927711          0.873494          0.927711          0.927711   \n",
       "57          0.897590          0.704819          0.909639          0.897590   \n",
       "1           0.921687          0.728916          0.927711          0.957831   \n",
       "63               NaN          0.602410          0.885542          0.855422   \n",
       "57          0.819277               NaN          0.891566          0.885542   \n",
       "57          0.819277          0.716867               NaN          0.927711   \n",
       "71          0.620482          0.783133          0.897590               NaN   \n",
       "52          0.873494          0.819277          0.801205          0.843373   \n",
       "65          0.903614          0.903614          0.740964          0.903614   \n",
       "57          0.542169          0.716867          0.753012          0.710843   \n",
       "53          0.891566          0.897590          0.963855          0.873494   \n",
       "63          0.548193          0.789157          0.945783          0.945783   \n",
       "\n",
       "    14_test_accuracy  15_test_accuracy  16_test_accuracy  17_test_accuracy  \\\n",
       "12          0.933735          0.963855          0.963855          0.957831   \n",
       "12          0.873494          0.867470          0.831325          0.674699   \n",
       "6           0.795181          0.686747          0.909639          0.927711   \n",
       "53          0.969880          0.969880          0.969880          0.879518   \n",
       "69          0.879518          0.867470          0.740964          0.704819   \n",
       "53          0.945783          0.945783          0.813253          0.945783   \n",
       "69          0.975904          0.975904          0.969880          0.524096   \n",
       "61          0.957831          0.909639          0.969880          0.873494   \n",
       "36          0.867470          0.867470          0.861446          0.867470   \n",
       "23          0.957831          0.951807          0.939759          0.957831   \n",
       "73          0.933735          0.933735          0.915663          0.795181   \n",
       "36          0.921687          0.921687          0.879518          0.885542   \n",
       "69          0.897590          0.879518          0.897590          0.885542   \n",
       "73          0.909639          0.909639          0.759036          0.807229   \n",
       "12               NaN          0.771084          0.771084          0.783133   \n",
       "73          0.927711               NaN          0.885542          0.927711   \n",
       "61          0.831325          0.825301               NaN          0.469880   \n",
       "2           0.939759          0.939759          0.861446               NaN   \n",
       "12          0.921687          0.921687          0.897590          0.873494   \n",
       "53          0.945783          0.945783          0.945783          0.945783   \n",
       "53          0.873494          0.867470          0.819277          0.746988   \n",
       "1           0.819277          0.753012          0.921687          0.945783   \n",
       "73          0.975904          0.975904          0.975904          0.903614   \n",
       "47          0.897590          0.903614          0.897590          0.837349   \n",
       "73          0.951807          0.945783          0.759036          0.951807   \n",
       "12          0.993976          0.993976          0.981928          0.548193   \n",
       "1           0.873494          0.873494          0.873494          0.873494   \n",
       "20          0.933735          0.927711          0.873494          0.927711   \n",
       "28          0.933735          0.927711          0.915663          0.933735   \n",
       "41          0.867470          0.867470          0.861446          0.656627   \n",
       "41          0.909639          0.909639          0.909639          0.903614   \n",
       "74          0.927711          0.819277          0.939759          0.915663   \n",
       "59          0.885542          0.879518          0.650602          0.716867   \n",
       "19               NaN          0.746988          0.885542          0.891566   \n",
       "27          0.891566               NaN          0.915663          0.927711   \n",
       "3           0.753012          0.753012               NaN          0.536145   \n",
       "77          0.927711          0.927711          0.867470               NaN   \n",
       "72          0.927711          0.927711          0.903614          0.849398   \n",
       "57          0.921687          0.981928          0.981928          0.975904   \n",
       "13          0.843373          0.843373          0.837349          0.746988   \n",
       "51          0.795181          0.722892          0.909639          0.921687   \n",
       "62          0.993976          0.993976          0.993976          0.885542   \n",
       "74          0.831325          0.825301          0.825301          0.807229   \n",
       "68          0.963855          0.951807          0.807229          0.963855   \n",
       "57          0.951807          0.951807          0.951807          0.542169   \n",
       "48          0.927711          0.885542          0.927711          0.921687   \n",
       "57          0.909639          0.909639          0.837349          0.909639   \n",
       "1           0.969880          0.963855          0.957831          0.969880   \n",
       "63          0.885542          0.885542          0.885542          0.740964   \n",
       "57          0.891566          0.891566          0.891566          0.891566   \n",
       "57          0.927711          0.885542          0.921687          0.897590   \n",
       "71          0.915663          0.909639          0.650602          0.897590   \n",
       "52               NaN          0.656627          0.867470          0.897590   \n",
       "65          0.867470               NaN          0.897590          0.903614   \n",
       "57          0.753012          0.753012               NaN          0.722892   \n",
       "53          0.963855          0.963855          0.807229               NaN   \n",
       "63          0.945783          0.945783          0.891566          0.801205   \n",
       "\n",
       "    18_test_accuracy  mean_test_accuracy  std_test_accuracy  \\\n",
       "12          0.963855            0.959505           0.007727   \n",
       "12          0.644578            0.744311           0.130556   \n",
       "6           0.897590            0.836011           0.115257   \n",
       "53          0.692771            0.875167           0.111835   \n",
       "69          0.457831            0.723896           0.148245   \n",
       "53          0.933735            0.920683           0.062837   \n",
       "69          0.813253            0.916667           0.114427   \n",
       "61          0.969880            0.879183           0.089023   \n",
       "36          0.849398            0.846720           0.037525   \n",
       "23          0.879518            0.895917           0.077584   \n",
       "73          0.566265            0.852744           0.103841   \n",
       "36          0.632530            0.845047           0.094159   \n",
       "69          0.765060            0.833668           0.065381   \n",
       "73          0.771084            0.857430           0.058681   \n",
       "12          0.783133            0.765395           0.020228   \n",
       "73          0.915663            0.877510           0.049554   \n",
       "61          0.560241            0.714190           0.113489   \n",
       "2           0.873494            0.895248           0.095168   \n",
       "12               NaN            0.866466           0.080177   \n",
       "53          0.945783            0.945783           0.000000   \n",
       "53          0.512048            0.731593           0.139731   \n",
       "1           0.933735            0.868474           0.105403   \n",
       "73          0.801205            0.897925           0.089453   \n",
       "47          0.879518            0.814592           0.116614   \n",
       "73          0.867470            0.902945           0.076540   \n",
       "12          0.734940            0.916667           0.126637   \n",
       "1           0.855422            0.857430           0.029306   \n",
       "20          0.801205            0.872490           0.075006   \n",
       "28          0.885542            0.885542           0.068715   \n",
       "41          0.662651            0.808902           0.073226   \n",
       "41          0.662651            0.870817           0.078448   \n",
       "74          0.783133            0.762048           0.130649   \n",
       "59          0.759036            0.835007           0.071169   \n",
       "19          0.897590            0.856760           0.078738   \n",
       "27          0.927711            0.883199           0.054814   \n",
       "3           0.566265            0.686747           0.070596   \n",
       "77          0.867470            0.889893           0.091378   \n",
       "72               NaN            0.875837           0.090320   \n",
       "57          0.981928            0.963855           0.036422   \n",
       "13          0.572289            0.735609           0.124787   \n",
       "51          0.909639            0.848059           0.108258   \n",
       "62          0.668675            0.877510           0.136546   \n",
       "74          0.686747            0.768072           0.082775   \n",
       "68          0.861446            0.914659           0.083599   \n",
       "57          0.927711            0.913320           0.096368   \n",
       "48          0.915663            0.886881           0.058114   \n",
       "57          0.873494            0.868474           0.064640   \n",
       "1           0.879518            0.915663           0.079868   \n",
       "63          0.542169            0.833333           0.098762   \n",
       "57          0.843373            0.867805           0.026976   \n",
       "57          0.837349            0.816600           0.094809   \n",
       "71          0.614458            0.837684           0.101151   \n",
       "52          0.879518            0.801539           0.110978   \n",
       "65          0.903614            0.852410           0.058138   \n",
       "57          0.524096            0.714859           0.066052   \n",
       "53          0.759036            0.890897           0.106840   \n",
       "63               NaN            0.891232           0.095745   \n",
       "\n",
       "    rank_test_accuracy  owner  run  0_test_eer  0_test_accuracy  \n",
       "12                   1      0    0         NaN              NaN  \n",
       "12                   1      1    0    0.201923         0.873494  \n",
       "6                    1      2    0    0.260417         0.777108  \n",
       "53                   1      3    0    0.056818         0.969880  \n",
       "69                   8      4    0    0.194175         0.879518  \n",
       "53                   1      5    0    0.097826         0.945783  \n",
       "69                   1      6    0    0.045977         0.975904  \n",
       "61                   1      7    0    0.315789         0.753012  \n",
       "36                   1      8    0    0.209524         0.867470  \n",
       "23                   2      9    0    0.078652         0.951807  \n",
       "73                   1     10    0    0.117021         0.933735  \n",
       "36                   1     11    0    0.135417         0.921687  \n",
       "69                   1     12    0    0.180851         0.861446  \n",
       "73                   1     13    0    0.153061         0.909639  \n",
       "12                   1     14    0    0.321429         0.740964  \n",
       "73                   1     15    0    0.129032         0.915663  \n",
       "61                   1     16    0    0.252252         0.831325  \n",
       "2                    1     17    0    0.107527         0.939759  \n",
       "12                   1     18    0    0.135417         0.921687  \n",
       "53                   1      0    1         NaN              NaN  \n",
       "53                   3      1    1    0.201923         0.873494  \n",
       "1                    1      2    1    0.107143         0.897590  \n",
       "73                   1      3    1    0.045977         0.975904  \n",
       "47                   1      4    1    0.166667         0.885542  \n",
       "73                   4      5    1    0.087912         0.951807  \n",
       "12                   1      6    1    0.011905         0.993976  \n",
       "1                    1      7    1    0.201923         0.873494  \n",
       "20                   2      8    1    0.117021         0.933735  \n",
       "28                   1      9    1    0.119565         0.921687  \n",
       "41                   2     10    1    0.209524         0.867470  \n",
       "41                   1     11    1    0.153061         0.909639  \n",
       "74                   2     12    1    0.475524         0.542169  \n",
       "59                   1     13    1    0.186275         0.885542  \n",
       "19                   1     14    1    0.193182         0.825301  \n",
       "27                   1     15    1    0.133333         0.897590  \n",
       "3                    1     16    1    0.330645         0.753012  \n",
       "77                   1     17    1    0.126316         0.927711  \n",
       "72                   1     18    1    0.126316         0.927711  \n",
       "57                   1      0    2         NaN              NaN  \n",
       "13                   1      1    2    0.238532         0.843373  \n",
       "51                   2      2    2    0.186047         0.825301  \n",
       "62                   1      3    2    0.011905         0.993976  \n",
       "74                   2      4    2    0.252252         0.831325  \n",
       "68                   1      5    2    0.067416         0.963855  \n",
       "57                   1      6    2    0.087912         0.951807  \n",
       "48                   1      7    2    0.126316         0.927711  \n",
       "57                   1      8    2    0.153061         0.909639  \n",
       "1                    1      9    2    0.057471         0.963855  \n",
       "63                   1     10    2    0.186275         0.885542  \n",
       "57                   1     11    2    0.178218         0.891566  \n",
       "57                   1     12    2    0.164706         0.843373  \n",
       "71                   1     13    2    0.144330         0.915663  \n",
       "52                   1     14    2    0.182796         0.855422  \n",
       "65                   3     15    2    0.316327         0.716867  \n",
       "57                   1     16    2    0.330645         0.753012  \n",
       "53                   2     17    2    0.067416         0.963855  \n",
       "63                   1     18    2    0.097826         0.945783  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# df_results[(df_results.rank_test_eer == 1) & (df_results.rank_test_accuracy == 1)].groupby('owner').head(40)[:269]\n",
    "df_results[(df_results.rank_test_eer == 1)].groupby('owner').head(232)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1*19*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 54)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results[df_results[\"owner\"] == 14].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_results[df_results[\"owner\"] == 14][ df_results[\"rank_test_eer\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_model__n_estimators</th>\n",
       "      <th>param_model__max_samples</th>\n",
       "      <th>param_model__max_features</th>\n",
       "      <th>param_model__contamination</th>\n",
       "      <th>1_test_eer</th>\n",
       "      <th>2_test_eer</th>\n",
       "      <th>3_test_eer</th>\n",
       "      <th>4_test_eer</th>\n",
       "      <th>5_test_eer</th>\n",
       "      <th>6_test_eer</th>\n",
       "      <th>7_test_eer</th>\n",
       "      <th>8_test_eer</th>\n",
       "      <th>9_test_eer</th>\n",
       "      <th>10_test_eer</th>\n",
       "      <th>11_test_eer</th>\n",
       "      <th>12_test_eer</th>\n",
       "      <th>13_test_eer</th>\n",
       "      <th>14_test_eer</th>\n",
       "      <th>15_test_eer</th>\n",
       "      <th>16_test_eer</th>\n",
       "      <th>17_test_eer</th>\n",
       "      <th>18_test_eer</th>\n",
       "      <th>mean_test_eer</th>\n",
       "      <th>std_test_eer</th>\n",
       "      <th>rank_test_eer</th>\n",
       "      <th>1_test_accuracy</th>\n",
       "      <th>2_test_accuracy</th>\n",
       "      <th>3_test_accuracy</th>\n",
       "      <th>4_test_accuracy</th>\n",
       "      <th>5_test_accuracy</th>\n",
       "      <th>6_test_accuracy</th>\n",
       "      <th>7_test_accuracy</th>\n",
       "      <th>8_test_accuracy</th>\n",
       "      <th>9_test_accuracy</th>\n",
       "      <th>10_test_accuracy</th>\n",
       "      <th>11_test_accuracy</th>\n",
       "      <th>12_test_accuracy</th>\n",
       "      <th>13_test_accuracy</th>\n",
       "      <th>14_test_accuracy</th>\n",
       "      <th>15_test_accuracy</th>\n",
       "      <th>16_test_accuracy</th>\n",
       "      <th>17_test_accuracy</th>\n",
       "      <th>18_test_accuracy</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>rank_test_accuracy</th>\n",
       "      <th>owner</th>\n",
       "      <th>run</th>\n",
       "      <th>0_test_eer</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.094760</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.024955</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>77</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.036145</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.051838</td>\n",
       "      <td>0.050040</td>\n",
       "      <td>1</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.036422</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.035941</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.010368</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.067982</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.959505</td>\n",
       "      <td>0.007727</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.239470</td>\n",
       "      <td>0.008075</td>\n",
       "      <td>0.331064</td>\n",
       "      <td>0.002431</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>84</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.206139</td>\n",
       "      <td>0.010410</td>\n",
       "      <td>0.311011</td>\n",
       "      <td>0.003016</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>12</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.359375</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.474359</td>\n",
       "      <td>0.344000</td>\n",
       "      <td>0.106638</td>\n",
       "      <td>0.146222</td>\n",
       "      <td>1</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.716867</td>\n",
       "      <td>0.777108</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.548193</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.126637</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.993976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.095958</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.025472</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>77</td>\n",
       "      <td>0.9</td>\n",
       "      <td>12</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.122222</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.288288</td>\n",
       "      <td>0.254717</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.487013</td>\n",
       "      <td>0.254717</td>\n",
       "      <td>0.111519</td>\n",
       "      <td>0.122217</td>\n",
       "      <td>1</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.813253</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.524096</td>\n",
       "      <td>0.813253</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.114427</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.975904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.262946</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.069063</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>215</td>\n",
       "      <td>0.7</td>\n",
       "      <td>24</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.376000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.058140</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.058140</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.111799</td>\n",
       "      <td>0.094085</td>\n",
       "      <td>1</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.728916</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.079868</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.963855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.266792</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.074419</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>215</td>\n",
       "      <td>0.7</td>\n",
       "      <td>60</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.144444</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.425373</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.252427</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.180851</td>\n",
       "      <td>0.114414</td>\n",
       "      <td>0.091029</td>\n",
       "      <td>1</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.620482</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.861446</td>\n",
       "      <td>0.914659</td>\n",
       "      <td>0.083599</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.963855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.214301</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>0.314889</td>\n",
       "      <td>0.003020</td>\n",
       "      <td>1000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.101124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.362069</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.098901</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.098901</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.120468</td>\n",
       "      <td>0.065775</td>\n",
       "      <td>1</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.692771</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.813253</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.920683</td>\n",
       "      <td>0.062837</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.945783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.094750</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.025022</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>77</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.184783</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.475524</td>\n",
       "      <td>0.091954</td>\n",
       "      <td>0.122194</td>\n",
       "      <td>0.092357</td>\n",
       "      <td>1</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.542169</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.913320</td>\n",
       "      <td>0.096368</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.951807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.272306</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.076335</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>215</td>\n",
       "      <td>0.9</td>\n",
       "      <td>72</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.395161</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.096386</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.299065</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.157303</td>\n",
       "      <td>0.128223</td>\n",
       "      <td>0.081662</td>\n",
       "      <td>1</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.656627</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.902945</td>\n",
       "      <td>0.076540</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.951807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.436656</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.113234</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.084337</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.289720</td>\n",
       "      <td>0.164835</td>\n",
       "      <td>0.164835</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.377049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.289720</td>\n",
       "      <td>0.105882</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.078652</td>\n",
       "      <td>0.080460</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.146067</td>\n",
       "      <td>0.134816</td>\n",
       "      <td>0.088766</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.680723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.895917</td>\n",
       "      <td>0.077584</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.078652</td>\n",
       "      <td>0.951807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.059389</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.016520</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>46</td>\n",
       "      <td>0.4</td>\n",
       "      <td>48</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.485915</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.112360</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.188889</td>\n",
       "      <td>0.109890</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.130952</td>\n",
       "      <td>0.138372</td>\n",
       "      <td>0.086710</td>\n",
       "      <td>1</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.524096</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.861446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.895248</td>\n",
       "      <td>0.095168</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.939759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.759016</td>\n",
       "      <td>0.002997</td>\n",
       "      <td>0.205988</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>599</td>\n",
       "      <td>0.9</td>\n",
       "      <td>60</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.486667</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.094118</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.134831</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.163043</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.252427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.306306</td>\n",
       "      <td>0.139664</td>\n",
       "      <td>0.109477</td>\n",
       "      <td>1</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.524096</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.890897</td>\n",
       "      <td>0.106840</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.963855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.098878</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.027996</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>77</td>\n",
       "      <td>0.5</td>\n",
       "      <td>72</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.108434</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.105882</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.108434</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.139867</td>\n",
       "      <td>0.093360</td>\n",
       "      <td>1</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.548193</td>\n",
       "      <td>0.789157</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.801205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.891232</td>\n",
       "      <td>0.095745</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.945783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.272066</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>0.076630</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>215</td>\n",
       "      <td>0.9</td>\n",
       "      <td>72</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.193878</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.288288</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.288288</td>\n",
       "      <td>0.268519</td>\n",
       "      <td>0.318966</td>\n",
       "      <td>0.318966</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.131868</td>\n",
       "      <td>0.268519</td>\n",
       "      <td>0.141128</td>\n",
       "      <td>0.113483</td>\n",
       "      <td>1</td>\n",
       "      <td>0.861446</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.801205</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.801205</td>\n",
       "      <td>0.897925</td>\n",
       "      <td>0.089453</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.975904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.746362</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.203167</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>599</td>\n",
       "      <td>0.6</td>\n",
       "      <td>60</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.134831</td>\n",
       "      <td>0.485507</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0.149980</td>\n",
       "      <td>0.081555</td>\n",
       "      <td>1</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.524096</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.889893</td>\n",
       "      <td>0.091378</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.927711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.022138</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.006563</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>16</td>\n",
       "      <td>0.4</td>\n",
       "      <td>84</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.144578</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.174419</td>\n",
       "      <td>0.236559</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.330189</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.134831</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.151411</td>\n",
       "      <td>0.050484</td>\n",
       "      <td>1</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.716867</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.883199</td>\n",
       "      <td>0.054814</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.897590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.743676</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.206942</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>599</td>\n",
       "      <td>0.7</td>\n",
       "      <td>72</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.120879</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.234043</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.120879</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.130952</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.120879</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.151482</td>\n",
       "      <td>0.067235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.813253</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.801205</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.068715</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.119565</td>\n",
       "      <td>0.921687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.036608</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.010847</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>27</td>\n",
       "      <td>0.6</td>\n",
       "      <td>72</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.154762</td>\n",
       "      <td>0.154762</td>\n",
       "      <td>0.154762</td>\n",
       "      <td>0.193182</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.303922</td>\n",
       "      <td>0.164706</td>\n",
       "      <td>0.183908</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.154762</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.153103</td>\n",
       "      <td>0.041704</td>\n",
       "      <td>1</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.740964</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.877510</td>\n",
       "      <td>0.049554</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.915663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.439226</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.116401</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>359</td>\n",
       "      <td>0.3</td>\n",
       "      <td>84</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.164706</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.252632</td>\n",
       "      <td>0.211111</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.144578</td>\n",
       "      <td>0.317308</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.153395</td>\n",
       "      <td>0.051918</td>\n",
       "      <td>1</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.813253</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.728916</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.886881</td>\n",
       "      <td>0.058114</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.927711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.035292</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.010231</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>27</td>\n",
       "      <td>0.1</td>\n",
       "      <td>60</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.178947</td>\n",
       "      <td>0.123596</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.152174</td>\n",
       "      <td>0.417910</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.178947</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.058140</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.157192</td>\n",
       "      <td>0.102726</td>\n",
       "      <td>1</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.632530</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.879183</td>\n",
       "      <td>0.089023</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.753012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1.227985</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>0.330484</td>\n",
       "      <td>0.002589</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>48</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.286957</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.405797</td>\n",
       "      <td>0.364341</td>\n",
       "      <td>0.410072</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>0.157320</td>\n",
       "      <td>0.167088</td>\n",
       "      <td>1</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.765060</td>\n",
       "      <td>0.656627</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.668675</td>\n",
       "      <td>0.877510</td>\n",
       "      <td>0.136546</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.993976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.263917</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.070670</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>215</td>\n",
       "      <td>0.5</td>\n",
       "      <td>36</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.129412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.482517</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.098901</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.105882</td>\n",
       "      <td>0.321101</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.221053</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.102273</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.162892</td>\n",
       "      <td>0.106994</td>\n",
       "      <td>1</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.789157</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.530120</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.868474</td>\n",
       "      <td>0.105403</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.897590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.267838</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.072216</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>215</td>\n",
       "      <td>0.7</td>\n",
       "      <td>84</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.131868</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.202247</td>\n",
       "      <td>0.202247</td>\n",
       "      <td>0.462121</td>\n",
       "      <td>0.275510</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.131868</td>\n",
       "      <td>0.154762</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.164703</td>\n",
       "      <td>0.082016</td>\n",
       "      <td>1</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.560241</td>\n",
       "      <td>0.765060</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.875837</td>\n",
       "      <td>0.090320</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.927711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.218555</td>\n",
       "      <td>0.002707</td>\n",
       "      <td>0.315039</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>1000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.309735</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.058140</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.297297</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.344538</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>0.167091</td>\n",
       "      <td>0.130762</td>\n",
       "      <td>1</td>\n",
       "      <td>0.813253</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.692771</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.692771</td>\n",
       "      <td>0.875167</td>\n",
       "      <td>0.111835</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.969880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.022140</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>16</td>\n",
       "      <td>0.3</td>\n",
       "      <td>84</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.339450</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.130952</td>\n",
       "      <td>0.242105</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.130952</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.234043</td>\n",
       "      <td>0.167973</td>\n",
       "      <td>0.074385</td>\n",
       "      <td>1</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.765060</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.740964</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.801205</td>\n",
       "      <td>0.872490</td>\n",
       "      <td>0.075006</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.933735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.035989</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.010433</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.147727</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.136842</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.357798</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.141304</td>\n",
       "      <td>0.147727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.173555</td>\n",
       "      <td>0.075628</td>\n",
       "      <td>1</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.813253</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.866466</td>\n",
       "      <td>0.080177</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.921687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.095225</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.024702</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>77</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.170455</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.174419</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.163043</td>\n",
       "      <td>0.180970</td>\n",
       "      <td>0.054697</td>\n",
       "      <td>1</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.740964</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.704819</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.868474</td>\n",
       "      <td>0.064640</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.909639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.036352</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.010720</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>27</td>\n",
       "      <td>0.6</td>\n",
       "      <td>72</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.208791</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.208791</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.123596</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.191011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.424000</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.152941</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.120879</td>\n",
       "      <td>0.242105</td>\n",
       "      <td>0.458647</td>\n",
       "      <td>0.181443</td>\n",
       "      <td>0.099967</td>\n",
       "      <td>1</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.566265</td>\n",
       "      <td>0.852744</td>\n",
       "      <td>0.103841</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.933735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.021937</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.006466</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>16</td>\n",
       "      <td>0.2</td>\n",
       "      <td>48</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.168539</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.381818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.154639</td>\n",
       "      <td>0.376147</td>\n",
       "      <td>0.181813</td>\n",
       "      <td>0.070278</td>\n",
       "      <td>1</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.813253</td>\n",
       "      <td>0.656627</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.870817</td>\n",
       "      <td>0.078448</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.909639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "57       0.094760      0.000295         0.024955        0.000272   \n",
       "12       0.035941      0.000147         0.010368        0.000508   \n",
       "53       1.239470      0.008075         0.331064        0.002431   \n",
       "12       1.206139      0.010410         0.311011        0.003016   \n",
       "69       0.095958      0.000631         0.025472        0.000344   \n",
       "1        0.262946      0.000798         0.069063        0.000439   \n",
       "68       0.266792      0.001050         0.074419        0.001034   \n",
       "53       1.214301      0.002859         0.314889        0.003020   \n",
       "57       0.094750      0.000874         0.025022        0.000358   \n",
       "73       0.272306      0.001412         0.076335        0.000651   \n",
       "23       0.436656      0.000914         0.113234        0.000744   \n",
       "2        0.059389      0.000410         0.016520        0.000247   \n",
       "53       0.759016      0.002997         0.205988        0.001313   \n",
       "63       0.098878      0.000977         0.027996        0.000313   \n",
       "73       0.272066      0.002379         0.076630        0.000582   \n",
       "77       0.746362      0.001559         0.203167        0.001314   \n",
       "27       0.022138      0.000112         0.006563        0.000167   \n",
       "28       0.743676      0.001352         0.206942        0.001150   \n",
       "73       0.036608      0.000108         0.010847        0.000158   \n",
       "48       0.439226      0.001717         0.116401        0.001011   \n",
       "61       0.035292      0.000141         0.010231        0.000162   \n",
       "62       1.227985      0.002379         0.330484        0.002589   \n",
       "1        0.263917      0.000757         0.070670        0.000473   \n",
       "72       0.267838      0.000817         0.072216        0.000532   \n",
       "53       1.218555      0.002707         0.315039        0.002055   \n",
       "20       0.022140      0.000159         0.006494        0.000180   \n",
       "12       0.035989      0.000154         0.010433        0.000248   \n",
       "57       0.095225      0.000456         0.024702        0.000280   \n",
       "73       0.036352      0.000466         0.010720        0.000221   \n",
       "41       0.021937      0.000598         0.006466        0.000199   \n",
       "\n",
       "   param_model__n_estimators param_model__max_samples  \\\n",
       "57                        77                      0.4   \n",
       "12                        27                      1.0   \n",
       "53                      1000                      0.9   \n",
       "12                      1000                      0.7   \n",
       "69                        77                      0.9   \n",
       "1                        215                      0.7   \n",
       "68                       215                      0.7   \n",
       "53                      1000                      1.0   \n",
       "57                        77                      0.4   \n",
       "73                       215                      0.9   \n",
       "23                       359                      1.0   \n",
       "2                         46                      0.4   \n",
       "53                       599                      0.9   \n",
       "63                        77                      0.5   \n",
       "73                       215                      0.9   \n",
       "77                       599                      0.6   \n",
       "27                        16                      0.4   \n",
       "28                       599                      0.7   \n",
       "73                        27                      0.6   \n",
       "48                       359                      0.3   \n",
       "61                        27                      0.1   \n",
       "62                      1000                      0.5   \n",
       "1                        215                      0.5   \n",
       "72                       215                      0.7   \n",
       "53                      1000                      1.0   \n",
       "20                        16                      0.3   \n",
       "12                        27                      1.0   \n",
       "57                        77                      0.4   \n",
       "73                        27                      0.6   \n",
       "41                        16                      0.2   \n",
       "\n",
       "   param_model__max_features param_model__contamination  1_test_eer  \\\n",
       "57                        12                        0.1    0.034884   \n",
       "12                        84                        0.1    0.067416   \n",
       "53                        84                        0.1    0.097826   \n",
       "12                        12                        0.1    0.046512   \n",
       "69                        12                        0.1    0.046512   \n",
       "1                         24                        0.2    0.059524   \n",
       "68                        60                        0.1    0.144444   \n",
       "53                        12                        0.1    0.100000   \n",
       "57                        12                        0.1    0.087912   \n",
       "73                        72                        0.1    0.107143   \n",
       "23                        12                        0.2    0.084337   \n",
       "2                         48                        0.2    0.107527   \n",
       "53                        60                        0.1    0.214286   \n",
       "63                        72                        0.1    0.100000   \n",
       "73                        72                        0.1    0.193878   \n",
       "77                        60                        0.2    0.130435   \n",
       "27                        84                        0.1    0.136364   \n",
       "28                        72                        0.2    0.120879   \n",
       "73                        72                        0.1    0.133333   \n",
       "48                        84                        0.1    0.164706   \n",
       "61                        60                        0.1    0.333333   \n",
       "62                        48                        0.1    0.286957   \n",
       "1                         36                        0.1    0.129412   \n",
       "72                        84                        0.1    0.131868   \n",
       "53                        12                        0.1    0.250000   \n",
       "20                        84                        0.4    0.118280   \n",
       "12                        84                        0.1    0.147727   \n",
       "57                        12                        0.1    0.153061   \n",
       "73                        72                        0.1    0.208791   \n",
       "41                        48                        0.4    0.156250   \n",
       "\n",
       "    2_test_eer  3_test_eer  4_test_eer  5_test_eer  6_test_eer  7_test_eer  \\\n",
       "57    0.047619    0.035714    0.035294    0.035714    0.035714    0.245283   \n",
       "12    0.067416    0.067416    0.067416    0.067416    0.067416    0.068966   \n",
       "53    0.097826    0.097826    0.097826    0.097826    0.097826    0.097826   \n",
       "12    0.011905    0.057471    0.023810    0.023810         NaN    0.011905   \n",
       "69    0.045977    0.047059    0.046512    0.045977         NaN    0.045977   \n",
       "1     0.056818    0.170213    0.133333    0.082353    0.056818    0.056818   \n",
       "68    0.067416    0.067416    0.153846         NaN    0.067416    0.068182   \n",
       "53    0.097826    0.097826    0.101124         NaN    0.097826    0.097826   \n",
       "57    0.087912    0.087912    0.088889    0.087912         NaN    0.088889   \n",
       "73    0.087912    0.087912    0.093023         NaN    0.087912    0.087912   \n",
       "23    0.077778    0.289720    0.164835    0.164835    0.077778    0.077778   \n",
       "2     0.107527    0.108696    0.107527    0.117647    0.485915    0.107527   \n",
       "53    0.067416    0.069767    0.067416    0.125000    0.486667    0.067416   \n",
       "63    0.097826    0.119048    0.097826    0.097826    0.108434    0.097826   \n",
       "73    0.045977         NaN    0.288288    0.045977    0.047059    0.047059   \n",
       "77    0.126316    0.129032    0.126316    0.134831    0.485507    0.126316   \n",
       "27    0.144578    0.126316    0.174419    0.236559    0.126316    0.330189   \n",
       "28    0.117021    0.217391    0.127907    0.234043    0.117021    0.117021   \n",
       "73    0.154762    0.154762    0.154762    0.193182    0.126316    0.303922   \n",
       "48    0.126316    0.252632    0.211111    0.127660    0.126316         NaN   \n",
       "61    0.082353    0.178947    0.123596    0.142857    0.113636         NaN   \n",
       "62    0.011905         NaN    0.327869    0.011905    0.023810    0.023810   \n",
       "1          NaN    0.097826    0.100000    0.260000    0.097826    0.482517   \n",
       "72    0.126316    0.139535    0.126316    0.126316    0.127660    0.126316   \n",
       "53    0.056818         NaN    0.309735    0.056818    0.058140    0.056818   \n",
       "20    0.117021    0.181818    0.280000    0.339450    0.118280    0.117021   \n",
       "12    0.135417    0.166667    0.142857    0.136842    0.142857    0.135417   \n",
       "57    0.153061    0.170455    0.276596    0.291667    0.153061    0.153061   \n",
       "73    0.117021    0.208791    0.125000    0.123596    0.125000    0.118280   \n",
       "41    0.153061    0.168539    0.153061    0.153061    0.153061    0.157895   \n",
       "\n",
       "    8_test_eer  9_test_eer  10_test_eer  11_test_eer  12_test_eer  \\\n",
       "57    0.034884    0.036145     0.034884     0.035714     0.035294   \n",
       "12    0.068182    0.068966     0.067416     0.067416     0.068966   \n",
       "53    0.097826    0.097826     0.097826     0.097826     0.097826   \n",
       "12    0.088889    0.088889     0.359375     0.305085     0.011905   \n",
       "69    0.122222    0.045977     0.288288     0.254717     0.045977   \n",
       "1     0.376000         NaN     0.093023     0.338983     0.082353   \n",
       "68    0.425373    0.069767     0.070588     0.083333     0.067416   \n",
       "53    0.362069    0.100000     0.097826     0.098901     0.097826   \n",
       "57    0.090909    0.088889     0.210526     0.184783     0.087912   \n",
       "73    0.395161    0.090909     0.088889     0.137931     0.096386   \n",
       "23    0.377049         NaN     0.095238     0.289720     0.105882   \n",
       "2     0.112360    0.108696     0.188889     0.109890     0.107527   \n",
       "53    0.094118    0.070588     0.134831     0.125000     0.067416   \n",
       "63    0.105882    0.119048     0.471429     0.260000     0.097826   \n",
       "73    0.288288    0.268519     0.318966     0.318966     0.045977   \n",
       "77    0.126316    0.126316     0.130435     0.129032     0.126316   \n",
       "27    0.139535    0.141176     0.126316     0.126316     0.141176   \n",
       "28    0.384615         NaN     0.120879     0.200000     0.130952   \n",
       "73    0.164706    0.183908     0.126316     0.129032     0.154762   \n",
       "48    0.144578    0.317308     0.126316     0.139535     0.126316   \n",
       "61    0.152174    0.417910     0.056818     0.220000     0.178947   \n",
       "62    0.316667    0.405797     0.364341     0.410072     0.011905   \n",
       "1     0.103448    0.098901     0.097826     0.105882     0.321101   \n",
       "72    0.202247    0.202247     0.462121     0.275510     0.126316   \n",
       "53    0.297297    0.333333     0.344538     0.370968     0.056818   \n",
       "20         NaN    0.130952     0.242105     0.307692     0.117021   \n",
       "12    0.186047    0.204545     0.401709     0.357798     0.135417   \n",
       "57         NaN    0.157895     0.156250     0.333333     0.153061   \n",
       "73    0.181818    0.191011          NaN     0.424000     0.117021   \n",
       "41    0.153061    0.190476     0.381818          NaN     0.156250   \n",
       "\n",
       "    13_test_eer  14_test_eer  15_test_eer  16_test_eer  17_test_eer  \\\n",
       "57     0.034884     0.111111     0.034884     0.034884     0.035294   \n",
       "12     0.067416     0.071429     0.067416     0.067416     0.068182   \n",
       "53     0.097826     0.097826     0.097826     0.097826     0.097826   \n",
       "12     0.012048     0.011905     0.011905     0.023810     0.474359   \n",
       "69     0.045977     0.045977     0.045977     0.046512     0.487013   \n",
       "1      0.058140     0.056818     0.057471     0.058140     0.056818   \n",
       "68     0.069767     0.067416     0.068966     0.252427     0.067416   \n",
       "53     0.098901     0.097826     0.097826     0.229167     0.097826   \n",
       "57     0.087912     0.087912     0.087912     0.087912     0.475524   \n",
       "73     0.137931     0.087912     0.088889     0.299065     0.087912   \n",
       "23     0.082353     0.077778     0.078652     0.080460     0.077778   \n",
       "2      0.116279     0.107527     0.107527     0.151163          NaN   \n",
       "53     0.163043     0.067416     0.067416     0.252427          NaN   \n",
       "63     0.097826     0.097826     0.097826     0.108434     0.244898   \n",
       "73     0.047059     0.045977     0.045977     0.045977     0.131868   \n",
       "77     0.141176     0.126316     0.126316     0.141176          NaN   \n",
       "27     0.126316     0.134831          NaN     0.129032     0.126316   \n",
       "28     0.118280     0.117021     0.118280     0.120879     0.117021   \n",
       "73     0.129032     0.126316          NaN     0.136364     0.126316   \n",
       "48     0.126316     0.126316     0.136364     0.126316     0.127660   \n",
       "61     0.057471     0.058140     0.113636     0.056818     0.170213   \n",
       "62     0.012048     0.011905     0.011905     0.011905     0.180000   \n",
       "1      0.107143     0.221053     0.301887     0.102273     0.097826   \n",
       "72     0.126316     0.126316     0.126316     0.131868     0.154762   \n",
       "53     0.056818     0.056818     0.056818     0.056818     0.161290   \n",
       "20     0.118280     0.117021     0.118280     0.130952     0.118280   \n",
       "12     0.135417     0.135417     0.135417     0.141304     0.147727   \n",
       "57     0.156250     0.153061     0.153061     0.174419     0.153061   \n",
       "73     0.152941     0.117021     0.117021     0.120879     0.242105   \n",
       "41     0.153061     0.153061     0.153061     0.153061     0.154639   \n",
       "\n",
       "    18_test_eer  mean_test_eer  std_test_eer  rank_test_eer  1_test_accuracy  \\\n",
       "57     0.034884       0.051838      0.050040              1         0.981928   \n",
       "12     0.067416       0.067982      0.001019              1         0.963855   \n",
       "53     0.097826       0.097826      0.000000              1         0.945783   \n",
       "12     0.344000       0.106638      0.146222              1         0.969880   \n",
       "69     0.254717       0.111519      0.122217              1         0.969880   \n",
       "1      0.161290       0.111799      0.094085              1         0.945783   \n",
       "68     0.180851       0.114414      0.091029              1         0.885542   \n",
       "53     0.100000       0.120468      0.065775              1         0.933735   \n",
       "57     0.091954       0.122194      0.092357              1         0.951807   \n",
       "73     0.157303       0.128223      0.081662              1         0.897590   \n",
       "23     0.146067       0.134816      0.088766              1         0.915663   \n",
       "2      0.130952       0.138372      0.086710              1         0.939759   \n",
       "53     0.306306       0.139664      0.109477              1         0.837349   \n",
       "63          NaN       0.139867      0.093360              1         0.933735   \n",
       "73     0.268519       0.141128      0.113483              1         0.861446   \n",
       "77     0.141176       0.149980      0.081555              1         0.909639   \n",
       "27     0.126316       0.151411      0.050484              1         0.885542   \n",
       "28     0.127907       0.151482      0.067235              1         0.915663   \n",
       "73     0.129032       0.153103      0.041704              1         0.897590   \n",
       "48     0.129032       0.153395      0.051918              1         0.843373   \n",
       "61     0.056818       0.157192      0.102726              1         0.734940   \n",
       "62     0.397059       0.157320      0.167088              1         0.795181   \n",
       "1      0.100000       0.162892      0.106994              1         0.879518   \n",
       "72          NaN       0.164703      0.082016              1         0.903614   \n",
       "53     0.370968       0.167091      0.130762              1         0.813253   \n",
       "20     0.234043       0.167973      0.074385              1         0.927711   \n",
       "12          NaN       0.173555      0.075628              1         0.873494   \n",
       "57     0.163043       0.180970      0.054697              1         0.909639   \n",
       "73     0.458647       0.181443      0.099967              1         0.819277   \n",
       "41     0.376147       0.181813      0.070278              1         0.897590   \n",
       "\n",
       "    2_test_accuracy  3_test_accuracy  4_test_accuracy  5_test_accuracy  \\\n",
       "57         0.957831         0.969880         0.975904         0.969880   \n",
       "12         0.963855         0.963855         0.963855         0.963855   \n",
       "53         0.945783         0.945783         0.945783         0.945783   \n",
       "12         0.993976         0.963855         0.981928         0.981928   \n",
       "69         0.975904         0.963855         0.969880         0.975904   \n",
       "1          0.969880         0.873494         0.897590         0.927711   \n",
       "68         0.963855         0.963855         0.879518              NaN   \n",
       "53         0.945783         0.945783         0.927711              NaN   \n",
       "57         0.951807         0.951807         0.945783         0.951807   \n",
       "73         0.951807         0.951807         0.921687              NaN   \n",
       "23         0.957831         0.771084         0.867470         0.867470   \n",
       "2          0.939759         0.933735         0.939759         0.891566   \n",
       "53         0.963855         0.945783         0.963855         0.897590   \n",
       "63         0.945783         0.885542         0.945783         0.945783   \n",
       "73         0.975904              NaN         0.783133         0.975904   \n",
       "77         0.927711         0.915663         0.927711         0.891566   \n",
       "27         0.855422         0.927711         0.837349         0.795181   \n",
       "28         0.933735         0.813253         0.885542         0.801205   \n",
       "73         0.849398         0.849398         0.849398         0.825301   \n",
       "48         0.927711         0.783133         0.813253         0.921687   \n",
       "61         0.927711         0.867470         0.903614         0.891566   \n",
       "62         0.993976              NaN         0.753012         0.993976   \n",
       "1               NaN         0.945783         0.933735         0.789157   \n",
       "72         0.927711         0.873494         0.927711         0.927711   \n",
       "53         0.969880              NaN         0.759036         0.969880   \n",
       "20         0.933735         0.837349         0.765060         0.710843   \n",
       "12         0.921687         0.837349         0.891566         0.915663   \n",
       "57         0.909639         0.849398         0.753012         0.740964   \n",
       "73         0.933735         0.819277         0.897590         0.903614   \n",
       "41         0.909639         0.855422         0.909639         0.909639   \n",
       "\n",
       "    6_test_accuracy  7_test_accuracy  8_test_accuracy  9_test_accuracy  \\\n",
       "57         0.969880         0.825301         0.981928         0.963855   \n",
       "12         0.963855         0.951807         0.957831         0.951807   \n",
       "53         0.945783         0.945783         0.945783         0.945783   \n",
       "12              NaN         0.993976         0.945783         0.945783   \n",
       "69              NaN         0.975904         0.909639         0.975904   \n",
       "1          0.969880         0.969880         0.686747              NaN   \n",
       "68         0.963855         0.957831         0.620482         0.945783   \n",
       "53         0.945783         0.945783         0.692771         0.933735   \n",
       "57              NaN         0.945783         0.933735         0.945783   \n",
       "73         0.951807         0.951807         0.656627         0.933735   \n",
       "23         0.957831         0.957831         0.680723              NaN   \n",
       "2          0.524096         0.939759         0.915663         0.933735   \n",
       "53         0.524096         0.963855         0.915663         0.939759   \n",
       "63         0.891566         0.945783         0.903614         0.885542   \n",
       "73         0.963855         0.963855         0.783133         0.801205   \n",
       "77         0.524096         0.927711         0.927711         0.927711   \n",
       "27         0.927711         0.716867         0.873494         0.867470   \n",
       "28         0.933735         0.933735         0.662651              NaN   \n",
       "73         0.927711         0.740964         0.843373         0.831325   \n",
       "48         0.927711              NaN         0.855422         0.728916   \n",
       "61         0.909639              NaN         0.885542         0.632530   \n",
       "62         0.981928         0.981928         0.765060         0.656627   \n",
       "1          0.945783         0.530120         0.915663         0.939759   \n",
       "72         0.921687         0.927711         0.819277         0.819277   \n",
       "53         0.957831         0.969880         0.771084         0.734940   \n",
       "20         0.927711         0.933735              NaN         0.873494   \n",
       "12         0.891566         0.921687         0.825301         0.813253   \n",
       "57         0.909639         0.909639              NaN         0.891566   \n",
       "73         0.897590         0.927711         0.837349         0.831325   \n",
       "41         0.909639         0.891566         0.909639         0.813253   \n",
       "\n",
       "    10_test_accuracy  11_test_accuracy  12_test_accuracy  13_test_accuracy  \\\n",
       "57          0.981928          0.969880          0.975904          0.981928   \n",
       "12          0.963855          0.963855          0.951807          0.963855   \n",
       "53          0.945783          0.945783          0.945783          0.945783   \n",
       "12          0.716867          0.777108          0.993976          0.987952   \n",
       "69          0.783133          0.813253          0.975904          0.975904   \n",
       "1           0.921687          0.728916          0.927711          0.957831   \n",
       "68          0.939759          0.921687          0.963855          0.945783   \n",
       "53          0.945783          0.939759          0.945783          0.939759   \n",
       "57          0.831325          0.849398          0.951807          0.951807   \n",
       "73          0.945783          0.879518          0.903614          0.879518   \n",
       "23          0.909639          0.771084          0.903614          0.927711   \n",
       "2           0.837349          0.927711          0.939759          0.897590   \n",
       "53          0.891566          0.897590          0.963855          0.873494   \n",
       "63          0.548193          0.789157          0.945783          0.945783   \n",
       "73          0.753012          0.753012          0.975904          0.963855   \n",
       "77          0.909639          0.915663          0.927711          0.867470   \n",
       "27          0.927711          0.927711          0.867470          0.927711   \n",
       "28          0.915663          0.825301          0.873494          0.927711   \n",
       "73          0.927711          0.915663          0.849398          0.915663   \n",
       "48          0.927711          0.873494          0.927711          0.927711   \n",
       "61          0.969880          0.837349          0.867470          0.963855   \n",
       "62          0.710843          0.650602          0.993976          0.987952   \n",
       "1           0.945783          0.903614          0.734940          0.897590   \n",
       "72          0.560241          0.765060          0.927711          0.927711   \n",
       "53          0.722892          0.692771          0.969880          0.969880   \n",
       "20          0.795181          0.740964          0.933735          0.927711   \n",
       "12          0.638554          0.686747          0.921687          0.921687   \n",
       "57          0.897590          0.704819          0.909639          0.897590   \n",
       "73               NaN          0.614458          0.933735          0.855422   \n",
       "41          0.656627               NaN          0.897590          0.909639   \n",
       "\n",
       "    14_test_accuracy  15_test_accuracy  16_test_accuracy  17_test_accuracy  \\\n",
       "57          0.921687          0.981928          0.981928          0.975904   \n",
       "12          0.933735          0.963855          0.963855          0.957831   \n",
       "53          0.945783          0.945783          0.945783          0.945783   \n",
       "12          0.993976          0.993976          0.981928          0.548193   \n",
       "69          0.975904          0.975904          0.969880          0.524096   \n",
       "1           0.969880          0.963855          0.957831          0.969880   \n",
       "68          0.963855          0.951807          0.807229          0.963855   \n",
       "53          0.945783          0.945783          0.813253          0.945783   \n",
       "57          0.951807          0.951807          0.951807          0.542169   \n",
       "73          0.951807          0.945783          0.759036          0.951807   \n",
       "23          0.957831          0.951807          0.939759          0.957831   \n",
       "2           0.939759          0.939759          0.861446               NaN   \n",
       "53          0.963855          0.963855          0.807229               NaN   \n",
       "63          0.945783          0.945783          0.891566          0.801205   \n",
       "73          0.975904          0.975904          0.975904          0.903614   \n",
       "77          0.927711          0.927711          0.867470               NaN   \n",
       "27          0.891566               NaN          0.915663          0.927711   \n",
       "28          0.933735          0.927711          0.915663          0.933735   \n",
       "73          0.927711               NaN          0.885542          0.927711   \n",
       "48          0.927711          0.885542          0.927711          0.921687   \n",
       "61          0.957831          0.909639          0.969880          0.873494   \n",
       "62          0.993976          0.993976          0.993976          0.885542   \n",
       "1           0.819277          0.753012          0.921687          0.945783   \n",
       "72          0.927711          0.927711          0.903614          0.849398   \n",
       "53          0.969880          0.969880          0.969880          0.879518   \n",
       "20          0.933735          0.927711          0.873494          0.927711   \n",
       "12          0.921687          0.921687          0.897590          0.873494   \n",
       "57          0.909639          0.909639          0.837349          0.909639   \n",
       "73          0.933735          0.933735          0.915663          0.795181   \n",
       "41          0.909639          0.909639          0.909639          0.903614   \n",
       "\n",
       "    18_test_accuracy  mean_test_accuracy  std_test_accuracy  \\\n",
       "57          0.981928            0.963855           0.036422   \n",
       "12          0.963855            0.959505           0.007727   \n",
       "53          0.945783            0.945783           0.000000   \n",
       "12          0.734940            0.916667           0.126637   \n",
       "69          0.813253            0.916667           0.114427   \n",
       "1           0.879518            0.915663           0.079868   \n",
       "68          0.861446            0.914659           0.083599   \n",
       "53          0.933735            0.920683           0.062837   \n",
       "57          0.927711            0.913320           0.096368   \n",
       "73          0.867470            0.902945           0.076540   \n",
       "23          0.879518            0.895917           0.077584   \n",
       "2           0.873494            0.895248           0.095168   \n",
       "53          0.759036            0.890897           0.106840   \n",
       "63               NaN            0.891232           0.095745   \n",
       "73          0.801205            0.897925           0.089453   \n",
       "77          0.867470            0.889893           0.091378   \n",
       "27          0.927711            0.883199           0.054814   \n",
       "28          0.885542            0.885542           0.068715   \n",
       "73          0.915663            0.877510           0.049554   \n",
       "48          0.915663            0.886881           0.058114   \n",
       "61          0.969880            0.879183           0.089023   \n",
       "62          0.668675            0.877510           0.136546   \n",
       "1           0.933735            0.868474           0.105403   \n",
       "72               NaN            0.875837           0.090320   \n",
       "53          0.692771            0.875167           0.111835   \n",
       "20          0.801205            0.872490           0.075006   \n",
       "12               NaN            0.866466           0.080177   \n",
       "57          0.873494            0.868474           0.064640   \n",
       "73          0.566265            0.852744           0.103841   \n",
       "41          0.662651            0.870817           0.078448   \n",
       "\n",
       "    rank_test_accuracy  owner  run  0_test_eer  0_test_accuracy  \n",
       "57                   1      0    2         NaN              NaN  \n",
       "12                   1      0    0         NaN              NaN  \n",
       "53                   1      0    1         NaN              NaN  \n",
       "12                   1      6    1    0.011905         0.993976  \n",
       "69                   1      6    0    0.045977         0.975904  \n",
       "1                    1      9    2    0.057471         0.963855  \n",
       "68                   1      5    2    0.067416         0.963855  \n",
       "53                   1      5    0    0.097826         0.945783  \n",
       "57                   1      6    2    0.087912         0.951807  \n",
       "73                   4      5    1    0.087912         0.951807  \n",
       "23                   2      9    0    0.078652         0.951807  \n",
       "2                    1     17    0    0.107527         0.939759  \n",
       "53                   2     17    2    0.067416         0.963855  \n",
       "63                   1     18    2    0.097826         0.945783  \n",
       "73                   1      3    1    0.045977         0.975904  \n",
       "77                   1     17    1    0.126316         0.927711  \n",
       "27                   1     15    1    0.133333         0.897590  \n",
       "28                   1      9    1    0.119565         0.921687  \n",
       "73                   1     15    0    0.129032         0.915663  \n",
       "48                   1      7    2    0.126316         0.927711  \n",
       "61                   1      7    0    0.315789         0.753012  \n",
       "62                   1      3    2    0.011905         0.993976  \n",
       "1                    1      2    1    0.107143         0.897590  \n",
       "72                   1     18    1    0.126316         0.927711  \n",
       "53                   1      3    0    0.056818         0.969880  \n",
       "20                   2      8    1    0.117021         0.933735  \n",
       "12                   1     18    0    0.135417         0.921687  \n",
       "57                   1      8    2    0.153061         0.909639  \n",
       "73                   1     10    0    0.117021         0.933735  \n",
       "41                   1     11    1    0.153061         0.909639  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    df_results[df_results[\"rank_test_eer\"] == 1]\n",
    "    .sort_values(\"mean_test_eer\")\n",
    "    .head(30)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display(\n",
    "#     df_results[df_results[\"rank_test_eer\"] == 1][\n",
    "#         [\n",
    "#             \"mean_fit_time\",\n",
    "#             \"param_model__nu\",\n",
    "#             \"param_model__gamma\",\n",
    "#             \"mean_test_accuracy\",\n",
    "#             \"std_test_accuracy\",\n",
    "#             \"mean_test_eer\",\n",
    "#             \"std_test_eer\",\n",
    "#         ]\n",
    "#     ].describe()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.median_n_estimators = None \n",
    "P.median_max_samples = None\n",
    "P.median_contamination = None\n",
    "P.median_max_features = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.9/site-packages (22.3.1)\n",
      "utility_functions imports setup complete\n",
      "Python 3.9.10\n",
      "EER: 0.333, Threshold: 0.600 <-- Arbitrary case\n",
      "EER: 0.000, Threshold: 0.900 <-- Best case\n",
      "EER: 1.000, Threshold: 0.900 <-- Worse case\n",
      "EER: 0.400, Threshold: 0.200 <-- Worse case\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utility functions imported\n",
      "Index(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time',\n",
      "       'n_estimators', 'max_samples', 'max_features', 'contamination',\n",
      "       '1_test_eer', '2_test_eer', '3_test_eer', '4_test_eer', '5_test_eer',\n",
      "       '6_test_eer', '7_test_eer', '8_test_eer', '9_test_eer', '10_test_eer',\n",
      "       '11_test_eer', '12_test_eer', '13_test_eer', '14_test_eer',\n",
      "       '15_test_eer', '16_test_eer', '17_test_eer', '18_test_eer',\n",
      "       'Mean Test EER', 'std_test_eer', 'rank_test_eer', '1_test_accuracy',\n",
      "       '2_test_accuracy', '3_test_accuracy', '4_test_accuracy',\n",
      "       '5_test_accuracy', '6_test_accuracy', '7_test_accuracy',\n",
      "       '8_test_accuracy', '9_test_accuracy', '10_test_accuracy',\n",
      "       '11_test_accuracy', '12_test_accuracy', '13_test_accuracy',\n",
      "       '14_test_accuracy', '15_test_accuracy', '16_test_accuracy',\n",
      "       '17_test_accuracy', '18_test_accuracy', 'Mean Test Acc.',\n",
      "       'std_test_accuracy', 'rank_test_accuracy', 'owner', 'run', '0_test_eer',\n",
      "       '0_test_accuracy'],\n",
      "      dtype='object')\n",
      "0, 0.28\n",
      "1, 0.24\n",
      "2, 0.20\n",
      "3, 0.16\n",
      "4, 0.12\n",
      "5, 0.08\n",
      "['0.240', '0.200', '0.160', '0.120', '0.080']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/951617972.py:92: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all axes decorations.\n",
      "  fig.tight_layout()\n",
      "/tmp/ipykernel_2624/951617972.py:92: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all axes decorations.\n",
      "  fig.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0.28\n",
      "1, 0.24\n",
      "2, 0.20\n",
      "3, 0.16\n",
      "4, 0.12\n",
      "5, 0.08\n",
      "['0.240', '0.200', '0.160', '0.120', '0.080']\n",
      "0, 0.28\n",
      "1, 0.24\n",
      "2, 0.20\n",
      "3, 0.16\n",
      "4, 0.12\n",
      "5, 0.08\n",
      "['0.240', '0.200', '0.160', '0.120', '0.080']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/951617972.py:92: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all axes decorations.\n",
      "  fig.tight_layout()\n",
      "/tmp/ipykernel_2624/951617972.py:92: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all axes decorations.\n",
      "  fig.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0.28\n",
      "1, 0.24\n",
      "2, 0.20\n",
      "3, 0.16\n",
      "4, 0.12\n",
      "5, 0.08\n",
      "['0.240', '0.200', '0.160', '0.120', '0.080']\n",
      "0, 0.28\n",
      "1, 0.24\n",
      "2, 0.20\n",
      "3, 0.16\n",
      "4, 0.12\n",
      "5, 0.08\n",
      "['0.240', '0.200', '0.160', '0.120', '0.080']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2624/951617972.py:92: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all axes decorations.\n",
      "  fig.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0.28\n",
      "1, 0.24\n",
      "2, 0.20\n",
      "3, 0.16\n",
      "4, 0.12\n",
      "5, 0.08\n",
      "['0.240', '0.200', '0.160', '0.120', '0.080']\n",
      "77.0 0.8 0.08 0.55\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Image size of 82513x200 pixels is too large. It must be less than 2^16 in each direction.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/formatters.py:339\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    341\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/pylabtools.py:151\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    149\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 151\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/backend_bases.py:2319\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2316\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2317\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2318\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[0;32m-> 2319\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2320\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2321\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2322\u001b[0m \u001b[43m            \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2323\u001b[0m \u001b[43m            \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2324\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2325\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/backend_bases.py:1648\u001b[0m, in \u001b[0;36m_check_savefig_extra_args.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     _api\u001b[38;5;241m.\u001b[39mwarn_deprecated(\n\u001b[1;32m   1641\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.3\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39mname, removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.6\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1642\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m() got unexpected keyword argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1643\u001b[0m                 \u001b[38;5;241m+\u001b[39m arg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m which is no longer supported as of \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1644\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m and will become an error \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1645\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1646\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(arg)\n\u001b[0;32m-> 1648\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/_api/deprecation.py:412\u001b[0m, in \u001b[0;36mdelete_parameter.<locals>.wrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m     deprecation_addendum \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf any parameter follows \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m, they should be passed as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyword, not positionally.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    405\u001b[0m     warn_deprecated(\n\u001b[1;32m    406\u001b[0m         since,\n\u001b[1;32m    407\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrepr\u001b[39m(name),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m                  \u001b[38;5;28;01melse\u001b[39;00m deprecation_addendum,\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minner_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minner_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:540\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;129m@_check_savefig_extra_args\u001b[39m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;129m@_api\u001b[39m\u001b[38;5;241m.\u001b[39mdelete_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    493\u001b[0m               metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 540\u001b[0m     \u001b[43mFigureCanvasAgg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m     mpl\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimsave(\n\u001b[1;32m    542\u001b[0m         filename_or_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_rgba(), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m\"\u001b[39m, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    543\u001b[0m         dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi, metadata\u001b[38;5;241m=\u001b[39mmetadata, pil_kwargs\u001b[38;5;241m=\u001b[39mpil_kwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:431\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;66;03m# docstring inherited\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_renderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m RendererAgg\u001b[38;5;241m.\u001b[39mlock, \\\n\u001b[1;32m    434\u001b[0m          (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\u001b[38;5;241m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\n\u001b[1;32m    435\u001b[0m           \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:447\u001b[0m, in \u001b[0;36mFigureCanvasAgg.get_renderer\u001b[0;34m(self, cleared)\u001b[0m\n\u001b[1;32m    444\u001b[0m reuse_renderer \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrenderer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    445\u001b[0m                   \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_lastKey\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m key)\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m reuse_renderer:\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderer \u001b[38;5;241m=\u001b[39m \u001b[43mRendererAgg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lastKey \u001b[38;5;241m=\u001b[39m key\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m cleared:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:93\u001b[0m, in \u001b[0;36mRendererAgg.__init__\u001b[0;34m(self, width, height, dpi)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m=\u001b[39m width\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m=\u001b[39m height\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_renderer \u001b[38;5;241m=\u001b[39m \u001b[43m_RendererAgg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_renderers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_methods()\n",
      "\u001b[0;31mValueError\u001b[0m: Image size of 82513x200 pixels is too large. It must be less than 2^16 in each direction."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 421x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Image size of 82513x200 pixels is too large. It must be less than 2^16 in each direction.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/formatters.py:339\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    341\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/pylabtools.py:151\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    149\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 151\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/backend_bases.py:2319\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2316\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2317\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2318\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[0;32m-> 2319\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2320\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2321\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2322\u001b[0m \u001b[43m            \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2323\u001b[0m \u001b[43m            \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2324\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2325\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/backend_bases.py:1648\u001b[0m, in \u001b[0;36m_check_savefig_extra_args.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     _api\u001b[38;5;241m.\u001b[39mwarn_deprecated(\n\u001b[1;32m   1641\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.3\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39mname, removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.6\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1642\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m() got unexpected keyword argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1643\u001b[0m                 \u001b[38;5;241m+\u001b[39m arg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m which is no longer supported as of \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1644\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m and will become an error \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1645\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1646\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(arg)\n\u001b[0;32m-> 1648\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/_api/deprecation.py:412\u001b[0m, in \u001b[0;36mdelete_parameter.<locals>.wrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m     deprecation_addendum \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf any parameter follows \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m, they should be passed as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyword, not positionally.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    405\u001b[0m     warn_deprecated(\n\u001b[1;32m    406\u001b[0m         since,\n\u001b[1;32m    407\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrepr\u001b[39m(name),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m                  \u001b[38;5;28;01melse\u001b[39;00m deprecation_addendum,\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minner_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minner_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:540\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;129m@_check_savefig_extra_args\u001b[39m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;129m@_api\u001b[39m\u001b[38;5;241m.\u001b[39mdelete_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    493\u001b[0m               metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 540\u001b[0m     \u001b[43mFigureCanvasAgg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m     mpl\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimsave(\n\u001b[1;32m    542\u001b[0m         filename_or_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_rgba(), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m\"\u001b[39m, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    543\u001b[0m         dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi, metadata\u001b[38;5;241m=\u001b[39mmetadata, pil_kwargs\u001b[38;5;241m=\u001b[39mpil_kwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:431\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;66;03m# docstring inherited\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_renderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m RendererAgg\u001b[38;5;241m.\u001b[39mlock, \\\n\u001b[1;32m    434\u001b[0m          (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\u001b[38;5;241m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\n\u001b[1;32m    435\u001b[0m           \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:447\u001b[0m, in \u001b[0;36mFigureCanvasAgg.get_renderer\u001b[0;34m(self, cleared)\u001b[0m\n\u001b[1;32m    444\u001b[0m reuse_renderer \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrenderer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    445\u001b[0m                   \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_lastKey\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m key)\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m reuse_renderer:\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderer \u001b[38;5;241m=\u001b[39m \u001b[43mRendererAgg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lastKey \u001b[38;5;241m=\u001b[39m key\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m cleared:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:93\u001b[0m, in \u001b[0;36mRendererAgg.__init__\u001b[0;34m(self, width, height, dpi)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m=\u001b[39m width\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m=\u001b[39m height\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_renderer \u001b[38;5;241m=\u001b[39m \u001b[43m_RendererAgg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_renderers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_methods()\n",
      "\u001b[0;31mValueError\u001b[0m: Image size of 82513x200 pixels is too large. It must be less than 2^16 in each direction."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 421x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Image size of 82513x200 pixels is too large. It must be less than 2^16 in each direction.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/formatters.py:339\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    341\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/IPython/core/pylabtools.py:151\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    149\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 151\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/backend_bases.py:2319\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2316\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2317\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2318\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[0;32m-> 2319\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2320\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2321\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2322\u001b[0m \u001b[43m            \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2323\u001b[0m \u001b[43m            \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2324\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2325\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/backend_bases.py:1648\u001b[0m, in \u001b[0;36m_check_savefig_extra_args.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     _api\u001b[38;5;241m.\u001b[39mwarn_deprecated(\n\u001b[1;32m   1641\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.3\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39mname, removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.6\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1642\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m() got unexpected keyword argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1643\u001b[0m                 \u001b[38;5;241m+\u001b[39m arg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m which is no longer supported as of \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1644\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m and will become an error \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1645\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1646\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(arg)\n\u001b[0;32m-> 1648\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/_api/deprecation.py:412\u001b[0m, in \u001b[0;36mdelete_parameter.<locals>.wrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m     deprecation_addendum \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf any parameter follows \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m, they should be passed as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyword, not positionally.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    405\u001b[0m     warn_deprecated(\n\u001b[1;32m    406\u001b[0m         since,\n\u001b[1;32m    407\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrepr\u001b[39m(name),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m                  \u001b[38;5;28;01melse\u001b[39;00m deprecation_addendum,\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minner_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minner_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:540\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;129m@_check_savefig_extra_args\u001b[39m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;129m@_api\u001b[39m\u001b[38;5;241m.\u001b[39mdelete_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    493\u001b[0m               metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 540\u001b[0m     \u001b[43mFigureCanvasAgg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m     mpl\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimsave(\n\u001b[1;32m    542\u001b[0m         filename_or_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_rgba(), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m\"\u001b[39m, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    543\u001b[0m         dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi, metadata\u001b[38;5;241m=\u001b[39mmetadata, pil_kwargs\u001b[38;5;241m=\u001b[39mpil_kwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:431\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;66;03m# docstring inherited\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_renderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m RendererAgg\u001b[38;5;241m.\u001b[39mlock, \\\n\u001b[1;32m    434\u001b[0m          (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\u001b[38;5;241m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\n\u001b[1;32m    435\u001b[0m           \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:447\u001b[0m, in \u001b[0;36mFigureCanvasAgg.get_renderer\u001b[0;34m(self, cleared)\u001b[0m\n\u001b[1;32m    444\u001b[0m reuse_renderer \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrenderer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    445\u001b[0m                   \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_lastKey\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m key)\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m reuse_renderer:\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderer \u001b[38;5;241m=\u001b[39m \u001b[43mRendererAgg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lastKey \u001b[38;5;241m=\u001b[39m key\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m cleared:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:93\u001b[0m, in \u001b[0;36mRendererAgg.__init__\u001b[0;34m(self, width, height, dpi)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m=\u001b[39m width\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m=\u001b[39m height\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_renderer \u001b[38;5;241m=\u001b[39m \u001b[43m_RendererAgg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_renderers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_methods()\n",
      "\u001b[0;31mValueError\u001b[0m: Image size of 82513x200 pixels is too large. It must be less than 2^16 in each direction."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 421x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 421x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 421x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 421x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ./utility-functions.ipynb\n",
    "# _, _, _, _, fig = utils_plot_randomsearch_results_IF(df_results, n_top=1)\n",
    "P.median_n_estimators, P.median_max_samples, P.median_contamination, P.median_max_features, fig = utils_plot_randomsearch_results_IF(df_results[(df_results.rank_test_eer == 1) & (df_results.mean_test_eer < 0.5)], n_top=1)\n",
    "# plt.savefig(f'WACA-IF-Nu_Gamma_plot-win_size={P.window_size}-step_width={P.IF_step_width}.png', bbox_inches='tight')\n",
    "print(P.median_n_estimators, P.median_max_samples, P.median_contamination, P.median_max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median_n_estimators: 77.0,  median_max_samples: 0.8,  median_contamination: 0.08,  median_max_features: 0.55\n"
     ]
    }
   ],
   "source": [
    "print(f\"median_n_estimators: {P.median_n_estimators},  median_max_samples: {P.median_max_samples},  median_contamination: {P.median_contamination},  median_max_features: {P.median_max_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_model__n_estimators</th>\n",
       "      <th>param_model__max_samples</th>\n",
       "      <th>param_model__max_features</th>\n",
       "      <th>param_model__contamination</th>\n",
       "      <th>1_test_eer</th>\n",
       "      <th>2_test_eer</th>\n",
       "      <th>...</th>\n",
       "      <th>16_test_accuracy</th>\n",
       "      <th>17_test_accuracy</th>\n",
       "      <th>18_test_accuracy</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>rank_test_accuracy</th>\n",
       "      <th>owner</th>\n",
       "      <th>run</th>\n",
       "      <th>0_test_eer</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.035616</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.010007</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>27</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.952811</td>\n",
       "      <td>0.008578</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.263808</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.068098</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>215</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.178218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.728916</td>\n",
       "      <td>0.692771</td>\n",
       "      <td>0.779786</td>\n",
       "      <td>0.117057</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.178218</td>\n",
       "      <td>0.891566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.021661</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.006318</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>16</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.861446</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.853748</td>\n",
       "      <td>0.063143</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.825301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.443437</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>0.118396</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.300885</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.801205</td>\n",
       "      <td>0.893240</td>\n",
       "      <td>0.096632</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.975904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.265926</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.068962</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>215</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.224299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.753012</td>\n",
       "      <td>0.578313</td>\n",
       "      <td>0.737282</td>\n",
       "      <td>0.131907</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.224299</td>\n",
       "      <td>0.855422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.250851</td>\n",
       "      <td>0.007144</td>\n",
       "      <td>0.342418</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>1000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.120879</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.888554</td>\n",
       "      <td>0.085105</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.933735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.443803</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.119217</td>\n",
       "      <td>0.001178</td>\n",
       "      <td>359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.530120</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.931392</td>\n",
       "      <td>0.101622</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.969880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.035753</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.010502</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>27</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.323810</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.858434</td>\n",
       "      <td>0.083647</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.174419</td>\n",
       "      <td>0.837349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.160724</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>0.043010</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>129</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.055721</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.897590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.262128</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.066888</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>215</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.102273</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.861446</td>\n",
       "      <td>0.885877</td>\n",
       "      <td>0.077185</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.933735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.035599</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.009856</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>27</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.078652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.941098</td>\n",
       "      <td>0.032968</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.164106</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.045290</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>129</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.578313</td>\n",
       "      <td>0.753681</td>\n",
       "      <td>0.138174</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.909639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.161674</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.044609</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>129</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.123596</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.859772</td>\n",
       "      <td>0.102347</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.191011</td>\n",
       "      <td>0.831325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.161650</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.044382</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>129</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.296610</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.875502</td>\n",
       "      <td>0.131721</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.022153</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.006634</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>16</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.224299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777108</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.752677</td>\n",
       "      <td>0.105388</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.224299</td>\n",
       "      <td>0.855422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.036140</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.010127</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>27</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.122222</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.879183</td>\n",
       "      <td>0.055520</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.933735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.035467</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.009935</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>27</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.081395</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.542169</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.914324</td>\n",
       "      <td>0.112693</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.975904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.265040</td>\n",
       "      <td>0.001388</td>\n",
       "      <td>0.073130</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>215</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.104651</td>\n",
       "      <td>0.101124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.902276</td>\n",
       "      <td>0.054903</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.945783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1.221493</td>\n",
       "      <td>0.001708</td>\n",
       "      <td>0.330719</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.877510</td>\n",
       "      <td>0.080722</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.933735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.161484</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.043824</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>129</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.813253</td>\n",
       "      <td>0.884538</td>\n",
       "      <td>0.077440</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.933735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.098068</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>77</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.951473</td>\n",
       "      <td>0.001380</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.035313</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.009798</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>27</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.231481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.805890</td>\n",
       "      <td>0.065085</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.231481</td>\n",
       "      <td>0.849398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.162612</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.046039</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>129</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.122222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.862115</td>\n",
       "      <td>0.105032</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.242105</td>\n",
       "      <td>0.795181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.445057</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.119107</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>359</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.884538</td>\n",
       "      <td>0.085507</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.957831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.442482</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.118386</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>359</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.351064</td>\n",
       "      <td>0.209524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777108</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.542169</td>\n",
       "      <td>0.732597</td>\n",
       "      <td>0.146051</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.209524</td>\n",
       "      <td>0.867470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.446025</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.120291</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>359</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.900268</td>\n",
       "      <td>0.082310</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.951807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.447202</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.119854</td>\n",
       "      <td>0.001039</td>\n",
       "      <td>359</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.058140</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.536145</td>\n",
       "      <td>0.740964</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.122313</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>0.987952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.036294</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.010620</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>27</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.160920</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>...</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.903949</td>\n",
       "      <td>0.043878</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.179775</td>\n",
       "      <td>0.843373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.096409</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.025882</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>77</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.101124</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.874498</td>\n",
       "      <td>0.090657</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.945783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.022667</td>\n",
       "      <td>0.001409</td>\n",
       "      <td>0.006755</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>16</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.281553</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.878849</td>\n",
       "      <td>0.059699</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.933735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows  54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "53       0.035616      0.000168         0.010007        0.000250   \n",
       "30       0.263808      0.000642         0.068098        0.000571   \n",
       "71       0.021661      0.000121         0.006318        0.000164   \n",
       "57       0.443437      0.002281         0.118396        0.000939   \n",
       "30       0.265926      0.000534         0.068962        0.000565   \n",
       "8        1.250851      0.007144         0.342418        0.002105   \n",
       "57       0.443803      0.000827         0.119217        0.001178   \n",
       "15       0.035753      0.000182         0.010502        0.000152   \n",
       "72       0.160724      0.001027         0.043010        0.000539   \n",
       "37       0.262128      0.000740         0.066888        0.000369   \n",
       "44       0.035599      0.000111         0.009856        0.000208   \n",
       "69       0.164106      0.001399         0.045290        0.000564   \n",
       "69       0.161674      0.000649         0.044609        0.000340   \n",
       "69       0.161650      0.000666         0.044382        0.000356   \n",
       "46       0.022153      0.000106         0.006634        0.000177   \n",
       "44       0.036140      0.000241         0.010127        0.000140   \n",
       "44       0.035467      0.000177         0.009935        0.000269   \n",
       "2        0.265040      0.001388         0.073130        0.000376   \n",
       "78       1.221493      0.001708         0.330719        0.001767   \n",
       "0        0.161484      0.000249         0.043824        0.000310   \n",
       "52       0.098068      0.000723         0.026828        0.000157   \n",
       "49       0.035313      0.000160         0.009798        0.000158   \n",
       "59       0.162612      0.000373         0.046039        0.000770   \n",
       "18       0.445057      0.001154         0.119107        0.000658   \n",
       "18       0.442482      0.001049         0.118386        0.001093   \n",
       "18       0.446025      0.001151         0.120291        0.000960   \n",
       "18       0.447202      0.000961         0.119854        0.001039   \n",
       "34       0.036294      0.000355         0.010620        0.000274   \n",
       "32       0.096409      0.000306         0.025882        0.000225   \n",
       "77       0.022667      0.001409         0.006755        0.000158   \n",
       "\n",
       "   param_model__n_estimators param_model__max_samples  \\\n",
       "53                        27                      0.8   \n",
       "30                       215                      1.0   \n",
       "71                        16                      0.5   \n",
       "57                       359                      1.0   \n",
       "30                       215                      1.0   \n",
       "8                       1000                      1.0   \n",
       "57                       359                      1.0   \n",
       "15                        27                      0.4   \n",
       "72                       129                      0.5   \n",
       "37                       215                      0.6   \n",
       "44                        27                      0.8   \n",
       "69                       129                      0.9   \n",
       "69                       129                      0.9   \n",
       "69                       129                      0.9   \n",
       "46                        16                      0.7   \n",
       "44                        27                      0.8   \n",
       "44                        27                      0.8   \n",
       "2                        215                      0.1   \n",
       "78                      1000                      0.3   \n",
       "0                        129                      0.5   \n",
       "52                        77                      0.9   \n",
       "49                        27                      0.7   \n",
       "59                       129                      0.4   \n",
       "18                       359                      0.9   \n",
       "18                       359                      0.9   \n",
       "18                       359                      0.9   \n",
       "18                       359                      0.9   \n",
       "34                        27                      0.9   \n",
       "32                        77                      0.5   \n",
       "77                        16                      0.8   \n",
       "\n",
       "   param_model__max_features param_model__contamination  1_test_eer  \\\n",
       "53                       0.4                        0.1    0.077778   \n",
       "30                       0.1                        0.1         NaN   \n",
       "71                       0.2                        0.2    0.155556   \n",
       "57                       0.4                        0.1    0.300885   \n",
       "30                       0.1                        0.1    0.305882   \n",
       "8                        0.7                        0.1    0.120879   \n",
       "57                       0.4                        0.1    0.057471   \n",
       "15                       0.7                        0.1    0.323810   \n",
       "72                       0.4                        0.2    0.170000   \n",
       "37                       0.1                        0.2    0.102273   \n",
       "44                       0.2                        0.1    0.077778   \n",
       "69                       0.6                        0.1         NaN   \n",
       "69                       0.6                        0.1    0.123596   \n",
       "69                       0.6                        0.1    0.296610   \n",
       "46                       0.5                        0.1    0.279070   \n",
       "44                       0.2                        0.1    0.122222   \n",
       "44                       0.2                        0.1    0.081395   \n",
       "2                        0.9                        0.1    0.104651   \n",
       "78                       0.6                        0.1    0.118280   \n",
       "0                        0.5                        0.2    0.088889   \n",
       "52                       0.6                        0.1    0.087912   \n",
       "49                       0.2                        0.2         NaN   \n",
       "59                       0.9                        0.2    0.122222   \n",
       "18                       0.4                        0.1    0.240000   \n",
       "18                       0.4                        0.1    0.351064   \n",
       "18                       0.4                        0.1    0.090909   \n",
       "18                       0.4                        0.1    0.058140   \n",
       "34                       0.7                        0.1    0.160920   \n",
       "32                       0.3                        0.1    0.101124   \n",
       "77                       1.0                        0.2    0.281553   \n",
       "\n",
       "    2_test_eer  ...  16_test_accuracy  17_test_accuracy  18_test_accuracy  \\\n",
       "53    0.077778  ...          0.957831          0.957831          0.957831   \n",
       "30    0.178218  ...          0.885542          0.728916          0.692771   \n",
       "71         NaN  ...          0.861446          0.915663          0.867470   \n",
       "57    0.045977  ...          0.975904          0.891566          0.801205   \n",
       "30    0.224299  ...          0.819277          0.753012          0.578313   \n",
       "8     0.117021  ...          0.783133          0.933735          0.849398   \n",
       "57    0.056818  ...          0.957831          0.530120          0.939759   \n",
       "15    0.126316  ...          0.915663          0.915663          0.879518   \n",
       "72    0.170000  ...          0.879518          0.897590          0.897590   \n",
       "37    0.097826  ...          0.915663          0.945783          0.861446   \n",
       "44    0.078652  ...          0.957831          0.951807          0.957831   \n",
       "69    0.153061  ...          0.849398          0.759036          0.578313   \n",
       "69         NaN  ...          0.921687          0.933735          0.921687   \n",
       "69    0.000000  ...          0.987952          0.897590          0.722892   \n",
       "46    0.224299  ...          0.777108          0.746988          0.734940   \n",
       "44    0.118280  ...          0.807229          0.921687          0.783133   \n",
       "44    0.045977  ...          0.957831          0.542169          0.909639   \n",
       "2     0.101124  ...          0.945783          0.945783          0.909639   \n",
       "78    0.117021  ...          0.903614          0.933735          0.867470   \n",
       "0     0.087912  ...          0.891566          0.945783          0.813253   \n",
       "52    0.087912  ...          0.951807          0.951807          0.951807   \n",
       "49    0.231481  ...          0.849398          0.849398          0.722892   \n",
       "59         NaN  ...          0.921687          0.933735          0.921687   \n",
       "18    0.077778  ...          0.957831          0.885542          0.783133   \n",
       "18    0.209524  ...          0.777108          0.807229          0.542169   \n",
       "18    0.087912  ...          0.807229          0.951807          0.855422   \n",
       "18    0.023529  ...          0.969880          0.536145          0.740964   \n",
       "34    0.107527  ...          0.939759          0.933735          0.891566   \n",
       "32    0.097826  ...          0.867470          0.945783          0.819277   \n",
       "77    0.097826  ...          0.927711          0.945783          0.867470   \n",
       "\n",
       "    mean_test_accuracy  std_test_accuracy  rank_test_accuracy  owner  run  \\\n",
       "53            0.952811           0.008578                   1      0    0   \n",
       "30            0.779786           0.117057                   2      1    0   \n",
       "71            0.853748           0.063143                   1      2    0   \n",
       "57            0.893240           0.096632                   1      3    0   \n",
       "30            0.737282           0.131907                   1      4    0   \n",
       "8             0.888554           0.085105                   1      5    0   \n",
       "57            0.931392           0.101622                   1      6    0   \n",
       "15            0.858434           0.083647                   2      7    0   \n",
       "72            0.867470           0.055721                   1      8    0   \n",
       "37            0.885877           0.077185                   1      9    0   \n",
       "44            0.941098           0.032968                   3      0    1   \n",
       "69            0.753681           0.138174                   1      1    1   \n",
       "69            0.859772           0.102347                   1      2    1   \n",
       "69            0.875502           0.131721                   1      3    1   \n",
       "46            0.752677           0.105388                   2      4    1   \n",
       "44            0.879183           0.055520                   2      5    1   \n",
       "44            0.914324           0.112693                   1      6    1   \n",
       "2             0.902276           0.054903                   1      7    1   \n",
       "78            0.877510           0.080722                   1      8    1   \n",
       "0             0.884538           0.077440                   2      9    1   \n",
       "52            0.951473           0.001380                   1      0    2   \n",
       "49            0.805890           0.065085                   1      1    2   \n",
       "59            0.862115           0.105032                   1      2    2   \n",
       "18            0.884538           0.085507                   1      3    2   \n",
       "18            0.732597           0.146051                   2      4    2   \n",
       "18            0.900268           0.082310                   1      5    2   \n",
       "18            0.916667           0.122313                   1      6    2   \n",
       "34            0.903949           0.043878                   1      7    2   \n",
       "32            0.874498           0.090657                   1      8    2   \n",
       "77            0.878849           0.059699                   1      9    2   \n",
       "\n",
       "    0_test_eer  0_test_accuracy  \n",
       "53         NaN              NaN  \n",
       "30    0.178218         0.891566  \n",
       "71    0.178571         0.825301  \n",
       "57    0.045977         0.975904  \n",
       "30    0.224299         0.855422  \n",
       "8     0.117021         0.933735  \n",
       "57    0.056818         0.969880  \n",
       "15    0.174419         0.837349  \n",
       "72    0.170000         0.897590  \n",
       "37    0.100000         0.933735  \n",
       "44         NaN              NaN  \n",
       "69    0.153061         0.909639  \n",
       "69    0.191011         0.831325  \n",
       "69    0.000000         1.000000  \n",
       "46    0.224299         0.855422  \n",
       "44    0.117021         0.933735  \n",
       "44    0.045977         0.975904  \n",
       "2     0.097826         0.945783  \n",
       "78    0.117021         0.933735  \n",
       "0     0.090909         0.933735  \n",
       "52         NaN              NaN  \n",
       "49    0.231481         0.849398  \n",
       "59    0.242105         0.795181  \n",
       "18    0.077778         0.957831  \n",
       "18    0.209524         0.867470  \n",
       "18    0.087912         0.951807  \n",
       "18    0.023529         0.987952  \n",
       "34    0.179775         0.843373  \n",
       "32    0.097826         0.945783  \n",
       "77    0.100000         0.933735  \n",
       "\n",
       "[30 rows x 54 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results[(df_results.rank_test_eer == 1) & (df_results.mean_test_eer != 0.5)].groupby('owner').head(232)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# P.IF_nu, P.IF_gamma = 0.098, 0.002\n",
    "# NORMALIZER-SCALER\n",
    "# P.IF_nu, P.IF_gamma = 0.074, 0.029\n",
    "# ROBUST-SCALER-250\n",
    "# P.IF_nu, P.IF_gamma = 0.037, 0.001\n",
    "# ROBUST-SCALER-1000-1000\n",
    "# P.IF_nu, P.IF_gamma = 0.141, 0.494\n",
    "# ROBUST-SCALER-1000-250\n",
    "# P.IF_nu, P.IF_gamma = 0.147, 0.655\n",
    "# P.IF_nu, P.IF_gamma = 0.098, 0.003\n",
    "# P.IF_nu, P.IF_gamma = 0.104, 0.004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>VALID-ROBUST-IF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frequency</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_subjects</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_test_subjects</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_ids</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_sample_points_per_exp</th>\n",
       "      <td>21000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp_begin_cutoff_idx</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp_end_cutoff_idx</th>\n",
       "      <td>-500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seconds_per_subject_train</th>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seconds_per_subject_test</th>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>window_size</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IF_step_width</th>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler</th>\n",
       "      <td>RobustScaler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler_scope</th>\n",
       "      <td>subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler_global</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IF_kernel</th>\n",
       "      <td>rbf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IF_nu</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IF_gamma</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_cols</th>\n",
       "      <td>[EMA_x_a, EMA_y_a, EMA_z_a, EMA_x_g, EMA_y_g, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exclude_subjects</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       Value\n",
       "name                                                         VALID-ROBUST-IF\n",
       "frequency                                                                100\n",
       "max_subjects                                                              29\n",
       "max_test_subjects                                                         10\n",
       "user_ids                   [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 2...\n",
       "num_sample_points_per_exp                                              21000\n",
       "exp_begin_cutoff_idx                                                     500\n",
       "exp_end_cutoff_idx                                                      -500\n",
       "seconds_per_subject_train                                                210\n",
       "seconds_per_subject_test                                                 210\n",
       "window_size                                                              500\n",
       "IF_step_width                                                            250\n",
       "scaler                                                          RobustScaler\n",
       "scaler_scope                                                         subject\n",
       "scaler_global                                                          False\n",
       "IF_kernel                                                                rbf\n",
       "IF_nu                                                                   None\n",
       "IF_gamma                                                                None\n",
       "feature_cols               [EMA_x_a, EMA_y_a, EMA_z_a, EMA_x_g, EMA_y_g, ...\n",
       "exclude_subjects                                                          []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils_ppp(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# utils_plot_acc_eer_dist(df_plot, \"Test Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# utils_plot_acc_eer_dist(df_results, \"Test EER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold \n",
    "# temp_split = KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "# temp_split.get_n_splits(X_exp2_test_dic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for train_index, test_index in temp_split.split(X_exp2_test_dic[0]):\n",
    "#     print(train_index, test_index)\n",
    "#     print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_vals_owner_idx['X_train'][162: 243].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_vals_owner_idx['X_train'][243: 324].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_test_cv_splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (4180915578.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [36]\u001b[0;36m\u001b[0m\n\u001b[0;31m    run_seed = SEED + run\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# test_set\n",
    "\n",
    "        \n",
    "#         run_seed = SEED + run\n",
    "#         train_dic, valid_test_dic = { owner_idx: X_exp_test_dic[owner_idx][\"profile_windows\"]}, X_exp_test_dic[owner_idx][\"unknown_users_dict\"] \n",
    "#         X_vals_owner_idx = utils_create_cv_splits(owner_idx, train_dic, valid_test_dic, seed=run_seed)\n",
    "        \n",
    "#         X_test_regular = X_vals_owner_idx['X_test_regular']\n",
    "#         X_test_anomalous = X_vals_owner_idx['X_test_anomalous']\n",
    "\n",
    "#         train_test_cv_splits = X_vals_owner_idx['cv_splits']\n",
    "        \n",
    "        \n",
    "#         pca = PCA(n_components = run+3)\n",
    "# #         X_train = pca.fit_transform(X_train)\n",
    "# #         X_test_regular = pca.transform(X_test_regular)\n",
    "# #         X_test_anomalous = pca.transform(X_test_anomalous)\n",
    "        \n",
    "#         clf = IsolationForest(n_estimators = np.int64(P.median_n_estimators), \n",
    "#                           max_samples = P.median_max_samples, \n",
    "#                           contamination = P.median_contamination, \n",
    "#                           max_features = np.int64(P.median_max_features), \n",
    "#                           random_state=run_seed, n_jobs=CORES, verbose=1)\n",
    "#         pipeline = Pipeline([\n",
    "#                             # ('scaler', get_new_scaler_dict[P.scaler]()), \n",
    "# #                             ('scaler2', Normalizer()),#best result\n",
    "# #                              ('pca', pca), \n",
    "# #                              ('selector', VarianceThreshold()), \n",
    "#                             ('model', clf)\n",
    "#                             ])\n",
    "        \n",
    "#         scores = cross_validate(\n",
    "#             pipeline,\n",
    "#             X_vals_owner_idx['X_train'],\n",
    "#             X_vals_owner_idx['y_train'],\n",
    "#             cv=train_test_cv_splits,\n",
    "#             scoring={\n",
    "#                 \"eer\": utils_eer_scorer,\n",
    "#                 \"accuracy\": \"accuracy\",\n",
    "#                 \"precision\": \"precision\",\n",
    "#                 \"recall\": \"recall\",\n",
    "#             },\n",
    "#             n_jobs=CORES,\n",
    "#             verbose=1,\n",
    "#             return_train_score=True,error_score='raise'\n",
    "#         )\n",
    "# #         print(df_score[\"test_eer\"])\n",
    "#         df_score = pd.DataFrame(scores)\n",
    "#         df_score[\"owner\"] = test_set[owner_idx]\n",
    "#         df_score[\"train_eer\"] = df_score[\"train_eer\"].abs()  # Revert scorer's signflip\n",
    "#         df_score[\"test_eer\"] = df_score[\"test_eer\"].abs()\n",
    "#         test_df_results = pd.concat([test_df_results, df_score], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_owner_IF_train_test(owner_idx, X_exp_test_dic, SEED, run, optimal_params, CORES):\n",
    "    run_seed = SEED + run\n",
    "    train_dic, valid_test_dic = { owner_idx: X_exp_test_dic[owner_idx][\"profile_windows\"]}, X_exp_test_dic[owner_idx][\"unknown_users_dict\"] \n",
    "                                #IF_train_train_WACA_features_dic, IF_train_valid_WACA_features_dic\n",
    "    X_vals_owner_idx = utils_create_cv_splits(owner_idx, train_dic, valid_test_dic, seed=run_seed)\n",
    "\n",
    "    X_test_regular = X_vals_owner_idx['X_test_regular'].copy()\n",
    "    X_test_anomalous = X_vals_owner_idx['X_test_anomalous'].copy()\n",
    "\n",
    "#         print(X_vals_owner_idx['cv_splits'])\n",
    "    train_test_cv_splits = X_vals_owner_idx['cv_splits']\n",
    "    # print(X_vals_owner_idx['y_train'])\n",
    "#         pca = PCA(n_components = run+3)\n",
    "#         X_train = pca.fit_transform(X_train)\n",
    "#         X_test_regular = pca.transform(X_test_regular)\n",
    "#         X_test_anomalous = pca.transform(X_test_anomalous)\n",
    "\n",
    "#         pca_fs.add_user_pca(owner_idx, pca)\n",
    "\n",
    "    clf = IsolationForest(n_estimators = np.int64(optimal_params.median_n_estimators), \n",
    "                          max_samples = optimal_params.median_max_samples, \n",
    "                          contamination = optimal_params.median_contamination, \n",
    "                          max_features = np.int64(optimal_params.median_max_features), \n",
    "                          random_state=run_seed, n_jobs=CORES, verbose=1)\n",
    "    pipeline = Pipeline([\n",
    "                        # ('scaler', get_new_scaler_dict[P.scaler]()), \n",
    "#                             ('scaler2', Normalizer()),#best result\n",
    "#                              ('pca', pca), \n",
    "#                              ('selector', VarianceThreshold()), \n",
    "                        ('model', clf)\n",
    "                        ])\n",
    "\n",
    "    \n",
    "    scores = cross_validate(\n",
    "        pipeline,\n",
    "        X_vals_owner_idx['X_train'],\n",
    "        X_vals_owner_idx['y_train'],\n",
    "        cv=train_test_cv_splits,\n",
    "        scoring={\n",
    "            \"eer\": utils_eer_scorer,\n",
    "            \"accuracy\": \"accuracy\",\n",
    "            \"precision\": \"precision\",\n",
    "            \"recall\": \"recall\",\n",
    "        },\n",
    "        n_jobs=CORES,\n",
    "        verbose=1,\n",
    "        return_train_score=True,error_score='raise'\n",
    "    )\n",
    "\n",
    "    # print(df_score[\"test_eer\"])\n",
    "    df_score = pd.DataFrame(scores)\n",
    "    df_score[\"owner\"] = test_set[owner_idx]\n",
    "    df_score[\"train_eer\"] = df_score[\"train_eer\"].abs()  # Revert scorer's signflip\n",
    "    df_score[\"test_eer\"] = df_score[\"test_eer\"].abs()\n",
    "    # df_score[\"run\"] = run\n",
    "    \n",
    "    print(df_score)\n",
    "    return df_score\n",
    "        \n",
    "    # df_report[\"run\"] = run shalll i go this route\n",
    "    \n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_owner_IF_train_test(2, X_exp_test_dic, SEED, run, param_dist, CORES=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median_n_estimators: 77.0,  median_max_samples: 0.8,  median_contamination: 0.08,  median_max_features: 0.55\n"
     ]
    }
   ],
   "source": [
    "test_set\n",
    "print(f\"median_n_estimators: {P.median_n_estimators},  median_max_samples: {P.median_max_samples},  median_contamination: {P.median_contamination},  median_max_features: {P.median_max_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_dist\n",
    "# P.median_n_estimators = 46 \n",
    "# P.median_max_samples = 0.6\n",
    "# P.median_contamination = 0.1\n",
    "# P.median_max_features = 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011764705882352941"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_score\n",
    "# P.median_n_estimators = 77.0\n",
    "# P.median_max_samples = 0.6\n",
    "# P.median_contamination = 0.1\n",
    "# P.median_max_features = 60.0\n",
    "\n",
    "# P.median_n_estimators = 77\n",
    "# P.median_max_samples = 0.6\n",
    "# P.median_contamination = 0.011\n",
    "# P.median_max_features = 60\n",
    "\n",
    "\n",
    "# P.median_n_estimators = 129.0  \n",
    "# P.median_max_samples = 0.8\n",
    "# P.median_contamination = 0.1\n",
    "# P.median_max_features = int(0.4 * 84)\n",
    "\n",
    "# P.median_n_estimators = 129.0\n",
    "# P.median_max_samples = 0.8\n",
    "# P.median_contamination = 0.1\n",
    "# P.median_max_features = int(0.5*84)\n",
    "\n",
    "#P.median_n_estimators = 1000.0 \n",
    "#P.median_max_samples = 0.8\n",
    "#P.median_contamination = 0.001\n",
    "#P.median_max_features = int(84)\n",
    "\n",
    "P.median_n_estimators = 129.0\n",
    "P.median_max_samples = .8\n",
    "P.median_contamination = 0.05\n",
    "P.median_max_features = int(0.55*84)\n",
    "\n",
    "P.median_n_estimators = 77.0\n",
    "P.median_max_samples = 0.8\n",
    "P.median_contamination = 0.08\n",
    "P.median_max_features = int(0.55*84)\n",
    "\n",
    "P.median_n_estimators = 77.0\n",
    "P.median_max_samples = 1.0\n",
    "P.median_contamination = 0.08\n",
    "P.median_max_features = 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 12, 14, 9, 18, 23, 2, 15, 10, 4]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set\n",
    "# X_exp_test_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s][Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.5s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.5s finished\n",
      " 20%|        | 1/5 [00:01<00:07,  1.80s/it][Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.111592    0.030324  0.125000  1.818989e-12       0.897590   \n",
      "1  0.099437    0.029282  0.208791  1.818989e-12       0.819277   \n",
      "2  0.099241    0.028959  0.117021  1.818989e-12       0.933735   \n",
      "3  0.098775    0.028745  0.117021  1.818989e-12       0.933735   \n",
      "4  0.099137    0.029068  0.142857  1.818989e-12       0.861446   \n",
      "5  0.099794    0.028922  0.320755  1.818989e-12       0.728916   \n",
      "6  0.099184    0.028906  0.217391  1.818989e-12       0.813253   \n",
      "7  0.099260    0.029456  0.118280  1.818989e-12       0.927711   \n",
      "8  0.099351    0.029338  0.172414  1.818989e-12       0.843373   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.923077              1.0      0.86747      0.915663   \n",
      "1        0.915663        0.791209              1.0      0.86747      0.915663   \n",
      "2        0.915663        1.000000              1.0      0.86747      0.915663   \n",
      "3        0.915663        1.000000              1.0      0.86747      0.915663   \n",
      "4        0.915663        0.857143              1.0      0.86747      0.915663   \n",
      "5        0.915663        0.679245              1.0      0.86747      0.915663   \n",
      "6        0.915663        0.782609              1.0      0.86747      0.915663   \n",
      "7        0.915663        0.986301              1.0      0.86747      0.915663   \n",
      "8        0.915663        0.827586              1.0      0.86747      0.915663   \n",
      "\n",
      "   owner  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n",
      "4      0  \n",
      "5      0  \n",
      "6      0  \n",
      "7      0  \n",
      "8      0  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.098537    0.028287  0.490566  1.818989e-12       0.518072   \n",
      "1  0.097890    0.028246  0.477419  1.818989e-12       0.542169   \n",
      "2  0.097584    0.028489  0.023810  1.818989e-12       0.981928   \n",
      "3  0.098477    0.028290  0.425532  1.818989e-12       0.626506   \n",
      "4  0.097750    0.028375  0.506098  1.818989e-12       0.487952   \n",
      "5  0.097404    0.028363  0.467105  1.818989e-12       0.560241   \n",
      "6  0.097545    0.028238  0.500000  1.818989e-12       0.500000   \n",
      "7  0.098160    0.028683  0.250000  1.818989e-12       0.825301   \n",
      "8  0.097888    0.028537  0.490566  1.818989e-12       0.518072   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.509434              1.0     0.975904      0.915663   \n",
      "1        0.915663        0.522581              1.0     0.975904      0.915663   \n",
      "2        0.915663        0.987805              1.0     0.975904      0.915663   \n",
      "3        0.915663        0.574468              1.0     0.975904      0.915663   \n",
      "4        0.915663        0.493902              1.0     0.975904      0.915663   \n",
      "5        0.915663        0.532895              1.0     0.975904      0.915663   \n",
      "6        0.915663        0.500000              1.0     0.975904      0.915663   \n",
      "7        0.915663        0.750000              1.0     0.975904      0.915663   \n",
      "8        0.915663        0.509434              1.0     0.975904      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     12  \n",
      "1     12  \n",
      "2     12  \n",
      "3     12  \n",
      "4     12  \n",
      "5     12  \n",
      "6     12  \n",
      "7     12  \n",
      "8     12  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.098688    0.028910  0.172414  1.818989e-12       0.843373   \n",
      "1  0.098061    0.028568  0.370370  1.818989e-12       0.668675   \n",
      "2  0.098088    0.028411  0.153061  1.818989e-12       0.909639   \n",
      "3  0.098105    0.028811  0.176471  1.818989e-12       0.831325   \n",
      "4  0.098488    0.028764  0.510791  1.818989e-12       0.481928   \n",
      "5  0.099384    0.028318  0.153061  1.818989e-12       0.909639   \n",
      "6  0.098283    0.028616  0.291667  1.818989e-12       0.740964   \n",
      "7  0.098719    0.028864  0.156250  1.818989e-12       0.897590   \n",
      "8  0.098312    0.028795  0.227273  1.818989e-12       0.789157   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.860759              1.0     0.819277      0.915663   \n",
      "1        0.915663        0.629630              1.0     0.819277      0.915663   \n",
      "2        0.915663        1.000000              1.0     0.819277      0.915663   \n",
      "3        0.915663        0.839506              1.0     0.819277      0.915663   \n",
      "4        0.915663        0.489209              1.0     0.819277      0.915663   \n",
      "5        0.915663        1.000000              1.0     0.819277      0.915663   \n",
      "6        0.915663        0.708333              1.0     0.819277      0.915663   \n",
      "7        0.915663        0.971429              1.0     0.819277      0.915663   \n",
      "8        0.915663        0.772727              1.0     0.819277      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     14  \n",
      "1     14  \n",
      "2     14  \n",
      "3     14  \n",
      "4     14  \n",
      "5     14  \n",
      "6     14  \n",
      "7     14  \n",
      "8     14  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.097983    0.028672  0.496732  1.818989e-12       0.506024   \n",
      "1  0.097512    0.028304  0.416667  1.818989e-12       0.632530   \n",
      "2  0.097648    0.028636  0.515723  1.818989e-12       0.469880   \n",
      "3  0.097776    0.028295  0.483221  1.818989e-12       0.530120   \n",
      "4  0.097712    0.028201  0.433824  1.818989e-12       0.608434   \n",
      "5  0.099049    0.028455  0.336207  1.818989e-12       0.728916   \n",
      "6  0.098736    0.028514  0.287037  1.818989e-12       0.777108   \n",
      "7  0.097875    0.028354  0.416667  1.818989e-12       0.632530   \n",
      "8  0.097594    0.028225  0.446043  1.818989e-12       0.590361   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.503268              1.0     0.927711      0.915663   \n",
      "1        0.915663        0.583333              1.0     0.927711      0.915663   \n",
      "2        0.915663        0.484277              1.0     0.927711      0.915663   \n",
      "3        0.915663        0.516779              1.0     0.927711      0.915663   \n",
      "4        0.915663        0.566176              1.0     0.927711      0.915663   \n",
      "5        0.915663        0.663793              1.0     0.927711      0.915663   \n",
      "6        0.915663        0.712963              1.0     0.927711      0.915663   \n",
      "7        0.915663        0.583333              1.0     0.927711      0.915663   \n",
      "8        0.915663        0.553957              1.0     0.927711      0.915663   \n",
      "\n",
      "   owner  \n",
      "0      9  \n",
      "1      9  \n",
      "2      9  \n",
      "3      9  \n",
      "4      9  \n",
      "5      9  \n",
      "6      9  \n",
      "7      9  \n",
      "8      9  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.102854    0.028503  0.170000  1.818989e-12       0.897590   \n",
      "1  0.097970    0.028874  0.173469  1.818989e-12       0.885542   \n",
      "2  0.098306    0.028402  0.170000  1.818989e-12       0.897590   \n",
      "3  0.098920    0.028709  0.170000  1.818989e-12       0.897590   \n",
      "4  0.098610    0.028748  0.175258  1.818989e-12       0.879518   \n",
      "5  0.098154    0.028455  0.170000  1.818989e-12       0.897590   \n",
      "6  0.098846    0.028117  0.170000  1.818989e-12       0.897590   \n",
      "7  0.098822    0.028206  0.170000  1.818989e-12       0.897590   \n",
      "8  0.098304    0.028802  0.178947  1.818989e-12       0.867470   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        1.000000              1.0     0.795181      0.915663   \n",
      "1        0.915663        0.970588              1.0     0.795181      0.915663   \n",
      "2        0.915663        1.000000              1.0     0.795181      0.915663   \n",
      "3        0.915663        1.000000              1.0     0.795181      0.915663   \n",
      "4        0.915663        0.956522              1.0     0.795181      0.915663   \n",
      "5        0.915663        1.000000              1.0     0.795181      0.915663   \n",
      "6        0.915663        1.000000              1.0     0.795181      0.915663   \n",
      "7        0.915663        1.000000              1.0     0.795181      0.915663   \n",
      "8        0.915663        0.929577              1.0     0.795181      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     18  \n",
      "1     18  \n",
      "2     18  \n",
      "3     18  \n",
      "4     18  \n",
      "5     18  \n",
      "6     18  \n",
      "7     18  \n",
      "8     18  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.099002    0.028448  0.278261  1.818989e-12       0.807229   \n",
      "1  0.098851    0.028778  0.280702  1.818989e-12       0.801205   \n",
      "2  0.098411    0.028508  0.445652  1.818989e-12       0.560241   \n",
      "3  0.098729    0.028242  0.278261  1.818989e-12       0.807229   \n",
      "4  0.098231    0.031037  0.278261  1.818989e-12       0.807229   \n",
      "5  0.097571    0.028049  0.278261  1.818989e-12       0.807229   \n",
      "6  0.097576    0.028902  0.329897  1.818989e-12       0.698795   \n",
      "7  0.097919    0.028360  0.278261  1.818989e-12       0.807229   \n",
      "8  0.097872    0.028819  0.283186  1.818989e-12       0.795181   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        1.000000              1.0     0.614458      0.915663   \n",
      "1        0.915663        0.980769              1.0     0.614458      0.915663   \n",
      "2        0.915663        0.554348              1.0     0.614458      0.915663   \n",
      "3        0.915663        1.000000              1.0     0.614458      0.915663   \n",
      "4        0.915663        1.000000              1.0     0.614458      0.915663   \n",
      "5        0.915663        1.000000              1.0     0.614458      0.915663   \n",
      "6        0.915663        0.739130              1.0     0.614458      0.915663   \n",
      "7        0.915663        1.000000              1.0     0.614458      0.915663   \n",
      "8        0.915663        0.962264              1.0     0.614458      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     23  \n",
      "1     23  \n",
      "2     23  \n",
      "3     23  \n",
      "4     23  \n",
      "5     23  \n",
      "6     23  \n",
      "7     23  \n",
      "8     23  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.181442    0.053366  0.175824  1.818989e-12       0.855422   \n",
      "1  0.098304    0.028482  0.166667  1.818989e-12       0.885542   \n",
      "2  0.097826    0.028746  0.179775  1.818989e-12       0.843373   \n",
      "3  0.096855    0.027996  0.161616  1.818989e-12       0.903614   \n",
      "4  0.097018    0.028144  0.163265  1.818989e-12       0.897590   \n",
      "5  0.096918    0.028465  0.166667  1.818989e-12       0.885542   \n",
      "6  0.097426    0.028674  0.279570  1.818989e-12       0.746988   \n",
      "7  0.097668    0.028588  0.168421  1.818989e-12       0.879518   \n",
      "8  0.097418    0.028655  0.177778  1.818989e-12       0.849398   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.893333              1.0     0.807229      0.915663   \n",
      "1        0.915663        0.957143              1.0     0.807229      0.915663   \n",
      "2        0.915663        0.870130              1.0     0.807229      0.915663   \n",
      "3        0.915663        1.000000              1.0     0.807229      0.915663   \n",
      "4        0.915663        0.985294              1.0     0.807229      0.915663   \n",
      "5        0.915663        0.957143              1.0     0.807229      0.915663   \n",
      "6        0.915663        0.720430              1.0     0.807229      0.915663   \n",
      "7        0.915663        0.943662              1.0     0.807229      0.915663   \n",
      "8        0.915663        0.881579              1.0     0.807229      0.915663   \n",
      "\n",
      "   owner  \n",
      "0      2  \n",
      "1      2  \n",
      "2      2  \n",
      "3      2  \n",
      "4      2  \n",
      "5      2  \n",
      "6      2  \n",
      "7      2  \n",
      "8      2  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.189050    0.052556  0.347368  1.818989e-12       0.674699   \n",
      "1  0.097403    0.028641  0.326733  1.818989e-12       0.710843   \n",
      "2  0.097473    0.028957  0.358696  1.818989e-12       0.656627   \n",
      "3  0.100667    0.027878  0.284483  1.818989e-12       0.801205   \n",
      "4  0.097165    0.028033  0.284483  1.818989e-12       0.801205   \n",
      "5  0.097463    0.028544  0.317308  1.818989e-12       0.728916   \n",
      "6  0.097728    0.028546  0.294643  1.818989e-12       0.777108   \n",
      "7  0.097572    0.028807  0.297297  1.818989e-12       0.771084   \n",
      "8  0.097509    0.028558  0.425287  1.818989e-12       0.578313   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.704225              1.0      0.60241      0.915663   \n",
      "1        0.915663        0.769231              1.0      0.60241      0.915663   \n",
      "2        0.915663        0.675676              1.0      0.60241      0.915663   \n",
      "3        0.915663        1.000000              1.0      0.60241      0.915663   \n",
      "4        0.915663        1.000000              1.0      0.60241      0.915663   \n",
      "5        0.915663        0.806452              1.0      0.60241      0.915663   \n",
      "6        0.915663        0.925926              1.0      0.60241      0.915663   \n",
      "7        0.915663        0.909091              1.0      0.60241      0.915663   \n",
      "8        0.915663        0.574713              1.0      0.60241      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     15  \n",
      "1     15  \n",
      "2     15  \n",
      "3     15  \n",
      "4     15  \n",
      "5     15  \n",
      "6     15  \n",
      "7     15  \n",
      "8     15  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.097913    0.028984  0.293103  1.818989e-12       0.789157   \n",
      "1  0.098017    0.028528  0.300885  1.818989e-12       0.771084   \n",
      "2  0.098084    0.028671  0.449438  1.818989e-12       0.554217   \n",
      "3  0.097966    0.028814  0.303571  1.818989e-12       0.765060   \n",
      "4  0.099524    0.028527  0.293103  1.818989e-12       0.789157   \n",
      "5  0.097873    0.028565  0.350515  1.818989e-12       0.674699   \n",
      "6  0.097434    0.028676  0.309091  1.818989e-12       0.753012   \n",
      "7  0.098156    0.028669  0.336634  1.818989e-12       0.698795   \n",
      "8  0.097875    0.028703  0.343434  1.818989e-12       0.686747   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.980000              1.0     0.590361      0.915663   \n",
      "1        0.915663        0.924528              1.0     0.590361      0.915663   \n",
      "2        0.915663        0.550562              1.0     0.590361      0.915663   \n",
      "3        0.915663        0.907407              1.0     0.590361      0.915663   \n",
      "4        0.915663        0.980000              1.0     0.590361      0.915663   \n",
      "5        0.915663        0.710145              1.0     0.590361      0.915663   \n",
      "6        0.915663        0.875000              1.0     0.590361      0.915663   \n",
      "7        0.915663        0.753846              1.0     0.590361      0.915663   \n",
      "8        0.915663        0.731343              1.0     0.590361      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     10  \n",
      "1     10  \n",
      "2     10  \n",
      "3     10  \n",
      "4     10  \n",
      "5     10  \n",
      "6     10  \n",
      "7     10  \n",
      "8     10  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.098908    0.028935  0.313953  1.818989e-12       0.692771   \n",
      "1  0.099866    0.028847  0.337079  1.818989e-12       0.674699   \n",
      "2  0.099328    0.029479  0.258065  1.818989e-12       0.771084   \n",
      "3  0.099751    0.029033  0.224299  1.818989e-12       0.855422   \n",
      "4  0.100210    0.028981  0.237624  1.818989e-12       0.819277   \n",
      "5  0.110931    0.029099  0.252632  1.818989e-12       0.783133   \n",
      "6  0.098374    0.028783  0.247423  1.818989e-12       0.795181   \n",
      "7  0.099029    0.028607  0.504202  1.818989e-12       0.493976   \n",
      "8  0.098284    0.028776  0.230769  1.818989e-12       0.837349   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.686047              1.0     0.710843      0.915663   \n",
      "1        0.915663        0.662921              1.0     0.710843      0.915663   \n",
      "2        0.915663        0.808219              1.0     0.710843      0.915663   \n",
      "3        0.915663        1.000000              1.0     0.710843      0.915663   \n",
      "4        0.915663        0.907692              1.0     0.710843      0.915663   \n",
      "5        0.915663        0.830986              1.0     0.710843      0.915663   \n",
      "6        0.915663        0.855072              1.0     0.710843      0.915663   \n",
      "7        0.915663        0.495798              1.0     0.710843      0.915663   \n",
      "8        0.915663        0.951613              1.0     0.710843      0.915663   \n",
      "\n",
      "   owner  \n",
      "0      4  \n",
      "1      4  \n",
      "2      4  \n",
      "3      4  \n",
      "4      4  \n",
      "5      4  \n",
      "6      4  \n",
      "7      4  \n",
      "8      4  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      " 40%|      | 2/5 [00:03<00:05,  1.77s/it][Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.106782    0.042599  0.130952  1.818989e-12       0.873494   \n",
      "1  0.100760    0.029624  0.132530  1.818989e-12       0.867470   \n",
      "2  0.097675    0.029113  0.118280  1.818989e-12       0.927711   \n",
      "3  0.097564    0.029187  0.120879  1.818989e-12       0.915663   \n",
      "4  0.097918    0.029045  0.132530  1.818989e-12       0.867470   \n",
      "5  0.097772    0.028868  0.272727  1.818989e-12       0.771084   \n",
      "6  0.097852    0.028899  0.120879  1.818989e-12       0.915663   \n",
      "7  0.098802    0.028753  0.117021  1.818989e-12       0.933735   \n",
      "8  0.098213    0.029085  0.132530  1.818989e-12       0.867470   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.878049              1.0      0.86747      0.915663   \n",
      "1        0.915663        0.867470              1.0      0.86747      0.915663   \n",
      "2        0.915663        0.986301              1.0      0.86747      0.915663   \n",
      "3        0.915663        0.960000              1.0      0.86747      0.915663   \n",
      "4        0.915663        0.867470              1.0      0.86747      0.915663   \n",
      "5        0.915663        0.727273              1.0      0.86747      0.915663   \n",
      "6        0.915663        0.960000              1.0      0.86747      0.915663   \n",
      "7        0.915663        1.000000              1.0      0.86747      0.915663   \n",
      "8        0.915663        0.867470              1.0      0.86747      0.915663   \n",
      "\n",
      "   owner  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n",
      "4      0  \n",
      "5      0  \n",
      "6      0  \n",
      "7      0  \n",
      "8      0  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.099041    0.028789  0.487179  1.818989e-12       0.524096   \n",
      "1  0.098203    0.028921  0.483871  1.818989e-12       0.530120   \n",
      "2  0.098465    0.029036  0.036145  1.818989e-12       0.963855   \n",
      "3  0.098418    0.028932  0.354839  1.818989e-12       0.716867   \n",
      "4  0.098590    0.028851  0.509202  1.818989e-12       0.481928   \n",
      "5  0.098920    0.028825  0.436620  1.818989e-12       0.608434   \n",
      "6  0.098830    0.028693  0.496855  1.818989e-12       0.506024   \n",
      "7  0.099352    0.028961  0.349593  1.818989e-12       0.722892   \n",
      "8  0.098603    0.028882  0.487179  1.818989e-12       0.524096   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.512821              1.0     0.963855      0.915663   \n",
      "1        0.915663        0.516129              1.0     0.963855      0.915663   \n",
      "2        0.915663        0.963855              1.0     0.963855      0.915663   \n",
      "3        0.915663        0.645161              1.0     0.963855      0.915663   \n",
      "4        0.915663        0.490798              1.0     0.963855      0.915663   \n",
      "5        0.915663        0.563380              1.0     0.963855      0.915663   \n",
      "6        0.915663        0.503145              1.0     0.963855      0.915663   \n",
      "7        0.915663        0.650407              1.0     0.963855      0.915663   \n",
      "8        0.915663        0.512821              1.0     0.963855      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     12  \n",
      "1     12  \n",
      "2     12  \n",
      "3     12  \n",
      "4     12  \n",
      "5     12  \n",
      "6     12  \n",
      "7     12  \n",
      "8     12  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.098603    0.029253  0.153846  1.818989e-12       0.879518   \n",
      "1  0.098186    0.028749  0.150538  1.818989e-12       0.891566   \n",
      "2  0.098640    0.028681  0.144330  1.818989e-12       0.915663   \n",
      "3  0.098243    0.028618  0.144330  1.818989e-12       0.915663   \n",
      "4  0.098857    0.028827  0.473282  1.818989e-12       0.542169   \n",
      "5  0.097943    0.028860  0.147368  1.818989e-12       0.903614   \n",
      "6  0.098736    0.029112  0.162791  1.818989e-12       0.849398   \n",
      "7  0.100791    0.028825  0.145833  1.818989e-12       0.909639   \n",
      "8  0.097871    0.029076  0.147368  1.818989e-12       0.903614   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.920000              1.0     0.831325      0.915663   \n",
      "1        0.915663        0.945205              1.0     0.831325      0.915663   \n",
      "2        0.915663        1.000000              1.0     0.831325      0.915663   \n",
      "3        0.915663        1.000000              1.0     0.831325      0.915663   \n",
      "4        0.915663        0.526718              1.0     0.831325      0.915663   \n",
      "5        0.915663        0.971831              1.0     0.831325      0.915663   \n",
      "6        0.915663        0.862500              1.0     0.831325      0.915663   \n",
      "7        0.915663        0.985714              1.0     0.831325      0.915663   \n",
      "8        0.915663        0.971831              1.0     0.831325      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     14  \n",
      "1     14  \n",
      "2     14  \n",
      "3     14  \n",
      "4     14  \n",
      "5     14  \n",
      "6     14  \n",
      "7     14  \n",
      "8     14  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.097750    0.028570  0.518750  1.818989e-12       0.463855   \n",
      "1  0.097518    0.028333  0.483221  1.818989e-12       0.530120   \n",
      "2  0.097719    0.028358  0.518750  1.818989e-12       0.463855   \n",
      "3  0.097481    0.028471  0.486667  1.818989e-12       0.524096   \n",
      "4  0.097350    0.028397  0.493421  1.818989e-12       0.512048   \n",
      "5  0.097447    0.028529  0.421053  1.818989e-12       0.626506   \n",
      "6  0.097624    0.028151  0.363636  1.818989e-12       0.698795   \n",
      "7  0.098222    0.028471  0.425373  1.818989e-12       0.620482   \n",
      "8  0.097542    0.028547  0.468966  1.818989e-12       0.554217   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.481250              1.0     0.927711      0.915663   \n",
      "1        0.915663        0.516779              1.0     0.927711      0.915663   \n",
      "2        0.915663        0.481250              1.0     0.927711      0.915663   \n",
      "3        0.915663        0.513333              1.0     0.927711      0.915663   \n",
      "4        0.915663        0.506579              1.0     0.927711      0.915663   \n",
      "5        0.915663        0.578947              1.0     0.927711      0.915663   \n",
      "6        0.915663        0.636364              1.0     0.927711      0.915663   \n",
      "7        0.915663        0.574627              1.0     0.927711      0.915663   \n",
      "8        0.915663        0.531034              1.0     0.927711      0.915663   \n",
      "\n",
      "   owner  \n",
      "0      9  \n",
      "1      9  \n",
      "2      9  \n",
      "3      9  \n",
      "4      9  \n",
      "5      9  \n",
      "6      9  \n",
      "7      9  \n",
      "8      9  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.102069    0.029408  0.138298  1.818989e-12       0.909639   \n",
      "1  0.099532    0.028990  0.144444  1.818989e-12       0.885542   \n",
      "2  0.099137    0.028863  0.141304  1.818989e-12       0.897590   \n",
      "3  0.099129    0.028720  0.135417  1.818989e-12       0.921687   \n",
      "4  0.099384    0.028646  0.139785  1.818989e-12       0.903614   \n",
      "5  0.098439    0.028391  0.136842  1.818989e-12       0.915663   \n",
      "6  0.101240    0.028255  0.135417  1.818989e-12       0.921687   \n",
      "7  0.099135    0.028269  0.135417  1.818989e-12       0.921687   \n",
      "8  0.098405    0.028782  0.141304  1.818989e-12       0.897590   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.972222              1.0     0.843373      0.915663   \n",
      "1        0.915663        0.921053              1.0     0.843373      0.915663   \n",
      "2        0.915663        0.945946              1.0     0.843373      0.915663   \n",
      "3        0.915663        1.000000              1.0     0.843373      0.915663   \n",
      "4        0.915663        0.958904              1.0     0.843373      0.915663   \n",
      "5        0.915663        0.985915              1.0     0.843373      0.915663   \n",
      "6        0.915663        1.000000              1.0     0.843373      0.915663   \n",
      "7        0.915663        1.000000              1.0     0.843373      0.915663   \n",
      "8        0.915663        0.945946              1.0     0.843373      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     18  \n",
      "1     18  \n",
      "2     18  \n",
      "3     18  \n",
      "4     18  \n",
      "5     18  \n",
      "6     18  \n",
      "7     18  \n",
      "8     18  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.098876    0.028247  0.224299  1.818989e-12       0.855422   \n",
      "1  0.097785    0.028625  0.226415  1.818989e-12       0.849398   \n",
      "2  0.097472    0.028446  0.473214  1.818989e-12       0.536145   \n",
      "3  0.097916    0.028247  0.224299  1.818989e-12       0.855422   \n",
      "4  0.097557    0.028208  0.224299  1.818989e-12       0.855422   \n",
      "5  0.097549    0.028135  0.224299  1.818989e-12       0.855422   \n",
      "6  0.097107    0.028585  0.263736  1.818989e-12       0.759036   \n",
      "7  0.098301    0.028298  0.224299  1.818989e-12       0.855422   \n",
      "8  0.098898    0.028745  0.233010  1.818989e-12       0.831325   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        1.000000              1.0     0.710843      0.915663   \n",
      "1        0.915663        0.983333              1.0     0.710843      0.915663   \n",
      "2        0.915663        0.526786              1.0     0.710843      0.915663   \n",
      "3        0.915663        1.000000              1.0     0.710843      0.915663   \n",
      "4        0.915663        1.000000              1.0     0.710843      0.915663   \n",
      "5        0.915663        1.000000              1.0     0.710843      0.915663   \n",
      "6        0.915663        0.786667              1.0     0.710843      0.915663   \n",
      "7        0.915663        1.000000              1.0     0.710843      0.915663   \n",
      "8        0.915663        0.936508              1.0     0.710843      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     23  \n",
      "1     23  \n",
      "2     23  \n",
      "3     23  \n",
      "4     23  \n",
      "5     23  \n",
      "6     23  \n",
      "7     23  \n",
      "8     23  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.097932    0.028821  0.117647  1.818989e-12       0.891566   \n",
      "1  0.097792    0.028947  0.094118  1.818989e-12       0.915663   \n",
      "2  0.098569    0.029003  0.147727  1.818989e-12       0.873494   \n",
      "3  0.097349    0.028286  0.087912  1.818989e-12       0.951807   \n",
      "4  0.097701    0.028702  0.089888  1.818989e-12       0.939759   \n",
      "5  0.097329    0.028834  0.127907  1.818989e-12       0.885542   \n",
      "6  0.098593    0.028777  0.299065  1.818989e-12       0.759036   \n",
      "7  0.097730    0.029123  0.096386  1.818989e-12       0.903614   \n",
      "8  0.097537    0.029892  0.193548  1.818989e-12       0.843373   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.882353              1.0     0.903614      0.915663   \n",
      "1        0.915663        0.925926              1.0     0.903614      0.915663   \n",
      "2        0.915663        0.852273              1.0     0.903614      0.915663   \n",
      "3        0.915663        1.000000              1.0     0.903614      0.915663   \n",
      "4        0.915663        0.974026              1.0     0.903614      0.915663   \n",
      "5        0.915663        0.872093              1.0     0.903614      0.915663   \n",
      "6        0.915663        0.700935              1.0     0.903614      0.915663   \n",
      "7        0.915663        0.903614              1.0     0.903614      0.915663   \n",
      "8        0.915663        0.806452              1.0     0.903614      0.915663   \n",
      "\n",
      "   owner  \n",
      "0      2  \n",
      "1      2  \n",
      "2      2  \n",
      "3      2  \n",
      "4      2  \n",
      "5      2  \n",
      "6      2  \n",
      "7      2  \n",
      "8      2  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.104383    0.028471  0.400000  1.818989e-12       0.638554   \n",
      "1  0.097074    0.028170  0.410256  1.818989e-12       0.626506   \n",
      "2  0.096689    0.028040  0.420168  1.818989e-12       0.614458   \n",
      "3  0.097132    0.027927  0.144330  1.818989e-12       0.915663   \n",
      "4  0.097063    0.028097  0.144330  1.818989e-12       0.915663   \n",
      "5  0.097627    0.028663  0.378378  1.818989e-12       0.662651   \n",
      "6  0.097980    0.031782  0.258065  1.818989e-12       0.771084   \n",
      "7  0.098883    0.028480  0.155556  1.818989e-12       0.873494   \n",
      "8  0.098286    0.028800  0.425000  1.818989e-12       0.608434   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.600000              1.0     0.831325      0.915663   \n",
      "1        0.915663        0.589744              1.0     0.831325      0.915663   \n",
      "2        0.915663        0.579832              1.0     0.831325      0.915663   \n",
      "3        0.915663        1.000000              1.0     0.831325      0.915663   \n",
      "4        0.915663        1.000000              1.0     0.831325      0.915663   \n",
      "5        0.915663        0.621622              1.0     0.831325      0.915663   \n",
      "6        0.915663        0.741935              1.0     0.831325      0.915663   \n",
      "7        0.915663        0.907895              1.0     0.831325      0.915663   \n",
      "8        0.915663        0.575000              1.0     0.831325      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     15  \n",
      "1     15  \n",
      "2     15  \n",
      "3     15  \n",
      "4     15  \n",
      "5     15  \n",
      "6     15  \n",
      "7     15  \n",
      "8     15  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.101378    0.029631  0.303922  1.818989e-12       0.740964   \n",
      "1  0.098212    0.029196  0.279279  1.818989e-12       0.795181   \n",
      "2  0.097630    0.029004  0.543860  1.818989e-12       0.439759   \n",
      "3  0.097582    0.029283  0.279279  1.818989e-12       0.795181   \n",
      "4  0.097368    0.028889  0.281818  1.818989e-12       0.789157   \n",
      "5  0.097643    0.028874  0.364706  1.818989e-12       0.638554   \n",
      "6  0.097669    0.028726  0.474747  1.818989e-12       0.530120   \n",
      "7  0.099819    0.029758  0.319588  1.818989e-12       0.710843   \n",
      "8  0.098222    0.029013  0.329787  1.818989e-12       0.692771   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.812500              1.0     0.626506      0.915663   \n",
      "1        0.915663        0.945455              1.0     0.626506      0.915663   \n",
      "2        0.915663        0.456140              1.0     0.626506      0.915663   \n",
      "3        0.915663        0.945455              1.0     0.626506      0.915663   \n",
      "4        0.915663        0.928571              1.0     0.626506      0.915663   \n",
      "5        0.915663        0.641975              1.0     0.626506      0.915663   \n",
      "6        0.915663        0.525253              1.0     0.626506      0.915663   \n",
      "7        0.915663        0.753623              1.0     0.626506      0.915663   \n",
      "8        0.915663        0.722222              1.0     0.626506      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     10  \n",
      "1     10  \n",
      "2     10  \n",
      "3     10  \n",
      "4     10  \n",
      "5     10  \n",
      "6     10  \n",
      "7     10  \n",
      "8     10  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.103201    0.028731  0.452174  1.818989e-12       0.566265   \n",
      "1  0.097803    0.028927  0.370000  1.818989e-12       0.656627   \n",
      "2  0.098373    0.028689  0.422018  1.818989e-12       0.602410   \n",
      "3  0.097969    0.028527  0.194175  1.818989e-12       0.879518   \n",
      "4  0.098122    0.028350  0.202020  1.818989e-12       0.855422   \n",
      "5  0.097766    0.028643  0.411215  1.818989e-12       0.614458   \n",
      "6  0.098600    0.028835  0.427273  1.818989e-12       0.596386   \n",
      "7  0.099165    0.028893  0.522727  1.818989e-12       0.463855   \n",
      "8  0.098846    0.029785  0.206186  1.818989e-12       0.843373   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.547826              1.0     0.759036      0.915663   \n",
      "1        0.915663        0.630000              1.0     0.759036      0.915663   \n",
      "2        0.915663        0.577982              1.0     0.759036      0.915663   \n",
      "3        0.915663        1.000000              1.0     0.759036      0.915663   \n",
      "4        0.915663        0.940299              1.0     0.759036      0.915663   \n",
      "5        0.915663        0.588785              1.0     0.759036      0.915663   \n",
      "6        0.915663        0.572727              1.0     0.759036      0.915663   \n",
      "7        0.915663        0.477273              1.0     0.759036      0.915663   \n",
      "8        0.915663        0.913043              1.0     0.759036      0.915663   \n",
      "\n",
      "   owner  \n",
      "0      4  \n",
      "1      4  \n",
      "2      4  \n",
      "3      4  \n",
      "4      4  \n",
      "5      4  \n",
      "6      4  \n",
      "7      4  \n",
      "8      4  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.5s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.5s finished\n",
      " 60%|    | 3/5 [00:05<00:03,  1.75s/it][Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.099307    0.032031  0.144330  1.818989e-12       0.915663   \n",
      "1  0.098059    0.029064  0.145833  1.818989e-12       0.909639   \n",
      "2  0.098070    0.028733  0.144330  1.818989e-12       0.915663   \n",
      "3  0.098365    0.028794  0.144330  1.818989e-12       0.915663   \n",
      "4  0.098370    0.028769  0.145833  1.818989e-12       0.909639   \n",
      "5  0.098021    0.028929  0.241758  1.818989e-12       0.783133   \n",
      "6  0.098102    0.028848  0.145833  1.818989e-12       0.909639   \n",
      "7  0.103292    0.028933  0.144330  1.818989e-12       0.915663   \n",
      "8  0.099332    0.029159  0.147368  1.818989e-12       0.903614   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        1.000000              1.0     0.831325      0.915663   \n",
      "1        0.915663        0.985714              1.0     0.831325      0.915663   \n",
      "2        0.915663        1.000000              1.0     0.831325      0.915663   \n",
      "3        0.915663        1.000000              1.0     0.831325      0.915663   \n",
      "4        0.915663        0.985714              1.0     0.831325      0.915663   \n",
      "5        0.915663        0.758242              1.0     0.831325      0.915663   \n",
      "6        0.915663        0.985714              1.0     0.831325      0.915663   \n",
      "7        0.915663        1.000000              1.0     0.831325      0.915663   \n",
      "8        0.915663        0.971831              1.0     0.831325      0.915663   \n",
      "\n",
      "   owner  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n",
      "4      0  \n",
      "5      0  \n",
      "6      0  \n",
      "7      0  \n",
      "8      0  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.099816    0.028790  0.500000  1.818989e-12       0.500000   \n",
      "1  0.097696    0.028476  0.456376  1.818989e-12       0.578313   \n",
      "2  0.097558    0.028884  0.023810  1.818989e-12       0.981928   \n",
      "3  0.097646    0.028601  0.235849  1.818989e-12       0.837349   \n",
      "4  0.097706    0.028359  0.506098  1.818989e-12       0.487952   \n",
      "5  0.097881    0.028409  0.448980  1.818989e-12       0.590361   \n",
      "6  0.096985    0.028356  0.496894  1.818989e-12       0.506024   \n",
      "7  0.097841    0.028337  0.341463  1.818989e-12       0.734940   \n",
      "8  0.101453    0.028505  0.477419  1.818989e-12       0.542169   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.500000              1.0     0.975904      0.915663   \n",
      "1        0.915663        0.543624              1.0     0.975904      0.915663   \n",
      "2        0.915663        0.987805              1.0     0.975904      0.915663   \n",
      "3        0.915663        0.764151              1.0     0.975904      0.915663   \n",
      "4        0.915663        0.493902              1.0     0.975904      0.915663   \n",
      "5        0.915663        0.551020              1.0     0.975904      0.915663   \n",
      "6        0.915663        0.503106              1.0     0.975904      0.915663   \n",
      "7        0.915663        0.658537              1.0     0.975904      0.915663   \n",
      "8        0.915663        0.522581              1.0     0.975904      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     12  \n",
      "1     12  \n",
      "2     12  \n",
      "3     12  \n",
      "4     12  \n",
      "5     12  \n",
      "6     12  \n",
      "7     12  \n",
      "8     12  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.098452    0.028852  0.247706  1.818989e-12       0.831325   \n",
      "1  0.097184    0.028048  0.245455  1.818989e-12       0.837349   \n",
      "2  0.097309    0.028136  0.245455  1.818989e-12       0.837349   \n",
      "3  0.097313    0.028120  0.245455  1.818989e-12       0.837349   \n",
      "4  0.097599    0.028735  0.317647  1.818989e-12       0.686747   \n",
      "5  0.097296    0.028601  0.247706  1.818989e-12       0.831325   \n",
      "6  0.097494    0.028721  0.247706  1.818989e-12       0.831325   \n",
      "7  0.098431    0.028129  0.245455  1.818989e-12       0.837349   \n",
      "8  0.097948    0.028596  0.245455  1.818989e-12       0.837349   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.982456              1.0     0.674699      0.915663   \n",
      "1        0.915663        1.000000              1.0     0.674699      0.915663   \n",
      "2        0.915663        1.000000              1.0     0.674699      0.915663   \n",
      "3        0.915663        1.000000              1.0     0.674699      0.915663   \n",
      "4        0.915663        0.691358              1.0     0.674699      0.915663   \n",
      "5        0.915663        0.982456              1.0     0.674699      0.915663   \n",
      "6        0.915663        0.982456              1.0     0.674699      0.915663   \n",
      "7        0.915663        1.000000              1.0     0.674699      0.915663   \n",
      "8        0.915663        1.000000              1.0     0.674699      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     14  \n",
      "1     14  \n",
      "2     14  \n",
      "3     14  \n",
      "4     14  \n",
      "5     14  \n",
      "6     14  \n",
      "7     14  \n",
      "8     14  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.144427    0.049659  0.474820  1.818989e-12       0.542169   \n",
      "1  0.117620    0.028727  0.179775  1.818989e-12       0.843373   \n",
      "2  0.098432    0.028268  0.522876  1.818989e-12       0.457831   \n",
      "3  0.098434    0.028368  0.359649  1.818989e-12       0.692771   \n",
      "4  0.097910    0.028475  0.130952  1.818989e-12       0.873494   \n",
      "5  0.097995    0.028447  0.381356  1.818989e-12       0.668675   \n",
      "6  0.098382    0.028755  0.111111  1.818989e-12       0.921687   \n",
      "7  0.100698    0.029128  0.434109  1.818989e-12       0.602410   \n",
      "8  0.103788    0.028764  0.215054  1.818989e-12       0.819277   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.525180              1.0     0.879518      0.915663   \n",
      "1        0.915663        0.820225              1.0     0.879518      0.915663   \n",
      "2        0.915663        0.477124              1.0     0.879518      0.915663   \n",
      "3        0.915663        0.640351              1.0     0.879518      0.915663   \n",
      "4        0.915663        0.869048              1.0     0.879518      0.915663   \n",
      "5        0.915663        0.618644              1.0     0.879518      0.915663   \n",
      "6        0.915663        0.960526              1.0     0.879518      0.915663   \n",
      "7        0.915663        0.565891              1.0     0.879518      0.915663   \n",
      "8        0.915663        0.784946              1.0     0.879518      0.915663   \n",
      "\n",
      "   owner  \n",
      "0      9  \n",
      "1      9  \n",
      "2      9  \n",
      "3      9  \n",
      "4      9  \n",
      "5      9  \n",
      "6      9  \n",
      "7      9  \n",
      "8      9  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.098442    0.028571  0.187500  1.818989e-12       0.861446   \n",
      "1  0.097816    0.028467  0.185567  1.818989e-12       0.867470   \n",
      "2  0.097784    0.028716  0.181818  1.818989e-12       0.879518   \n",
      "3  0.097764    0.028620  0.180000  1.818989e-12       0.885542   \n",
      "4  0.097521    0.028494  0.180000  1.818989e-12       0.885542   \n",
      "5  0.097457    0.027836  0.178218  1.818989e-12       0.891566   \n",
      "6  0.097705    0.028039  0.178218  1.818989e-12       0.891566   \n",
      "7  0.098034    0.028383  0.178218  1.818989e-12       0.891566   \n",
      "8  0.098268    0.028197  0.178218  1.818989e-12       0.891566   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.928571              1.0     0.783133      0.915663   \n",
      "1        0.915663        0.942029              1.0     0.783133      0.915663   \n",
      "2        0.915663        0.970149              1.0     0.783133      0.915663   \n",
      "3        0.915663        0.984848              1.0     0.783133      0.915663   \n",
      "4        0.915663        0.984848              1.0     0.783133      0.915663   \n",
      "5        0.915663        1.000000              1.0     0.783133      0.915663   \n",
      "6        0.915663        1.000000              1.0     0.783133      0.915663   \n",
      "7        0.915663        1.000000              1.0     0.783133      0.915663   \n",
      "8        0.915663        1.000000              1.0     0.783133      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     18  \n",
      "1     18  \n",
      "2     18  \n",
      "3     18  \n",
      "4     18  \n",
      "5     18  \n",
      "6     18  \n",
      "7     18  \n",
      "8     18  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.098453    0.028381  0.271930  1.818989e-12       0.813253   \n",
      "1  0.097974    0.028927  0.274336  1.818989e-12       0.807229   \n",
      "2  0.098285    0.028610  0.490196  1.818989e-12       0.512048   \n",
      "3  0.097909    0.028395  0.271930  1.818989e-12       0.813253   \n",
      "4  0.097900    0.028453  0.271930  1.818989e-12       0.813253   \n",
      "5  0.097795    0.028512  0.271930  1.818989e-12       0.813253   \n",
      "6  0.098200    0.028810  0.306931  1.818989e-12       0.734940   \n",
      "7  0.098258    0.028910  0.274336  1.818989e-12       0.807229   \n",
      "8  0.099872    0.034492  0.279279  1.818989e-12       0.795181   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        1.000000              1.0     0.626506      0.915663   \n",
      "1        0.915663        0.981132              1.0     0.626506      0.915663   \n",
      "2        0.915663        0.509804              1.0     0.626506      0.915663   \n",
      "3        0.915663        1.000000              1.0     0.626506      0.915663   \n",
      "4        0.915663        1.000000              1.0     0.626506      0.915663   \n",
      "5        0.915663        1.000000              1.0     0.626506      0.915663   \n",
      "6        0.915663        0.800000              1.0     0.626506      0.915663   \n",
      "7        0.915663        0.981132              1.0     0.626506      0.915663   \n",
      "8        0.915663        0.945455              1.0     0.626506      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     23  \n",
      "1     23  \n",
      "2     23  \n",
      "3     23  \n",
      "4     23  \n",
      "5     23  \n",
      "6     23  \n",
      "7     23  \n",
      "8     23  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.187413    0.032517  0.129412  1.818989e-12       0.879518   \n",
      "1  0.097577    0.028790  0.118280  1.818989e-12       0.927711   \n",
      "2  0.097465    0.028821  0.127907  1.818989e-12       0.885542   \n",
      "3  0.097210    0.028065  0.117021  1.818989e-12       0.933735   \n",
      "4  0.097155    0.027925  0.117021  1.818989e-12       0.933735   \n",
      "5  0.097500    0.028109  0.117021  1.818989e-12       0.933735   \n",
      "6  0.097661    0.028600  0.200000  1.818989e-12       0.825301   \n",
      "7  0.097989    0.028577  0.129412  1.818989e-12       0.879518   \n",
      "8  0.100243    0.028708  0.129412  1.818989e-12       0.879518   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.888889              1.0      0.86747      0.915663   \n",
      "1        0.915663        0.986301              1.0      0.86747      0.915663   \n",
      "2        0.915663        0.900000              1.0      0.86747      0.915663   \n",
      "3        0.915663        1.000000              1.0      0.86747      0.915663   \n",
      "4        0.915663        1.000000              1.0      0.86747      0.915663   \n",
      "5        0.915663        1.000000              1.0      0.86747      0.915663   \n",
      "6        0.915663        0.800000              1.0      0.86747      0.915663   \n",
      "7        0.915663        0.888889              1.0      0.86747      0.915663   \n",
      "8        0.915663        0.888889              1.0      0.86747      0.915663   \n",
      "\n",
      "   owner  \n",
      "0      2  \n",
      "1      2  \n",
      "2      2  \n",
      "3      2  \n",
      "4      2  \n",
      "5      2  \n",
      "6      2  \n",
      "7      2  \n",
      "8      2  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.098455    0.028644  0.350515  1.818989e-12       0.674699   \n",
      "1  0.098683    0.029011  0.307692  1.818989e-12       0.710843   \n",
      "2  0.097801    0.028526  0.357143  1.818989e-12       0.668675   \n",
      "3  0.098094    0.028437  0.194175  1.818989e-12       0.879518   \n",
      "4  0.099219    0.028313  0.194175  1.818989e-12       0.879518   \n",
      "5  0.098118    0.028620  0.232558  1.818989e-12       0.777108   \n",
      "6  0.098539    0.028721  0.212766  1.818989e-12       0.825301   \n",
      "7  0.098368    0.028622  0.212766  1.818989e-12       0.825301   \n",
      "8  0.104082    0.028596  0.422018  1.818989e-12       0.602410   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.649485              1.0     0.759036      0.915663   \n",
      "1        0.915663        0.692308              1.0     0.759036      0.915663   \n",
      "2        0.915663        0.642857              1.0     0.759036      0.915663   \n",
      "3        0.915663        1.000000              1.0     0.759036      0.915663   \n",
      "4        0.915663        1.000000              1.0     0.759036      0.915663   \n",
      "5        0.915663        0.787500              1.0     0.759036      0.915663   \n",
      "6        0.915663        0.875000              1.0     0.759036      0.915663   \n",
      "7        0.915663        0.875000              1.0     0.759036      0.915663   \n",
      "8        0.915663        0.577982              1.0     0.759036      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     15  \n",
      "1     15  \n",
      "2     15  \n",
      "3     15  \n",
      "4     15  \n",
      "5     15  \n",
      "6     15  \n",
      "7     15  \n",
      "8     15  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.098106    0.028802  0.282828  1.818989e-12       0.759036   \n",
      "1  0.097670    0.028750  0.261682  1.818989e-12       0.807229   \n",
      "2  0.097501    0.028301  0.375000  1.818989e-12       0.632530   \n",
      "3  0.097965    0.029129  0.269231  1.818989e-12       0.789157   \n",
      "4  0.097798    0.028618  0.254545  1.818989e-12       0.825301   \n",
      "5  0.098170    0.028652  0.269231  1.818989e-12       0.789157   \n",
      "6  0.097373    0.028639  0.280000  1.818989e-12       0.765060   \n",
      "7  0.098718    0.029074  0.277228  1.818989e-12       0.771084   \n",
      "8  0.103320    0.028811  0.285714  1.818989e-12       0.753012   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.820896              1.0     0.662651      0.915663   \n",
      "1        0.915663        0.932203              1.0     0.662651      0.915663   \n",
      "2        0.915663        0.625000              1.0     0.662651      0.915663   \n",
      "3        0.915663        0.887097              1.0     0.662651      0.915663   \n",
      "4        0.915663        0.982143              1.0     0.662651      0.915663   \n",
      "5        0.915663        0.887097              1.0     0.662651      0.915663   \n",
      "6        0.915663        0.833333              1.0     0.662651      0.915663   \n",
      "7        0.915663        0.846154              1.0     0.662651      0.915663   \n",
      "8        0.915663        0.808824              1.0     0.662651      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     10  \n",
      "1     10  \n",
      "2     10  \n",
      "3     10  \n",
      "4     10  \n",
      "5     10  \n",
      "6     10  \n",
      "7     10  \n",
      "8     10  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.097592    0.028638  0.409091  1.818989e-12       0.620482   \n",
      "1  0.097344    0.028713  0.235294  1.818989e-12       0.771084   \n",
      "2  0.096893    0.028756  0.214286  1.818989e-12       0.789157   \n",
      "3  0.097455    0.028584  0.180000  1.818989e-12       0.885542   \n",
      "4  0.097332    0.028569  0.181818  1.818989e-12       0.879518   \n",
      "5  0.096936    0.028547  0.202247  1.818989e-12       0.819277   \n",
      "6  0.097203    0.028640  0.252874  1.818989e-12       0.759036   \n",
      "7  0.099554    0.028773  0.480000  1.818989e-12       0.530120   \n",
      "8  0.101583    0.028495  0.187500  1.818989e-12       0.861446   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.590909              1.0     0.783133      0.915663   \n",
      "1        0.915663        0.764706              1.0     0.783133      0.915663   \n",
      "2        0.915663        0.792683              1.0     0.783133      0.915663   \n",
      "3        0.915663        0.984848              1.0     0.783133      0.915663   \n",
      "4        0.915663        0.970149              1.0     0.783133      0.915663   \n",
      "5        0.915663        0.844156              1.0     0.783133      0.915663   \n",
      "6        0.915663        0.747126              1.0     0.783133      0.915663   \n",
      "7        0.915663        0.520000              1.0     0.783133      0.915663   \n",
      "8        0.915663        0.928571              1.0     0.783133      0.915663   \n",
      "\n",
      "   owner  \n",
      "0      4  \n",
      "1      4  \n",
      "2      4  \n",
      "3      4  \n",
      "4      4  \n",
      "5      4  \n",
      "6      4  \n",
      "7      4  \n",
      "8      4  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      " 80%|  | 4/5 [00:06<00:01,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.099140    0.028989  0.186275  1.818989e-12       0.885542   \n",
      "1  0.098334    0.028664  0.186275  1.818989e-12       0.885542   \n",
      "2  0.098533    0.028377  0.186275  1.818989e-12       0.885542   \n",
      "3  0.098216    0.028504  0.186275  1.818989e-12       0.885542   \n",
      "4  0.098479    0.028770  0.186275  1.818989e-12       0.885542   \n",
      "5  0.100773    0.029234  0.211111  1.818989e-12       0.813253   \n",
      "6  0.098234    0.028386  0.186275  1.818989e-12       0.885542   \n",
      "7  0.099303    0.028615  0.186275  1.818989e-12       0.885542   \n",
      "8  0.098769    0.029186  0.190000  1.818989e-12       0.873494   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        1.000000              1.0     0.771084      0.915663   \n",
      "1        0.915663        1.000000              1.0     0.771084      0.915663   \n",
      "2        0.915663        1.000000              1.0     0.771084      0.915663   \n",
      "3        0.915663        1.000000              1.0     0.771084      0.915663   \n",
      "4        0.915663        1.000000              1.0     0.771084      0.915663   \n",
      "5        0.915663        0.842105              1.0     0.771084      0.915663   \n",
      "6        0.915663        1.000000              1.0     0.771084      0.915663   \n",
      "7        0.915663        1.000000              1.0     0.771084      0.915663   \n",
      "8        0.915663        0.969697              1.0     0.771084      0.915663   \n",
      "\n",
      "   owner  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n",
      "4      0  \n",
      "5      0  \n",
      "6      0  \n",
      "7      0  \n",
      "8      0  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.099015    0.029724  0.506250  1.818989e-12       0.487952   \n",
      "1  0.097683    0.028395  0.466216  1.818989e-12       0.560241   \n",
      "2  0.097795    0.028701  0.047059  1.818989e-12       0.963855   \n",
      "3  0.098602    0.028361  0.368000  1.818989e-12       0.698795   \n",
      "4  0.098172    0.028507  0.509317  1.818989e-12       0.481928   \n",
      "5  0.097519    0.028426  0.443662  1.818989e-12       0.596386   \n",
      "6  0.097551    0.028498  0.496815  1.818989e-12       0.506024   \n",
      "7  0.098527    0.028284  0.233010  1.818989e-12       0.831325   \n",
      "8  0.098347    0.028527  0.496815  1.818989e-12       0.506024   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.493750              1.0     0.951807      0.915663   \n",
      "1        0.915663        0.533784              1.0     0.951807      0.915663   \n",
      "2        0.915663        0.975309              1.0     0.951807      0.915663   \n",
      "3        0.915663        0.632000              1.0     0.951807      0.915663   \n",
      "4        0.915663        0.490683              1.0     0.951807      0.915663   \n",
      "5        0.915663        0.556338              1.0     0.951807      0.915663   \n",
      "6        0.915663        0.503185              1.0     0.951807      0.915663   \n",
      "7        0.915663        0.766990              1.0     0.951807      0.915663   \n",
      "8        0.915663        0.503185              1.0     0.951807      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     12  \n",
      "1     12  \n",
      "2     12  \n",
      "3     12  \n",
      "4     12  \n",
      "5     12  \n",
      "6     12  \n",
      "7     12  \n",
      "8     12  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.099328    0.028963  0.163043  1.818989e-12       0.873494   \n",
      "1  0.098632    0.028560  0.153061  1.818989e-12       0.909639   \n",
      "2  0.108718    0.028414  0.153061  1.818989e-12       0.909639   \n",
      "3  0.099089    0.028562  0.153061  1.818989e-12       0.909639   \n",
      "4  0.098823    0.030604  0.472868  1.818989e-12       0.542169   \n",
      "5  0.098491    0.028741  0.161290  1.818989e-12       0.879518   \n",
      "6  0.098215    0.029005  0.163043  1.818989e-12       0.873494   \n",
      "7  0.099092    0.029025  0.156250  1.818989e-12       0.897590   \n",
      "8  0.098214    0.032624  0.164835  1.818989e-12       0.867470   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.918919              1.0     0.819277      0.915663   \n",
      "1        0.915663        1.000000              1.0     0.819277      0.915663   \n",
      "2        0.915663        1.000000              1.0     0.819277      0.915663   \n",
      "3        0.915663        1.000000              1.0     0.819277      0.915663   \n",
      "4        0.915663        0.527132              1.0     0.819277      0.915663   \n",
      "5        0.915663        0.931507              1.0     0.819277      0.915663   \n",
      "6        0.915663        0.918919              1.0     0.819277      0.915663   \n",
      "7        0.915663        0.971429              1.0     0.819277      0.915663   \n",
      "8        0.915663        0.906667              1.0     0.819277      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     14  \n",
      "1     14  \n",
      "2     14  \n",
      "3     14  \n",
      "4     14  \n",
      "5     14  \n",
      "6     14  \n",
      "7     14  \n",
      "8     14  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.107729    0.028826  0.483444  1.818989e-12       0.530120   \n",
      "1  0.098204    0.028506  0.264151  1.818989e-12       0.801205   \n",
      "2  0.097750    0.036167  0.458333  1.818989e-12       0.572289   \n",
      "3  0.099842    0.028993  0.496774  1.818989e-12       0.506024   \n",
      "4  0.098482    0.028578  0.355372  1.818989e-12       0.710843   \n",
      "5  0.098284    0.028680  0.426471  1.818989e-12       0.620482   \n",
      "6  0.098709    0.029098  0.142857  1.818989e-12       0.891566   \n",
      "7  0.099097    0.028869  0.430657  1.818989e-12       0.614458   \n",
      "8  0.114663    0.028668  0.442857  1.818989e-12       0.596386   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.516556              1.0     0.939759      0.915663   \n",
      "1        0.915663        0.735849              1.0     0.939759      0.915663   \n",
      "2        0.915663        0.541667              1.0     0.939759      0.915663   \n",
      "3        0.915663        0.503226              1.0     0.939759      0.915663   \n",
      "4        0.915663        0.644628              1.0     0.939759      0.915663   \n",
      "5        0.915663        0.573529              1.0     0.939759      0.915663   \n",
      "6        0.915663        0.857143              1.0     0.939759      0.915663   \n",
      "7        0.915663        0.569343              1.0     0.939759      0.915663   \n",
      "8        0.915663        0.557143              1.0     0.939759      0.915663   \n",
      "\n",
      "   owner  \n",
      "0      9  \n",
      "1      9  \n",
      "2      9  \n",
      "3      9  \n",
      "4      9  \n",
      "5      9  \n",
      "6      9  \n",
      "7      9  \n",
      "8      9  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.099328    0.028721  0.216981  1.818989e-12       0.861446   \n",
      "1  0.098731    0.028502  0.216981  1.818989e-12       0.861446   \n",
      "2  0.098464    0.028376  0.216981  1.818989e-12       0.861446   \n",
      "3  0.098560    0.028752  0.216981  1.818989e-12       0.861446   \n",
      "4  0.099345    0.028189  0.216981  1.818989e-12       0.861446   \n",
      "5  0.098242    0.028588  0.216981  1.818989e-12       0.861446   \n",
      "6  0.098641    0.028399  0.216981  1.818989e-12       0.861446   \n",
      "7  0.100786    0.028615  0.216981  1.818989e-12       0.861446   \n",
      "8  0.100579    0.028906  0.219048  1.818989e-12       0.855422   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        1.000000              1.0     0.722892      0.915663   \n",
      "1        0.915663        1.000000              1.0     0.722892      0.915663   \n",
      "2        0.915663        1.000000              1.0     0.722892      0.915663   \n",
      "3        0.915663        1.000000              1.0     0.722892      0.915663   \n",
      "4        0.915663        1.000000              1.0     0.722892      0.915663   \n",
      "5        0.915663        1.000000              1.0     0.722892      0.915663   \n",
      "6        0.915663        1.000000              1.0     0.722892      0.915663   \n",
      "7        0.915663        1.000000              1.0     0.722892      0.915663   \n",
      "8        0.915663        0.983607              1.0     0.722892      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     18  \n",
      "1     18  \n",
      "2     18  \n",
      "3     18  \n",
      "4     18  \n",
      "5     18  \n",
      "6     18  \n",
      "7     18  \n",
      "8     18  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.098526    0.028156  0.178218  1.818989e-12       0.891566   \n",
      "1  0.097825    0.028477  0.178218  1.818989e-12       0.891566   \n",
      "2  0.097946    0.028439  0.429825  1.818989e-12       0.596386   \n",
      "3  0.097921    0.028153  0.178218  1.818989e-12       0.891566   \n",
      "4  0.098684    0.028115  0.178218  1.818989e-12       0.891566   \n",
      "5  0.098415    0.028217  0.178218  1.818989e-12       0.891566   \n",
      "6  0.097577    0.028692  0.206897  1.818989e-12       0.807229   \n",
      "7  0.098078    0.028073  0.178218  1.818989e-12       0.891566   \n",
      "8  0.097740    0.028732  0.180000  1.818989e-12       0.885542   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        1.000000              1.0     0.783133      0.915663   \n",
      "1        0.915663        1.000000              1.0     0.783133      0.915663   \n",
      "2        0.915663        0.570175              1.0     0.783133      0.915663   \n",
      "3        0.915663        1.000000              1.0     0.783133      0.915663   \n",
      "4        0.915663        1.000000              1.0     0.783133      0.915663   \n",
      "5        0.915663        1.000000              1.0     0.783133      0.915663   \n",
      "6        0.915663        0.822785              1.0     0.783133      0.915663   \n",
      "7        0.915663        1.000000              1.0     0.783133      0.915663   \n",
      "8        0.915663        0.984848              1.0     0.783133      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     23  \n",
      "1     23  \n",
      "2     23  \n",
      "3     23  \n",
      "4     23  \n",
      "5     23  \n",
      "6     23  \n",
      "7     23  \n",
      "8     23  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.098982    0.028988  0.184783  1.818989e-12       0.849398   \n",
      "1  0.099030    0.028608  0.170000  1.818989e-12       0.897590   \n",
      "2  0.098410    0.028776  0.177083  1.818989e-12       0.873494   \n",
      "3  0.098905    0.028187  0.170000  1.818989e-12       0.897590   \n",
      "4  0.098578    0.028187  0.170000  1.818989e-12       0.897590   \n",
      "5  0.098037    0.028513  0.170000  1.818989e-12       0.897590   \n",
      "6  0.098325    0.029298  0.223529  1.818989e-12       0.783133   \n",
      "7  0.098997    0.028955  0.175258  1.818989e-12       0.879518   \n",
      "8  0.102598    0.028710  0.188889  1.818989e-12       0.837349   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.891892              1.0     0.795181      0.915663   \n",
      "1        0.915663        1.000000              1.0     0.795181      0.915663   \n",
      "2        0.915663        0.942857              1.0     0.795181      0.915663   \n",
      "3        0.915663        1.000000              1.0     0.795181      0.915663   \n",
      "4        0.915663        1.000000              1.0     0.795181      0.915663   \n",
      "5        0.915663        1.000000              1.0     0.795181      0.915663   \n",
      "6        0.915663        0.776471              1.0     0.795181      0.915663   \n",
      "7        0.915663        0.956522              1.0     0.795181      0.915663   \n",
      "8        0.915663        0.868421              1.0     0.795181      0.915663   \n",
      "\n",
      "   owner  \n",
      "0      2  \n",
      "1      2  \n",
      "2      2  \n",
      "3      2  \n",
      "4      2  \n",
      "5      2  \n",
      "6      2  \n",
      "7      2  \n",
      "8      2  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.097857    0.028363  0.232558  1.818989e-12       0.777108   \n",
      "1  0.097743    0.028377  0.290323  1.818989e-12       0.734940   \n",
      "2  0.097579    0.028275  0.282609  1.818989e-12       0.740964   \n",
      "3  0.098768    0.027872  0.170000  1.818989e-12       0.897590   \n",
      "4  0.097645    0.028030  0.170000  1.818989e-12       0.897590   \n",
      "5  0.097290    0.028265  0.250000  1.818989e-12       0.765060   \n",
      "6  0.098002    0.030309  0.180851  1.818989e-12       0.861446   \n",
      "7  0.098256    0.028546  0.182796  1.818989e-12       0.855422   \n",
      "8  0.101520    0.028394  0.405405  1.818989e-12       0.626506   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.767442              1.0     0.795181      0.915663   \n",
      "1        0.915663        0.709677              1.0     0.795181      0.915663   \n",
      "2        0.915663        0.717391              1.0     0.795181      0.915663   \n",
      "3        0.915663        1.000000              1.0     0.795181      0.915663   \n",
      "4        0.915663        1.000000              1.0     0.795181      0.915663   \n",
      "5        0.915663        0.750000              1.0     0.795181      0.915663   \n",
      "6        0.915663        0.916667              1.0     0.795181      0.915663   \n",
      "7        0.915663        0.904110              1.0     0.795181      0.915663   \n",
      "8        0.915663        0.594595              1.0     0.795181      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     15  \n",
      "1     15  \n",
      "2     15  \n",
      "3     15  \n",
      "4     15  \n",
      "5     15  \n",
      "6     15  \n",
      "7     15  \n",
      "8     15  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.100641    0.031810  0.303922  1.818989e-12       0.740964   \n",
      "1  0.098842    0.028915  0.279279  1.818989e-12       0.795181   \n",
      "2  0.097417    0.028589  0.428571  1.818989e-12       0.578313   \n",
      "3  0.097760    0.029928  0.295238  1.818989e-12       0.759036   \n",
      "4  0.097857    0.028685  0.281818  1.818989e-12       0.789157   \n",
      "5  0.097377    0.028708  0.303922  1.818989e-12       0.740964   \n",
      "6  0.099275    0.028767  0.300971  1.818989e-12       0.746988   \n",
      "7  0.098572    0.028964  0.292453  1.818989e-12       0.765060   \n",
      "8  0.097798    0.028613  0.313131  1.818989e-12       0.722892   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.812500              1.0     0.626506      0.915663   \n",
      "1        0.915663        0.945455              1.0     0.626506      0.915663   \n",
      "2        0.915663        0.571429              1.0     0.626506      0.915663   \n",
      "3        0.915663        0.852459              1.0     0.626506      0.915663   \n",
      "4        0.915663        0.928571              1.0     0.626506      0.915663   \n",
      "5        0.915663        0.812500              1.0     0.626506      0.915663   \n",
      "6        0.915663        0.825397              1.0     0.626506      0.915663   \n",
      "7        0.915663        0.866667              1.0     0.626506      0.915663   \n",
      "8        0.915663        0.776119              1.0     0.626506      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     10  \n",
      "1     10  \n",
      "2     10  \n",
      "3     10  \n",
      "4     10  \n",
      "5     10  \n",
      "6     10  \n",
      "7     10  \n",
      "8     10  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.099726    0.029552  0.238095  1.818989e-12       0.765060   \n",
      "1  0.098402    0.029026  0.247059  1.818989e-12       0.759036   \n",
      "2  0.098240    0.028725  0.188119  1.818989e-12       0.879518   \n",
      "3  0.098484    0.028855  0.188119  1.818989e-12       0.879518   \n",
      "4  0.098254    0.028492  0.186275  1.818989e-12       0.885542   \n",
      "5  0.098369    0.029079  0.204301  1.818989e-12       0.831325   \n",
      "6  0.099280    0.028892  0.223529  1.818989e-12       0.783133   \n",
      "7  0.098741    0.034814  0.452991  1.818989e-12       0.566265   \n",
      "8  0.097966    0.028919  0.191919  1.818989e-12       0.867470   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.761905              1.0     0.771084      0.915663   \n",
      "1        0.915663        0.752941              1.0     0.771084      0.915663   \n",
      "2        0.915663        0.984615              1.0     0.771084      0.915663   \n",
      "3        0.915663        0.984615              1.0     0.771084      0.915663   \n",
      "4        0.915663        1.000000              1.0     0.771084      0.915663   \n",
      "5        0.915663        0.876712              1.0     0.771084      0.915663   \n",
      "6        0.915663        0.790123              1.0     0.771084      0.915663   \n",
      "7        0.915663        0.547009              1.0     0.771084      0.915663   \n",
      "8        0.915663        0.955224              1.0     0.771084      0.915663   \n",
      "\n",
      "   owner  \n",
      "0      4  \n",
      "1      4  \n",
      "2      4  \n",
      "3      4  \n",
      "4      4  \n",
      "5      4  \n",
      "6      4  \n",
      "7      4  \n",
      "8      4  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.4s finished\n",
      "100%|| 5/5 [00:08<00:00,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.099868    0.029801  0.082353  1.818989e-12       0.927711   \n",
      "1  0.098134    0.028609  0.262136  1.818989e-12       0.795181   \n",
      "2  0.097872    0.028272  0.077778  1.818989e-12       0.957831   \n",
      "3  0.097514    0.028368  0.077778  1.818989e-12       0.957831   \n",
      "4  0.097978    0.028954  0.081395  1.818989e-12       0.933735   \n",
      "5  0.098419    0.028897  0.350427  1.818989e-12       0.710843   \n",
      "6  0.097621    0.028662  0.082353  1.818989e-12       0.927711   \n",
      "7  0.098404    0.028764  0.084337  1.818989e-12       0.915663   \n",
      "8  0.097938    0.028782  0.191489  1.818989e-12       0.849398   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.938272              1.0     0.915663      0.915663   \n",
      "1        0.915663        0.737864              1.0     0.915663      0.915663   \n",
      "2        0.915663        1.000000              1.0     0.915663      0.915663   \n",
      "3        0.915663        1.000000              1.0     0.915663      0.915663   \n",
      "4        0.915663        0.950000              1.0     0.915663      0.915663   \n",
      "5        0.915663        0.649573              1.0     0.915663      0.915663   \n",
      "6        0.915663        0.938272              1.0     0.915663      0.915663   \n",
      "7        0.915663        0.915663              1.0     0.915663      0.915663   \n",
      "8        0.915663        0.808511              1.0     0.915663      0.915663   \n",
      "\n",
      "   owner  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n",
      "4      0  \n",
      "5      0  \n",
      "6      0  \n",
      "7      0  \n",
      "8      0  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.098484    0.028665  0.506173  1.818989e-12       0.487952   \n",
      "1  0.097006    0.029006  0.496855  1.818989e-12       0.506024   \n",
      "2  0.096912    0.028093  0.111111  1.818989e-12       0.921687   \n",
      "3  0.096588    0.027885  0.402985  1.818989e-12       0.656627   \n",
      "4  0.097258    0.028574  0.509202  1.818989e-12       0.481928   \n",
      "5  0.096719    0.027944  0.473684  1.818989e-12       0.548193   \n",
      "6  0.096469    0.027992  0.500000  1.818989e-12       0.500000   \n",
      "7  0.097425    0.028095  0.424460  1.818989e-12       0.626506   \n",
      "8  0.096878    0.028432  0.496855  1.818989e-12       0.506024   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.493827              1.0     0.963855      0.915663   \n",
      "1        0.915663        0.503145              1.0     0.963855      0.915663   \n",
      "2        0.915663        0.888889              1.0     0.963855      0.915663   \n",
      "3        0.915663        0.597015              1.0     0.963855      0.915663   \n",
      "4        0.915663        0.490798              1.0     0.963855      0.915663   \n",
      "5        0.915663        0.526316              1.0     0.963855      0.915663   \n",
      "6        0.915663        0.500000              1.0     0.963855      0.915663   \n",
      "7        0.915663        0.575540              1.0     0.963855      0.915663   \n",
      "8        0.915663        0.503145              1.0     0.963855      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     12  \n",
      "1     12  \n",
      "2     12  \n",
      "3     12  \n",
      "4     12  \n",
      "5     12  \n",
      "6     12  \n",
      "7     12  \n",
      "8     12  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.099557    0.029401  0.205882  1.818989e-12       0.861446   \n",
      "1  0.099367    0.028983  0.201923  1.818989e-12       0.873494   \n",
      "2  0.099001    0.029048  0.201923  1.818989e-12       0.873494   \n",
      "3  0.099430    0.028947  0.201923  1.818989e-12       0.873494   \n",
      "4  0.099348    0.029408  0.478992  1.818989e-12       0.530120   \n",
      "5  0.098777    0.028986  0.201923  1.818989e-12       0.873494   \n",
      "6  0.098852    0.029374  0.218750  1.818989e-12       0.825301   \n",
      "7  0.099835    0.028932  0.201923  1.818989e-12       0.873494   \n",
      "8  0.099290    0.029669  0.210000  1.818989e-12       0.849398   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.968750              1.0     0.746988      0.915663   \n",
      "1        0.915663        1.000000              1.0     0.746988      0.915663   \n",
      "2        0.915663        1.000000              1.0     0.746988      0.915663   \n",
      "3        0.915663        1.000000              1.0     0.746988      0.915663   \n",
      "4        0.915663        0.521008              1.0     0.746988      0.915663   \n",
      "5        0.915663        1.000000              1.0     0.746988      0.915663   \n",
      "6        0.915663        0.885714              1.0     0.746988      0.915663   \n",
      "7        0.915663        1.000000              1.0     0.746988      0.915663   \n",
      "8        0.915663        0.939394              1.0     0.746988      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     14  \n",
      "1     14  \n",
      "2     14  \n",
      "3     14  \n",
      "4     14  \n",
      "5     14  \n",
      "6     14  \n",
      "7     14  \n",
      "8     14  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.106851    0.028925  0.512500  1.818989e-12       0.475904   \n",
      "1  0.097948    0.028806  0.442857  1.818989e-12       0.596386   \n",
      "2  0.102665    0.029324  0.515528  1.818989e-12       0.469880   \n",
      "3  0.098020    0.036825  0.483444  1.818989e-12       0.530120   \n",
      "4  0.098354    0.028661  0.442857  1.818989e-12       0.596386   \n",
      "5  0.097957    0.028579  0.469388  1.818989e-12       0.554217   \n",
      "6  0.097810    0.028739  0.309735  1.818989e-12       0.759036   \n",
      "7  0.098844    0.028556  0.458333  1.818989e-12       0.572289   \n",
      "8  0.098509    0.028706  0.426471  1.818989e-12       0.620482   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.487500              1.0     0.939759      0.915663   \n",
      "1        0.915663        0.557143              1.0     0.939759      0.915663   \n",
      "2        0.915663        0.484472              1.0     0.939759      0.915663   \n",
      "3        0.915663        0.516556              1.0     0.939759      0.915663   \n",
      "4        0.915663        0.557143              1.0     0.939759      0.915663   \n",
      "5        0.915663        0.530612              1.0     0.939759      0.915663   \n",
      "6        0.915663        0.690265              1.0     0.939759      0.915663   \n",
      "7        0.915663        0.541667              1.0     0.939759      0.915663   \n",
      "8        0.915663        0.573529              1.0     0.939759      0.915663   \n",
      "\n",
      "   owner  \n",
      "0      9  \n",
      "1      9  \n",
      "2      9  \n",
      "3      9  \n",
      "4      9  \n",
      "5      9  \n",
      "6      9  \n",
      "7      9  \n",
      "8      9  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.098145    0.028733  0.330097  1.818989e-12       0.710843   \n",
      "1  0.097981    0.028772  0.166667  1.818989e-12       0.837349   \n",
      "2  0.097890    0.028672  0.188235  1.818989e-12       0.819277   \n",
      "3  0.097909    0.028692  0.162791  1.818989e-12       0.849398   \n",
      "4  0.097579    0.028684  0.162791  1.818989e-12       0.849398   \n",
      "5  0.097838    0.028533  0.147368  1.818989e-12       0.903614   \n",
      "6  0.097361    0.028339  0.144330  1.818989e-12       0.915663   \n",
      "7  0.098283    0.028583  0.145833  1.818989e-12       0.909639   \n",
      "8  0.097574    0.028648  0.160920  1.818989e-12       0.855422   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.669903              1.0     0.831325      0.915663   \n",
      "1        0.915663        0.841463              1.0     0.831325      0.915663   \n",
      "2        0.915663        0.811765              1.0     0.831325      0.915663   \n",
      "3        0.915663        0.862500              1.0     0.831325      0.915663   \n",
      "4        0.915663        0.862500              1.0     0.831325      0.915663   \n",
      "5        0.915663        0.971831              1.0     0.831325      0.915663   \n",
      "6        0.915663        1.000000              1.0     0.831325      0.915663   \n",
      "7        0.915663        0.985714              1.0     0.831325      0.915663   \n",
      "8        0.915663        0.873418              1.0     0.831325      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     18  \n",
      "1     18  \n",
      "2     18  \n",
      "3     18  \n",
      "4     18  \n",
      "5     18  \n",
      "6     18  \n",
      "7     18  \n",
      "8     18  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.097992    0.028708  0.203883  1.818989e-12       0.867470   \n",
      "1  0.097737    0.028693  0.205882  1.818989e-12       0.861446   \n",
      "2  0.097677    0.028509  0.483333  1.818989e-12       0.524096   \n",
      "3  0.097873    0.028506  0.201923  1.818989e-12       0.873494   \n",
      "4  0.097654    0.028209  0.201923  1.818989e-12       0.873494   \n",
      "5  0.097628    0.028639  0.203883  1.818989e-12       0.867470   \n",
      "6  0.097842    0.028933  0.241379  1.818989e-12       0.771084   \n",
      "7  0.098304    0.028723  0.203883  1.818989e-12       0.867470   \n",
      "8  0.097991    0.029010  0.214286  1.818989e-12       0.837349   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.984127              1.0     0.746988      0.915663   \n",
      "1        0.915663        0.968750              1.0     0.746988      0.915663   \n",
      "2        0.915663        0.516667              1.0     0.746988      0.915663   \n",
      "3        0.915663        1.000000              1.0     0.746988      0.915663   \n",
      "4        0.915663        1.000000              1.0     0.746988      0.915663   \n",
      "5        0.915663        0.984127              1.0     0.746988      0.915663   \n",
      "6        0.915663        0.784810              1.0     0.746988      0.915663   \n",
      "7        0.915663        0.984127              1.0     0.746988      0.915663   \n",
      "8        0.915663        0.911765              1.0     0.746988      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     23  \n",
      "1     23  \n",
      "2     23  \n",
      "3     23  \n",
      "4     23  \n",
      "5     23  \n",
      "6     23  \n",
      "7     23  \n",
      "8     23  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.098973    0.029295  0.126437  1.818989e-12       0.891566   \n",
      "1  0.098199    0.028806  0.079545  1.818989e-12       0.945783   \n",
      "2  0.097368    0.028528  0.082353  1.818989e-12       0.927711   \n",
      "3  0.097802    0.028605  0.077778  1.818989e-12       0.957831   \n",
      "4  0.097703    0.028006  0.077778  1.818989e-12       0.957831   \n",
      "5  0.098549    0.028145  0.077778  1.818989e-12       0.957831   \n",
      "6  0.097669    0.028745  0.232323  1.818989e-12       0.819277   \n",
      "7  0.098072    0.029003  0.105882  1.818989e-12       0.903614   \n",
      "8  0.097757    0.028664  0.146067  1.818989e-12       0.879518   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.873563              1.0     0.915663      0.915663   \n",
      "1        0.915663        0.974359              1.0     0.915663      0.915663   \n",
      "2        0.915663        0.938272              1.0     0.915663      0.915663   \n",
      "3        0.915663        1.000000              1.0     0.915663      0.915663   \n",
      "4        0.915663        1.000000              1.0     0.915663      0.915663   \n",
      "5        0.915663        1.000000              1.0     0.915663      0.915663   \n",
      "6        0.915663        0.767677              1.0     0.915663      0.915663   \n",
      "7        0.915663        0.894118              1.0     0.915663      0.915663   \n",
      "8        0.915663        0.853933              1.0     0.915663      0.915663   \n",
      "\n",
      "   owner  \n",
      "0      2  \n",
      "1      2  \n",
      "2      2  \n",
      "3      2  \n",
      "4      2  \n",
      "5      2  \n",
      "6      2  \n",
      "7      2  \n",
      "8      2  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.107575    0.028872  0.351064  1.818989e-12       0.668675   \n",
      "1  0.098943    0.028738  0.298851  1.818989e-12       0.710843   \n",
      "2  0.097605    0.028435  0.440367  1.818989e-12       0.578313   \n",
      "3  0.097835    0.028690  0.211538  1.818989e-12       0.861446   \n",
      "4  0.098011    0.028104  0.209524  1.818989e-12       0.867470   \n",
      "5  0.097988    0.028397  0.314607  1.818989e-12       0.698795   \n",
      "6  0.097579    0.028602  0.247191  1.818989e-12       0.771084   \n",
      "7  0.098216    0.028817  0.234043  1.818989e-12       0.801205   \n",
      "8  0.098283    0.028891  0.401961  1.818989e-12       0.620482   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.648936              1.0      0.73494      0.915663   \n",
      "1        0.915663        0.701149              1.0      0.73494      0.915663   \n",
      "2        0.915663        0.559633              1.0      0.73494      0.915663   \n",
      "3        0.915663        0.983871              1.0      0.73494      0.915663   \n",
      "4        0.915663        1.000000              1.0      0.73494      0.915663   \n",
      "5        0.915663        0.685393              1.0      0.73494      0.915663   \n",
      "6        0.915663        0.792208              1.0      0.73494      0.915663   \n",
      "7        0.915663        0.847222              1.0      0.73494      0.915663   \n",
      "8        0.915663        0.598039              1.0      0.73494      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     15  \n",
      "1     15  \n",
      "2     15  \n",
      "3     15  \n",
      "4     15  \n",
      "5     15  \n",
      "6     15  \n",
      "7     15  \n",
      "8     15  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.097073    0.028515  0.341176  1.818989e-12       0.662651   \n",
      "1  0.096909    0.028556  0.329545  1.818989e-12       0.680723   \n",
      "2  0.096862    0.028475  0.490566  1.818989e-12       0.512048   \n",
      "3  0.097203    0.028753  0.292929  1.818989e-12       0.746988   \n",
      "4  0.096617    0.028809  0.273585  1.818989e-12       0.789157   \n",
      "5  0.096696    0.028419  0.379310  1.818989e-12       0.626506   \n",
      "6  0.096867    0.028431  0.329545  1.818989e-12       0.680723   \n",
      "7  0.097281    0.028594  0.341176  1.818989e-12       0.662651   \n",
      "8  0.097643    0.028510  0.345238  1.818989e-12       0.656627   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.666667              1.0     0.650602      0.915663   \n",
      "1        0.915663        0.692308              1.0     0.650602      0.915663   \n",
      "2        0.915663        0.509434              1.0     0.650602      0.915663   \n",
      "3        0.915663        0.805970              1.0     0.650602      0.915663   \n",
      "4        0.915663        0.900000              1.0     0.650602      0.915663   \n",
      "5        0.915663        0.620690              1.0     0.650602      0.915663   \n",
      "6        0.915663        0.692308              1.0     0.650602      0.915663   \n",
      "7        0.915663        0.666667              1.0     0.650602      0.915663   \n",
      "8        0.915663        0.658537              1.0     0.650602      0.915663   \n",
      "\n",
      "   owner  \n",
      "0     10  \n",
      "1     10  \n",
      "2     10  \n",
      "3     10  \n",
      "4     10  \n",
      "5     10  \n",
      "6     10  \n",
      "7     10  \n",
      "8     10  ,    fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
      "0  0.105323    0.030762  0.447368  1.818989e-12       0.572289   \n",
      "1  0.098402    0.028818  0.300000  1.818989e-12       0.716867   \n",
      "2  0.097789    0.028814  0.370000  1.818989e-12       0.656627   \n",
      "3  0.098425    0.028713  0.198020  1.818989e-12       0.867470   \n",
      "4  0.097906    0.028777  0.202020  1.818989e-12       0.855422   \n",
      "5  0.097698    0.028899  0.222222  1.818989e-12       0.801205   \n",
      "6  0.098435    0.028729  0.357143  1.818989e-12       0.668675   \n",
      "7  0.098709    0.028751  0.466102  1.818989e-12       0.548193   \n",
      "8  0.098515    0.029128  0.224719  1.818989e-12       0.795181   \n",
      "\n",
      "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
      "0        0.915663        0.552632              1.0     0.759036      0.915663   \n",
      "1        0.915663        0.700000              1.0     0.759036      0.915663   \n",
      "2        0.915663        0.630000              1.0     0.759036      0.915663   \n",
      "3        0.915663        0.969231              1.0     0.759036      0.915663   \n",
      "4        0.915663        0.940299              1.0     0.759036      0.915663   \n",
      "5        0.915663        0.828947              1.0     0.759036      0.915663   \n",
      "6        0.915663        0.642857              1.0     0.759036      0.915663   \n",
      "7        0.915663        0.533898              1.0     0.759036      0.915663   \n",
      "8        0.915663        0.818182              1.0     0.759036      0.915663   \n",
      "\n",
      "   owner  \n",
      "0      4  \n",
      "1      4  \n",
      "2      4  \n",
      "3      4  \n",
      "4      4  \n",
      "5      4  \n",
      "6      4  \n",
      "7      4  \n",
      "8      4  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_eer</th>\n",
       "      <th>train_eer</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>owner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.111592</td>\n",
       "      <td>0.030324</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.818989e-12</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.86747</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.099437</td>\n",
       "      <td>0.029282</td>\n",
       "      <td>0.208791</td>\n",
       "      <td>1.818989e-12</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.791209</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.86747</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.099241</td>\n",
       "      <td>0.028959</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>1.818989e-12</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.86747</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.098775</td>\n",
       "      <td>0.028745</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>1.818989e-12</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.86747</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.099137</td>\n",
       "      <td>0.029068</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.818989e-12</td>\n",
       "      <td>0.861446</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.86747</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
       "0  0.111592    0.030324  0.125000  1.818989e-12       0.897590   \n",
       "1  0.099437    0.029282  0.208791  1.818989e-12       0.819277   \n",
       "2  0.099241    0.028959  0.117021  1.818989e-12       0.933735   \n",
       "3  0.098775    0.028745  0.117021  1.818989e-12       0.933735   \n",
       "4  0.099137    0.029068  0.142857  1.818989e-12       0.861446   \n",
       "\n",
       "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
       "0        0.915663        0.923077              1.0      0.86747      0.915663   \n",
       "1        0.915663        0.791209              1.0      0.86747      0.915663   \n",
       "2        0.915663        1.000000              1.0      0.86747      0.915663   \n",
       "3        0.915663        1.000000              1.0      0.86747      0.915663   \n",
       "4        0.915663        0.857143              1.0      0.86747      0.915663   \n",
       "\n",
       "   owner  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_results = None  # Will be filled with randomsearch scores\n",
    "for run in tqdm(range(5)):\n",
    "\n",
    "    df_score = Parallel(n_jobs=-1, verbose=0)(delayed(evaluate_owner_IF_train_test)(owner_idx, X_exp_test_dic, SEED, run, optimal_params=P, CORES=1) for owner_idx in range(len(test_set)))\n",
    "    # test_df_results = pd.concat([test_df_results, df_score], axis=0)\n",
    "    print(df_score)\n",
    "    test_df_results = pd.concat([test_df_results] + df_score, sort=False, axis=0)\n",
    "\n",
    "\n",
    "test_df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [79]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_df_results\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m()\n\u001b[1;32m      2\u001b[0m test_df_results\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      3\u001b[0m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m+\u001b[39m df_score, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'head'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 264\n"
     ]
    }
   ],
   "source": [
    "test_df_results.head()\n",
    "test_df_results.shape\n",
    "pd.concat([None] + df_score, sort=False, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([None,\n",
       "          fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
       "0  0.060355    0.017924  0.135417  1.818989e-12       0.921687   \n",
       "1  0.059660    0.018121  0.156627  1.818989e-12       0.843373   \n",
       "2  0.059146    0.017721  0.135417  1.818989e-12       0.921687   \n",
       "3  0.059037    0.017716  0.135417  1.818989e-12       0.921687   \n",
       "4  0.059126    0.018265  0.139785  1.818989e-12       0.903614   \n",
       "5  0.059093    0.018161  0.326923  1.818989e-12       0.716867   \n",
       "6  0.059430    0.018186  0.147727  1.818989e-12       0.873494   \n",
       "7  0.059211    0.018280  0.141304  1.818989e-12       0.897590   \n",
       "8  0.059111    0.017993  0.139785  1.818989e-12       0.903614   \n",
       "\n",
       "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
       "0        0.891566        1.000000              1.0     0.843373      0.891566   \n",
       "1        0.891566        0.843373              1.0     0.843373      0.891566   \n",
       "2        0.891566        1.000000              1.0     0.843373      0.891566   \n",
       "3        0.891566        1.000000              1.0     0.843373      0.891566   \n",
       "4        0.891566        0.958904              1.0     0.843373      0.891566   \n",
       "5        0.891566        0.673077              1.0     0.843373      0.891566   \n",
       "6        0.891566        0.897436              1.0     0.843373      0.891566   \n",
       "7        0.891566        0.945946              1.0     0.843373      0.891566   \n",
       "8        0.891566        0.958904              1.0     0.843373      0.891566   \n",
       "\n",
       "   owner  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "5      0  \n",
       "6      0  \n",
       "7      0  \n",
       "8      0  ,\n",
       "          fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
       "0  0.061423    0.017751  0.500000  1.818989e-12       0.500000   \n",
       "1  0.068899    0.031055  0.571429  1.818989e-12       0.427711   \n",
       "2  0.118025    0.025438  0.366412  1.818989e-12       0.710843   \n",
       "3  0.060273    0.017740  0.366412  1.818989e-12       0.710843   \n",
       "4  0.060670    0.018245  0.646465  1.818989e-12       0.325301   \n",
       "5  0.059972    0.018111  0.396694  1.818989e-12       0.650602   \n",
       "6  0.059867    0.028232  0.631579  1.818989e-12       0.349398   \n",
       "7  0.060345    0.017860  0.372093  1.818989e-12       0.698795   \n",
       "8  0.060048    0.017956  0.606742  1.818989e-12       0.385542   \n",
       "\n",
       "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
       "0        0.891566        0.500000              1.0     0.421687      0.891566   \n",
       "1        0.891566        0.426829              1.0     0.421687      0.891566   \n",
       "2        0.891566        1.000000              1.0     0.421687      0.891566   \n",
       "3        0.891566        1.000000              1.0     0.421687      0.891566   \n",
       "4        0.891566        0.353535              1.0     0.421687      0.891566   \n",
       "5        0.891566        0.777778              1.0     0.421687      0.891566   \n",
       "6        0.891566        0.368421              1.0     0.421687      0.891566   \n",
       "7        0.891566        0.945946              1.0     0.421687      0.891566   \n",
       "8        0.891566        0.393258              1.0     0.421687      0.891566   \n",
       "\n",
       "   owner  \n",
       "0     12  \n",
       "1     12  \n",
       "2     12  \n",
       "3     12  \n",
       "4     12  \n",
       "5     12  \n",
       "6     12  \n",
       "7     12  \n",
       "8     12  ,\n",
       "          fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
       "0  0.060162    0.018397  0.225490  1.818989e-12       0.837349   \n",
       "1  0.060088    0.017889  0.216981  1.818989e-12       0.861446   \n",
       "2  0.059486    0.017810  0.216981  1.818989e-12       0.861446   \n",
       "3  0.059342    0.017971  0.216981  1.818989e-12       0.861446   \n",
       "4  0.059538    0.018054  0.439252  1.818989e-12       0.578313   \n",
       "5  0.059386    0.017917  0.216981  1.818989e-12       0.861446   \n",
       "6  0.059613    0.018134  0.230000  1.818989e-12       0.825301   \n",
       "7  0.059429    0.018234  0.221154  1.818989e-12       0.849398   \n",
       "8  0.059329    0.018075  0.219048  1.818989e-12       0.855422   \n",
       "\n",
       "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
       "0        0.891566        0.937500              1.0     0.722892      0.891566   \n",
       "1        0.891566        1.000000              1.0     0.722892      0.891566   \n",
       "2        0.891566        1.000000              1.0     0.722892      0.891566   \n",
       "3        0.891566        1.000000              1.0     0.722892      0.891566   \n",
       "4        0.891566        0.560748              1.0     0.722892      0.891566   \n",
       "5        0.891566        1.000000              1.0     0.722892      0.891566   \n",
       "6        0.891566        0.909091              1.0     0.722892      0.891566   \n",
       "7        0.891566        0.967742              1.0     0.722892      0.891566   \n",
       "8        0.891566        0.983607              1.0     0.722892      0.891566   \n",
       "\n",
       "   owner  \n",
       "0     14  \n",
       "1     14  \n",
       "2     14  \n",
       "3     14  \n",
       "4     14  \n",
       "5     14  \n",
       "6     14  \n",
       "7     14  \n",
       "8     14  ,\n",
       "          fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
       "0  0.060239    0.017528  0.500000  1.818989e-12       0.500000   \n",
       "1  0.059500    0.017821  0.412214  1.818989e-12       0.638554   \n",
       "2  0.059558    0.017831  0.515723  1.818989e-12       0.469880   \n",
       "3  0.059726    0.017787  0.453901  1.818989e-12       0.578313   \n",
       "4  0.060254    0.017718  0.437956  1.818989e-12       0.602410   \n",
       "5  0.059839    0.017680  0.388889  1.818989e-12       0.668675   \n",
       "6  0.059434    0.017960  0.312500  1.818989e-12       0.753012   \n",
       "7  0.059502    0.018113  0.457746  1.818989e-12       0.572289   \n",
       "8  0.061533    0.018012  0.398438  1.818989e-12       0.656627   \n",
       "\n",
       "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
       "0        0.891566        0.500000              1.0     0.927711      0.891566   \n",
       "1        0.891566        0.587786              1.0     0.927711      0.891566   \n",
       "2        0.891566        0.484277              1.0     0.927711      0.891566   \n",
       "3        0.891566        0.546099              1.0     0.927711      0.891566   \n",
       "4        0.891566        0.562044              1.0     0.927711      0.891566   \n",
       "5        0.891566        0.611111              1.0     0.927711      0.891566   \n",
       "6        0.891566        0.687500              1.0     0.927711      0.891566   \n",
       "7        0.891566        0.542254              1.0     0.927711      0.891566   \n",
       "8        0.891566        0.601562              1.0     0.927711      0.891566   \n",
       "\n",
       "   owner  \n",
       "0      9  \n",
       "1      9  \n",
       "2      9  \n",
       "3      9  \n",
       "4      9  \n",
       "5      9  \n",
       "6      9  \n",
       "7      9  \n",
       "8      9  ,\n",
       "          fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
       "0  0.060068    0.017953  0.247619  1.818989e-12       0.819277   \n",
       "1  0.059829    0.017997  0.240741  1.818989e-12       0.837349   \n",
       "2  0.059576    0.017880  0.240741  1.818989e-12       0.837349   \n",
       "3  0.059772    0.018064  0.245283  1.818989e-12       0.825301   \n",
       "4  0.059368    0.018104  0.247619  1.818989e-12       0.819277   \n",
       "5  0.061543    0.017723  0.238532  1.818989e-12       0.843373   \n",
       "6  0.061692    0.018344  0.238532  1.818989e-12       0.843373   \n",
       "7  0.059427    0.018260  0.242991  1.818989e-12       0.831325   \n",
       "8  0.060345    0.017981  0.242991  1.818989e-12       0.831325   \n",
       "\n",
       "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
       "0        0.891566        0.934426              1.0     0.686747      0.891566   \n",
       "1        0.891566        0.982759              1.0     0.686747      0.891566   \n",
       "2        0.891566        0.982759              1.0     0.686747      0.891566   \n",
       "3        0.891566        0.950000              1.0     0.686747      0.891566   \n",
       "4        0.891566        0.934426              1.0     0.686747      0.891566   \n",
       "5        0.891566        1.000000              1.0     0.686747      0.891566   \n",
       "6        0.891566        1.000000              1.0     0.686747      0.891566   \n",
       "7        0.891566        0.966102              1.0     0.686747      0.891566   \n",
       "8        0.891566        0.966102              1.0     0.686747      0.891566   \n",
       "\n",
       "   owner  \n",
       "0     18  \n",
       "1     18  \n",
       "2     18  \n",
       "3     18  \n",
       "4     18  \n",
       "5     18  \n",
       "6     18  \n",
       "7     18  \n",
       "8     18  ,\n",
       "          fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
       "0  0.059894    0.017560  0.296610  1.818989e-12       0.789157   \n",
       "1  0.059287    0.017872  0.304348  1.818989e-12       0.771084   \n",
       "2  0.059376    0.017967  0.494737  1.818989e-12       0.506024   \n",
       "3  0.059800    0.017739  0.296610  1.818989e-12       0.789157   \n",
       "4  0.059602    0.017665  0.296610  1.818989e-12       0.789157   \n",
       "5  0.059519    0.017885  0.304348  1.818989e-12       0.771084   \n",
       "6  0.059605    0.018086  0.330189  1.818989e-12       0.716867   \n",
       "7  0.059223    0.017897  0.301724  1.818989e-12       0.777108   \n",
       "8  0.060338    0.017861  0.296610  1.818989e-12       0.789157   \n",
       "\n",
       "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
       "0        0.891566        1.000000              1.0     0.578313      0.891566   \n",
       "1        0.891566        0.941176              1.0     0.578313      0.891566   \n",
       "2        0.891566        0.505263              1.0     0.578313      0.891566   \n",
       "3        0.891566        1.000000              1.0     0.578313      0.891566   \n",
       "4        0.891566        1.000000              1.0     0.578313      0.891566   \n",
       "5        0.891566        0.941176              1.0     0.578313      0.891566   \n",
       "6        0.891566        0.800000              1.0     0.578313      0.891566   \n",
       "7        0.891566        0.960000              1.0     0.578313      0.891566   \n",
       "8        0.891566        1.000000              1.0     0.578313      0.891566   \n",
       "\n",
       "   owner  \n",
       "0     23  \n",
       "1     23  \n",
       "2     23  \n",
       "3     23  \n",
       "4     23  \n",
       "5     23  \n",
       "6     23  \n",
       "7     23  \n",
       "8     23  ,\n",
       "          fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
       "0  0.115826    0.030987  0.120482  1.818989e-12       0.879518   \n",
       "1  0.080752    0.017747  0.107527  1.818989e-12       0.939759   \n",
       "2  0.061779    0.018380  0.117647  1.818989e-12       0.891566   \n",
       "3  0.061624    0.017859  0.107527  1.818989e-12       0.939759   \n",
       "4  0.060753    0.017957  0.107527  1.818989e-12       0.939759   \n",
       "5  0.061381    0.018490  0.109890  1.818989e-12       0.927711   \n",
       "6  0.060721    0.019482  0.141176  1.818989e-12       0.867470   \n",
       "7  0.070176    0.018722  0.116279  1.818989e-12       0.897590   \n",
       "8  0.060866    0.018563  0.112360  1.818989e-12       0.915663   \n",
       "\n",
       "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
       "0        0.891566        0.879518              1.0     0.879518      0.891566   \n",
       "1        0.891566        1.000000              1.0     0.879518      0.891566   \n",
       "2        0.891566        0.901235              1.0     0.879518      0.891566   \n",
       "3        0.891566        1.000000              1.0     0.879518      0.891566   \n",
       "4        0.891566        1.000000              1.0     0.879518      0.891566   \n",
       "5        0.891566        0.973333              1.0     0.879518      0.891566   \n",
       "6        0.891566        0.858824              1.0     0.879518      0.891566   \n",
       "7        0.891566        0.912500              1.0     0.879518      0.891566   \n",
       "8        0.891566        0.948052              1.0     0.879518      0.891566   \n",
       "\n",
       "   owner  \n",
       "0      2  \n",
       "1      2  \n",
       "2      2  \n",
       "3      2  \n",
       "4      2  \n",
       "5      2  \n",
       "6      2  \n",
       "7      2  \n",
       "8      2  ,\n",
       "          fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
       "0  0.077209    0.018130  0.292929  1.818989e-12       0.746988   \n",
       "1  0.060306    0.018167  0.271028  1.818989e-12       0.795181   \n",
       "2  0.059496    0.018095  0.311828  1.818989e-12       0.710843   \n",
       "3  0.059850    0.017834  0.258929  1.818989e-12       0.825301   \n",
       "4  0.059348    0.017722  0.258929  1.818989e-12       0.825301   \n",
       "5  0.059808    0.018301  0.271028  1.818989e-12       0.795181   \n",
       "6  0.059231    0.017993  0.276190  1.818989e-12       0.783133   \n",
       "7  0.061984    0.018658  0.263636  1.818989e-12       0.813253   \n",
       "8  0.059869    0.019516  0.308511  1.818989e-12       0.716867   \n",
       "\n",
       "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
       "0        0.891566        0.805970              1.0     0.650602      0.891566   \n",
       "1        0.891566        0.915254              1.0     0.650602      0.891566   \n",
       "2        0.891566        0.739726              1.0     0.650602      0.891566   \n",
       "3        0.891566        1.000000              1.0     0.650602      0.891566   \n",
       "4        0.891566        1.000000              1.0     0.650602      0.891566   \n",
       "5        0.891566        0.915254              1.0     0.650602      0.891566   \n",
       "6        0.891566        0.885246              1.0     0.650602      0.891566   \n",
       "7        0.891566        0.964286              1.0     0.650602      0.891566   \n",
       "8        0.891566        0.750000              1.0     0.650602      0.891566   \n",
       "\n",
       "   owner  \n",
       "0     15  \n",
       "1     15  \n",
       "2     15  \n",
       "3     15  \n",
       "4     15  \n",
       "5     15  \n",
       "6     15  \n",
       "7     15  \n",
       "8     15  ,\n",
       "          fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
       "0  0.059404    0.018015  0.336538  1.818989e-12       0.704819   \n",
       "1  0.059214    0.018256  0.299145  1.818989e-12       0.783133   \n",
       "2  0.059422    0.018110  0.524752  1.818989e-12       0.469880   \n",
       "3  0.059229    0.018095  0.315315  1.818989e-12       0.746988   \n",
       "4  0.059018    0.017700  0.296610  1.818989e-12       0.789157   \n",
       "5  0.059138    0.018115  0.339806  1.818989e-12       0.698795   \n",
       "6  0.059104    0.017904  0.330189  1.818989e-12       0.716867   \n",
       "7  0.059110    0.018101  0.301724  1.818989e-12       0.777108   \n",
       "8  0.059554    0.018196  0.307018  1.818989e-12       0.765060   \n",
       "\n",
       "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
       "0        0.891566        0.774194              1.0     0.578313      0.891566   \n",
       "1        0.891566        0.979592              1.0     0.578313      0.891566   \n",
       "2        0.891566        0.475248              1.0     0.578313      0.891566   \n",
       "3        0.891566        0.872727              1.0     0.578313      0.891566   \n",
       "4        0.891566        1.000000              1.0     0.578313      0.891566   \n",
       "5        0.891566        0.761905              1.0     0.578313      0.891566   \n",
       "6        0.891566        0.800000              1.0     0.578313      0.891566   \n",
       "7        0.891566        0.960000              1.0     0.578313      0.891566   \n",
       "8        0.891566        0.923077              1.0     0.578313      0.891566   \n",
       "\n",
       "   owner  \n",
       "0     10  \n",
       "1     10  \n",
       "2     10  \n",
       "3     10  \n",
       "4     10  \n",
       "5     10  \n",
       "6     10  \n",
       "7     10  \n",
       "8     10  ,\n",
       "          fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
       "0  0.062664    0.018142  0.420455  1.818989e-12       0.584337   \n",
       "1  0.060006    0.018237  0.288288  1.818989e-12       0.783133   \n",
       "2  0.059803    0.018153  0.376471  1.818989e-12       0.626506   \n",
       "3  0.059381    0.017782  0.278261  1.818989e-12       0.807229   \n",
       "4  0.059910    0.018105  0.293578  1.818989e-12       0.771084   \n",
       "5  0.059805    0.018044  0.283186  1.818989e-12       0.795181   \n",
       "6  0.060704    0.018189  0.355556  1.818989e-12       0.656627   \n",
       "7  0.059894    0.018111  0.344086  1.818989e-12       0.674699   \n",
       "8  0.060009    0.018349  0.336842  1.818989e-12       0.686747   \n",
       "\n",
       "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
       "0        0.891566        0.579545              1.0     0.614458      0.891566   \n",
       "1        0.891566        0.927273              1.0     0.614458      0.891566   \n",
       "2        0.891566        0.629630              1.0     0.614458      0.891566   \n",
       "3        0.891566        1.000000              1.0     0.614458      0.891566   \n",
       "4        0.891566        0.894737              1.0     0.614458      0.891566   \n",
       "5        0.891566        0.962264              1.0     0.614458      0.891566   \n",
       "6        0.891566        0.671053              1.0     0.614458      0.891566   \n",
       "7        0.891566        0.698630              1.0     0.614458      0.891566   \n",
       "8        0.891566        0.718310              1.0     0.614458      0.891566   \n",
       "\n",
       "   owner  \n",
       "0      4  \n",
       "1      4  \n",
       "2      4  \n",
       "3      4  \n",
       "4      4  \n",
       "5      4  \n",
       "6      4  \n",
       "7      4  \n",
       "8      4  ], dtype=object)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060484    0.018193  0.342105  ...     0.903614      0.891566      9\n",
      "1  0.060284    0.018091  0.318182  ...     0.903614      0.891566      9\n",
      "2  0.059676    0.017817  0.468085  ...     0.903614      0.891566      9\n",
      "3  0.059575    0.017673  0.440299  ...     0.903614      0.891566      9\n",
      "4  0.059599    0.017560  0.318182  ...     0.903614      0.891566      9\n",
      "5  0.059345    0.017869  0.147727  ...     0.903614      0.891566      9\n",
      "6  0.059538    0.017788  0.157303  ...     0.903614      0.891566      9\n",
      "7  0.059699    0.017746  0.299065  ...     0.903614      0.891566      9\n",
      "8  0.060037    0.017734  0.358974  ...     0.903614      0.891566      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060143    0.017807  0.342105  ...     0.903614      0.891566      9\n",
      "1  0.059403    0.017803  0.318182  ...     0.903614      0.891566      9\n",
      "2  0.059162    0.017710  0.468085  ...     0.903614      0.891566      9\n",
      "3  0.059485    0.017694  0.440299  ...     0.903614      0.891566      9\n",
      "4  0.059443    0.017584  0.318182  ...     0.903614      0.891566      9\n",
      "5  0.059705    0.017881  0.147727  ...     0.903614      0.891566      9\n",
      "6  0.059339    0.017975  0.157303  ...     0.903614      0.891566      9\n",
      "7  0.059047    0.017851  0.299065  ...     0.903614      0.891566      9\n",
      "8  0.059297    0.017702  0.358974  ...     0.903614      0.891566      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.059566    0.018262  0.308411  ...      0.60241      0.891566     15\n",
      "1  0.059742    0.017755  0.302752  ...      0.60241      0.891566     15\n",
      "2  0.059139    0.017755  0.351064  ...      0.60241      0.891566     15\n",
      "3  0.059343    0.017509  0.284483  ...      0.60241      0.891566     15\n",
      "4  0.059330    0.017387  0.284483  ...      0.60241      0.891566     15\n",
      "5  0.059411    0.017858  0.330000  ...      0.60241      0.891566     15\n",
      "6  0.060562    0.017792  0.292035  ...      0.60241      0.891566     15\n",
      "7  0.059588    0.017932  0.289474  ...      0.60241      0.891566     15\n",
      "8  0.059290    0.017813  0.343750  ...      0.60241      0.891566     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.061417    0.018764  0.184783  ...     0.795181      0.891566      2\n",
      "1  0.060809    0.018514  0.188889  ...     0.795181      0.891566      2\n",
      "2  0.060547    0.018314  0.184783  ...     0.795181      0.891566      2\n",
      "3  0.060289    0.018165  0.171717  ...     0.795181      0.891566      2\n",
      "4  0.060081    0.018123  0.171717  ...     0.795181      0.891566      2\n",
      "5  0.060088    0.018785  0.241379  ...     0.795181      0.891566      2\n",
      "6  0.060219    0.017954  0.352941  ...     0.795181      0.891566      2\n",
      "7  0.060005    0.018173  0.182796  ...     0.795181      0.891566      2\n",
      "8  0.060084    0.017996  0.214286  ...     0.795181      0.891566      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.061136    0.018022  0.282828  ...     0.855422      0.891566      0\n",
      "1  0.060267    0.017963  0.154762  ...     0.855422      0.891566      0\n",
      "2  0.059968    0.018206  0.127660  ...     0.855422      0.891566      0\n",
      "3  0.060006    0.017948  0.129032  ...     0.855422      0.891566      0\n",
      "4  0.059811    0.018978  0.260417  ...     0.855422      0.891566      0\n",
      "5  0.059835    0.017975  0.244681  ...     0.855422      0.891566      0\n",
      "6  0.060245    0.017851  0.303922  ...     0.855422      0.891566      0\n",
      "7  0.059990    0.017790  0.126316  ...     0.855422      0.891566      0\n",
      "8  0.059918    0.018049  0.244681  ...     0.855422      0.891566      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060485    0.017954  0.290598  ...     0.590361      0.891566     18\n",
      "1  0.060352    0.018287  0.295652  ...     0.590361      0.891566     18\n",
      "2  0.059879    0.018277  0.293103  ...     0.590361      0.891566     18\n",
      "3  0.060521    0.017855  0.290598  ...     0.590361      0.891566     18\n",
      "4  0.060624    0.018139  0.298246  ...     0.590361      0.891566     18\n",
      "5  0.060539    0.018974  0.290598  ...     0.590361      0.891566     18\n",
      "6  0.060182    0.017959  0.290598  ...     0.590361      0.891566     18\n",
      "7  0.060312    0.018103  0.290598  ...     0.590361      0.891566     18\n",
      "8  0.059711    0.018183  0.295652  ...     0.590361      0.891566     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.059834    0.018032  0.284404  ...     0.626506      0.891566     10\n",
      "1  0.059675    0.017699  0.271930  ...     0.626506      0.891566     10\n",
      "2  0.059635    0.017819  0.458333  ...     0.626506      0.891566     10\n",
      "3  0.059446    0.018049  0.298077  ...     0.626506      0.891566     10\n",
      "4  0.059469    0.018136  0.276786  ...     0.626506      0.891566     10\n",
      "5  0.059595    0.018015  0.276786  ...     0.626506      0.891566     10\n",
      "6  0.059466    0.018023  0.295238  ...     0.626506      0.891566     10\n",
      "7  0.059993    0.018094  0.279279  ...     0.626506      0.891566     10\n",
      "8  0.060016    0.018126  0.292453  ...     0.626506      0.891566     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060917    0.018348  0.305882  ...     0.710843      0.891566     15\n",
      "1  0.062328    0.018147  0.372340  ...     0.710843      0.891566     15\n",
      "2  0.060247    0.018043  0.397959  ...     0.710843      0.891566     15\n",
      "3  0.060152    0.017978  0.230769  ...     0.710843      0.891566     15\n",
      "4  0.059867    0.017594  0.224299  ...     0.710843      0.891566     15\n",
      "5  0.059785    0.017945  0.378947  ...     0.710843      0.891566     15\n",
      "6  0.059980    0.017972  0.233010  ...     0.710843      0.891566     15\n",
      "7  0.059768    0.018315  0.275862  ...     0.710843      0.891566     15\n",
      "8  0.060020    0.017973  0.421569  ...     0.710843      0.891566     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060610    0.017807  0.381679  ...      0.39759      0.891566     12\n",
      "1  0.061816    0.017695  0.446429  ...      0.39759      0.891566     12\n",
      "2  0.059625    0.017476  0.375940  ...      0.39759      0.891566     12\n",
      "3  0.059791    0.017580  0.375940  ...      0.39759      0.891566     12\n",
      "4  0.060312    0.017826  0.679612  ...      0.39759      0.891566     12\n",
      "5  0.059634    0.017546  0.375940  ...      0.39759      0.891566     12\n",
      "6  0.059249    0.017824  0.450450  ...      0.39759      0.891566     12\n",
      "7  0.059283    0.017869  0.375940  ...      0.39759      0.891566     12\n",
      "8  0.059432    0.017966  0.485437  ...      0.39759      0.891566     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060282    0.018131  0.296610  ...     0.578313      0.891566     23\n",
      "1  0.060171    0.018065  0.304348  ...     0.578313      0.891566     23\n",
      "2  0.060695    0.018699  0.411765  ...     0.578313      0.891566     23\n",
      "3  0.061355    0.017815  0.296610  ...     0.578313      0.891566     23\n",
      "4  0.059798    0.017989  0.296610  ...     0.578313      0.891566     23\n",
      "5  0.059588    0.018884  0.296610  ...     0.578313      0.891566     23\n",
      "6  0.059422    0.018263  0.307018  ...     0.578313      0.891566     23\n",
      "7  0.059663    0.017863  0.296610  ...     0.578313      0.891566     23\n",
      "8  0.059393    0.017832  0.296610  ...     0.578313      0.891566     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.059538    0.017950  0.430000  ...     0.686747      0.891566      4\n",
      "1  0.059937    0.018059  0.313253  ...     0.686747      0.891566      4\n",
      "2  0.059434    0.018153  0.268041  ...     0.686747      0.891566      4\n",
      "3  0.059311    0.018023  0.240741  ...     0.686747      0.891566      4\n",
      "4  0.059363    0.017987  0.240741  ...     0.686747      0.891566      4\n",
      "5  0.059600    0.018081  0.254902  ...     0.686747      0.891566      4\n",
      "6  0.059305    0.017967  0.276596  ...     0.686747      0.891566      4\n",
      "7  0.059572    0.017998  0.512821  ...     0.686747      0.891566      4\n",
      "8  0.059232    0.018336  0.247619  ...     0.686747      0.891566      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060807    0.018507  0.195652  ...     0.783133      0.891566     14\n",
      "1  0.060609    0.018465  0.211765  ...     0.783133      0.891566     14\n",
      "2  0.060036    0.018100  0.180000  ...     0.783133      0.891566     14\n",
      "3  0.059894    0.018066  0.202247  ...     0.783133      0.891566     14\n",
      "4  0.059550    0.017782  0.492188  ...     0.783133      0.891566     14\n",
      "5  0.059799    0.018084  0.180000  ...     0.783133      0.891566     14\n",
      "6  0.059701    0.017929  0.204545  ...     0.783133      0.891566     14\n",
      "7  0.059551    0.018298  0.183673  ...     0.783133      0.891566     14\n",
      "8  0.059864    0.018280  0.214286  ...     0.783133      0.891566     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060045    0.018050  0.195652  ...     0.783133      0.891566     14\n",
      "1  0.060014    0.018194  0.211765  ...     0.783133      0.891566     14\n",
      "2  0.059719    0.017995  0.180000  ...     0.783133      0.891566     14\n",
      "3  0.059781    0.017920  0.202247  ...     0.783133      0.891566     14\n",
      "4  0.059550    0.017688  0.492188  ...     0.783133      0.891566     14\n",
      "5  0.059020    0.017840  0.180000  ...     0.783133      0.891566     14\n",
      "6  0.059366    0.018032  0.204545  ...     0.783133      0.891566     14\n",
      "7  0.059267    0.018035  0.183673  ...     0.783133      0.891566     14\n",
      "8  0.059410    0.017987  0.214286  ...     0.783133      0.891566     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.059568    0.018649  0.244898  ...     0.710843      0.891566      2\n",
      "1  0.060126    0.018218  0.226415  ...     0.710843      0.891566      2\n",
      "2  0.059507    0.018055  0.237624  ...     0.710843      0.891566      2\n",
      "3  0.059468    0.017670  0.224299  ...     0.710843      0.891566      2\n",
      "4  0.059683    0.017484  0.224299  ...     0.710843      0.891566      2\n",
      "5  0.059433    0.018151  0.228571  ...     0.710843      0.891566      2\n",
      "6  0.073564    0.018494  0.279070  ...     0.710843      0.891566      2\n",
      "7  0.060617    0.018678  0.235294  ...     0.710843      0.891566      2\n",
      "8  0.061580    0.018736  0.233010  ...     0.710843      0.891566      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.061767    0.021953  0.304762  ...     0.614458      0.891566     10\n",
      "1  0.061245    0.018571  0.359551  ...     0.614458      0.891566     10\n",
      "2  0.060751    0.018197  0.504854  ...     0.614458      0.891566     10\n",
      "3  0.060501    0.018283  0.304762  ...     0.614458      0.891566     10\n",
      "4  0.060356    0.018415  0.288288  ...     0.614458      0.891566     10\n",
      "5  0.060576    0.018184  0.540541  ...     0.614458      0.891566     10\n",
      "6  0.060675    0.018191  0.457447  ...     0.614458      0.891566     10\n",
      "7  0.061079    0.018431  0.439560  ...     0.614458      0.891566     10\n",
      "8  0.060975    0.018178  0.433333  ...     0.614458      0.891566     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060432    0.018612  0.195652  ...     0.783133      0.891566     14\n",
      "1  0.060560    0.018331  0.211765  ...     0.783133      0.891566     14\n",
      "2  0.060602    0.018372  0.180000  ...     0.783133      0.891566     14\n",
      "3  0.061044    0.018393  0.202247  ...     0.783133      0.891566     14\n",
      "4  0.065442    0.020007  0.492188  ...     0.783133      0.891566     14\n",
      "5  0.060687    0.019227  0.180000  ...     0.783133      0.891566     14\n",
      "6  0.063418    0.019074  0.204545  ...     0.783133      0.891566     14\n",
      "7  0.060045    0.018444  0.183673  ...     0.783133      0.891566     14\n",
      "8  0.059935    0.018258  0.214286  ...     0.783133      0.891566     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060834    0.018353  0.305882  ...     0.710843      0.891566     15\n",
      "1  0.060047    0.018255  0.372340  ...     0.710843      0.891566     15\n",
      "2  0.059872    0.018075  0.397959  ...     0.710843      0.891566     15\n",
      "3  0.060314    0.018072  0.230769  ...     0.710843      0.891566     15\n",
      "4  0.060297    0.017692  0.224299  ...     0.710843      0.891566     15\n",
      "5  0.059933    0.018115  0.378947  ...     0.710843      0.891566     15\n",
      "6  0.059582    0.018137  0.233010  ...     0.710843      0.891566     15\n",
      "7  0.059876    0.018232  0.275862  ...     0.710843      0.891566     15\n",
      "8  0.059723    0.018011  0.421569  ...     0.710843      0.891566     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060131    0.018109  0.417582  ...     0.542169      0.891566     12\n",
      "1  0.059961    0.017890  0.316667  ...     0.542169      0.891566     12\n",
      "2  0.075401    0.021030  0.314050  ...     0.542169      0.891566     12\n",
      "3  0.061769    0.018859  0.322034  ...     0.542169      0.891566     12\n",
      "4  0.059657    0.018126  0.531250  ...     0.542169      0.891566     12\n",
      "5  0.060450    0.017917  0.314050  ...     0.542169      0.891566     12\n",
      "6  0.060059    0.018141  0.441860  ...     0.542169      0.891566     12\n",
      "7  0.059693    0.017836  0.314050  ...     0.542169      0.891566     12\n",
      "8  0.059432    0.018177  0.510870  ...     0.542169      0.891566     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.061697    0.019203  0.285714  ...     0.662651      0.891566      4\n",
      "1  0.061386    0.018980  0.321839  ...     0.662651      0.891566      4\n",
      "2  0.061394    0.018748  0.297872  ...     0.662651      0.891566      4\n",
      "3  0.061059    0.018470  0.254545  ...     0.662651      0.891566      4\n",
      "4  0.060786    0.018579  0.256881  ...     0.662651      0.891566      4\n",
      "5  0.061116    0.018854  0.291667  ...     0.662651      0.891566      4\n",
      "6  0.060786    0.018726  0.259259  ...     0.662651      0.891566      4\n",
      "7  0.060989    0.018710  0.490741  ...     0.662651      0.891566      4\n",
      "8  0.061136    0.018497  0.254545  ...     0.662651      0.891566      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060712    0.018027  0.342105  ...     0.903614      0.891566      9\n",
      "1  0.060317    0.018217  0.318182  ...     0.903614      0.891566      9\n",
      "2  0.060363    0.018201  0.468085  ...     0.903614      0.891566      9\n",
      "3  0.060627    0.018250  0.440299  ...     0.903614      0.891566      9\n",
      "4  0.061944    0.018229  0.318182  ...     0.903614      0.891566      9\n",
      "5  0.060522    0.018193  0.147727  ...     0.903614      0.891566      9\n",
      "6  0.060463    0.018340  0.157303  ...     0.903614      0.891566      9\n",
      "7  0.060302    0.018208  0.299065  ...     0.903614      0.891566      9\n",
      "8  0.060572    0.018333  0.358974  ...     0.903614      0.891566      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.061232    0.019068  0.184783  ...     0.795181      0.891566      2\n",
      "1  0.060846    0.018534  0.188889  ...     0.795181      0.891566      2\n",
      "2  0.061343    0.018487  0.184783  ...     0.795181      0.891566      2\n",
      "3  0.060606    0.018344  0.171717  ...     0.795181      0.891566      2\n",
      "4  0.060672    0.018359  0.171717  ...     0.795181      0.891566      2\n",
      "5  0.061853    0.018407  0.241379  ...     0.795181      0.891566      2\n",
      "6  0.060637    0.018627  0.352941  ...     0.795181      0.891566      2\n",
      "7  0.060452    0.018662  0.182796  ...     0.795181      0.891566      2\n",
      "8  0.060385    0.018506  0.214286  ...     0.795181      0.891566      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.065803    0.018262  0.144330  ...     0.831325      0.891566      0\n",
      "1  0.060965    0.018635  0.150538  ...     0.831325      0.891566      0\n",
      "2  0.067174    0.018450  0.144330  ...     0.831325      0.891566      0\n",
      "3  0.060556    0.018449  0.144330  ...     0.831325      0.891566      0\n",
      "4  0.060524    0.018452  0.144330  ...     0.831325      0.891566      0\n",
      "5  0.060559    0.018595  0.429752  ...     0.831325      0.891566      0\n",
      "6  0.070467    0.022813  0.147368  ...     0.831325      0.891566      0\n",
      "7  0.060844    0.018212  0.144330  ...     0.831325      0.891566      0\n",
      "8  0.060445    0.018604  0.147368  ...     0.831325      0.891566      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.061533    0.018337  0.381679  ...      0.39759      0.891566     12\n",
      "1  0.060376    0.018014  0.446429  ...      0.39759      0.891566     12\n",
      "2  0.060030    0.017549  0.375940  ...      0.39759      0.891566     12\n",
      "3  0.060630    0.017615  0.375940  ...      0.39759      0.891566     12\n",
      "4  0.059803    0.018104  0.679612  ...      0.39759      0.891566     12\n",
      "5  0.059745    0.017594  0.375940  ...      0.39759      0.891566     12\n",
      "6  0.059916    0.017727  0.450450  ...      0.39759      0.891566     12\n",
      "7  0.059757    0.017467  0.375940  ...      0.39759      0.891566     12\n",
      "8  0.059768    0.017675  0.485437  ...      0.39759      0.891566     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060585    0.017992  0.290598  ...     0.590361      0.891566     18\n",
      "1  0.060013    0.018413  0.295652  ...     0.590361      0.891566     18\n",
      "2  0.059756    0.018142  0.293103  ...     0.590361      0.891566     18\n",
      "3  0.060721    0.017915  0.290598  ...     0.590361      0.891566     18\n",
      "4  0.060032    0.027388  0.298246  ...     0.590361      0.891566     18\n",
      "5  0.062327    0.018046  0.290598  ...     0.590361      0.891566     18\n",
      "6  0.059809    0.018029  0.290598  ...     0.590361      0.891566     18\n",
      "7  0.059862    0.017852  0.290598  ...     0.590361      0.891566     18\n",
      "8  0.059628    0.018138  0.295652  ...     0.590361      0.891566     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060720    0.018777  0.304762  ...     0.614458      0.891566     10\n",
      "1  0.060870    0.018251  0.359551  ...     0.614458      0.891566     10\n",
      "2  0.060221    0.018218  0.504854  ...     0.614458      0.891566     10\n",
      "3  0.059952    0.018174  0.304762  ...     0.614458      0.891566     10\n",
      "4  0.060248    0.018222  0.288288  ...     0.614458      0.891566     10\n",
      "5  0.059661    0.018056  0.540541  ...     0.614458      0.891566     10\n",
      "6  0.059798    0.017987  0.457447  ...     0.614458      0.891566     10\n",
      "7  0.059551    0.017915  0.439560  ...     0.614458      0.891566     10\n",
      "8  0.059714    0.017982  0.433333  ...     0.614458      0.891566     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060068    0.018012  0.310345  ...     0.566265      0.891566     14\n",
      "1  0.060163    0.017857  0.302521  ...     0.566265      0.891566     14\n",
      "2  0.061325    0.017937  0.302521  ...     0.566265      0.891566     14\n",
      "3  0.060673    0.017758  0.302521  ...     0.566265      0.891566     14\n",
      "4  0.060328    0.018151  0.371134  ...     0.566265      0.891566     14\n",
      "5  0.059621    0.018303  0.305085  ...     0.566265      0.891566     14\n",
      "6  0.059870    0.018134  0.307692  ...     0.566265      0.891566     14\n",
      "7  0.059983    0.017973  0.302521  ...     0.566265      0.891566     14\n",
      "8  0.059547    0.017932  0.305085  ...     0.566265      0.891566     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.062440    0.018672  0.282828  ...     0.855422      0.891566      0\n",
      "1  0.061211    0.018503  0.154762  ...     0.855422      0.891566      0\n",
      "2  0.062291    0.018507  0.127660  ...     0.855422      0.891566      0\n",
      "3  0.099501    0.022111  0.129032  ...     0.855422      0.891566      0\n",
      "4  0.060240    0.018124  0.260417  ...     0.855422      0.891566      0\n",
      "5  0.060346    0.018205  0.244681  ...     0.855422      0.891566      0\n",
      "6  0.060131    0.018014  0.303922  ...     0.855422      0.891566      0\n",
      "7  0.059999    0.017824  0.126316  ...     0.855422      0.891566      0\n",
      "8  0.060228    0.018308  0.244681  ...     0.855422      0.891566      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060769    0.018189  0.296610  ...     0.578313      0.891566     23\n",
      "1  0.061329    0.018373  0.304348  ...     0.578313      0.891566     23\n",
      "2  0.060556    0.018446  0.411765  ...     0.578313      0.891566     23\n",
      "3  0.061219    0.025841  0.296610  ...     0.578313      0.891566     23\n",
      "4  0.065817    0.019541  0.296610  ...     0.578313      0.891566     23\n",
      "5  0.060630    0.017997  0.296610  ...     0.578313      0.891566     23\n",
      "6  0.060205    0.018347  0.307018  ...     0.578313      0.891566     23\n",
      "7  0.060333    0.017963  0.296610  ...     0.578313      0.891566     23\n",
      "8  0.060075    0.017834  0.296610  ...     0.578313      0.891566     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060523    0.018676  0.285714  ...     0.662651      0.891566      4\n",
      "1  0.060149    0.018304  0.321839  ...     0.662651      0.891566      4\n",
      "2  0.059965    0.018141  0.297872  ...     0.662651      0.891566      4\n",
      "3  0.060044    0.018092  0.254545  ...     0.662651      0.891566      4\n",
      "4  0.061214    0.018370  0.256881  ...     0.662651      0.891566      4\n",
      "5  0.059855    0.018446  0.291667  ...     0.662651      0.891566      4\n",
      "6  0.059900    0.018149  0.259259  ...     0.662651      0.891566      4\n",
      "7  0.059639    0.017985  0.490741  ...     0.662651      0.891566      4\n",
      "8  0.059911    0.018265  0.254545  ...     0.662651      0.891566      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060046    0.018093  0.223529  ...     0.771084      0.891566      9\n",
      "1  0.059865    0.017715  0.186275  ...     0.771084      0.891566      9\n",
      "2  0.059687    0.017765  0.288889  ...     0.771084      0.891566      9\n",
      "3  0.060715    0.018267  0.218391  ...     0.771084      0.891566      9\n",
      "4  0.060189    0.018532  0.190000  ...     0.771084      0.891566      9\n",
      "5  0.063985    0.018932  0.206522  ...     0.771084      0.891566      9\n",
      "6  0.060353    0.017864  0.186275  ...     0.771084      0.891566      9\n",
      "7  0.060114    0.018071  0.326316  ...     0.771084      0.891566      9\n",
      "8  0.059730    0.018066  0.193878  ...     0.771084      0.891566      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.065801    0.019292  0.195652  ...     0.783133      0.891566     14\n",
      "1  0.064180    0.019040  0.211765  ...     0.783133      0.891566     14\n",
      "2  0.060758    0.018386  0.180000  ...     0.783133      0.891566     14\n",
      "3  0.064987    0.018346  0.202247  ...     0.783133      0.891566     14\n",
      "4  0.060908    0.018104  0.492188  ...     0.783133      0.891566     14\n",
      "5  0.060863    0.018468  0.180000  ...     0.783133      0.891566     14\n",
      "6  0.061499    0.018607  0.204545  ...     0.783133      0.891566     14\n",
      "7  0.061270    0.018234  0.183673  ...     0.783133      0.891566     14\n",
      "8  0.060972    0.018219  0.214286  ...     0.783133      0.891566     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.061783    0.018285  0.305882  ...     0.710843      0.891566     15\n",
      "1  0.061432    0.018232  0.372340  ...     0.710843      0.891566     15\n",
      "2  0.060684    0.017942  0.397959  ...     0.710843      0.891566     15\n",
      "3  0.060122    0.032870  0.230769  ...     0.710843      0.891566     15\n",
      "4  0.069273    0.019063  0.224299  ...     0.710843      0.891566     15\n",
      "5  0.061412    0.017953  0.378947  ...     0.710843      0.891566     15\n",
      "6  0.060399    0.018260  0.233010  ...     0.710843      0.891566     15\n",
      "7  0.059659    0.018239  0.275862  ...     0.710843      0.891566     15\n",
      "8  0.059922    0.018162  0.421569  ...     0.710843      0.891566     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060061    0.017967  0.500000  ...     0.626506      0.891566     12\n",
      "1  0.059948    0.017771  0.551724  ...     0.626506      0.891566     12\n",
      "2  0.059610    0.017691  0.276786  ...     0.626506      0.891566     12\n",
      "3  0.059626    0.017840  0.287037  ...     0.626506      0.891566     12\n",
      "4  0.060121    0.023168  0.600000  ...     0.626506      0.891566     12\n",
      "5  0.059470    0.018075  0.356322  ...     0.626506      0.891566     12\n",
      "6  0.059274    0.017965  0.543860  ...     0.626506      0.891566     12\n",
      "7  0.059232    0.017803  0.333333  ...     0.626506      0.891566     12\n",
      "8  0.059152    0.017691  0.509434  ...     0.626506      0.891566     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060515    0.017830  0.178218  ...     0.783133      0.891566     18\n",
      "1  0.060087    0.017713  0.178218  ...     0.783133      0.891566     18\n",
      "2  0.059765    0.017569  0.178218  ...     0.783133      0.891566     18\n",
      "3  0.060041    0.017802  0.178218  ...     0.783133      0.891566     18\n",
      "4  0.060394    0.017604  0.178218  ...     0.783133      0.891566     18\n",
      "5  0.060274    0.017455  0.178218  ...     0.783133      0.891566     18\n",
      "6  0.059944    0.017434  0.178218  ...     0.783133      0.891566     18\n",
      "7  0.060264    0.017616  0.178218  ...     0.783133      0.891566     18\n",
      "8  0.059571    0.017812  0.178218  ...     0.783133      0.891566     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.061243    0.018116  0.342105  ...     0.903614      0.891566      9\n",
      "1  0.060399    0.017830  0.318182  ...     0.903614      0.891566      9\n",
      "2  0.060328    0.017760  0.468085  ...     0.903614      0.891566      9\n",
      "3  0.063144    0.017694  0.440299  ...     0.903614      0.891566      9\n",
      "4  0.059585    0.017693  0.318182  ...     0.903614      0.891566      9\n",
      "5  0.059776    0.017768  0.147727  ...     0.903614      0.891566      9\n",
      "6  0.059546    0.017869  0.157303  ...     0.903614      0.891566      9\n",
      "7  0.059567    0.017718  0.299065  ...     0.903614      0.891566      9\n",
      "8  0.059727    0.017743  0.358974  ...     0.903614      0.891566      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.061455    0.018431  0.184783  ...     0.795181      0.891566      2\n",
      "1  0.060624    0.018316  0.188889  ...     0.795181      0.891566      2\n",
      "2  0.060059    0.018401  0.184783  ...     0.795181      0.891566      2\n",
      "3  0.060145    0.018221  0.171717  ...     0.795181      0.891566      2\n",
      "4  0.067901    0.025966  0.171717  ...     0.795181      0.891566      2\n",
      "5  0.061275    0.018166  0.241379  ...     0.795181      0.891566      2\n",
      "6  0.060125    0.018117  0.352941  ...     0.795181      0.891566      2\n",
      "7  0.059837    0.018096  0.182796  ...     0.795181      0.891566      2\n",
      "8  0.059631    0.017886  0.214286  ...     0.795181      0.891566      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060651    0.018600  0.177083  ...     0.795181      0.891566      0\n",
      "1  0.060179    0.018405  0.186813  ...     0.795181      0.891566      0\n",
      "2  0.060231    0.017870  0.170000  ...     0.795181      0.891566      0\n",
      "3  0.059979    0.017775  0.170000  ...     0.795181      0.891566      0\n",
      "4  0.059921    0.018141  0.195402  ...     0.795181      0.891566      0\n",
      "5  0.085894    0.019388  0.197674  ...     0.795181      0.891566      0\n",
      "6  0.061049    0.018147  0.175258  ...     0.795181      0.891566      0\n",
      "7  0.060743    0.017838  0.170000  ...     0.795181      0.891566      0\n",
      "8  0.061876    0.018230  0.182796  ...     0.795181      0.891566      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060240    0.017689  0.186275  ...     0.771084      0.891566     23\n",
      "1  0.060281    0.017800  0.186275  ...     0.771084      0.891566     23\n",
      "2  0.059925    0.017810  0.423423  ...     0.771084      0.891566     23\n",
      "3  0.059820    0.017658  0.186275  ...     0.771084      0.891566     23\n",
      "4  0.059620    0.017756  0.186275  ...     0.771084      0.891566     23\n",
      "5  0.059597    0.017615  0.186275  ...     0.771084      0.891566     23\n",
      "6  0.059765    0.018169  0.200000  ...     0.771084      0.891566     23\n",
      "7  0.060038    0.017790  0.186275  ...     0.771084      0.891566     23\n",
      "8  0.060295    0.018016  0.190000  ...     0.771084      0.891566     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060895    0.018254  0.290598  ...     0.590361      0.891566     18\n",
      "1  0.060559    0.018330  0.295652  ...     0.590361      0.891566     18\n",
      "2  0.060383    0.018449  0.293103  ...     0.590361      0.891566     18\n",
      "3  0.062615    0.017875  0.290598  ...     0.590361      0.891566     18\n",
      "4  0.060138    0.018147  0.298246  ...     0.590361      0.891566     18\n",
      "5  0.059968    0.017678  0.290598  ...     0.590361      0.891566     18\n",
      "6  0.059564    0.017825  0.290598  ...     0.590361      0.891566     18\n",
      "7  0.059761    0.017725  0.290598  ...     0.590361      0.891566     18\n",
      "8  0.059881    0.018255  0.295652  ...     0.590361      0.891566     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.062084    0.018198  0.304762  ...     0.614458      0.891566     10\n",
      "1  0.059023    0.018177  0.359551  ...     0.614458      0.891566     10\n",
      "2  0.059502    0.018084  0.504854  ...     0.614458      0.891566     10\n",
      "3  0.060476    0.018018  0.304762  ...     0.614458      0.891566     10\n",
      "4  0.065703    0.020156  0.288288  ...     0.614458      0.891566     10\n",
      "5  0.059922    0.017986  0.540541  ...     0.614458      0.891566     10\n",
      "6  0.059460    0.017956  0.457447  ...     0.614458      0.891566     10\n",
      "7  0.059067    0.018075  0.439560  ...     0.614458      0.891566     10\n",
      "8  0.059805    0.018149  0.433333  ...     0.614458      0.891566     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060314    0.018728  0.252336  ...     0.674699      0.891566     14\n",
      "1  0.060104    0.018251  0.262136  ...     0.674699      0.891566     14\n",
      "2  0.059818    0.017836  0.245455  ...     0.674699      0.891566     14\n",
      "3  0.060056    0.018201  0.259615  ...     0.674699      0.891566     14\n",
      "4  0.064551    0.018054  0.504425  ...     0.674699      0.891566     14\n",
      "5  0.080340    0.018896  0.247706  ...     0.674699      0.891566     14\n",
      "6  0.061506    0.018760  0.254717  ...     0.674699      0.891566     14\n",
      "7  0.059594    0.017955  0.245455  ...     0.674699      0.891566     14\n",
      "8  0.059802    0.018507  0.259615  ...     0.674699      0.891566     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060701    0.018953  0.220000  ...      0.73494      0.891566      2\n",
      "1  0.059789    0.018053  0.211538  ...      0.73494      0.891566      2\n",
      "2  0.059557    0.017634  0.209524  ...      0.73494      0.891566      2\n",
      "3  0.059609    0.017746  0.209524  ...      0.73494      0.891566      2\n",
      "4  0.058990    0.017590  0.209524  ...      0.73494      0.891566      2\n",
      "5  0.059417    0.017669  0.209524  ...      0.73494      0.891566      2\n",
      "6  0.061876    0.019675  0.217822  ...      0.73494      0.891566      2\n",
      "7  0.059311    0.018068  0.213592  ...      0.73494      0.891566      2\n",
      "8  0.060865    0.019210  0.220000  ...      0.73494      0.891566      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060889    0.018544  0.305882  ...     0.710843      0.891566     15\n",
      "1  0.060377    0.018239  0.372340  ...     0.710843      0.891566     15\n",
      "2  0.064147    0.017957  0.397959  ...     0.710843      0.891566     15\n",
      "3  0.059840    0.018044  0.230769  ...     0.710843      0.891566     15\n",
      "4  0.059719    0.017643  0.224299  ...     0.710843      0.891566     15\n",
      "5  0.059730    0.018001  0.378947  ...     0.710843      0.891566     15\n",
      "6  0.059570    0.018121  0.233010  ...     0.710843      0.891566     15\n",
      "7  0.059775    0.018264  0.275862  ...     0.710843      0.891566     15\n",
      "8  0.060062    0.018272  0.421569  ...     0.710843      0.891566     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.061163    0.018300  0.381679  ...      0.39759      0.891566     12\n",
      "1  0.059803    0.018028  0.446429  ...      0.39759      0.891566     12\n",
      "2  0.059940    0.017879  0.375940  ...      0.39759      0.891566     12\n",
      "3  0.060525    0.018042  0.375940  ...      0.39759      0.891566     12\n",
      "4  0.063966    0.020687  0.679612  ...      0.39759      0.891566     12\n",
      "5  0.061461    0.018201  0.375940  ...      0.39759      0.891566     12\n",
      "6  0.065437    0.018024  0.450450  ...      0.39759      0.891566     12\n",
      "7  0.059707    0.017610  0.375940  ...      0.39759      0.891566     12\n",
      "8  0.059355    0.017759  0.485437  ...      0.39759      0.891566     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060236    0.018364  0.135417  ...     0.843373      0.891566     23\n",
      "1  0.059782    0.018322  0.144444  ...     0.843373      0.891566     23\n",
      "2  0.060488    0.018146  0.481481  ...     0.843373      0.891566     23\n",
      "3  0.059998    0.017702  0.135417  ...     0.843373      0.891566     23\n",
      "4  0.060547    0.018140  0.141304  ...     0.843373      0.891566     23\n",
      "5  0.064077    0.019094  0.136842  ...     0.843373      0.891566     23\n",
      "6  0.060636    0.018539  0.152941  ...     0.843373      0.891566     23\n",
      "7  0.060197    0.018161  0.135417  ...     0.843373      0.891566     23\n",
      "8  0.059920    0.018548  0.146067  ...     0.843373      0.891566     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.061059    0.018306  0.324324  ...     0.566265      0.891566     10\n",
      "1  0.059769    0.017939  0.302521  ...     0.566265      0.891566     10\n",
      "2  0.060751    0.018502  0.342857  ...     0.566265      0.891566     10\n",
      "3  0.059496    0.018283  0.333333  ...     0.566265      0.891566     10\n",
      "4  0.059752    0.018285  0.315789  ...     0.566265      0.891566     10\n",
      "5  0.061159    0.018122  0.318584  ...     0.566265      0.891566     10\n",
      "6  0.059934    0.018188  0.333333  ...     0.566265      0.891566     10\n",
      "7  0.059875    0.018043  0.310345  ...     0.566265      0.891566     10\n",
      "8  0.059703    0.018383  0.327273  ...     0.566265      0.891566     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.061186    0.018209  0.296610  ...     0.578313      0.891566     23\n",
      "1  0.060619    0.018351  0.304348  ...     0.578313      0.891566     23\n",
      "2  0.060887    0.021525  0.411765  ...     0.578313      0.891566     23\n",
      "3  0.060131    0.017847  0.296610  ...     0.578313      0.891566     23\n",
      "4  0.060110    0.017774  0.296610  ...     0.578313      0.891566     23\n",
      "5  0.059891    0.017577  0.296610  ...     0.578313      0.891566     23\n",
      "6  0.059877    0.018235  0.307018  ...     0.578313      0.891566     23\n",
      "7  0.059888    0.018107  0.296610  ...     0.578313      0.891566     23\n",
      "8  0.060032    0.017976  0.296610  ...     0.578313      0.891566     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060633    0.018413  0.285714  ...     0.662651      0.891566      4\n",
      "1  0.059742    0.018264  0.321839  ...     0.662651      0.891566      4\n",
      "2  0.059706    0.018095  0.297872  ...     0.662651      0.891566      4\n",
      "3  0.060386    0.018131  0.254545  ...     0.662651      0.891566      4\n",
      "4  0.060309    0.024235  0.256881  ...     0.662651      0.891566      4\n",
      "5  0.061188    0.018458  0.291667  ...     0.662651      0.891566      4\n",
      "6  0.059795    0.018635  0.259259  ...     0.662651      0.891566      4\n",
      "7  0.061909    0.019932  0.490741  ...     0.662651      0.891566      4\n",
      "8  0.060482    0.018562  0.254545  ...     0.662651      0.891566      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060808    0.018447  0.512346  ...     0.951807      0.891566      9\n",
      "1  0.059707    0.017840  0.509317  ...     0.951807      0.891566      9\n",
      "2  0.059620    0.017850  0.512346  ...     0.951807      0.891566      9\n",
      "3  0.059959    0.017918  0.506250  ...     0.951807      0.891566      9\n",
      "4  0.065159    0.017879  0.509317  ...     0.951807      0.891566      9\n",
      "5  0.068042    0.020417  0.490323  ...     0.951807      0.891566      9\n",
      "6  0.061554    0.018098  0.500000  ...     0.951807      0.891566      9\n",
      "7  0.059779    0.018083  0.451389  ...     0.951807      0.891566      9\n",
      "8  0.060112    0.018029  0.503145  ...     0.951807      0.891566      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060885    0.018179  0.318182  ...     0.662651      0.891566     15\n",
      "1  0.060102    0.018126  0.280000  ...     0.662651      0.891566     15\n",
      "2  0.059772    0.018104  0.345238  ...     0.662651      0.891566     15\n",
      "3  0.059712    0.018236  0.254545  ...     0.662651      0.891566     15\n",
      "4  0.060116    0.018427  0.252252  ...     0.662651      0.891566     15\n",
      "5  0.060694    0.018189  0.318182  ...     0.662651      0.891566     15\n",
      "6  0.068413    0.020494  0.271845  ...     0.662651      0.891566     15\n",
      "7  0.070349    0.018159  0.259259  ...     0.662651      0.891566     15\n",
      "8  0.067951    0.025984  0.427083  ...     0.662651      0.891566     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060761    0.018764  0.184783  ...     0.795181      0.891566      2\n",
      "1  0.060739    0.018439  0.188889  ...     0.795181      0.891566      2\n",
      "2  0.064363    0.018259  0.184783  ...     0.795181      0.891566      2\n",
      "3  0.060145    0.018173  0.171717  ...     0.795181      0.891566      2\n",
      "4  0.060145    0.018120  0.171717  ...     0.795181      0.891566      2\n",
      "5  0.059893    0.018217  0.241379  ...     0.795181      0.891566      2\n",
      "6  0.059910    0.018221  0.352941  ...     0.795181      0.891566      2\n",
      "7  0.059997    0.018430  0.182796  ...     0.795181      0.891566      2\n",
      "8  0.060149    0.018418  0.214286  ...     0.795181      0.891566      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.061153    0.018358  0.282828  ...     0.855422      0.891566      0\n",
      "1  0.059817    0.018248  0.154762  ...     0.855422      0.891566      0\n",
      "2  0.059674    0.018345  0.127660  ...     0.855422      0.891566      0\n",
      "3  0.060049    0.018501  0.129032  ...     0.855422      0.891566      0\n",
      "4  0.064586    0.020295  0.260417  ...     0.855422      0.891566      0\n",
      "5  0.061004    0.018367  0.244681  ...     0.855422      0.891566      0\n",
      "6  0.060915    0.019206  0.303922  ...     0.855422      0.891566      0\n",
      "7  0.059519    0.017942  0.126316  ...     0.855422      0.891566      0\n",
      "8  0.060010    0.018219  0.244681  ...     0.855422      0.891566      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060159    0.018758  0.182796  ...     0.795181      0.891566     18\n",
      "1  0.060203    0.018600  0.182796  ...     0.795181      0.891566     18\n",
      "2  0.060132    0.018097  0.274725  ...     0.795181      0.891566     18\n",
      "3  0.060107    0.024506  0.171717  ...     0.795181      0.891566     18\n",
      "4  0.060220    0.018391  0.204819  ...     0.795181      0.891566     18\n",
      "5  0.079936    0.020494  0.170000  ...     0.795181      0.891566     18\n",
      "6  0.060935    0.018289  0.170000  ...     0.795181      0.891566     18\n",
      "7  0.059684    0.018883  0.170000  ...     0.795181      0.891566     18\n",
      "8  0.060535    0.018616  0.180851  ...     0.795181      0.891566     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.059733    0.018639  0.216867  ...     0.783133      0.891566      4\n",
      "1  0.059618    0.028741  0.189474  ...     0.783133      0.891566      4\n",
      "2  0.060742    0.018530  0.185567  ...     0.783133      0.891566      4\n",
      "3  0.060426    0.018507  0.180000  ...     0.783133      0.891566      4\n",
      "4  0.061446    0.018556  0.181818  ...     0.783133      0.891566      4\n",
      "5  0.060015    0.018403  0.187500  ...     0.783133      0.891566      4\n",
      "6  0.061238    0.018553  0.226190  ...     0.783133      0.891566      4\n",
      "7  0.059943    0.018312  0.398148  ...     0.783133      0.891566      4\n",
      "8  0.059768    0.018401  0.185567  ...     0.783133      0.891566      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.061600    0.018762  0.304762  ...     0.614458      0.891566     10\n",
      "1  0.061260    0.021658  0.359551  ...     0.614458      0.891566     10\n",
      "2  0.060892    0.018104  0.504854  ...     0.614458      0.891566     10\n",
      "3  0.061045    0.018279  0.304762  ...     0.614458      0.891566     10\n",
      "4  0.060251    0.018314  0.288288  ...     0.614458      0.891566     10\n",
      "5  0.061140    0.018118  0.540541  ...     0.614458      0.891566     10\n",
      "6  0.060957    0.018014  0.457447  ...     0.614458      0.891566     10\n",
      "7  0.061182    0.018412  0.439560  ...     0.614458      0.891566     10\n",
      "8  0.061003    0.018007  0.433333  ...     0.614458      0.891566     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060525    0.018277  0.195652  ...     0.783133      0.891566     14\n",
      "1  0.059797    0.018177  0.211765  ...     0.783133      0.891566     14\n",
      "2  0.060396    0.018219  0.180000  ...     0.783133      0.891566     14\n",
      "3  0.059667    0.018084  0.202247  ...     0.783133      0.891566     14\n",
      "4  0.063883    0.019898  0.492188  ...     0.783133      0.891566     14\n",
      "5  0.060781    0.018316  0.180000  ...     0.783133      0.891566     14\n",
      "6  0.060438    0.018322  0.204545  ...     0.783133      0.891566     14\n",
      "7  0.059545    0.018226  0.183673  ...     0.783133      0.891566     14\n",
      "8  0.059425    0.018160  0.214286  ...     0.783133      0.891566     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.061127    0.018758  0.108434  ...     0.891566      0.891566      2\n",
      "1  0.060677    0.018220  0.102273  ...     0.891566      0.891566      2\n",
      "2  0.062511    0.019929  0.177778  ...     0.891566      0.891566      2\n",
      "3  0.060065    0.017819  0.097826  ...     0.891566      0.891566      2\n",
      "4  0.060737    0.018073  0.098901  ...     0.891566      0.891566      2\n",
      "5  0.060883    0.018403  0.168539  ...     0.891566      0.891566      2\n",
      "6  0.062548    0.018729  0.356522  ...     0.891566      0.891566      2\n",
      "7  0.059979    0.018329  0.108434  ...     0.891566      0.891566      2\n",
      "8  0.060187    0.018040  0.139535  ...     0.891566      0.891566      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060355    0.017924  0.135417  ...     0.843373      0.891566      0\n",
      "1  0.059660    0.018121  0.156627  ...     0.843373      0.891566      0\n",
      "2  0.059146    0.017721  0.135417  ...     0.843373      0.891566      0\n",
      "3  0.059037    0.017716  0.135417  ...     0.843373      0.891566      0\n",
      "4  0.059126    0.018265  0.139785  ...     0.843373      0.891566      0\n",
      "5  0.059093    0.018161  0.326923  ...     0.843373      0.891566      0\n",
      "6  0.059430    0.018186  0.147727  ...     0.843373      0.891566      0\n",
      "7  0.059211    0.018280  0.141304  ...     0.843373      0.891566      0\n",
      "8  0.059111    0.017993  0.139785  ...     0.843373      0.891566      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.063286    0.019001  0.285714  ...     0.662651      0.891566      4\n",
      "1  0.064437    0.018349  0.321839  ...     0.662651      0.891566      4\n",
      "2  0.060308    0.018217  0.297872  ...     0.662651      0.891566      4\n",
      "3  0.060060    0.017962  0.254545  ...     0.662651      0.891566      4\n",
      "4  0.060108    0.018173  0.256881  ...     0.662651      0.891566      4\n",
      "5  0.059853    0.018195  0.291667  ...     0.662651      0.891566      4\n",
      "6  0.059893    0.018142  0.259259  ...     0.662651      0.891566      4\n",
      "7  0.060341    0.018051  0.490741  ...     0.662651      0.891566      4\n",
      "8  0.061028    0.018505  0.254545  ...     0.662651      0.891566      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.061410    0.018103  0.342105  ...     0.903614      0.891566      9\n",
      "1  0.063421    0.018031  0.318182  ...     0.903614      0.891566      9\n",
      "2  0.060136    0.018259  0.468085  ...     0.903614      0.891566      9\n",
      "3  0.060859    0.018044  0.440299  ...     0.903614      0.891566      9\n",
      "4  0.069314    0.020265  0.318182  ...     0.903614      0.891566      9\n",
      "5  0.062663    0.018142  0.147727  ...     0.903614      0.891566      9\n",
      "6  0.060431    0.018387  0.157303  ...     0.903614      0.891566      9\n",
      "7  0.060213    0.018117  0.299065  ...     0.903614      0.891566      9\n",
      "8  0.059865    0.017897  0.358974  ...     0.903614      0.891566      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060981    0.018810  0.347368  ...      0.60241      0.891566     10\n",
      "1  0.060542    0.018288  0.358696  ...      0.60241      0.891566     10\n",
      "2  0.060097    0.018371  0.523810  ...      0.60241      0.891566     10\n",
      "3  0.059844    0.018566  0.300000  ...      0.60241      0.891566     10\n",
      "4  0.059944    0.018185  0.308411  ...      0.60241      0.891566     10\n",
      "5  0.060097    0.018241  0.537037  ...      0.60241      0.891566     10\n",
      "6  0.059947    0.018244  0.484536  ...      0.60241      0.891566     10\n",
      "7  0.061114    0.018749  0.388235  ...      0.60241      0.891566     10\n",
      "8  0.060124    0.018282  0.411765  ...      0.60241      0.891566     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060162    0.018397  0.225490  ...     0.722892      0.891566     14\n",
      "1  0.060088    0.017889  0.216981  ...     0.722892      0.891566     14\n",
      "2  0.059486    0.017810  0.216981  ...     0.722892      0.891566     14\n",
      "3  0.059342    0.017971  0.216981  ...     0.722892      0.891566     14\n",
      "4  0.059538    0.018054  0.439252  ...     0.722892      0.891566     14\n",
      "5  0.059386    0.017917  0.216981  ...     0.722892      0.891566     14\n",
      "6  0.059613    0.018134  0.230000  ...     0.722892      0.891566     14\n",
      "7  0.059429    0.018234  0.221154  ...     0.722892      0.891566     14\n",
      "8  0.059329    0.018075  0.219048  ...     0.722892      0.891566     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060997    0.018430  0.381679  ...      0.39759      0.891566     12\n",
      "1  0.060150    0.018028  0.446429  ...      0.39759      0.891566     12\n",
      "2  0.060047    0.017588  0.375940  ...      0.39759      0.891566     12\n",
      "3  0.059977    0.017533  0.375940  ...      0.39759      0.891566     12\n",
      "4  0.059816    0.018057  0.679612  ...      0.39759      0.891566     12\n",
      "5  0.059839    0.017728  0.375940  ...      0.39759      0.891566     12\n",
      "6  0.060088    0.017917  0.450450  ...      0.39759      0.891566     12\n",
      "7  0.059935    0.017674  0.375940  ...      0.39759      0.891566     12\n",
      "8  0.059494    0.017883  0.485437  ...      0.39759      0.891566     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060486    0.017713  0.296610  ...     0.578313      0.891566     23\n",
      "1  0.059721    0.018062  0.304348  ...     0.578313      0.891566     23\n",
      "2  0.059567    0.017996  0.411765  ...     0.578313      0.891566     23\n",
      "3  0.059830    0.017802  0.296610  ...     0.578313      0.891566     23\n",
      "4  0.059650    0.017713  0.296610  ...     0.578313      0.891566     23\n",
      "5  0.059529    0.017758  0.296610  ...     0.578313      0.891566     23\n",
      "6  0.059594    0.018292  0.307018  ...     0.578313      0.891566     23\n",
      "7  0.059706    0.017640  0.296610  ...     0.578313      0.891566     23\n",
      "8  0.059424    0.017578  0.296610  ...     0.578313      0.891566     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060014    0.018060  0.296610  ...     0.578313      0.891566     23\n",
      "1  0.060161    0.017998  0.304348  ...     0.578313      0.891566     23\n",
      "2  0.059871    0.018245  0.411765  ...     0.578313      0.891566     23\n",
      "3  0.059712    0.017756  0.296610  ...     0.578313      0.891566     23\n",
      "4  0.059865    0.017838  0.296610  ...     0.578313      0.891566     23\n",
      "5  0.059447    0.017809  0.296610  ...     0.578313      0.891566     23\n",
      "6  0.059478    0.018176  0.307018  ...     0.578313      0.891566     23\n",
      "7  0.059098    0.017573  0.296610  ...     0.578313      0.891566     23\n",
      "8  0.059465    0.017857  0.296610  ...     0.578313      0.891566     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060422    0.017989  0.455357  ...      0.73494      0.891566      4\n",
      "1  0.060230    0.018209  0.364583  ...      0.73494      0.891566      4\n",
      "2  0.061452    0.017888  0.371134  ...      0.73494      0.891566      4\n",
      "3  0.059907    0.018156  0.211538  ...      0.73494      0.891566      4\n",
      "4  0.059622    0.018162  0.217822  ...      0.73494      0.891566      4\n",
      "5  0.059760    0.018182  0.419048  ...      0.73494      0.891566      4\n",
      "6  0.060598    0.018329  0.390000  ...      0.73494      0.891566      4\n",
      "7  0.070424    0.020467  0.504065  ...      0.73494      0.891566      4\n",
      "8  0.060245    0.018454  0.226804  ...      0.73494      0.891566      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060239    0.017528  0.500000  ...     0.927711      0.891566      9\n",
      "1  0.059500    0.017821  0.412214  ...     0.927711      0.891566      9\n",
      "2  0.059558    0.017831  0.515723  ...     0.927711      0.891566      9\n",
      "3  0.059726    0.017787  0.453901  ...     0.927711      0.891566      9\n",
      "4  0.060254    0.017718  0.437956  ...     0.927711      0.891566      9\n",
      "5  0.059839    0.017680  0.388889  ...     0.927711      0.891566      9\n",
      "6  0.059434    0.017960  0.312500  ...     0.927711      0.891566      9\n",
      "7  0.059502    0.018113  0.457746  ...     0.927711      0.891566      9\n",
      "8  0.061533    0.018012  0.398438  ...     0.927711      0.891566      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060866    0.018435  0.195652  ...     0.783133      0.891566     14\n",
      "1  0.060623    0.018248  0.211765  ...     0.783133      0.891566     14\n",
      "2  0.059971    0.021409  0.180000  ...     0.783133      0.891566     14\n",
      "3  0.059946    0.018067  0.202247  ...     0.783133      0.891566     14\n",
      "4  0.059512    0.017788  0.492188  ...     0.783133      0.891566     14\n",
      "5  0.059992    0.018096  0.180000  ...     0.783133      0.891566     14\n",
      "6  0.059527    0.017968  0.204545  ...     0.783133      0.891566     14\n",
      "7  0.059558    0.018049  0.183673  ...     0.783133      0.891566     14\n",
      "8  0.060104    0.017968  0.214286  ...     0.783133      0.891566     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060229    0.018014  0.184783  ...     0.795181      0.891566      2\n",
      "1  0.059464    0.018074  0.188889  ...     0.795181      0.891566      2\n",
      "2  0.059616    0.018117  0.184783  ...     0.795181      0.891566      2\n",
      "3  0.059603    0.017990  0.171717  ...     0.795181      0.891566      2\n",
      "4  0.060531    0.019082  0.171717  ...     0.795181      0.891566      2\n",
      "5  0.060127    0.018040  0.241379  ...     0.795181      0.891566      2\n",
      "6  0.059542    0.018053  0.352941  ...     0.795181      0.891566      2\n",
      "7  0.059426    0.018084  0.182796  ...     0.795181      0.891566      2\n",
      "8  0.059447    0.017976  0.214286  ...     0.795181      0.891566      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.059920    0.018488  0.184783  ...     0.795181      0.891566      2\n",
      "1  0.059594    0.018287  0.188889  ...     0.795181      0.891566      2\n",
      "2  0.059852    0.018129  0.184783  ...     0.795181      0.891566      2\n",
      "3  0.059462    0.018010  0.171717  ...     0.795181      0.891566      2\n",
      "4  0.059919    0.018130  0.171717  ...     0.795181      0.891566      2\n",
      "5  0.059418    0.018034  0.241379  ...     0.795181      0.891566      2\n",
      "6  0.059731    0.018261  0.352941  ...     0.795181      0.891566      2\n",
      "7  0.059121    0.018191  0.182796  ...     0.795181      0.891566      2\n",
      "8  0.059491    0.018071  0.214286  ...     0.795181      0.891566      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060045    0.017881  0.201923  ...     0.746988      0.891566      0\n",
      "1  0.059845    0.017847  0.201923  ...     0.746988      0.891566      0\n",
      "2  0.060109    0.017751  0.201923  ...     0.746988      0.891566      0\n",
      "3  0.059088    0.017547  0.201923  ...     0.746988      0.891566      0\n",
      "4  0.059297    0.017576  0.201923  ...     0.746988      0.891566      0\n",
      "5  0.059226    0.018043  0.247059  ...     0.746988      0.891566      0\n",
      "6  0.059447    0.017941  0.203883  ...     0.746988      0.891566      0\n",
      "7  0.060730    0.017637  0.201923  ...     0.746988      0.891566      0\n",
      "8  0.059311    0.017975  0.205882  ...     0.746988      0.891566      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060068    0.017953  0.247619  ...     0.686747      0.891566     18\n",
      "1  0.059829    0.017997  0.240741  ...     0.686747      0.891566     18\n",
      "2  0.059576    0.017880  0.240741  ...     0.686747      0.891566     18\n",
      "3  0.059772    0.018064  0.245283  ...     0.686747      0.891566     18\n",
      "4  0.059368    0.018104  0.247619  ...     0.686747      0.891566     18\n",
      "5  0.061543    0.017723  0.238532  ...     0.686747      0.891566     18\n",
      "6  0.061692    0.018344  0.238532  ...     0.686747      0.891566     18\n",
      "7  0.059427    0.018260  0.242991  ...     0.686747      0.891566     18\n",
      "8  0.060345    0.017981  0.242991  ...     0.686747      0.891566     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.061781    0.018494  0.282828  ...     0.855422      0.891566      0\n",
      "1  0.060721    0.018162  0.154762  ...     0.855422      0.891566      0\n",
      "2  0.060202    0.018085  0.127660  ...     0.855422      0.891566      0\n",
      "3  0.061643    0.018046  0.129032  ...     0.855422      0.891566      0\n",
      "4  0.060070    0.017961  0.260417  ...     0.855422      0.891566      0\n",
      "5  0.059899    0.018070  0.244681  ...     0.855422      0.891566      0\n",
      "6  0.059521    0.017842  0.303922  ...     0.855422      0.891566      0\n",
      "7  0.059737    0.017736  0.126316  ...     0.855422      0.891566      0\n",
      "8  0.060197    0.018486  0.244681  ...     0.855422      0.891566      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060336    0.017661  0.290598  ...     0.590361      0.891566     18\n",
      "1  0.059853    0.018060  0.295652  ...     0.590361      0.891566     18\n",
      "2  0.059683    0.018201  0.293103  ...     0.590361      0.891566     18\n",
      "3  0.059792    0.017806  0.290598  ...     0.590361      0.891566     18\n",
      "4  0.059627    0.019193  0.298246  ...     0.590361      0.891566     18\n",
      "5  0.059719    0.017730  0.290598  ...     0.590361      0.891566     18\n",
      "6  0.059847    0.017665  0.290598  ...     0.590361      0.891566     18\n",
      "7  0.059879    0.017788  0.290598  ...     0.590361      0.891566     18\n",
      "8  0.059697    0.018156  0.295652  ...     0.590361      0.891566     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060016    0.018158  0.290598  ...     0.590361      0.891566     18\n",
      "1  0.060085    0.018153  0.295652  ...     0.590361      0.891566     18\n",
      "2  0.059953    0.018052  0.293103  ...     0.590361      0.891566     18\n",
      "3  0.059694    0.017648  0.290598  ...     0.590361      0.891566     18\n",
      "4  0.060145    0.018261  0.298246  ...     0.590361      0.891566     18\n",
      "5  0.059778    0.017530  0.290598  ...     0.590361      0.891566     18\n",
      "6  0.059726    0.017788  0.290598  ...     0.590361      0.891566     18\n",
      "7  0.059772    0.017533  0.290598  ...     0.590361      0.891566     18\n",
      "8  0.059546    0.018092  0.295652  ...     0.590361      0.891566     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060869    0.017976  0.337079  ...     0.710843      0.891566     15\n",
      "1  0.059823    0.018084  0.269663  ...     0.710843      0.891566     15\n",
      "2  0.060054    0.020270  0.421569  ...     0.710843      0.891566     15\n",
      "3  0.059231    0.017615  0.224299  ...     0.710843      0.891566     15\n",
      "4  0.060095    0.017956  0.226415  ...     0.710843      0.891566     15\n",
      "5  0.060364    0.018172  0.427184  ...     0.710843      0.891566     15\n",
      "6  0.060019    0.018117  0.260870  ...     0.710843      0.891566     15\n",
      "7  0.067730    0.020207  0.240000  ...     0.710843      0.891566     15\n",
      "8  0.059613    0.018160  0.410000  ...     0.710843      0.891566     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.061423    0.017751  0.500000  ...     0.421687      0.891566     12\n",
      "1  0.068899    0.031055  0.571429  ...     0.421687      0.891566     12\n",
      "2  0.118025    0.025438  0.366412  ...     0.421687      0.891566     12\n",
      "3  0.060273    0.017740  0.366412  ...     0.421687      0.891566     12\n",
      "4  0.060670    0.018245  0.646465  ...     0.421687      0.891566     12\n",
      "5  0.059972    0.018111  0.396694  ...     0.421687      0.891566     12\n",
      "6  0.059867    0.028232  0.631579  ...     0.421687      0.891566     12\n",
      "7  0.060345    0.017860  0.372093  ...     0.421687      0.891566     12\n",
      "8  0.060048    0.017956  0.606742  ...     0.421687      0.891566     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060702    0.018361  0.342105  ...     0.903614      0.891566      9\n",
      "1  0.061823    0.017847  0.318182  ...     0.903614      0.891566      9\n",
      "2  0.060287    0.017709  0.468085  ...     0.903614      0.891566      9\n",
      "3  0.059503    0.017664  0.440299  ...     0.903614      0.891566      9\n",
      "4  0.059609    0.017802  0.318182  ...     0.903614      0.891566      9\n",
      "5  0.059634    0.017894  0.147727  ...     0.903614      0.891566      9\n",
      "6  0.059762    0.017852  0.157303  ...     0.903614      0.891566      9\n",
      "7  0.059446    0.017857  0.299065  ...     0.903614      0.891566      9\n",
      "8  0.060197    0.017790  0.358974  ...     0.903614      0.891566      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060314    0.018010  0.305882  ...     0.710843      0.891566     15\n",
      "1  0.059549    0.017624  0.372340  ...     0.710843      0.891566     15\n",
      "2  0.059346    0.017939  0.397959  ...     0.710843      0.891566     15\n",
      "3  0.059474    0.017814  0.230769  ...     0.710843      0.891566     15\n",
      "4  0.059272    0.017502  0.224299  ...     0.710843      0.891566     15\n",
      "5  0.059190    0.017639  0.378947  ...     0.710843      0.891566     15\n",
      "6  0.059328    0.017848  0.233010  ...     0.710843      0.891566     15\n",
      "7  0.059183    0.017850  0.275862  ...     0.710843      0.891566     15\n",
      "8  0.059342    0.017824  0.421569  ...     0.710843      0.891566     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.059677    0.018253  0.305882  ...     0.710843      0.891566     15\n",
      "1  0.059518    0.017891  0.372340  ...     0.710843      0.891566     15\n",
      "2  0.059414    0.017763  0.397959  ...     0.710843      0.891566     15\n",
      "3  0.059506    0.017911  0.230769  ...     0.710843      0.891566     15\n",
      "4  0.059556    0.017516  0.224299  ...     0.710843      0.891566     15\n",
      "5  0.059475    0.017744  0.378947  ...     0.710843      0.891566     15\n",
      "6  0.060214    0.017988  0.233010  ...     0.710843      0.891566     15\n",
      "7  0.059263    0.017983  0.275862  ...     0.710843      0.891566     15\n",
      "8  0.059547    0.018149  0.421569  ...     0.710843      0.891566     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.059309    0.018001  0.321839  ...     0.662651      0.891566     12\n",
      "1  0.059272    0.017999  0.521739  ...     0.662651      0.891566     12\n",
      "2  0.058969    0.017383  0.252252  ...     0.662651      0.891566     12\n",
      "3  0.058719    0.017779  0.254545  ...     0.662651      0.891566     12\n",
      "4  0.058806    0.017762  0.570312  ...     0.662651      0.891566     12\n",
      "5  0.058810    0.017846  0.256881  ...     0.662651      0.891566     12\n",
      "6  0.058920    0.017867  0.513274  ...     0.662651      0.891566     12\n",
      "7  0.060075    0.017971  0.264151  ...     0.662651      0.891566     12\n",
      "8  0.059006    0.017600  0.455446  ...     0.662651      0.891566     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.059894    0.017560  0.296610  ...     0.578313      0.891566     23\n",
      "1  0.059287    0.017872  0.304348  ...     0.578313      0.891566     23\n",
      "2  0.059376    0.017967  0.494737  ...     0.578313      0.891566     23\n",
      "3  0.059800    0.017739  0.296610  ...     0.578313      0.891566     23\n",
      "4  0.059602    0.017665  0.296610  ...     0.578313      0.891566     23\n",
      "5  0.059519    0.017885  0.304348  ...     0.578313      0.891566     23\n",
      "6  0.059605    0.018086  0.330189  ...     0.578313      0.891566     23\n",
      "7  0.059223    0.017897  0.301724  ...     0.578313      0.891566     23\n",
      "8  0.060338    0.017861  0.296610  ...     0.578313      0.891566     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060986    0.017962  0.296610  ...     0.578313      0.891566     23\n",
      "1  0.060453    0.018262  0.304348  ...     0.578313      0.891566     23\n",
      "2  0.060648    0.018246  0.411765  ...     0.578313      0.891566     23\n",
      "3  0.059925    0.017629  0.296610  ...     0.578313      0.891566     23\n",
      "4  0.059639    0.017829  0.296610  ...     0.578313      0.891566     23\n",
      "5  0.059528    0.017640  0.296610  ...     0.578313      0.891566     23\n",
      "6  0.059327    0.018219  0.307018  ...     0.578313      0.891566     23\n",
      "7  0.060540    0.017705  0.296610  ...     0.578313      0.891566     23\n",
      "8  0.059593    0.017616  0.296610  ...     0.578313      0.891566     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.059841    0.018261  0.285714  ...     0.662651      0.891566      4\n",
      "1  0.059272    0.017998  0.321839  ...     0.662651      0.891566      4\n",
      "2  0.059252    0.017996  0.297872  ...     0.662651      0.891566      4\n",
      "3  0.059846    0.017966  0.254545  ...     0.662651      0.891566      4\n",
      "4  0.059350    0.017843  0.256881  ...     0.662651      0.891566      4\n",
      "5  0.059185    0.017886  0.291667  ...     0.662651      0.891566      4\n",
      "6  0.059324    0.018030  0.259259  ...     0.662651      0.891566      4\n",
      "7  0.059239    0.017799  0.490741  ...     0.662651      0.891566      4\n",
      "8  0.059580    0.018000  0.254545  ...     0.662651      0.891566      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.059591    0.018375  0.285714  ...     0.662651      0.891566      4\n",
      "1  0.060176    0.018218  0.321839  ...     0.662651      0.891566      4\n",
      "2  0.059567    0.017980  0.297872  ...     0.662651      0.891566      4\n",
      "3  0.059744    0.017902  0.254545  ...     0.662651      0.891566      4\n",
      "4  0.059658    0.017975  0.256881  ...     0.662651      0.891566      4\n",
      "5  0.070510    0.018571  0.291667  ...     0.662651      0.891566      4\n",
      "6  0.063236    0.019036  0.259259  ...     0.662651      0.891566      4\n",
      "7  0.059921    0.018253  0.490741  ...     0.662651      0.891566      4\n",
      "8  0.059719    0.018403  0.254545  ...     0.662651      0.891566      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.061007    0.018213  0.492647  ...     0.831325      0.891566      9\n",
      "1  0.060046    0.018193  0.155556  ...     0.831325      0.891566      9\n",
      "2  0.060342    0.018157  0.536913  ...     0.831325      0.891566      9\n",
      "3  0.059474    0.017780  0.469231  ...     0.831325      0.891566      9\n",
      "4  0.059482    0.018132  0.150538  ...     0.831325      0.891566      9\n",
      "5  0.059603    0.018178  0.361111  ...     0.831325      0.891566      9\n",
      "6  0.066260    0.019611  0.147368  ...     0.831325      0.891566      9\n",
      "7  0.059617    0.018043  0.383929  ...     0.831325      0.891566      9\n",
      "8  0.059837    0.017991  0.178571  ...     0.831325      0.891566      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.077209    0.018130  0.292929  ...     0.650602      0.891566     15\n",
      "1  0.060306    0.018167  0.271028  ...     0.650602      0.891566     15\n",
      "2  0.059496    0.018095  0.311828  ...     0.650602      0.891566     15\n",
      "3  0.059850    0.017834  0.258929  ...     0.650602      0.891566     15\n",
      "4  0.059348    0.017722  0.258929  ...     0.650602      0.891566     15\n",
      "5  0.059808    0.018301  0.271028  ...     0.650602      0.891566     15\n",
      "6  0.059231    0.017993  0.276190  ...     0.650602      0.891566     15\n",
      "7  0.061984    0.018658  0.263636  ...     0.650602      0.891566     15\n",
      "8  0.059869    0.019516  0.308511  ...     0.650602      0.891566     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.061044    0.018407  0.282828  ...     0.855422      0.891566      0\n",
      "1  0.060772    0.018329  0.154762  ...     0.855422      0.891566      0\n",
      "2  0.060039    0.018077  0.127660  ...     0.855422      0.891566      0\n",
      "3  0.059603    0.018094  0.129032  ...     0.855422      0.891566      0\n",
      "4  0.059951    0.017938  0.260417  ...     0.855422      0.891566      0\n",
      "5  0.060087    0.017886  0.244681  ...     0.855422      0.891566      0\n",
      "6  0.060062    0.017808  0.303922  ...     0.855422      0.891566      0\n",
      "7  0.060184    0.017811  0.126316  ...     0.855422      0.891566      0\n",
      "8  0.059906    0.017851  0.244681  ...     0.855422      0.891566      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.061098    0.018399  0.282828  ...     0.855422      0.891566      0\n",
      "1  0.059981    0.018246  0.154762  ...     0.855422      0.891566      0\n",
      "2  0.060258    0.018409  0.127660  ...     0.855422      0.891566      0\n",
      "3  0.060154    0.018358  0.129032  ...     0.855422      0.891566      0\n",
      "4  0.059956    0.027213  0.260417  ...     0.855422      0.891566      0\n",
      "5  0.062109    0.018366  0.244681  ...     0.855422      0.891566      0\n",
      "6  0.060343    0.018435  0.303922  ...     0.855422      0.891566      0\n",
      "7  0.059658    0.018013  0.126316  ...     0.855422      0.891566      0\n",
      "8  0.060035    0.018252  0.244681  ...     0.855422      0.891566      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.061186    0.019037  0.235294  ...     0.710843      0.891566     18\n",
      "1  0.060503    0.018385  0.228571  ...     0.710843      0.891566     18\n",
      "2  0.060428    0.017773  0.224299  ...     0.710843      0.891566     18\n",
      "3  0.061184    0.017899  0.224299  ...     0.710843      0.891566     18\n",
      "4  0.060817    0.017944  0.224299  ...     0.710843      0.891566     18\n",
      "5  0.063562    0.018469  0.224299  ...     0.710843      0.891566     18\n",
      "6  0.061708    0.017859  0.224299  ...     0.710843      0.891566     18\n",
      "7  0.059110    0.018139  0.228571  ...     0.710843      0.891566     18\n",
      "8  0.059125    0.018078  0.233010  ...     0.710843      0.891566     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.059404    0.018015  0.336538  ...     0.578313      0.891566     10\n",
      "1  0.059214    0.018256  0.299145  ...     0.578313      0.891566     10\n",
      "2  0.059422    0.018110  0.524752  ...     0.578313      0.891566     10\n",
      "3  0.059229    0.018095  0.315315  ...     0.578313      0.891566     10\n",
      "4  0.059018    0.017700  0.296610  ...     0.578313      0.891566     10\n",
      "5  0.059138    0.018115  0.339806  ...     0.578313      0.891566     10\n",
      "6  0.059104    0.017904  0.330189  ...     0.578313      0.891566     10\n",
      "7  0.059110    0.018101  0.301724  ...     0.578313      0.891566     10\n",
      "8  0.059554    0.018196  0.307018  ...     0.578313      0.891566     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060350    0.018181  0.381679  ...      0.39759      0.891566     12\n",
      "1  0.060135    0.017827  0.446429  ...      0.39759      0.891566     12\n",
      "2  0.059901    0.017742  0.375940  ...      0.39759      0.891566     12\n",
      "3  0.059909    0.017484  0.375940  ...      0.39759      0.891566     12\n",
      "4  0.059529    0.017890  0.679612  ...      0.39759      0.891566     12\n",
      "5  0.059518    0.017489  0.375940  ...      0.39759      0.891566     12\n",
      "6  0.060118    0.017896  0.450450  ...      0.39759      0.891566     12\n",
      "7  0.059559    0.017734  0.375940  ...      0.39759      0.891566     12\n",
      "8  0.059541    0.017921  0.485437  ...      0.39759      0.891566     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060651    0.018310  0.381679  ...      0.39759      0.891566     12\n",
      "1  0.060585    0.018048  0.446429  ...      0.39759      0.891566     12\n",
      "2  0.060567    0.018153  0.375940  ...      0.39759      0.891566     12\n",
      "3  0.061367    0.017978  0.375940  ...      0.39759      0.891566     12\n",
      "4  0.060904    0.019384  0.679612  ...      0.39759      0.891566     12\n",
      "5  0.060774    0.018039  0.375940  ...      0.39759      0.891566     12\n",
      "6  0.061162    0.018327  0.450450  ...      0.39759      0.891566     12\n",
      "7  0.061013    0.018043  0.375940  ...      0.39759      0.891566     12\n",
      "8  0.062126    0.018273  0.485437  ...      0.39759      0.891566     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.059709    0.018298  0.336066  ...     0.506024      0.891566     23\n",
      "1  0.059678    0.017885  0.336066  ...     0.506024      0.891566     23\n",
      "2  0.059637    0.017870  0.538462  ...     0.506024      0.891566     23\n",
      "3  0.059715    0.017965  0.333333  ...     0.506024      0.891566     23\n",
      "4  0.059705    0.017966  0.341667  ...     0.506024      0.891566     23\n",
      "5  0.060501    0.023839  0.333333  ...     0.506024      0.891566     23\n",
      "6  0.062347    0.018630  0.455556  ...     0.506024      0.891566     23\n",
      "7  0.059743    0.018445  0.341667  ...     0.506024      0.891566     23\n",
      "8  0.059623    0.018113  0.383178  ...     0.506024      0.891566     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.062664    0.018142  0.420455  ...     0.614458      0.891566      4\n",
      "1  0.060006    0.018237  0.288288  ...     0.614458      0.891566      4\n",
      "2  0.059803    0.018153  0.376471  ...     0.614458      0.891566      4\n",
      "3  0.059381    0.017782  0.278261  ...     0.614458      0.891566      4\n",
      "4  0.059910    0.018105  0.293578  ...     0.614458      0.891566      4\n",
      "5  0.059805    0.018044  0.283186  ...     0.614458      0.891566      4\n",
      "6  0.060704    0.018189  0.355556  ...     0.614458      0.891566      4\n",
      "7  0.059894    0.018111  0.344086  ...     0.614458      0.891566      4\n",
      "8  0.060009    0.018349  0.336842  ...     0.614458      0.891566      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.061494    0.018384  0.290598  ...     0.590361      0.891566     18\n",
      "1  0.060726    0.018401  0.295652  ...     0.590361      0.891566     18\n",
      "2  0.060575    0.018200  0.293103  ...     0.590361      0.891566     18\n",
      "3  0.060464    0.017844  0.290598  ...     0.590361      0.891566     18\n",
      "4  0.060963    0.018244  0.298246  ...     0.590361      0.891566     18\n",
      "5  0.060696    0.017768  0.290598  ...     0.590361      0.891566     18\n",
      "6  0.060883    0.019207  0.290598  ...     0.590361      0.891566     18\n",
      "7  0.060463    0.017889  0.290598  ...     0.590361      0.891566     18\n",
      "8  0.060165    0.018492  0.295652  ...     0.590361      0.891566     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060492    0.018197  0.304762  ...     0.614458      0.891566     10\n",
      "1  0.059932    0.018057  0.359551  ...     0.614458      0.891566     10\n",
      "2  0.060221    0.018064  0.504854  ...     0.614458      0.891566     10\n",
      "3  0.060035    0.018199  0.304762  ...     0.614458      0.891566     10\n",
      "4  0.060938    0.018109  0.288288  ...     0.614458      0.891566     10\n",
      "5  0.059720    0.018100  0.540541  ...     0.614458      0.891566     10\n",
      "6  0.059997    0.018004  0.457447  ...     0.614458      0.891566     10\n",
      "7  0.060009    0.017972  0.439560  ...     0.614458      0.891566     10\n",
      "8  0.059987    0.017928  0.433333  ...     0.614458      0.891566     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060812    0.018578  0.304762  ...     0.614458      0.891566     10\n",
      "1  0.060727    0.018293  0.359551  ...     0.614458      0.891566     10\n",
      "2  0.060330    0.018094  0.504854  ...     0.614458      0.891566     10\n",
      "3  0.060435    0.018286  0.304762  ...     0.614458      0.891566     10\n",
      "4  0.060288    0.018155  0.288288  ...     0.614458      0.891566     10\n",
      "5  0.059969    0.019219  0.540541  ...     0.614458      0.891566     10\n",
      "6  0.060026    0.018102  0.457447  ...     0.614458      0.891566     10\n",
      "7  0.059889    0.018096  0.439560  ...     0.614458      0.891566     10\n",
      "8  0.059867    0.018071  0.433333  ...     0.614458      0.891566     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.060271    0.018488  0.235849  ...     0.698795      0.891566     14\n",
      "1  0.060100    0.017980  0.231481  ...     0.698795      0.891566     14\n",
      "2  0.060424    0.017798  0.231481  ...     0.698795      0.891566     14\n",
      "3  0.059673    0.018067  0.235849  ...     0.698795      0.891566     14\n",
      "4  0.060170    0.018032  0.382979  ...     0.698795      0.891566     14\n",
      "5  0.059679    0.018306  0.233645  ...     0.698795      0.891566     14\n",
      "6  0.059963    0.019361  0.250000  ...     0.698795      0.891566     14\n",
      "7  0.060139    0.018088  0.233645  ...     0.698795      0.891566     14\n",
      "8  0.059656    0.018084  0.238095  ...     0.698795      0.891566     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.115826    0.030987  0.120482  ...     0.879518      0.891566      2\n",
      "1  0.080752    0.017747  0.107527  ...     0.879518      0.891566      2\n",
      "2  0.061779    0.018380  0.117647  ...     0.879518      0.891566      2\n",
      "3  0.061624    0.017859  0.107527  ...     0.879518      0.891566      2\n",
      "4  0.060753    0.017957  0.107527  ...     0.879518      0.891566      2\n",
      "5  0.061381    0.018490  0.109890  ...     0.879518      0.891566      2\n",
      "6  0.060721    0.019482  0.141176  ...     0.879518      0.891566      2\n",
      "7  0.070176    0.018722  0.116279  ...     0.879518      0.891566      2\n",
      "8  0.060866    0.018563  0.112360  ...     0.879518      0.891566      2\n",
      "\n",
      "[9 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "np.array([None] + df_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "ERROR! Session/line number was not unique in database. History logging moved to new session 234\n"
     ]
    }
   ],
   "source": [
    "print(CORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df_results = None  # Will be filled with randomsearch scores\n",
    "for run in tqdm(range(5)):\n",
    "\n",
    "\n",
    "    for owner_idx in tqdm(\n",
    "        range(len(test_set)),\n",
    "        desc=\"Owner\",\n",
    "    ):\n",
    "\n",
    "        \n",
    "        run_seed = SEED + run\n",
    "        train_dic, valid_test_dic = { owner_idx: X_exp_test_dic[owner_idx][\"profile_windows\"]}, X_exp_test_dic[owner_idx][\"unknown_users_dict\"] \n",
    "        X_vals_owner_idx = utils_create_cv_splits(owner_idx, train_dic, valid_test_dic, seed=run_seed)\n",
    "        \n",
    "        X_test_regular = X_vals_owner_idx['X_test_regular']\n",
    "        X_test_anomalous = X_vals_owner_idx['X_test_anomalous']\n",
    "\n",
    "        train_test_cv_splits = X_vals_owner_idx['cv_splits']\n",
    "        \n",
    "        \n",
    "        pca = PCA(n_components = run+3)\n",
    "#         X_train = pca.fit_transform(X_train)\n",
    "#         X_test_regular = pca.transform(X_test_regular)\n",
    "#         X_test_anomalous = pca.transform(X_test_anomalous)\n",
    "        \n",
    "        clf = IsolationForest(n_estimators = np.int64(P.median_n_estimators), \n",
    "                          max_samples = P.median_max_samples, \n",
    "                          contamination = P.median_contamination, \n",
    "                          max_features = np.int64(P.median_max_features), \n",
    "                          random_state=run_seed, n_jobs=CORES, verbose=1)\n",
    "        pipeline = Pipeline([\n",
    "                            # ('scaler', get_new_scaler_dict[P.scaler]()), \n",
    "#                             ('scaler2', Normalizer()),#best result\n",
    "#                              ('pca', pca), \n",
    "#                              ('selector', VarianceThreshold()), \n",
    "                            ('model', clf)\n",
    "                            ])\n",
    "        \n",
    "        scores = cross_validate(\n",
    "            pipeline,\n",
    "            X_vals_owner_idx['X_train'],\n",
    "            X_vals_owner_idx['y_train'],\n",
    "            cv=train_test_cv_splits,\n",
    "            scoring={\n",
    "                \"eer\": utils_eer_scorer,\n",
    "                \"accuracy\": \"accuracy\",\n",
    "                \"precision\": \"precision\",\n",
    "                \"recall\": \"recall\",\n",
    "            },\n",
    "            n_jobs=CORES,\n",
    "            verbose=1,\n",
    "            return_train_score=True,error_score='raise'\n",
    "        )\n",
    "#         print(df_score[\"test_eer\"])\n",
    "        df_score = pd.DataFrame(scores)\n",
    "        df_score[\"owner\"] = test_set[owner_idx]\n",
    "        df_score[\"train_eer\"] = df_score[\"train_eer\"].abs()  # Revert scorer's signflip\n",
    "        df_score[\"test_eer\"] = df_score[\"test_eer\"].abs()\n",
    "        test_df_results = pd.concat([test_df_results, df_score], axis=0)\n",
    "        \n",
    "\n",
    "test_df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_eer</th>\n",
       "      <th>train_eer</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>owner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.101073</td>\n",
       "      <td>0.029287</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>1.818989e-12</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.101488</td>\n",
       "      <td>0.030540</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>1.818989e-12</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.898734</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.099403</td>\n",
       "      <td>0.028908</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>1.818989e-12</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.100581</td>\n",
       "      <td>0.029050</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>1.818989e-12</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.100344</td>\n",
       "      <td>0.028847</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>1.818989e-12</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_eer     train_eer  test_accuracy  \\\n",
       "0  0.101073    0.029287  0.127660  1.818989e-12       0.921687   \n",
       "1  0.101488    0.030540  0.137931  1.818989e-12       0.879518   \n",
       "2  0.099403    0.028908  0.126316  1.818989e-12       0.927711   \n",
       "3  0.100581    0.029050  0.126316  1.818989e-12       0.927711   \n",
       "4  0.100344    0.028847  0.130435  1.818989e-12       0.909639   \n",
       "\n",
       "   train_accuracy  test_precision  train_precision  test_recall  train_recall  \\\n",
       "0        0.915663        0.986111              1.0     0.855422      0.915663   \n",
       "1        0.915663        0.898734              1.0     0.855422      0.915663   \n",
       "2        0.915663        1.000000              1.0     0.855422      0.915663   \n",
       "3        0.915663        1.000000              1.0     0.855422      0.915663   \n",
       "4        0.915663        0.959459              1.0     0.855422      0.915663   \n",
       "\n",
       "   owner  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df_plot = test_df_results.rename(\n",
    "    columns={\"test_accuracy\": \"Test Accuracy\", \"test_eer\": \"Test EER\", \"owner\": \"Owner\"}\n",
    ").astype({\"Owner\": str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean: 0.7717\n",
      "Overall mean: 0.2692\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 985.14x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 985.14x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100654    0.029534  0.337079  ...     0.638554      0.915663     10\n",
      "1  0.099944    0.028930  0.348837  ...     0.638554      0.915663     10\n",
      "2  0.098777    0.028886  0.411111  ...     0.638554      0.915663     10\n",
      "3  0.099078    0.028995  0.288462  ...     0.638554      0.915663     10\n",
      "4  0.098919    0.028823  0.288462  ...     0.638554      0.915663     10\n",
      "5  0.098792    0.028777  0.344828  ...     0.638554      0.915663     10\n",
      "6  0.098577    0.028442  0.361446  ...     0.638554      0.915663     10\n",
      "7  0.099441    0.029146  0.485437  ...     0.638554      0.915663     10\n",
      "8  0.100005    0.028837  0.417582  ...     0.638554      0.915663     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.171655    0.028574  0.509202  ...     0.963855      0.915663     12\n",
      "1  0.098915    0.028554  0.506173  ...     0.963855      0.915663     12\n",
      "2  0.098670    0.028602  0.101124  ...     0.963855      0.915663     12\n",
      "3  0.098515    0.028445  0.398496  ...     0.963855      0.915663     12\n",
      "4  0.098599    0.028749  0.509202  ...     0.963855      0.915663     12\n",
      "5  0.098758    0.028508  0.487179  ...     0.963855      0.915663     12\n",
      "6  0.098882    0.028506  0.503106  ...     0.963855      0.915663     12\n",
      "7  0.100134    0.028419  0.432624  ...     0.963855      0.915663     12\n",
      "8  0.099646    0.028481  0.500000  ...     0.963855      0.915663     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.105443    0.028830  0.156250  ...     0.819277      0.915663     18\n",
      "1  0.098270    0.028581  0.159574  ...     0.819277      0.915663     18\n",
      "2  0.098740    0.029891  0.153061  ...     0.819277      0.915663     18\n",
      "3  0.100614    0.029276  0.156250  ...     0.819277      0.915663     18\n",
      "4  0.100561    0.029344  0.170455  ...     0.819277      0.915663     18\n",
      "5  0.099325    0.028763  0.153061  ...     0.819277      0.915663     18\n",
      "6  0.099211    0.028640  0.153061  ...     0.819277      0.915663     18\n",
      "7  0.100177    0.028698  0.153061  ...     0.819277      0.915663     18\n",
      "8  0.101319    0.029022  0.157895  ...     0.819277      0.915663     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.265857    0.336341  0.201923  ...     0.746988      0.915663     23\n",
      "1  1.242568    0.336684  0.203883  ...     0.746988      0.915663     23\n",
      "2  1.251620    0.338404  0.436364  ...     0.746988      0.915663     23\n",
      "3  1.244867    0.337612  0.201923  ...     0.746988      0.915663     23\n",
      "4  1.251926    0.335838  0.201923  ...     0.746988      0.915663     23\n",
      "5  1.248179    0.338782  0.201923  ...     0.746988      0.915663     23\n",
      "6  1.251311    0.339732  0.228261  ...     0.746988      0.915663     23\n",
      "7  1.251393    0.339249  0.201923  ...     0.746988      0.915663     23\n",
      "8  1.249668    0.338671  0.203883  ...     0.746988      0.915663     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.246145    0.337770  0.273585  ...     0.650602      0.915663     10\n",
      "1  1.240129    0.336283  0.263636  ...     0.650602      0.915663     10\n",
      "2  1.242831    0.337709  0.413043  ...     0.650602      0.915663     10\n",
      "3  1.240729    0.336379  0.273585  ...     0.650602      0.915663     10\n",
      "4  1.239010    0.338540  0.261261  ...     0.650602      0.915663     10\n",
      "5  1.238914    0.337853  0.292929  ...     0.650602      0.915663     10\n",
      "6  1.237939    0.336910  0.281553  ...     0.650602      0.915663     10\n",
      "7  1.242178    0.339129  0.278846  ...     0.650602      0.915663     10\n",
      "8  1.241006    0.337697  0.292929  ...     0.650602      0.915663     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.103201    0.028731  0.452174  ...     0.759036      0.915663      4\n",
      "1  0.097803    0.028927  0.370000  ...     0.759036      0.915663      4\n",
      "2  0.098373    0.028689  0.422018  ...     0.759036      0.915663      4\n",
      "3  0.097969    0.028527  0.194175  ...     0.759036      0.915663      4\n",
      "4  0.098122    0.028350  0.202020  ...     0.759036      0.915663      4\n",
      "5  0.097766    0.028643  0.411215  ...     0.759036      0.915663      4\n",
      "6  0.098600    0.028835  0.427273  ...     0.759036      0.915663      4\n",
      "7  0.099165    0.028893  0.522727  ...     0.759036      0.915663      4\n",
      "8  0.098846    0.029785  0.206186  ...     0.759036      0.915663      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100900    0.029505  0.223404  ...     0.746988      0.915663      2\n",
      "1  0.099571    0.029027  0.201923  ...     0.746988      0.915663      2\n",
      "2  0.099634    0.029122  0.212121  ...     0.746988      0.915663      2\n",
      "3  0.100096    0.028957  0.203883  ...     0.746988      0.915663      2\n",
      "4  0.098994    0.028485  0.201923  ...     0.746988      0.915663      2\n",
      "5  0.098888    0.028836  0.205882  ...     0.746988      0.915663      2\n",
      "6  0.098834    0.029131  0.270588  ...     0.746988      0.915663      2\n",
      "7  0.098451    0.028971  0.228261  ...     0.746988      0.915663      2\n",
      "8  0.099698    0.033810  0.218750  ...     0.746988      0.915663      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098793    0.028886  0.290000  ...     0.650602      0.915663     10\n",
      "1  0.098427    0.028654  0.258929  ...     0.650602      0.915663     10\n",
      "2  0.098922    0.029003  0.273585  ...     0.650602      0.915663     10\n",
      "3  0.098284    0.028594  0.263636  ...     0.650602      0.915663     10\n",
      "4  0.098650    0.028884  0.263636  ...     0.650602      0.915663     10\n",
      "5  0.098324    0.037879  0.266055  ...     0.650602      0.915663     10\n",
      "6  0.101957    0.029124  0.337209  ...     0.650602      0.915663     10\n",
      "7  0.099235    0.028682  0.284314  ...     0.650602      0.915663     10\n",
      "8  0.098423    0.028817  0.302083  ...     0.650602      0.915663     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098498    0.040114  0.305556  ...      0.60241      0.915663     10\n",
      "1  0.099340    0.029404  0.294643  ...      0.60241      0.915663     10\n",
      "2  0.106392    0.030297  0.549550  ...      0.60241      0.915663     10\n",
      "3  0.098922    0.029202  0.326733  ...      0.60241      0.915663     10\n",
      "4  0.098289    0.028961  0.297297  ...      0.60241      0.915663     10\n",
      "5  0.098820    0.029124  0.314286  ...      0.60241      0.915663     10\n",
      "6  0.098646    0.028750  0.311321  ...      0.60241      0.915663     10\n",
      "7  0.099492    0.028851  0.300000  ...      0.60241      0.915663     10\n",
      "8  0.098733    0.028860  0.314286  ...      0.60241      0.915663     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.251135    0.343759  0.126316  ...     0.855422      0.915663      0\n",
      "1  1.247320    0.343974  0.127660  ...     0.855422      0.915663      0\n",
      "2  1.245408    0.345108  0.126316  ...     0.855422      0.915663      0\n",
      "3  1.250025    0.347098  0.126316  ...     0.855422      0.915663      0\n",
      "4  1.248951    0.344472  0.126316  ...     0.855422      0.915663      0\n",
      "5  1.251960    0.345712  0.252632  ...     0.855422      0.915663      0\n",
      "6  1.243374    0.342639  0.130435  ...     0.855422      0.915663      0\n",
      "7  1.245943    0.343611  0.126316  ...     0.855422      0.915663      0\n",
      "8  1.236322    0.341244  0.134831  ...     0.855422      0.915663      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.264698    0.338688  0.265487  ...     0.638554      0.915663     23\n",
      "1  1.243947    0.337487  0.267857  ...     0.638554      0.915663     23\n",
      "2  1.239930    0.338073  0.464646  ...     0.638554      0.915663     23\n",
      "3  1.232703    0.333885  0.265487  ...     0.638554      0.915663     23\n",
      "4  1.233013    0.336925  0.265487  ...     0.638554      0.915663     23\n",
      "5  1.232300    0.336044  0.265487  ...     0.638554      0.915663     23\n",
      "6  1.235794    0.339467  0.275229  ...     0.638554      0.915663     23\n",
      "7  1.239169    0.339209  0.265487  ...     0.638554      0.915663     23\n",
      "8  1.236566    0.338622  0.267857  ...     0.638554      0.915663     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.101378    0.029631  0.303922  ...     0.626506      0.915663     10\n",
      "1  0.098212    0.029196  0.279279  ...     0.626506      0.915663     10\n",
      "2  0.097630    0.029004  0.543860  ...     0.626506      0.915663     10\n",
      "3  0.097582    0.029283  0.279279  ...     0.626506      0.915663     10\n",
      "4  0.097368    0.028889  0.281818  ...     0.626506      0.915663     10\n",
      "5  0.097643    0.028874  0.364706  ...     0.626506      0.915663     10\n",
      "6  0.097669    0.028726  0.474747  ...     0.626506      0.915663     10\n",
      "7  0.099819    0.029758  0.319588  ...     0.626506      0.915663     10\n",
      "8  0.098222    0.029013  0.329787  ...     0.626506      0.915663     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.101821    0.029775  0.270588  ...     0.746988      0.915663     15\n",
      "1  0.100462    0.029444  0.253012  ...     0.746988      0.915663     15\n",
      "2  0.099543    0.029069  0.360825  ...     0.746988      0.915663     15\n",
      "3  0.099718    0.028924  0.201923  ...     0.746988      0.915663     15\n",
      "4  0.099538    0.028666  0.201923  ...     0.746988      0.915663     15\n",
      "5  0.099082    0.029174  0.238636  ...     0.746988      0.915663     15\n",
      "6  0.099116    0.028666  0.218750  ...     0.746988      0.915663     15\n",
      "7  0.099645    0.028945  0.225806  ...     0.746988      0.915663     15\n",
      "8  0.102953    0.031402  0.261905  ...     0.746988      0.915663     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099243    0.028821  0.396226  ...     0.771084      0.915663      4\n",
      "1  0.098803    0.029160  0.272727  ...     0.771084      0.915663      4\n",
      "2  0.099346    0.028874  0.215909  ...     0.771084      0.915663      4\n",
      "3  0.098949    0.028676  0.188119  ...     0.771084      0.915663      4\n",
      "4  0.098876    0.028941  0.190000  ...     0.771084      0.915663      4\n",
      "5  0.099317    0.028709  0.288889  ...     0.771084      0.915663      4\n",
      "6  0.098900    0.028756  0.296703  ...     0.771084      0.915663      4\n",
      "7  0.100159    0.028806  0.503876  ...     0.771084      0.915663      4\n",
      "8  0.099279    0.029366  0.200000  ...     0.771084      0.915663      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099432    0.029155  0.367347  ...     0.746988      0.915663      4\n",
      "1  0.098752    0.028995  0.415094  ...     0.746988      0.915663      4\n",
      "2  0.100770    0.029092  0.398058  ...     0.746988      0.915663      4\n",
      "3  0.098955    0.028929  0.201923  ...     0.746988      0.915663      4\n",
      "4  0.098782    0.034412  0.203883  ...     0.746988      0.915663      4\n",
      "5  0.099313    0.029340  0.303371  ...     0.746988      0.915663      4\n",
      "6  0.098770    0.029296  0.253012  ...     0.746988      0.915663      4\n",
      "7  0.099566    0.028863  0.460870  ...     0.746988      0.915663      4\n",
      "8  0.099601    0.029490  0.214286  ...     0.746988      0.915663      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.253314    0.343408  0.506098  ...     0.975904      0.915663     12\n",
      "1  1.250570    0.340971  0.484076  ...     0.975904      0.915663     12\n",
      "2  1.260686    0.339253  0.058140  ...     0.975904      0.915663     12\n",
      "3  1.249972    0.341943  0.367188  ...     0.975904      0.915663     12\n",
      "4  1.247016    0.340020  0.506098  ...     0.975904      0.915663     12\n",
      "5  1.238329    0.339709  0.470588  ...     0.975904      0.915663     12\n",
      "6  1.235622    0.337767  0.500000  ...     0.975904      0.915663     12\n",
      "7  1.247213    0.340753  0.362205  ...     0.975904      0.915663     12\n",
      "8  1.239381    0.337520  0.496894  ...     0.975904      0.915663     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.260061    0.340386  0.186275  ...     0.771084      0.915663     18\n",
      "1  1.248177    0.341172  0.188119  ...     0.771084      0.915663     18\n",
      "2  1.256581    0.346940  0.186275  ...     0.771084      0.915663     18\n",
      "3  1.247898    0.341378  0.186275  ...     0.771084      0.915663     18\n",
      "4  1.256332    0.345408  0.186275  ...     0.771084      0.915663     18\n",
      "5  1.250686    0.337955  0.186275  ...     0.771084      0.915663     18\n",
      "6  1.244858    0.340635  0.186275  ...     0.771084      0.915663     18\n",
      "7  1.253882    0.342530  0.186275  ...     0.771084      0.915663     18\n",
      "8  1.252843    0.341404  0.186275  ...     0.771084      0.915663     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099307    0.032031  0.144330  ...     0.831325      0.915663      0\n",
      "1  0.098059    0.029064  0.145833  ...     0.831325      0.915663      0\n",
      "2  0.098070    0.028733  0.144330  ...     0.831325      0.915663      0\n",
      "3  0.098365    0.028794  0.144330  ...     0.831325      0.915663      0\n",
      "4  0.098370    0.028769  0.145833  ...     0.831325      0.915663      0\n",
      "5  0.098021    0.028929  0.241758  ...     0.831325      0.915663      0\n",
      "6  0.098102    0.028848  0.145833  ...     0.831325      0.915663      0\n",
      "7  0.103292    0.028933  0.144330  ...     0.831325      0.915663      0\n",
      "8  0.099332    0.029159  0.147368  ...     0.831325      0.915663      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.101460    0.029620  0.141304  ...     0.843373      0.915663      0\n",
      "1  0.104288    0.029042  0.146067  ...     0.843373      0.915663      0\n",
      "2  0.099240    0.028686  0.135417  ...     0.843373      0.915663      0\n",
      "3  0.102153    0.028486  0.135417  ...     0.843373      0.915663      0\n",
      "4  0.098958    0.028185  0.135417  ...     0.843373      0.915663      0\n",
      "5  0.098866    0.028921  0.166667  ...     0.843373      0.915663      0\n",
      "6  0.098317    0.028641  0.136842  ...     0.843373      0.915663      0\n",
      "7  0.098238    0.028476  0.135417  ...     0.843373      0.915663      0\n",
      "8  0.098418    0.034435  0.147727  ...     0.843373      0.915663      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100007    0.028774  0.503268  ...     0.915663      0.915663      9\n",
      "1  0.099253    0.028385  0.401575  ...     0.915663      0.915663      9\n",
      "2  0.099201    0.028906  0.515924  ...     0.915663      0.915663      9\n",
      "3  0.100217    0.028696  0.464789  ...     0.915663      0.915663      9\n",
      "4  0.099362    0.028539  0.479452  ...     0.915663      0.915663      9\n",
      "5  0.099551    0.028476  0.415385  ...     0.915663      0.915663      9\n",
      "6  0.099127    0.028381  0.392000  ...     0.915663      0.915663      9\n",
      "7  0.110985    0.029771  0.424242  ...     0.915663      0.915663      9\n",
      "8  0.099677    0.028738  0.387097  ...     0.915663      0.915663      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099478    0.029157  0.512987  ...     0.903614      0.915663      9\n",
      "1  0.098564    0.028123  0.385246  ...     0.903614      0.915663      9\n",
      "2  0.097888    0.028349  0.506579  ...     0.903614      0.915663      9\n",
      "3  0.098091    0.028538  0.509804  ...     0.903614      0.915663      9\n",
      "4  0.098823    0.028546  0.431818  ...     0.903614      0.915663      9\n",
      "5  0.098108    0.028407  0.452555  ...     0.903614      0.915663      9\n",
      "6  0.098390    0.028588  0.271845  ...     0.903614      0.915663      9\n",
      "7  0.098424    0.028622  0.427481  ...     0.903614      0.915663      9\n",
      "8  0.098045    0.028314  0.503311  ...     0.903614      0.915663      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.316115    0.340838  0.245455  ...     0.674699      0.915663     18\n",
      "1  1.241047    0.341766  0.245455  ...     0.674699      0.915663     18\n",
      "2  1.250890    0.340572  0.245455  ...     0.674699      0.915663     18\n",
      "3  1.241880    0.344791  0.245455  ...     0.674699      0.915663     18\n",
      "4  1.243771    0.342980  0.245455  ...     0.674699      0.915663     18\n",
      "5  1.239447    0.340952  0.245455  ...     0.674699      0.915663     18\n",
      "6  1.241608    0.341444  0.245455  ...     0.674699      0.915663     18\n",
      "7  1.245090    0.341564  0.245455  ...     0.674699      0.915663     18\n",
      "8  1.242683    0.341720  0.245455  ...     0.674699      0.915663     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.249997    0.341620  0.412844  ...     0.771084      0.915663      4\n",
      "1  1.242773    0.339828  0.311828  ...     0.771084      0.915663      4\n",
      "2  1.245825    0.343144  0.280899  ...     0.771084      0.915663      4\n",
      "3  1.242929    0.338303  0.188119  ...     0.771084      0.915663      4\n",
      "4  1.240598    0.339424  0.191919  ...     0.771084      0.915663      4\n",
      "5  1.239942    0.339111  0.238095  ...     0.771084      0.915663      4\n",
      "6  1.239603    0.340825  0.255814  ...     0.771084      0.915663      4\n",
      "7  1.246363    0.343585  0.492063  ...     0.771084      0.915663      4\n",
      "8  1.240095    0.340075  0.195876  ...     0.771084      0.915663      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099816    0.028790  0.500000  ...     0.975904      0.915663     12\n",
      "1  0.097696    0.028476  0.456376  ...     0.975904      0.915663     12\n",
      "2  0.097558    0.028884  0.023810  ...     0.975904      0.915663     12\n",
      "3  0.097646    0.028601  0.235849  ...     0.975904      0.915663     12\n",
      "4  0.097706    0.028359  0.506098  ...     0.975904      0.915663     12\n",
      "5  0.097881    0.028409  0.448980  ...     0.975904      0.915663     12\n",
      "6  0.096985    0.028356  0.496894  ...     0.975904      0.915663     12\n",
      "7  0.097841    0.028337  0.341463  ...     0.975904      0.915663     12\n",
      "8  0.101453    0.028505  0.477419  ...     0.975904      0.915663     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.106223    0.028879  0.503106  ...     0.963855      0.915663      9\n",
      "1  0.099603    0.028598  0.285714  ...     0.963855      0.915663      9\n",
      "2  0.099342    0.033256  0.509202  ...     0.963855      0.915663      9\n",
      "3  0.098541    0.028441  0.477124  ...     0.963855      0.915663      9\n",
      "4  0.098417    0.028283  0.448276  ...     0.963855      0.915663      9\n",
      "5  0.098417    0.028176  0.455782  ...     0.963855      0.915663      9\n",
      "6  0.098110    0.028282  0.279279  ...     0.963855      0.915663      9\n",
      "7  0.098587    0.028520  0.428571  ...     0.963855      0.915663      9\n",
      "8  0.104451    0.029723  0.379845  ...     0.963855      0.915663      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099021    0.028671  0.270000  ...     0.674699      0.915663     15\n",
      "1  0.098754    0.029008  0.293478  ...     0.674699      0.915663     15\n",
      "2  0.098301    0.028666  0.321429  ...     0.674699      0.915663     15\n",
      "3  0.098752    0.028315  0.245455  ...     0.674699      0.915663     15\n",
      "4  0.098361    0.028232  0.245455  ...     0.674699      0.915663     15\n",
      "5  0.098669    0.028658  0.272727  ...     0.674699      0.915663     15\n",
      "6  0.098515    0.028841  0.252336  ...     0.674699      0.915663     15\n",
      "7  0.099669    0.028874  0.254717  ...     0.674699      0.915663     15\n",
      "8  0.099092    0.029181  0.356322  ...     0.674699      0.915663     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.184192    0.031733  0.237624  ...     0.927711      0.915663      2\n",
      "1  0.098785    0.028796  0.134831  ...     0.927711      0.915663      2\n",
      "2  0.098007    0.029208  0.153846  ...     0.927711      0.915663      2\n",
      "3  0.098249    0.029039  0.068966  ...     0.927711      0.915663      2\n",
      "4  0.098418    0.029204  0.069767  ...     0.927711      0.915663      2\n",
      "5  0.098615    0.029999  0.153846  ...     0.927711      0.915663      2\n",
      "6  0.098437    0.028870  0.403101  ...     0.927711      0.915663      2\n",
      "7  0.098738    0.028856  0.189474  ...     0.927711      0.915663      2\n",
      "8  0.099356    0.028818  0.237624  ...     0.927711      0.915663      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.261887    0.338441  0.135417  ...     0.843373      0.915663      0\n",
      "1  1.244041    0.340092  0.136842  ...     0.843373      0.915663      0\n",
      "2  1.246132    0.340133  0.135417  ...     0.843373      0.915663      0\n",
      "3  1.244578    0.339163  0.135417  ...     0.843373      0.915663      0\n",
      "4  1.242610    0.340862  0.135417  ...     0.843373      0.915663      0\n",
      "5  1.238303    0.339014  0.204545  ...     0.843373      0.915663      0\n",
      "6  1.236891    0.340493  0.136842  ...     0.843373      0.915663      0\n",
      "7  1.239015    0.348074  0.135417  ...     0.843373      0.915663      0\n",
      "8  1.235693    0.337950  0.146067  ...     0.843373      0.915663      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.246968    0.336483  0.512658  ...     0.927711      0.915663      9\n",
      "1  1.234954    0.333191  0.457746  ...     0.927711      0.915663      9\n",
      "2  1.240692    0.332528  0.518750  ...     0.927711      0.915663      9\n",
      "3  1.239815    0.334169  0.483221  ...     0.927711      0.915663      9\n",
      "4  1.235115    0.337433  0.468966  ...     0.927711      0.915663      9\n",
      "5  1.235433    0.332627  0.457746  ...     0.927711      0.915663      9\n",
      "6  1.234304    0.334347  0.330435  ...     0.927711      0.915663      9\n",
      "7  1.236158    0.337497  0.437956  ...     0.927711      0.915663      9\n",
      "8  1.238947    0.334584  0.461538  ...     0.927711      0.915663      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098452    0.028852  0.247706  ...     0.674699      0.915663     14\n",
      "1  0.097184    0.028048  0.245455  ...     0.674699      0.915663     14\n",
      "2  0.097309    0.028136  0.245455  ...     0.674699      0.915663     14\n",
      "3  0.097313    0.028120  0.245455  ...     0.674699      0.915663     14\n",
      "4  0.097599    0.028735  0.317647  ...     0.674699      0.915663     14\n",
      "5  0.097296    0.028601  0.247706  ...     0.674699      0.915663     14\n",
      "6  0.097494    0.028721  0.247706  ...     0.674699      0.915663     14\n",
      "7  0.098431    0.028129  0.245455  ...     0.674699      0.915663     14\n",
      "8  0.097948    0.028596  0.245455  ...     0.674699      0.915663     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.102291    0.028757  0.198020  ...     0.759036      0.915663     23\n",
      "1  0.099628    0.028566  0.204082  ...     0.759036      0.915663     23\n",
      "2  0.098512    0.028540  0.503937  ...     0.759036      0.915663     23\n",
      "3  0.098796    0.028096  0.194175  ...     0.759036      0.915663     23\n",
      "4  0.098669    0.027931  0.194175  ...     0.759036      0.915663     23\n",
      "5  0.097800    0.027858  0.194175  ...     0.759036      0.915663     23\n",
      "6  0.097590    0.028084  0.343750  ...     0.759036      0.915663     23\n",
      "7  0.098046    0.028365  0.198020  ...     0.759036      0.915663     23\n",
      "8  0.097754    0.031941  0.212766  ...     0.759036      0.915663     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.155571    0.028702  0.173913  ...     0.915663      0.915663      2\n",
      "1  0.099269    0.028841  0.082353  ...     0.915663      0.915663      2\n",
      "2  0.098722    0.028705  0.116279  ...     0.915663      0.915663      2\n",
      "3  0.098906    0.028252  0.079545  ...     0.915663      0.915663      2\n",
      "4  0.099068    0.028049  0.077778  ...     0.915663      0.915663      2\n",
      "5  0.099183    0.028745  0.083333  ...     0.915663      0.915663      2\n",
      "6  0.098681    0.028622  0.377049  ...     0.915663      0.915663      2\n",
      "7  0.107677    0.030499  0.173913  ...     0.915663      0.915663      2\n",
      "8  0.099438    0.029246  0.240000  ...     0.915663      0.915663      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098786    0.028743  0.396396  ...     0.807229      0.915663      4\n",
      "1  0.098741    0.028773  0.367925  ...     0.807229      0.915663      4\n",
      "2  0.098752    0.029021  0.177778  ...     0.807229      0.915663      4\n",
      "3  0.098213    0.028792  0.166667  ...     0.807229      0.915663      4\n",
      "4  0.098190    0.028893  0.166667  ...     0.807229      0.915663      4\n",
      "5  0.098722    0.028817  0.302083  ...     0.807229      0.915663      4\n",
      "6  0.098528    0.028789  0.302083  ...     0.807229      0.915663      4\n",
      "7  0.098858    0.028470  0.503704  ...     0.807229      0.915663      4\n",
      "8  0.099413    0.028738  0.183908  ...     0.807229      0.915663      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.244297    0.334586  0.506173  ...     0.963855      0.915663     12\n",
      "1  1.237948    0.331286  0.483871  ...     0.963855      0.915663     12\n",
      "2  1.245790    0.331927  0.047619  ...     0.963855      0.915663     12\n",
      "3  1.238025    0.334338  0.354839  ...     0.963855      0.915663     12\n",
      "4  1.242156    0.336555  0.509202  ...     0.963855      0.915663     12\n",
      "5  1.238087    0.336880  0.448276  ...     0.963855      0.915663     12\n",
      "6  1.232943    0.333353  0.500000  ...     0.963855      0.915663     12\n",
      "7  1.231265    0.344521  0.316239  ...     0.963855      0.915663     12\n",
      "8  1.229600    0.331645  0.496855  ...     0.963855      0.915663     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.310997    0.334379  0.185567  ...     0.783133      0.915663     14\n",
      "1  1.233081    0.334632  0.180000  ...     0.783133      0.915663     14\n",
      "2  1.234741    0.333118  0.178218  ...     0.783133      0.915663     14\n",
      "3  1.237100    0.333279  0.178218  ...     0.783133      0.915663     14\n",
      "4  1.235378    0.337440  0.471545  ...     0.783133      0.915663     14\n",
      "5  1.234495    0.334583  0.181818  ...     0.783133      0.915663     14\n",
      "6  1.236084    0.336145  0.191489  ...     0.783133      0.915663     14\n",
      "7  1.237784    0.337821  0.180000  ...     0.783133      0.915663     14\n",
      "8  1.238206    0.335398  0.191489  ...     0.783133      0.915663     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098442    0.028571  0.187500  ...     0.783133      0.915663     18\n",
      "1  0.097816    0.028467  0.185567  ...     0.783133      0.915663     18\n",
      "2  0.097784    0.028716  0.181818  ...     0.783133      0.915663     18\n",
      "3  0.097764    0.028620  0.180000  ...     0.783133      0.915663     18\n",
      "4  0.097521    0.028494  0.180000  ...     0.783133      0.915663     18\n",
      "5  0.097457    0.027836  0.178218  ...     0.783133      0.915663     18\n",
      "6  0.097705    0.028039  0.178218  ...     0.783133      0.915663     18\n",
      "7  0.098034    0.028383  0.178218  ...     0.783133      0.915663     18\n",
      "8  0.098268    0.028197  0.178218  ...     0.783133      0.915663     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100725    0.034690  0.219780  ...     0.759036      0.915663     14\n",
      "1  0.099625    0.029132  0.210526  ...     0.759036      0.915663     14\n",
      "2  0.098598    0.028616  0.194175  ...     0.759036      0.915663     14\n",
      "3  0.101686    0.028692  0.208333  ...     0.759036      0.915663     14\n",
      "4  0.098089    0.028612  0.442478  ...     0.759036      0.915663     14\n",
      "5  0.098658    0.028546  0.198020  ...     0.759036      0.915663     14\n",
      "6  0.097584    0.028586  0.229885  ...     0.759036      0.915663     14\n",
      "7  0.098106    0.028702  0.200000  ...     0.759036      0.915663     14\n",
      "8  0.098109    0.030769  0.198020  ...     0.759036      0.915663     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.156148    0.028745  0.265487  ...     0.638554      0.915663     18\n",
      "1  0.098560    0.028975  0.267857  ...     0.638554      0.915663     18\n",
      "2  0.098794    0.028721  0.265487  ...     0.638554      0.915663     18\n",
      "3  0.098877    0.029157  0.267857  ...     0.638554      0.915663     18\n",
      "4  0.099539    0.028781  0.265487  ...     0.638554      0.915663     18\n",
      "5  0.099413    0.028700  0.265487  ...     0.638554      0.915663     18\n",
      "6  0.098954    0.028679  0.265487  ...     0.638554      0.915663     18\n",
      "7  0.123189    0.029592  0.265487  ...     0.638554      0.915663     18\n",
      "8  0.099485    0.029374  0.272727  ...     0.638554      0.915663     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.109715    0.028958  0.409091  ...     0.783133      0.915663     15\n",
      "1  0.098825    0.028502  0.386792  ...     0.783133      0.915663     15\n",
      "2  0.099940    0.028849  0.375000  ...     0.783133      0.915663     15\n",
      "3  0.098543    0.028664  0.178218  ...     0.783133      0.915663     15\n",
      "4  0.101411    0.028677  0.178218  ...     0.783133      0.915663     15\n",
      "5  0.098602    0.028758  0.343434  ...     0.783133      0.915663     15\n",
      "6  0.098396    0.028555  0.214286  ...     0.783133      0.915663     15\n",
      "7  0.098737    0.028766  0.209302  ...     0.783133      0.915663     15\n",
      "8  0.098551    0.028616  0.453782  ...     0.783133      0.915663     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.241206    0.340727  0.298969  ...     0.650602      0.915663     10\n",
      "1  1.237394    0.341747  0.287129  ...     0.650602      0.915663     10\n",
      "2  1.246108    0.338438  0.454545  ...     0.650602      0.915663     10\n",
      "3  1.238494    0.342137  0.281553  ...     0.650602      0.915663     10\n",
      "4  1.265261    0.344083  0.268519  ...     0.650602      0.915663     10\n",
      "5  1.246895    0.341136  0.364706  ...     0.650602      0.915663     10\n",
      "6  1.264388    0.344438  0.337209  ...     0.650602      0.915663     10\n",
      "7  1.255342    0.349804  0.364706  ...     0.650602      0.915663     10\n",
      "8  1.240556    0.341262  0.364706  ...     0.650602      0.915663     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.258864    0.338948  0.135417  ...     0.843373      0.915663      0\n",
      "1  1.247158    0.338739  0.139785  ...     0.843373      0.915663      0\n",
      "2  1.246764    0.341772  0.135417  ...     0.843373      0.915663      0\n",
      "3  1.248110    0.339898  0.135417  ...     0.843373      0.915663      0\n",
      "4  1.243769    0.339288  0.135417  ...     0.843373      0.915663      0\n",
      "5  1.243199    0.338955  0.195402  ...     0.843373      0.915663      0\n",
      "6  1.241122    0.338381  0.136842  ...     0.843373      0.915663      0\n",
      "7  1.248751    0.339810  0.135417  ...     0.843373      0.915663      0\n",
      "8  1.247616    0.339099  0.142857  ...     0.843373      0.915663      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098453    0.028381  0.271930  ...     0.626506      0.915663     23\n",
      "1  0.097974    0.028927  0.274336  ...     0.626506      0.915663     23\n",
      "2  0.098285    0.028610  0.490196  ...     0.626506      0.915663     23\n",
      "3  0.097909    0.028395  0.271930  ...     0.626506      0.915663     23\n",
      "4  0.097900    0.028453  0.271930  ...     0.626506      0.915663     23\n",
      "5  0.097795    0.028512  0.271930  ...     0.626506      0.915663     23\n",
      "6  0.098200    0.028810  0.306931  ...     0.626506      0.915663     23\n",
      "7  0.098258    0.028910  0.274336  ...     0.626506      0.915663     23\n",
      "8  0.099872    0.034492  0.279279  ...     0.626506      0.915663     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100602    0.028752  0.487342  ...     0.975904      0.915663     12\n",
      "1  0.100078    0.028521  0.470588  ...     0.975904      0.915663     12\n",
      "2  0.099037    0.028772  0.024096  ...     0.975904      0.915663     12\n",
      "3  0.100932    0.028449  0.307692  ...     0.975904      0.915663     12\n",
      "4  0.098638    0.028494  0.506098  ...     0.975904      0.915663     12\n",
      "5  0.099195    0.028298  0.441379  ...     0.975904      0.915663     12\n",
      "6  0.098904    0.028131  0.500000  ...     0.975904      0.915663     12\n",
      "7  0.098815    0.028354  0.336066  ...     0.975904      0.915663     12\n",
      "8  0.098516    0.030241  0.487342  ...     0.975904      0.915663     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.171750    0.028490  0.187500  ...     0.783133      0.915663     14\n",
      "1  0.097991    0.028620  0.197802  ...     0.783133      0.915663     14\n",
      "2  0.098024    0.027987  0.178218  ...     0.783133      0.915663     14\n",
      "3  0.098419    0.043268  0.183673  ...     0.783133      0.915663     14\n",
      "4  0.163880    0.028484  0.492188  ...     0.783133      0.915663     14\n",
      "5  0.098158    0.028644  0.185567  ...     0.783133      0.915663     14\n",
      "6  0.098162    0.029510  0.216867  ...     0.783133      0.915663     14\n",
      "7  0.098390    0.029646  0.181818  ...     0.783133      0.915663     14\n",
      "8  0.098626    0.029444  0.193548  ...     0.783133      0.915663     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099498    0.028457  0.170000  ...     0.795181      0.915663      0\n",
      "1  0.098529    0.028273  0.170000  ...     0.795181      0.915663      0\n",
      "2  0.098035    0.028500  0.170000  ...     0.795181      0.915663      0\n",
      "3  0.097998    0.028730  0.171717  ...     0.795181      0.915663      0\n",
      "4  0.098049    0.028290  0.170000  ...     0.795181      0.915663      0\n",
      "5  0.098880    0.028946  0.193182  ...     0.795181      0.915663      0\n",
      "6  0.098057    0.028578  0.170000  ...     0.795181      0.915663      0\n",
      "7  0.099358    0.028281  0.170000  ...     0.795181      0.915663      0\n",
      "8  0.097832    0.028727  0.177083  ...     0.795181      0.915663      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.262798    0.337837  0.205882  ...     0.746988      0.915663     14\n",
      "1  1.245816    0.337005  0.203883  ...     0.746988      0.915663     14\n",
      "2  1.248328    0.337168  0.201923  ...     0.746988      0.915663     14\n",
      "3  1.249780    0.340098  0.201923  ...     0.746988      0.915663     14\n",
      "4  1.245651    0.339579  0.456140  ...     0.746988      0.915663     14\n",
      "5  1.241384    0.337439  0.203883  ...     0.746988      0.915663     14\n",
      "6  1.243454    0.339812  0.207921  ...     0.746988      0.915663     14\n",
      "7  1.242663    0.343466  0.201923  ...     0.746988      0.915663     14\n",
      "8  1.242922    0.339932  0.203883  ...     0.746988      0.915663     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.315745    0.337448  0.209524  ...      0.73494      0.915663     18\n",
      "1  1.242444    0.336273  0.209524  ...      0.73494      0.915663     18\n",
      "2  1.236146    0.334296  0.209524  ...      0.73494      0.915663     18\n",
      "3  1.241556    0.339456  0.209524  ...      0.73494      0.915663     18\n",
      "4  1.236126    0.336375  0.209524  ...      0.73494      0.915663     18\n",
      "5  1.237046    0.334551  0.209524  ...      0.73494      0.915663     18\n",
      "6  1.237139    0.334605  0.209524  ...      0.73494      0.915663     18\n",
      "7  1.234166    0.335972  0.209524  ...      0.73494      0.915663     18\n",
      "8  1.240407    0.336101  0.209524  ...      0.73494      0.915663     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098455    0.028644  0.350515  ...     0.759036      0.915663     15\n",
      "1  0.098683    0.029011  0.307692  ...     0.759036      0.915663     15\n",
      "2  0.097801    0.028526  0.357143  ...     0.759036      0.915663     15\n",
      "3  0.098094    0.028437  0.194175  ...     0.759036      0.915663     15\n",
      "4  0.099219    0.028313  0.194175  ...     0.759036      0.915663     15\n",
      "5  0.098118    0.028620  0.232558  ...     0.759036      0.915663     15\n",
      "6  0.098539    0.028721  0.212766  ...     0.759036      0.915663     15\n",
      "7  0.098368    0.028622  0.212766  ...     0.759036      0.915663     15\n",
      "8  0.104082    0.028596  0.422018  ...     0.759036      0.915663     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099895    0.029176  0.292035  ...      0.60241      0.915663     10\n",
      "1  0.099654    0.029053  0.294643  ...      0.60241      0.915663     10\n",
      "2  0.103493    0.028820  0.523810  ...      0.60241      0.915663     10\n",
      "3  0.098393    0.028639  0.317308  ...      0.60241      0.915663     10\n",
      "4  0.098314    0.028453  0.289474  ...      0.60241      0.915663     10\n",
      "5  0.098058    0.028457  0.340206  ...      0.60241      0.915663     10\n",
      "6  0.097751    0.028289  0.302752  ...      0.60241      0.915663     10\n",
      "7  0.098533    0.028274  0.333333  ...      0.60241      0.915663     10\n",
      "8  0.110068    0.031281  0.311321  ...      0.60241      0.915663     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099503    0.028822  0.436975  ...     0.807229      0.915663      4\n",
      "1  0.099161    0.028669  0.417391  ...     0.807229      0.915663      4\n",
      "2  0.099157    0.028900  0.396396  ...     0.807229      0.915663      4\n",
      "3  0.099275    0.028788  0.166667  ...     0.807229      0.915663      4\n",
      "4  0.099092    0.028793  0.175824  ...     0.807229      0.915663      4\n",
      "5  0.099027    0.028815  0.316327  ...     0.807229      0.915663      4\n",
      "6  0.098890    0.028652  0.279570  ...     0.807229      0.915663      4\n",
      "7  0.120634    0.029754  0.484615  ...     0.807229      0.915663      4\n",
      "8  0.099575    0.028910  0.190476  ...     0.807229      0.915663      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.106757    0.028460  0.493750  ...     0.975904      0.915663     12\n",
      "1  0.098418    0.028554  0.493750  ...     0.975904      0.915663     12\n",
      "2  0.097770    0.028404  0.047059  ...     0.975904      0.915663     12\n",
      "3  0.098393    0.028343  0.381679  ...     0.975904      0.915663     12\n",
      "4  0.100938    0.028861  0.506098  ...     0.975904      0.915663     12\n",
      "5  0.101403    0.028519  0.474026  ...     0.975904      0.915663     12\n",
      "6  0.098066    0.028172  0.500000  ...     0.975904      0.915663     12\n",
      "7  0.098621    0.028505  0.325000  ...     0.975904      0.915663     12\n",
      "8  0.098482    0.028306  0.496894  ...     0.975904      0.915663     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.246665    0.336690  0.503226  ...     0.927711      0.915663      9\n",
      "1  1.256210    0.337247  0.352941  ...     0.927711      0.915663      9\n",
      "2  1.242947    0.338372  0.515723  ...     0.927711      0.915663      9\n",
      "3  1.251639    0.343608  0.476190  ...     0.927711      0.915663      9\n",
      "4  1.246651    0.340833  0.324561  ...     0.927711      0.915663      9\n",
      "5  1.241706    0.337592  0.368852  ...     0.927711      0.915663      9\n",
      "6  1.240462    0.340780  0.189474  ...     0.927711      0.915663      9\n",
      "7  1.242477    0.340983  0.421053  ...     0.927711      0.915663      9\n",
      "8  1.240983    0.336205  0.407692  ...     0.927711      0.915663      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.244239    0.341115  0.136364  ...     0.855422      0.915663      2\n",
      "1  1.241054    0.339223  0.127660  ...     0.855422      0.915663      2\n",
      "2  1.240308    0.337560  0.131868  ...     0.855422      0.915663      2\n",
      "3  1.246560    0.362895  0.126316  ...     0.855422      0.915663      2\n",
      "4  1.239041    0.336248  0.126316  ...     0.855422      0.915663      2\n",
      "5  1.238471    0.336860  0.127660  ...     0.855422      0.915663      2\n",
      "6  1.240742    0.341197  0.219780  ...     0.855422      0.915663      2\n",
      "7  1.239495    0.341298  0.134831  ...     0.855422      0.915663      2\n",
      "8  1.242993    0.339302  0.139535  ...     0.855422      0.915663      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098106    0.028802  0.282828  ...     0.662651      0.915663     10\n",
      "1  0.097670    0.028750  0.261682  ...     0.662651      0.915663     10\n",
      "2  0.097501    0.028301  0.375000  ...     0.662651      0.915663     10\n",
      "3  0.097965    0.029129  0.269231  ...     0.662651      0.915663     10\n",
      "4  0.097798    0.028618  0.254545  ...     0.662651      0.915663     10\n",
      "5  0.098170    0.028652  0.269231  ...     0.662651      0.915663     10\n",
      "6  0.097373    0.028639  0.280000  ...     0.662651      0.915663     10\n",
      "7  0.098718    0.029074  0.277228  ...     0.662651      0.915663     10\n",
      "8  0.103320    0.028811  0.285714  ...     0.662651      0.915663     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100037    0.028599  0.493590  ...     0.951807      0.915663     12\n",
      "1  0.099598    0.028539  0.483660  ...     0.951807      0.915663     12\n",
      "2  0.099356    0.028654  0.047619  ...     0.951807      0.915663     12\n",
      "3  0.098459    0.028480  0.294643  ...     0.951807      0.915663     12\n",
      "4  0.098358    0.028625  0.512346  ...     0.951807      0.915663     12\n",
      "5  0.098177    0.028160  0.435714  ...     0.951807      0.915663     12\n",
      "6  0.098265    0.028407  0.503145  ...     0.951807      0.915663     12\n",
      "7  0.099056    0.028173  0.225490  ...     0.951807      0.915663     12\n",
      "8  0.098412    0.032862  0.503145  ...     0.951807      0.915663     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.107062    0.049361  0.503226  ...     0.927711      0.915663     12\n",
      "1  0.152091    0.028756  0.476190  ...     0.927711      0.915663     12\n",
      "2  0.099219    0.028627  0.071429  ...     0.927711      0.915663     12\n",
      "3  0.098363    0.028347  0.425373  ...     0.927711      0.915663     12\n",
      "4  0.098933    0.028720  0.518750  ...     0.927711      0.915663     12\n",
      "5  0.109238    0.029195  0.412214  ...     0.927711      0.915663     12\n",
      "6  0.098714    0.028888  0.503226  ...     0.927711      0.915663     12\n",
      "7  0.098092    0.028629  0.252427  ...     0.927711      0.915663     12\n",
      "8  0.097973    0.028514  0.486667  ...     0.927711      0.915663     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.256389    0.339776  0.278846  ...     0.650602      0.915663     15\n",
      "1  1.241569    0.337507  0.276190  ...     0.650602      0.915663     15\n",
      "2  1.246222    0.336621  0.308511  ...     0.650602      0.915663     15\n",
      "3  1.235948    0.336144  0.258929  ...     0.650602      0.915663     15\n",
      "4  1.281565    0.351442  0.258929  ...     0.650602      0.915663     15\n",
      "5  1.248443    0.335935  0.278846  ...     0.650602      0.915663     15\n",
      "6  1.265938    0.341330  0.263636  ...     0.650602      0.915663     15\n",
      "7  1.276721    0.343922  0.261261  ...     0.650602      0.915663     15\n",
      "8  1.247989    0.340392  0.329545  ...     0.650602      0.915663     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.244567    0.336382  0.503067  ...     0.975904      0.915663     12\n",
      "1  1.240865    0.336779  0.484076  ...     0.975904      0.915663     12\n",
      "2  1.241289    0.333416  0.047059  ...     0.975904      0.915663     12\n",
      "3  1.247614    0.335411  0.400000  ...     0.975904      0.915663     12\n",
      "4  1.243627    0.339516  0.506098  ...     0.975904      0.915663     12\n",
      "5  1.237923    0.332630  0.460000  ...     0.975904      0.915663     12\n",
      "6  1.240498    0.340864  0.496894  ...     0.975904      0.915663     12\n",
      "7  1.243391    0.336225  0.362205  ...     0.975904      0.915663     12\n",
      "8  1.241739    0.339546  0.496894  ...     0.975904      0.915663     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.144427    0.049659  0.474820  ...     0.879518      0.915663      9\n",
      "1  0.117620    0.028727  0.179775  ...     0.879518      0.915663      9\n",
      "2  0.098432    0.028268  0.522876  ...     0.879518      0.915663      9\n",
      "3  0.098434    0.028368  0.359649  ...     0.879518      0.915663      9\n",
      "4  0.097910    0.028475  0.130952  ...     0.879518      0.915663      9\n",
      "5  0.097995    0.028447  0.381356  ...     0.879518      0.915663      9\n",
      "6  0.098382    0.028755  0.111111  ...     0.879518      0.915663      9\n",
      "7  0.100698    0.029128  0.434109  ...     0.879518      0.915663      9\n",
      "8  0.103788    0.028764  0.215054  ...     0.879518      0.915663      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.101118    0.028806  0.252252  ...     0.662651      0.915663     23\n",
      "1  0.100015    0.029081  0.261682  ...     0.662651      0.915663     23\n",
      "2  0.099288    0.028824  0.481132  ...     0.662651      0.915663     23\n",
      "3  0.099160    0.028594  0.252252  ...     0.662651      0.915663     23\n",
      "4  0.099113    0.028607  0.254545  ...     0.662651      0.915663     23\n",
      "5  0.098543    0.028244  0.252252  ...     0.662651      0.915663     23\n",
      "6  0.098725    0.028843  0.271845  ...     0.662651      0.915663     23\n",
      "7  0.099121    0.028714  0.252252  ...     0.662651      0.915663     23\n",
      "8  0.098847    0.033856  0.261682  ...     0.662651      0.915663     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.118158    0.028705  0.228916  ...     0.771084      0.915663     15\n",
      "1  0.098624    0.028501  0.311828  ...     0.771084      0.915663     15\n",
      "2  0.097874    0.028377  0.378641  ...     0.771084      0.915663     15\n",
      "3  0.098069    0.028395  0.188119  ...     0.771084      0.915663     15\n",
      "4  0.098115    0.027989  0.186275  ...     0.771084      0.915663     15\n",
      "5  0.098254    0.028463  0.280899  ...     0.771084      0.915663     15\n",
      "6  0.098310    0.028395  0.200000  ...     0.771084      0.915663     15\n",
      "7  0.099230    0.028530  0.197917  ...     0.771084      0.915663     15\n",
      "8  0.098457    0.028821  0.264368  ...     0.771084      0.915663     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098515    0.028594  0.366337  ...     0.771084      0.915663      4\n",
      "1  0.098322    0.028701  0.215909  ...     0.771084      0.915663      4\n",
      "2  0.097871    0.028670  0.208791  ...     0.771084      0.915663      4\n",
      "3  0.097842    0.027942  0.186275  ...     0.771084      0.915663      4\n",
      "4  0.097741    0.028145  0.186275  ...     0.771084      0.915663      4\n",
      "5  0.098241    0.028926  0.226190  ...     0.771084      0.915663      4\n",
      "6  0.098089    0.029733  0.220930  ...     0.771084      0.915663      4\n",
      "7  0.098047    0.028611  0.471074  ...     0.771084      0.915663      4\n",
      "8  0.097724    0.028526  0.193878  ...     0.771084      0.915663      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.258401    0.337499  0.386792  ...     0.783133      0.915663      4\n",
      "1  1.248010    0.338084  0.293478  ...     0.783133      0.915663      4\n",
      "2  1.246398    0.338146  0.261364  ...     0.783133      0.915663      4\n",
      "3  1.244528    0.339730  0.180000  ...     0.783133      0.915663      4\n",
      "4  1.243640    0.336770  0.181818  ...     0.783133      0.915663      4\n",
      "5  1.242944    0.341414  0.277778  ...     0.783133      0.915663      4\n",
      "6  1.241063    0.341105  0.244186  ...     0.783133      0.915663      4\n",
      "7  1.248401    0.343194  0.496124  ...     0.783133      0.915663      4\n",
      "8  1.240447    0.338254  0.187500  ...     0.783133      0.915663      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.253384    0.345333  0.273585  ...     0.650602      0.915663     10\n",
      "1  1.239063    0.336872  0.268519  ...     0.650602      0.915663     10\n",
      "2  1.238422    0.338322  0.485714  ...     0.650602      0.915663     10\n",
      "3  1.242602    0.337872  0.278846  ...     0.650602      0.915663     10\n",
      "4  1.238488    0.338614  0.263636  ...     0.650602      0.915663     10\n",
      "5  1.234961    0.335920  0.322222  ...     0.650602      0.915663     10\n",
      "6  1.236870    0.338422  0.287129  ...     0.650602      0.915663     10\n",
      "7  1.238215    0.339557  0.311828  ...     0.650602      0.915663     10\n",
      "8  1.241735    0.338315  0.302083  ...     0.650602      0.915663     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.097592    0.028638  0.409091  ...     0.783133      0.915663      4\n",
      "1  0.097344    0.028713  0.235294  ...     0.783133      0.915663      4\n",
      "2  0.096893    0.028756  0.214286  ...     0.783133      0.915663      4\n",
      "3  0.097455    0.028584  0.180000  ...     0.783133      0.915663      4\n",
      "4  0.097332    0.028569  0.181818  ...     0.783133      0.915663      4\n",
      "5  0.096936    0.028547  0.202247  ...     0.783133      0.915663      4\n",
      "6  0.097203    0.028640  0.252874  ...     0.783133      0.915663      4\n",
      "7  0.099554    0.028773  0.480000  ...     0.783133      0.915663      4\n",
      "8  0.101583    0.028495  0.187500  ...     0.783133      0.915663      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100523    0.029081  0.129412  ...      0.86747      0.915663      0\n",
      "1  0.099490    0.028764  0.225806  ...      0.86747      0.915663      0\n",
      "2  0.098902    0.028923  0.118280  ...      0.86747      0.915663      0\n",
      "3  0.098896    0.028688  0.119565  ...      0.86747      0.915663      0\n",
      "4  0.098361    0.028317  0.118280  ...      0.86747      0.915663      0\n",
      "5  0.098294    0.028466  0.142857  ...      0.86747      0.915663      0\n",
      "6  0.098445    0.028314  0.120879  ...      0.86747      0.915663      0\n",
      "7  0.098231    0.028498  0.119565  ...      0.86747      0.915663      0\n",
      "8  0.099961    0.028320  0.162791  ...      0.86747      0.915663      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.114216    0.029122  0.183908  ...     0.807229      0.915663     14\n",
      "1  0.103993    0.028990  0.173913  ...     0.807229      0.915663     14\n",
      "2  0.100053    0.028368  0.161616  ...     0.807229      0.915663     14\n",
      "3  0.099434    0.028923  0.164948  ...     0.807229      0.915663     14\n",
      "4  0.099751    0.028875  0.496241  ...     0.807229      0.915663     14\n",
      "5  0.098935    0.029002  0.164948  ...     0.807229      0.915663     14\n",
      "6  0.099162    0.028925  0.170213  ...     0.807229      0.915663     14\n",
      "7  0.103902    0.031156  0.161616  ...     0.807229      0.915663     14\n",
      "8  0.103805    0.030372  0.172043  ...     0.807229      0.915663     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098999    0.028954  0.164835  ...     0.819277      0.915663      2\n",
      "1  0.098455    0.028739  0.154639  ...     0.819277      0.915663      2\n",
      "2  0.098255    0.028793  0.157895  ...     0.819277      0.915663      2\n",
      "3  0.098271    0.028390  0.153061  ...     0.819277      0.915663      2\n",
      "4  0.098508    0.028081  0.153061  ...     0.819277      0.915663      2\n",
      "5  0.098889    0.028916  0.161290  ...     0.819277      0.915663      2\n",
      "6  0.100002    0.028829  0.306122  ...     0.819277      0.915663      2\n",
      "7  0.099230    0.028922  0.166667  ...     0.819277      0.915663      2\n",
      "8  0.098949    0.028924  0.170455  ...     0.819277      0.915663      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.249255    0.336145  0.300971  ...     0.626506      0.915663     15\n",
      "1  1.238473    0.336657  0.281818  ...     0.626506      0.915663     15\n",
      "2  1.244157    0.334579  0.319588  ...     0.626506      0.915663     15\n",
      "3  1.246617    0.340101  0.271930  ...     0.626506      0.915663     15\n",
      "4  1.237679    0.334891  0.271930  ...     0.626506      0.915663     15\n",
      "5  1.233959    0.336618  0.284404  ...     0.626506      0.915663     15\n",
      "6  1.235698    0.334785  0.279279  ...     0.626506      0.915663     15\n",
      "7  1.241636    0.341321  0.274336  ...     0.626506      0.915663     15\n",
      "8  1.232053    0.336607  0.352273  ...     0.626506      0.915663     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.244924    0.336062  0.278261  ...     0.614458      0.915663     23\n",
      "1  1.239531    0.336185  0.280702  ...     0.614458      0.915663     23\n",
      "2  1.240196    0.338368  0.484848  ...     0.614458      0.915663     23\n",
      "3  1.242074    0.332356  0.278261  ...     0.614458      0.915663     23\n",
      "4  1.238391    0.334714  0.278261  ...     0.614458      0.915663     23\n",
      "5  1.235897    0.331909  0.278261  ...     0.614458      0.915663     23\n",
      "6  1.246562    0.337471  0.283186  ...     0.614458      0.915663     23\n",
      "7  1.254230    0.337190  0.278261  ...     0.614458      0.915663     23\n",
      "8  1.239905    0.338311  0.280702  ...     0.614458      0.915663     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.187413    0.032517  0.129412  ...      0.86747      0.915663      2\n",
      "1  0.097577    0.028790  0.118280  ...      0.86747      0.915663      2\n",
      "2  0.097465    0.028821  0.127907  ...      0.86747      0.915663      2\n",
      "3  0.097210    0.028065  0.117021  ...      0.86747      0.915663      2\n",
      "4  0.097155    0.027925  0.117021  ...      0.86747      0.915663      2\n",
      "5  0.097500    0.028109  0.117021  ...      0.86747      0.915663      2\n",
      "6  0.097661    0.028600  0.200000  ...      0.86747      0.915663      2\n",
      "7  0.097989    0.028577  0.129412  ...      0.86747      0.915663      2\n",
      "8  0.100243    0.028708  0.129412  ...      0.86747      0.915663      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100533    0.029246  0.263158  ...     0.698795      0.915663     15\n",
      "1  0.099707    0.028770  0.317647  ...     0.698795      0.915663     15\n",
      "2  0.103119    0.028529  0.389474  ...     0.698795      0.915663     15\n",
      "3  0.098828    0.028265  0.231481  ...     0.698795      0.915663     15\n",
      "4  0.098991    0.028330  0.231481  ...     0.698795      0.915663     15\n",
      "5  0.098607    0.028669  0.274725  ...     0.698795      0.915663     15\n",
      "6  0.098224    0.028515  0.242718  ...     0.698795      0.915663     15\n",
      "7  0.098493    0.028459  0.250000  ...     0.698795      0.915663     15\n",
      "8  0.111645    0.030598  0.414141  ...     0.698795      0.915663     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099276    0.028486  0.506250  ...     0.951807      0.915663     12\n",
      "1  0.098469    0.028430  0.490323  ...     0.951807      0.915663     12\n",
      "2  0.098924    0.028421  0.091954  ...     0.951807      0.915663     12\n",
      "3  0.097937    0.028388  0.336134  ...     0.951807      0.915663     12\n",
      "4  0.098627    0.028901  0.509317  ...     0.951807      0.915663     12\n",
      "5  0.098817    0.028240  0.362903  ...     0.951807      0.915663     12\n",
      "6  0.098608    0.028405  0.503145  ...     0.951807      0.915663     12\n",
      "7  0.099792    0.028575  0.294643  ...     0.951807      0.915663     12\n",
      "8  0.098953    0.028456  0.483660  ...     0.951807      0.915663     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100775    0.029401  0.439394  ...     0.891566      0.915663      9\n",
      "1  0.101083    0.029060  0.098901  ...     0.891566      0.915663      9\n",
      "2  0.100843    0.029380  0.471429  ...     0.891566      0.915663      9\n",
      "3  0.100363    0.029266  0.393443  ...     0.891566      0.915663      9\n",
      "4  0.099816    0.029563  0.098901  ...     0.891566      0.915663      9\n",
      "5  0.100204    0.029249  0.204301  ...     0.891566      0.915663      9\n",
      "6  0.101035    0.028666  0.097826  ...     0.891566      0.915663      9\n",
      "7  0.101674    0.029267  0.430769  ...     0.891566      0.915663      9\n",
      "8  0.101124    0.030188  0.104651  ...     0.891566      0.915663      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.262478    0.338988  0.238532  ...     0.686747      0.915663     23\n",
      "1  1.267446    0.339368  0.240741  ...     0.686747      0.915663     23\n",
      "2  1.262111    0.342835  0.457143  ...     0.686747      0.915663     23\n",
      "3  1.249196    0.338862  0.238532  ...     0.686747      0.915663     23\n",
      "4  1.246977    0.345789  0.238532  ...     0.686747      0.915663     23\n",
      "5  1.245373    0.339046  0.238532  ...     0.686747      0.915663     23\n",
      "6  1.243280    0.343621  0.260000  ...     0.686747      0.915663     23\n",
      "7  1.249912    0.341825  0.238532  ...     0.686747      0.915663     23\n",
      "8  1.243651    0.340766  0.240741  ...     0.686747      0.915663     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.252171    0.340710  0.323810  ...     0.590361      0.915663     15\n",
      "1  1.242581    0.341825  0.309091  ...     0.590361      0.915663     15\n",
      "2  1.240862    0.338753  0.361702  ...     0.590361      0.915663     15\n",
      "3  1.244108    0.336020  0.290598  ...     0.590361      0.915663     15\n",
      "4  1.244030    0.339305  0.290598  ...     0.590361      0.915663     15\n",
      "5  1.243528    0.338099  0.309091  ...     0.590361      0.915663     15\n",
      "6  1.241026    0.343270  0.298246  ...     0.590361      0.915663     15\n",
      "7  1.242340    0.344037  0.295652  ...     0.590361      0.915663     15\n",
      "8  1.238608    0.339927  0.365591  ...     0.590361      0.915663     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099140    0.028989  0.186275  ...     0.771084      0.915663      0\n",
      "1  0.098334    0.028664  0.186275  ...     0.771084      0.915663      0\n",
      "2  0.098533    0.028377  0.186275  ...     0.771084      0.915663      0\n",
      "3  0.098216    0.028504  0.186275  ...     0.771084      0.915663      0\n",
      "4  0.098479    0.028770  0.186275  ...     0.771084      0.915663      0\n",
      "5  0.100773    0.029234  0.211111  ...     0.771084      0.915663      0\n",
      "6  0.098234    0.028386  0.186275  ...     0.771084      0.915663      0\n",
      "7  0.099303    0.028615  0.186275  ...     0.771084      0.915663      0\n",
      "8  0.098769    0.029186  0.190000  ...     0.771084      0.915663      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.101405    0.029617  0.371134  ...      0.73494      0.915663      4\n",
      "1  0.100282    0.029401  0.322222  ...      0.73494      0.915663      4\n",
      "2  0.104515    0.029150  0.383838  ...      0.73494      0.915663      4\n",
      "3  0.099355    0.029329  0.209524  ...      0.73494      0.915663      4\n",
      "4  0.099727    0.029080  0.215686  ...      0.73494      0.915663      4\n",
      "5  0.099133    0.028887  0.314607  ...      0.73494      0.915663      4\n",
      "6  0.098914    0.029012  0.241758  ...      0.73494      0.915663      4\n",
      "7  0.099188    0.028810  0.424528  ...      0.73494      0.915663      4\n",
      "8  0.104504    0.030869  0.222222  ...      0.73494      0.915663      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.101013    0.029140  0.148936  ...     0.831325      0.915663      0\n",
      "1  0.102505    0.028908  0.160920  ...     0.831325      0.915663      0\n",
      "2  0.099366    0.028593  0.144330  ...     0.831325      0.915663      0\n",
      "3  0.098975    0.028658  0.144330  ...     0.831325      0.915663      0\n",
      "4  0.099306    0.028611  0.157303  ...     0.831325      0.915663      0\n",
      "5  0.098620    0.028640  0.258065  ...     0.831325      0.915663      0\n",
      "6  0.099588    0.028780  0.160920  ...     0.831325      0.915663      0\n",
      "7  0.099484    0.028996  0.150538  ...     0.831325      0.915663      0\n",
      "8  0.099220    0.029319  0.147368  ...     0.831325      0.915663      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099121    0.028841  0.188889  ...     0.795181      0.915663     14\n",
      "1  0.098990    0.028834  0.184783  ...     0.795181      0.915663     14\n",
      "2  0.098928    0.028385  0.170000  ...     0.795181      0.915663     14\n",
      "3  0.098528    0.028863  0.171717  ...     0.795181      0.915663     14\n",
      "4  0.098459    0.028823  0.484375  ...     0.795181      0.915663     14\n",
      "5  0.098869    0.028990  0.178947  ...     0.795181      0.915663     14\n",
      "6  0.099830    0.028862  0.266667  ...     0.795181      0.915663     14\n",
      "7  0.099458    0.028692  0.180851  ...     0.795181      0.915663     14\n",
      "8  0.098114    0.028535  0.232558  ...     0.795181      0.915663     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.268718    0.342510  0.224299  ...     0.710843      0.915663     18\n",
      "1  1.257158    0.340351  0.226415  ...     0.710843      0.915663     18\n",
      "2  1.256040    0.351429  0.224299  ...     0.710843      0.915663     18\n",
      "3  1.254158    0.344315  0.224299  ...     0.710843      0.915663     18\n",
      "4  1.252078    0.342198  0.224299  ...     0.710843      0.915663     18\n",
      "5  1.245972    0.339814  0.224299  ...     0.710843      0.915663     18\n",
      "6  1.244540    0.341854  0.224299  ...     0.710843      0.915663     18\n",
      "7  1.252901    0.342746  0.224299  ...     0.710843      0.915663     18\n",
      "8  1.244658    0.341308  0.224299  ...     0.710843      0.915663     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.272629    0.347911  0.383178  ...     0.795181      0.915663      4\n",
      "1  1.263190    0.342485  0.297872  ...     0.795181      0.915663      4\n",
      "2  1.248382    0.342047  0.297872  ...     0.795181      0.915663      4\n",
      "3  1.253939    0.343336  0.171717  ...     0.795181      0.915663      4\n",
      "4  1.251450    0.342221  0.175258  ...     0.795181      0.915663      4\n",
      "5  1.246487    0.343759  0.232558  ...     0.795181      0.915663      4\n",
      "6  1.245193    0.347063  0.266667  ...     0.795181      0.915663      4\n",
      "7  1.247190    0.345971  0.484375  ...     0.795181      0.915663      4\n",
      "8  1.245286    0.344323  0.180851  ...     0.795181      0.915663      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099015    0.029724  0.506250  ...     0.951807      0.915663     12\n",
      "1  0.097683    0.028395  0.466216  ...     0.951807      0.915663     12\n",
      "2  0.097795    0.028701  0.047059  ...     0.951807      0.915663     12\n",
      "3  0.098602    0.028361  0.368000  ...     0.951807      0.915663     12\n",
      "4  0.098172    0.028507  0.509317  ...     0.951807      0.915663     12\n",
      "5  0.097519    0.028426  0.443662  ...     0.951807      0.915663     12\n",
      "6  0.097551    0.028498  0.496815  ...     0.951807      0.915663     12\n",
      "7  0.098527    0.028284  0.233010  ...     0.951807      0.915663     12\n",
      "8  0.098347    0.028527  0.496815  ...     0.951807      0.915663     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100502    0.029110  0.242718  ...     0.698795      0.915663     14\n",
      "1  0.099413    0.028806  0.271739  ...     0.698795      0.915663     14\n",
      "2  0.098835    0.028388  0.231481  ...     0.698795      0.915663     14\n",
      "3  0.098375    0.028564  0.238095  ...     0.698795      0.915663     14\n",
      "4  0.098689    0.028444  0.425743  ...     0.698795      0.915663     14\n",
      "5  0.098459    0.028312  0.235849  ...     0.698795      0.915663     14\n",
      "6  0.097999    0.028600  0.233645  ...     0.698795      0.915663     14\n",
      "7  0.098292    0.028084  0.231481  ...     0.698795      0.915663     14\n",
      "8  0.098496    0.028721  0.240385  ...     0.698795      0.915663     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.109096    0.028220  0.500000  ...     0.915663      0.915663      9\n",
      "1  0.099267    0.027866  0.289720  ...     0.915663      0.915663      9\n",
      "2  0.097841    0.027921  0.453237  ...     0.915663      0.915663      9\n",
      "3  0.098382    0.028193  0.506494  ...     0.915663      0.915663      9\n",
      "4  0.098365    0.028037  0.366667  ...     0.915663      0.915663      9\n",
      "5  0.098024    0.028036  0.302752  ...     0.915663      0.915663      9\n",
      "6  0.098236    0.028039  0.208333  ...     0.915663      0.915663      9\n",
      "7  0.098268    0.028168  0.371901  ...     0.915663      0.915663      9\n",
      "8  0.099303    0.028209  0.392000  ...     0.915663      0.915663      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100034    0.028721  0.194175  ...     0.759036      0.915663     18\n",
      "1  0.099468    0.028539  0.194175  ...     0.759036      0.915663     18\n",
      "2  0.099466    0.028324  0.194175  ...     0.759036      0.915663     18\n",
      "3  0.099121    0.028583  0.194175  ...     0.759036      0.915663     18\n",
      "4  0.099522    0.029055  0.196078  ...     0.759036      0.915663     18\n",
      "5  0.113109    0.031310  0.194175  ...     0.759036      0.915663     18\n",
      "6  0.100249    0.028774  0.194175  ...     0.759036      0.915663     18\n",
      "7  0.100294    0.028366  0.194175  ...     0.759036      0.915663     18\n",
      "8  0.099494    0.028957  0.202020  ...     0.759036      0.915663     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.268099    0.349691  0.133333  ...     0.855422      0.915663      2\n",
      "1  1.250418    0.339662  0.127660  ...     0.855422      0.915663      2\n",
      "2  1.256066    0.340995  0.131868  ...     0.855422      0.915663      2\n",
      "3  1.251716    0.341276  0.126316  ...     0.855422      0.915663      2\n",
      "4  1.244925    0.340763  0.126316  ...     0.855422      0.915663      2\n",
      "5  1.242991    0.338688  0.126316  ...     0.855422      0.915663      2\n",
      "6  1.633305    0.343983  0.275510  ...     0.855422      0.915663      2\n",
      "7  1.250761    0.343572  0.133333  ...     0.855422      0.915663      2\n",
      "8  1.243228    0.341273  0.136364  ...     0.855422      0.915663      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098537    0.028287  0.490566  ...     0.975904      0.915663     12\n",
      "1  0.097890    0.028246  0.477419  ...     0.975904      0.915663     12\n",
      "2  0.097584    0.028489  0.023810  ...     0.975904      0.915663     12\n",
      "3  0.098477    0.028290  0.425532  ...     0.975904      0.915663     12\n",
      "4  0.097750    0.028375  0.506098  ...     0.975904      0.915663     12\n",
      "5  0.097404    0.028363  0.467105  ...     0.975904      0.915663     12\n",
      "6  0.097545    0.028238  0.500000  ...     0.975904      0.915663     12\n",
      "7  0.098160    0.028683  0.250000  ...     0.975904      0.915663     12\n",
      "8  0.097888    0.028537  0.490566  ...     0.975904      0.915663     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099328    0.028963  0.163043  ...     0.819277      0.915663     14\n",
      "1  0.098632    0.028560  0.153061  ...     0.819277      0.915663     14\n",
      "2  0.108718    0.028414  0.153061  ...     0.819277      0.915663     14\n",
      "3  0.099089    0.028562  0.153061  ...     0.819277      0.915663     14\n",
      "4  0.098823    0.030604  0.472868  ...     0.819277      0.915663     14\n",
      "5  0.098491    0.028741  0.161290  ...     0.819277      0.915663     14\n",
      "6  0.098215    0.029005  0.163043  ...     0.819277      0.915663     14\n",
      "7  0.099092    0.029025  0.156250  ...     0.819277      0.915663     14\n",
      "8  0.098214    0.032624  0.164835  ...     0.819277      0.915663     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100602    0.029151  0.509434  ...     0.939759      0.915663      9\n",
      "1  0.099992    0.028645  0.422222  ...     0.939759      0.915663      9\n",
      "2  0.099179    0.028548  0.493506  ...     0.939759      0.915663      9\n",
      "3  0.099229    0.028813  0.503185  ...     0.939759      0.915663      9\n",
      "4  0.099261    0.028437  0.321739  ...     0.939759      0.915663      9\n",
      "5  0.099224    0.028572  0.426471  ...     0.939759      0.915663      9\n",
      "6  0.099333    0.028469  0.212121  ...     0.939759      0.915663      9\n",
      "7  0.098803    0.028609  0.404580  ...     0.939759      0.915663      9\n",
      "8  0.099017    0.038163  0.480000  ...     0.939759      0.915663      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100065    0.029143  0.186047  ...     0.843373      0.915663      2\n",
      "1  0.098603    0.028787  0.138298  ...     0.843373      0.915663      2\n",
      "2  0.099530    0.029134  0.147727  ...     0.843373      0.915663      2\n",
      "3  0.098798    0.028833  0.136842  ...     0.843373      0.915663      2\n",
      "4  0.099324    0.028361  0.135417  ...     0.843373      0.915663      2\n",
      "5  0.099293    0.034014  0.136842  ...     0.843373      0.915663      2\n",
      "6  0.100830    0.029087  0.320388  ...     0.843373      0.915663      2\n",
      "7  0.098642    0.028872  0.195402  ...     0.843373      0.915663      2\n",
      "8  0.099042    0.029075  0.152941  ...     0.843373      0.915663      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099745    0.029341  0.257143  ...     0.674699      0.915663     15\n",
      "1  0.099070    0.029503  0.254717  ...     0.674699      0.915663     15\n",
      "2  0.099081    0.029334  0.278351  ...     0.674699      0.915663     15\n",
      "3  0.098814    0.028979  0.250000  ...     0.674699      0.915663     15\n",
      "4  0.099827    0.033345  0.245455  ...     0.674699      0.915663     15\n",
      "5  0.112569    0.029016  0.262136  ...     0.674699      0.915663     15\n",
      "6  0.100220    0.029132  0.262136  ...     0.674699      0.915663     15\n",
      "7  0.100608    0.028952  0.275510  ...     0.674699      0.915663     15\n",
      "8  0.099125    0.028660  0.410526  ...     0.674699      0.915663     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.244107    0.337367  0.500000  ...     0.975904      0.915663     12\n",
      "1  1.237824    0.336685  0.480769  ...     0.975904      0.915663     12\n",
      "2  1.237443    0.333438  0.035714  ...     0.975904      0.915663     12\n",
      "3  1.242176    0.335065  0.367188  ...     0.975904      0.915663     12\n",
      "4  1.236135    0.334544  0.506098  ...     0.975904      0.915663     12\n",
      "5  1.236813    0.335535  0.467105  ...     0.975904      0.915663     12\n",
      "6  1.236242    0.334357  0.493750  ...     0.975904      0.915663     12\n",
      "7  1.238258    0.337025  0.313559  ...     0.975904      0.915663     12\n",
      "8  1.234311    0.335246  0.496894  ...     0.975904      0.915663     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098688    0.028910  0.172414  ...     0.819277      0.915663     14\n",
      "1  0.098061    0.028568  0.370370  ...     0.819277      0.915663     14\n",
      "2  0.098088    0.028411  0.153061  ...     0.819277      0.915663     14\n",
      "3  0.098105    0.028811  0.176471  ...     0.819277      0.915663     14\n",
      "4  0.098488    0.028764  0.510791  ...     0.819277      0.915663     14\n",
      "5  0.099384    0.028318  0.153061  ...     0.819277      0.915663     14\n",
      "6  0.098283    0.028616  0.291667  ...     0.819277      0.915663     14\n",
      "7  0.098719    0.028864  0.156250  ...     0.819277      0.915663     14\n",
      "8  0.098312    0.028795  0.227273  ...     0.819277      0.915663     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099328    0.028721  0.216981  ...     0.722892      0.915663     18\n",
      "1  0.098731    0.028502  0.216981  ...     0.722892      0.915663     18\n",
      "2  0.098464    0.028376  0.216981  ...     0.722892      0.915663     18\n",
      "3  0.098560    0.028752  0.216981  ...     0.722892      0.915663     18\n",
      "4  0.099345    0.028189  0.216981  ...     0.722892      0.915663     18\n",
      "5  0.098242    0.028588  0.216981  ...     0.722892      0.915663     18\n",
      "6  0.098641    0.028399  0.216981  ...     0.722892      0.915663     18\n",
      "7  0.100786    0.028615  0.216981  ...     0.722892      0.915663     18\n",
      "8  0.100579    0.028906  0.219048  ...     0.722892      0.915663     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.101115    0.029103  0.274725  ...     0.698795      0.915663     15\n",
      "1  0.100544    0.029024  0.263158  ...     0.698795      0.915663     15\n",
      "2  0.099341    0.028768  0.271739  ...     0.698795      0.915663     15\n",
      "3  0.098573    0.028535  0.231481  ...     0.698795      0.915663     15\n",
      "4  0.098795    0.028380  0.231481  ...     0.698795      0.915663     15\n",
      "5  0.098440    0.028780  0.263158  ...     0.698795      0.915663     15\n",
      "6  0.098563    0.028498  0.242718  ...     0.698795      0.915663     15\n",
      "7  0.098546    0.028965  0.252525  ...     0.698795      0.915663     15\n",
      "8  0.099044    0.031961  0.420000  ...     0.698795      0.915663     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098632    0.028731  0.429752  ...     0.831325      0.915663      4\n",
      "1  0.098403    0.028478  0.439024  ...     0.831325      0.915663      4\n",
      "2  0.098192    0.028552  0.389381  ...     0.831325      0.915663      4\n",
      "3  0.097954    0.028697  0.147368  ...     0.831325      0.915663      4\n",
      "4  0.098032    0.028707  0.159091  ...     0.831325      0.915663      4\n",
      "5  0.097873    0.028717  0.336538  ...     0.831325      0.915663      4\n",
      "6  0.098212    0.028585  0.258065  ...     0.831325      0.915663      4\n",
      "7  0.098096    0.029560  0.507143  ...     0.831325      0.915663      4\n",
      "8  0.098515    0.029349  0.215909  ...     0.831325      0.915663      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099745    0.028595  0.097826  ...     0.891566      0.915663      0\n",
      "1  0.098786    0.029481  0.129412  ...     0.891566      0.915663      0\n",
      "2  0.098830    0.028657  0.097826  ...     0.891566      0.915663      0\n",
      "3  0.098365    0.028279  0.097826  ...     0.891566      0.915663      0\n",
      "4  0.098272    0.028738  0.100000  ...     0.891566      0.915663      0\n",
      "5  0.098288    0.028885  0.274510  ...     0.891566      0.915663      0\n",
      "6  0.098148    0.029221  0.107143  ...     0.891566      0.915663      0\n",
      "7  0.098893    0.028234  0.097826  ...     0.891566      0.915663      0\n",
      "8  0.098332    0.028615  0.119048  ...     0.891566      0.915663      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.244642    0.340722  0.207921  ...     0.746988      0.915663     14\n",
      "1  1.243548    0.340850  0.201923  ...     0.746988      0.915663     14\n",
      "2  1.241997    0.337286  0.201923  ...     0.746988      0.915663     14\n",
      "3  1.241432    0.340178  0.201923  ...     0.746988      0.915663     14\n",
      "4  1.237415    0.344260  0.465517  ...     0.746988      0.915663     14\n",
      "5  1.236605    0.336610  0.203883  ...     0.746988      0.915663     14\n",
      "6  1.234610    0.337794  0.205882  ...     0.746988      0.915663     14\n",
      "7  1.238720    0.342885  0.201923  ...     0.746988      0.915663     14\n",
      "8  1.234217    0.339579  0.201923  ...     0.746988      0.915663     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.097983    0.028672  0.496732  ...     0.927711      0.915663      9\n",
      "1  0.097512    0.028304  0.416667  ...     0.927711      0.915663      9\n",
      "2  0.097648    0.028636  0.515723  ...     0.927711      0.915663      9\n",
      "3  0.097776    0.028295  0.483221  ...     0.927711      0.915663      9\n",
      "4  0.097712    0.028201  0.433824  ...     0.927711      0.915663      9\n",
      "5  0.099049    0.028455  0.336207  ...     0.927711      0.915663      9\n",
      "6  0.098736    0.028514  0.287037  ...     0.927711      0.915663      9\n",
      "7  0.097875    0.028354  0.416667  ...     0.927711      0.915663      9\n",
      "8  0.097594    0.028225  0.446043  ...     0.927711      0.915663      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098526    0.028156  0.178218  ...     0.783133      0.915663     23\n",
      "1  0.097825    0.028477  0.178218  ...     0.783133      0.915663     23\n",
      "2  0.097946    0.028439  0.429825  ...     0.783133      0.915663     23\n",
      "3  0.097921    0.028153  0.178218  ...     0.783133      0.915663     23\n",
      "4  0.098684    0.028115  0.178218  ...     0.783133      0.915663     23\n",
      "5  0.098415    0.028217  0.178218  ...     0.783133      0.915663     23\n",
      "6  0.097577    0.028692  0.206897  ...     0.783133      0.915663     23\n",
      "7  0.098078    0.028073  0.178218  ...     0.783133      0.915663     23\n",
      "8  0.097740    0.028732  0.180000  ...     0.783133      0.915663     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.103344    0.029214  0.493671  ...     0.963855      0.915663     12\n",
      "1  0.100623    0.028777  0.477124  ...     0.963855      0.915663     12\n",
      "2  0.099684    0.029125  0.058824  ...     0.963855      0.915663     12\n",
      "3  0.099451    0.028533  0.407407  ...     0.963855      0.915663     12\n",
      "4  0.099505    0.028777  0.509202  ...     0.963855      0.915663     12\n",
      "5  0.099473    0.028397  0.370079  ...     0.963855      0.915663     12\n",
      "6  0.102043    0.028534  0.500000  ...     0.963855      0.915663     12\n",
      "7  0.099442    0.028567  0.338843  ...     0.963855      0.915663     12\n",
      "8  0.099231    0.033038  0.487179  ...     0.963855      0.915663     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099723    0.029086  0.159574  ...     0.819277      0.915663     18\n",
      "1  0.099483    0.028536  0.161290  ...     0.819277      0.915663     18\n",
      "2  0.098511    0.028228  0.153061  ...     0.819277      0.915663     18\n",
      "3  0.098606    0.028515  0.153061  ...     0.819277      0.915663     18\n",
      "4  0.098468    0.028880  0.161290  ...     0.819277      0.915663     18\n",
      "5  0.097888    0.028378  0.153061  ...     0.819277      0.915663     18\n",
      "6  0.098489    0.028261  0.153061  ...     0.819277      0.915663     18\n",
      "7  0.098283    0.029512  0.153061  ...     0.819277      0.915663     18\n",
      "8  0.098408    0.028870  0.159574  ...     0.819277      0.915663     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100633    0.029354  0.166667  ...     0.807229      0.915663     23\n",
      "1  0.099399    0.029452  0.163265  ...     0.807229      0.915663     23\n",
      "2  0.100207    0.029025  0.468254  ...     0.807229      0.915663     23\n",
      "3  0.099696    0.028602  0.161616  ...     0.807229      0.915663     23\n",
      "4  0.099682    0.028777  0.161616  ...     0.807229      0.915663     23\n",
      "5  0.099166    0.029124  0.166667  ...     0.807229      0.915663     23\n",
      "6  0.099574    0.029424  0.355769  ...     0.807229      0.915663     23\n",
      "7  0.102916    0.029608  0.166667  ...     0.807229      0.915663     23\n",
      "8  0.099115    0.028827  0.271739  ...     0.807229      0.915663     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.249165    0.341835  0.278846  ...     0.650602      0.915663     10\n",
      "1  1.256263    0.343790  0.271028  ...     0.650602      0.915663     10\n",
      "2  1.245789    0.339629  0.419355  ...     0.650602      0.915663     10\n",
      "3  1.246719    0.342957  0.281553  ...     0.650602      0.915663     10\n",
      "4  1.247802    0.340451  0.266055  ...     0.650602      0.915663     10\n",
      "5  1.246102    0.341159  0.302083  ...     0.650602      0.915663     10\n",
      "6  1.243052    0.345298  0.292929  ...     0.650602      0.915663     10\n",
      "7  1.246818    0.340318  0.292929  ...     0.650602      0.915663     10\n",
      "8  1.242373    0.342293  0.318681  ...     0.650602      0.915663     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.111592    0.030324  0.125000  ...      0.86747      0.915663      0\n",
      "1  0.099437    0.029282  0.208791  ...      0.86747      0.915663      0\n",
      "2  0.099241    0.028959  0.117021  ...      0.86747      0.915663      0\n",
      "3  0.098775    0.028745  0.117021  ...      0.86747      0.915663      0\n",
      "4  0.099137    0.029068  0.142857  ...      0.86747      0.915663      0\n",
      "5  0.099794    0.028922  0.320755  ...      0.86747      0.915663      0\n",
      "6  0.099184    0.028906  0.217391  ...      0.86747      0.915663      0\n",
      "7  0.099260    0.029456  0.118280  ...      0.86747      0.915663      0\n",
      "8  0.099351    0.029338  0.172414  ...      0.86747      0.915663      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.107729    0.028826  0.483444  ...     0.939759      0.915663      9\n",
      "1  0.098204    0.028506  0.264151  ...     0.939759      0.915663      9\n",
      "2  0.097750    0.036167  0.458333  ...     0.939759      0.915663      9\n",
      "3  0.099842    0.028993  0.496774  ...     0.939759      0.915663      9\n",
      "4  0.098482    0.028578  0.355372  ...     0.939759      0.915663      9\n",
      "5  0.098284    0.028680  0.426471  ...     0.939759      0.915663      9\n",
      "6  0.098709    0.029098  0.142857  ...     0.939759      0.915663      9\n",
      "7  0.099097    0.028869  0.430657  ...     0.939759      0.915663      9\n",
      "8  0.114663    0.028668  0.442857  ...     0.939759      0.915663      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100805    0.029205  0.160920  ...     0.879518      0.915663      2\n",
      "1  0.099504    0.028701  0.119048  ...     0.879518      0.915663      2\n",
      "2  0.098925    0.028591  0.111111  ...     0.879518      0.915663      2\n",
      "3  0.098644    0.028674  0.113636  ...     0.879518      0.915663      2\n",
      "4  0.098866    0.028294  0.108696  ...     0.879518      0.915663      2\n",
      "5  0.098546    0.028714  0.112360  ...     0.879518      0.915663      2\n",
      "6  0.098193    0.028566  0.365217  ...     0.879518      0.915663      2\n",
      "7  0.098318    0.028696  0.179775  ...     0.879518      0.915663      2\n",
      "8  0.098958    0.038856  0.277228  ...     0.879518      0.915663      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.128057    0.029094  0.347368  ...      0.60241      0.915663     10\n",
      "1  0.098896    0.028964  0.297297  ...      0.60241      0.915663     10\n",
      "2  0.099269    0.029009  0.330000  ...      0.60241      0.915663     10\n",
      "3  0.099229    0.029156  0.297297  ...      0.60241      0.915663     10\n",
      "4  0.099124    0.028947  0.292035  ...      0.60241      0.915663     10\n",
      "5  0.098739    0.029050  0.323529  ...      0.60241      0.915663     10\n",
      "6  0.099270    0.029105  0.340206  ...      0.60241      0.915663     10\n",
      "7  0.107415    0.031343  0.370787  ...      0.60241      0.915663     10\n",
      "8  0.100503    0.029603  0.351064  ...      0.60241      0.915663     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099209    0.029083  0.506098  ...     0.975904      0.915663     12\n",
      "1  0.098363    0.028179  0.500000  ...     0.975904      0.915663     12\n",
      "2  0.098166    0.028681  0.109890  ...     0.975904      0.915663     12\n",
      "3  0.097732    0.028092  0.372093  ...     0.975904      0.915663     12\n",
      "4  0.097738    0.028447  0.506098  ...     0.975904      0.915663     12\n",
      "5  0.098155    0.028349  0.484076  ...     0.975904      0.915663     12\n",
      "6  0.097956    0.028299  0.500000  ...     0.975904      0.915663     12\n",
      "7  0.098113    0.028073  0.433566  ...     0.975904      0.915663     12\n",
      "8  0.097565    0.028190  0.496894  ...     0.975904      0.915663     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.255487    0.336755  0.512658  ...     0.927711      0.915663      9\n",
      "1  1.246581    0.336503  0.450000  ...     0.927711      0.915663      9\n",
      "2  1.244463    0.336080  0.518750  ...     0.927711      0.915663      9\n",
      "3  1.244949    0.340172  0.493421  ...     0.927711      0.915663      9\n",
      "4  1.243967    0.339041  0.461538  ...     0.927711      0.915663      9\n",
      "5  1.239229    0.336531  0.442029  ...     0.927711      0.915663      9\n",
      "6  1.245652    0.338992  0.287037  ...     0.927711      0.915663      9\n",
      "7  1.241599    0.343097  0.433824  ...     0.927711      0.915663      9\n",
      "8  1.241435    0.337352  0.490066  ...     0.927711      0.915663      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.102854    0.028503  0.170000  ...     0.795181      0.915663     18\n",
      "1  0.097970    0.028874  0.173469  ...     0.795181      0.915663     18\n",
      "2  0.098306    0.028402  0.170000  ...     0.795181      0.915663     18\n",
      "3  0.098920    0.028709  0.170000  ...     0.795181      0.915663     18\n",
      "4  0.098610    0.028748  0.175258  ...     0.795181      0.915663     18\n",
      "5  0.098154    0.028455  0.170000  ...     0.795181      0.915663     18\n",
      "6  0.098846    0.028117  0.170000  ...     0.795181      0.915663     18\n",
      "7  0.098822    0.028206  0.170000  ...     0.795181      0.915663     18\n",
      "8  0.098304    0.028802  0.178947  ...     0.795181      0.915663     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098982    0.028988  0.184783  ...     0.795181      0.915663      2\n",
      "1  0.099030    0.028608  0.170000  ...     0.795181      0.915663      2\n",
      "2  0.098410    0.028776  0.177083  ...     0.795181      0.915663      2\n",
      "3  0.098905    0.028187  0.170000  ...     0.795181      0.915663      2\n",
      "4  0.098578    0.028187  0.170000  ...     0.795181      0.915663      2\n",
      "5  0.098037    0.028513  0.170000  ...     0.795181      0.915663      2\n",
      "6  0.098325    0.029298  0.223529  ...     0.795181      0.915663      2\n",
      "7  0.098997    0.028955  0.175258  ...     0.795181      0.915663      2\n",
      "8  0.102598    0.028710  0.188889  ...     0.795181      0.915663      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100654    0.029218  0.291262  ...     0.638554      0.915663     10\n",
      "1  0.099945    0.028908  0.326087  ...     0.638554      0.915663     10\n",
      "2  0.099015    0.028761  0.329670  ...     0.638554      0.915663     10\n",
      "3  0.098554    0.028805  0.297030  ...     0.638554      0.915663     10\n",
      "4  0.098967    0.028350  0.275229  ...     0.638554      0.915663     10\n",
      "5  0.098644    0.028380  0.337079  ...     0.638554      0.915663     10\n",
      "6  0.098146    0.028516  0.306122  ...     0.638554      0.915663     10\n",
      "7  0.099377    0.029065  0.319149  ...     0.638554      0.915663     10\n",
      "8  0.099305    0.033872  0.417582  ...     0.638554      0.915663     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099943    0.028903  0.130435  ...     0.855422      0.915663      0\n",
      "1  0.099608    0.028918  0.134831  ...     0.855422      0.915663      0\n",
      "2  0.098748    0.028688  0.126316  ...     0.855422      0.915663      0\n",
      "3  0.099337    0.028473  0.126316  ...     0.855422      0.915663      0\n",
      "4  0.099273    0.028606  0.126316  ...     0.855422      0.915663      0\n",
      "5  0.098755    0.028674  0.174419  ...     0.855422      0.915663      0\n",
      "6  0.099324    0.028821  0.130435  ...     0.855422      0.915663      0\n",
      "7  0.109350    0.029727  0.126316  ...     0.855422      0.915663      0\n",
      "8  0.099347    0.029114  0.136364  ...     0.855422      0.915663      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100362    0.028987  0.184783  ...     0.795181      0.915663     14\n",
      "1  0.098785    0.028704  0.182796  ...     0.795181      0.915663     14\n",
      "2  0.099982    0.028285  0.170000  ...     0.795181      0.915663     14\n",
      "3  0.098142    0.028706  0.178947  ...     0.795181      0.915663     14\n",
      "4  0.098012    0.029791  0.472000  ...     0.795181      0.915663     14\n",
      "5  0.101078    0.028559  0.178947  ...     0.795181      0.915663     14\n",
      "6  0.098578    0.028522  0.184783  ...     0.795181      0.915663     14\n",
      "7  0.099006    0.028857  0.171717  ...     0.795181      0.915663     14\n",
      "8  0.098602    0.028626  0.193182  ...     0.795181      0.915663     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.269452    0.335805  0.201923  ...     0.746988      0.915663     18\n",
      "1  1.239393    0.338624  0.203883  ...     0.746988      0.915663     18\n",
      "2  1.239339    0.334077  0.201923  ...     0.746988      0.915663     18\n",
      "3  1.242584    0.342615  0.201923  ...     0.746988      0.915663     18\n",
      "4  1.238558    0.336503  0.201923  ...     0.746988      0.915663     18\n",
      "5  1.245610    0.339845  0.201923  ...     0.746988      0.915663     18\n",
      "6  1.242294    0.335989  0.201923  ...     0.746988      0.915663     18\n",
      "7  1.235337    0.344894  0.201923  ...     0.746988      0.915663     18\n",
      "8  1.237630    0.337172  0.203883  ...     0.746988      0.915663     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099002    0.028448  0.278261  ...     0.614458      0.915663     23\n",
      "1  0.098851    0.028778  0.280702  ...     0.614458      0.915663     23\n",
      "2  0.098411    0.028508  0.445652  ...     0.614458      0.915663     23\n",
      "3  0.098729    0.028242  0.278261  ...     0.614458      0.915663     23\n",
      "4  0.098231    0.031037  0.278261  ...     0.614458      0.915663     23\n",
      "5  0.097571    0.028049  0.278261  ...     0.614458      0.915663     23\n",
      "6  0.097576    0.028902  0.329897  ...     0.614458      0.915663     23\n",
      "7  0.097919    0.028360  0.278261  ...     0.614458      0.915663     23\n",
      "8  0.097872    0.028819  0.283186  ...     0.614458      0.915663     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.097857    0.028363  0.232558  ...     0.795181      0.915663     15\n",
      "1  0.097743    0.028377  0.290323  ...     0.795181      0.915663     15\n",
      "2  0.097579    0.028275  0.282609  ...     0.795181      0.915663     15\n",
      "3  0.098768    0.027872  0.170000  ...     0.795181      0.915663     15\n",
      "4  0.097645    0.028030  0.170000  ...     0.795181      0.915663     15\n",
      "5  0.097290    0.028265  0.250000  ...     0.795181      0.915663     15\n",
      "6  0.098002    0.030309  0.180851  ...     0.795181      0.915663     15\n",
      "7  0.098256    0.028546  0.182796  ...     0.795181      0.915663     15\n",
      "8  0.101520    0.028394  0.405405  ...     0.795181      0.915663     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100819    0.029328  0.403670  ...     0.783133      0.915663      4\n",
      "1  0.100133    0.029367  0.380952  ...     0.783133      0.915663      4\n",
      "2  0.099267    0.029039  0.216867  ...     0.783133      0.915663      4\n",
      "3  0.098596    0.029136  0.180000  ...     0.783133      0.915663      4\n",
      "4  0.098622    0.028844  0.187500  ...     0.783133      0.915663      4\n",
      "5  0.098661    0.028945  0.315789  ...     0.783133      0.915663      4\n",
      "6  0.098710    0.028774  0.285714  ...     0.783133      0.915663      4\n",
      "7  0.098969    0.028786  0.488189  ...     0.783133      0.915663      4\n",
      "8  0.098651    0.039267  0.195652  ...     0.783133      0.915663      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099006    0.028236  0.478873  ...     0.891566      0.915663     12\n",
      "1  0.097868    0.028412  0.478873  ...     0.891566      0.915663     12\n",
      "2  0.097834    0.028374  0.098901  ...     0.891566      0.915663     12\n",
      "3  0.099874    0.028355  0.356522  ...     0.891566      0.915663     12\n",
      "4  0.097856    0.028527  0.525641  ...     0.891566      0.915663     12\n",
      "5  0.098088    0.033145  0.339286  ...     0.891566      0.915663     12\n",
      "6  0.099936    0.028964  0.516340  ...     0.891566      0.915663     12\n",
      "7  0.098801    0.028642  0.237113  ...     0.891566      0.915663     12\n",
      "8  0.097811    0.028484  0.493151  ...     0.891566      0.915663     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099741    0.028920  0.522581  ...     0.891566      0.915663      9\n",
      "1  0.098908    0.028651  0.482517  ...     0.891566      0.915663      9\n",
      "2  0.100868    0.028922  0.528662  ...     0.891566      0.915663      9\n",
      "3  0.098648    0.028799  0.467626  ...     0.891566      0.915663      9\n",
      "4  0.098742    0.028864  0.503356  ...     0.891566      0.915663      9\n",
      "5  0.098771    0.028654  0.467626  ...     0.891566      0.915663      9\n",
      "6  0.098510    0.028918  0.439394  ...     0.891566      0.915663      9\n",
      "7  0.099668    0.029089  0.439394  ...     0.891566      0.915663      9\n",
      "8  0.099051    0.028467  0.463768  ...     0.891566      0.915663      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.245025    0.336986  0.216981  ...     0.722892      0.915663     23\n",
      "1  1.237818    0.348927  0.219048  ...     0.722892      0.915663     23\n",
      "2  1.237717    0.338523  0.433962  ...     0.722892      0.915663     23\n",
      "3  1.250132    0.338818  0.216981  ...     0.722892      0.915663     23\n",
      "4  1.239919    0.338928  0.216981  ...     0.722892      0.915663     23\n",
      "5  1.238577    0.340653  0.216981  ...     0.722892      0.915663     23\n",
      "6  1.241707    0.338147  0.232323  ...     0.722892      0.915663     23\n",
      "7  1.237283    0.346171  0.216981  ...     0.722892      0.915663     23\n",
      "8  1.236045    0.336441  0.219048  ...     0.722892      0.915663     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.097913    0.028984  0.293103  ...     0.590361      0.915663     10\n",
      "1  0.098017    0.028528  0.300885  ...     0.590361      0.915663     10\n",
      "2  0.098084    0.028671  0.449438  ...     0.590361      0.915663     10\n",
      "3  0.097966    0.028814  0.303571  ...     0.590361      0.915663     10\n",
      "4  0.099524    0.028527  0.293103  ...     0.590361      0.915663     10\n",
      "5  0.097873    0.028565  0.350515  ...     0.590361      0.915663     10\n",
      "6  0.097434    0.028676  0.309091  ...     0.590361      0.915663     10\n",
      "7  0.098156    0.028669  0.336634  ...     0.590361      0.915663     10\n",
      "8  0.097875    0.028703  0.343434  ...     0.590361      0.915663     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100641    0.031810  0.303922  ...     0.626506      0.915663     10\n",
      "1  0.098842    0.028915  0.279279  ...     0.626506      0.915663     10\n",
      "2  0.097417    0.028589  0.428571  ...     0.626506      0.915663     10\n",
      "3  0.097760    0.029928  0.295238  ...     0.626506      0.915663     10\n",
      "4  0.097857    0.028685  0.281818  ...     0.626506      0.915663     10\n",
      "5  0.097377    0.028708  0.303922  ...     0.626506      0.915663     10\n",
      "6  0.099275    0.028767  0.300971  ...     0.626506      0.915663     10\n",
      "7  0.098572    0.028964  0.292453  ...     0.626506      0.915663     10\n",
      "8  0.097798    0.028613  0.313131  ...     0.626506      0.915663     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.102138    0.029512  0.203883  ...     0.746988      0.915663     18\n",
      "1  0.100919    0.029591  0.212121  ...     0.746988      0.915663     18\n",
      "2  0.100692    0.028727  0.201923  ...     0.746988      0.915663     18\n",
      "3  0.100313    0.028921  0.201923  ...     0.746988      0.915663     18\n",
      "4  0.099698    0.028819  0.207921  ...     0.746988      0.915663     18\n",
      "5  0.099601    0.028449  0.201923  ...     0.746988      0.915663     18\n",
      "6  0.099302    0.028549  0.201923  ...     0.746988      0.915663     18\n",
      "7  0.099538    0.028599  0.201923  ...     0.746988      0.915663     18\n",
      "8  0.099088    0.029777  0.210000  ...     0.746988      0.915663     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.104043    0.029032  0.209524  ...      0.73494      0.915663     23\n",
      "1  0.100760    0.029006  0.211538  ...      0.73494      0.915663     23\n",
      "2  0.099837    0.029238  0.464912  ...      0.73494      0.915663     23\n",
      "3  0.100790    0.028726  0.209524  ...      0.73494      0.915663     23\n",
      "4  0.100794    0.028527  0.209524  ...      0.73494      0.915663     23\n",
      "5  0.100514    0.038827  0.209524  ...      0.73494      0.915663     23\n",
      "6  0.102327    0.029576  0.239130  ...      0.73494      0.915663     23\n",
      "7  0.099111    0.028671  0.209524  ...      0.73494      0.915663     23\n",
      "8  0.100028    0.029529  0.211538  ...      0.73494      0.915663     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099251    0.029238  0.294118  ...     0.638554      0.915663     10\n",
      "1  0.099242    0.028982  0.272727  ...     0.638554      0.915663     10\n",
      "2  0.098465    0.029348  0.291262  ...     0.638554      0.915663     10\n",
      "3  0.098459    0.029144  0.277778  ...     0.638554      0.915663     10\n",
      "4  0.105185    0.032499  0.267857  ...     0.638554      0.915663     10\n",
      "5  0.099643    0.029447  0.275229  ...     0.638554      0.915663     10\n",
      "6  0.099388    0.029372  0.306122  ...     0.638554      0.915663     10\n",
      "7  0.101140    0.029355  0.291262  ...     0.638554      0.915663     10\n",
      "8  0.099261    0.029240  0.322581  ...     0.638554      0.915663     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.270686    0.345996  0.135417  ...     0.843373      0.915663      0\n",
      "1  1.259556    0.344841  0.138298  ...     0.843373      0.915663      0\n",
      "2  1.255697    0.343023  0.135417  ...     0.843373      0.915663      0\n",
      "3  1.256134    0.346834  0.135417  ...     0.843373      0.915663      0\n",
      "4  1.257002    0.344896  0.135417  ...     0.843373      0.915663      0\n",
      "5  1.255931    0.349224  0.204545  ...     0.843373      0.915663      0\n",
      "6  1.258221    0.342956  0.136842  ...     0.843373      0.915663      0\n",
      "7  1.258196    0.349968  0.135417  ...     0.843373      0.915663      0\n",
      "8  1.255145    0.344150  0.149425  ...     0.843373      0.915663      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098908    0.028935  0.313953  ...     0.710843      0.915663      4\n",
      "1  0.099866    0.028847  0.337079  ...     0.710843      0.915663      4\n",
      "2  0.099328    0.029479  0.258065  ...     0.710843      0.915663      4\n",
      "3  0.099751    0.029033  0.224299  ...     0.710843      0.915663      4\n",
      "4  0.100210    0.028981  0.237624  ...     0.710843      0.915663      4\n",
      "5  0.110931    0.029099  0.252632  ...     0.710843      0.915663      4\n",
      "6  0.098374    0.028783  0.247423  ...     0.710843      0.915663      4\n",
      "7  0.099029    0.028607  0.504202  ...     0.710843      0.915663      4\n",
      "8  0.098284    0.028776  0.230769  ...     0.710843      0.915663      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099726    0.029552  0.238095  ...     0.771084      0.915663      4\n",
      "1  0.098402    0.029026  0.247059  ...     0.771084      0.915663      4\n",
      "2  0.098240    0.028725  0.188119  ...     0.771084      0.915663      4\n",
      "3  0.098484    0.028855  0.188119  ...     0.771084      0.915663      4\n",
      "4  0.098254    0.028492  0.186275  ...     0.771084      0.915663      4\n",
      "5  0.098369    0.029079  0.204301  ...     0.771084      0.915663      4\n",
      "6  0.099280    0.028892  0.223529  ...     0.771084      0.915663      4\n",
      "7  0.098741    0.034814  0.452991  ...     0.771084      0.915663      4\n",
      "8  0.097966    0.028919  0.191919  ...     0.771084      0.915663      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099841    0.028425  0.500000  ...     0.951807      0.915663     12\n",
      "1  0.099309    0.028406  0.506250  ...     0.951807      0.915663     12\n",
      "2  0.098586    0.028461  0.048193  ...     0.951807      0.915663     12\n",
      "3  0.098619    0.028255  0.307018  ...     0.951807      0.915663     12\n",
      "4  0.097514    0.028333  0.512346  ...     0.951807      0.915663     12\n",
      "5  0.097735    0.027951  0.341667  ...     0.951807      0.915663     12\n",
      "6  0.097762    0.028398  0.503145  ...     0.951807      0.915663     12\n",
      "7  0.098255    0.028254  0.341667  ...     0.951807      0.915663     12\n",
      "8  0.099611    0.027984  0.476821  ...     0.951807      0.915663     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099461    0.028920  0.204301  ...     0.771084      0.915663     14\n",
      "1  0.098383    0.028975  0.202128  ...     0.771084      0.915663     14\n",
      "2  0.098116    0.028207  0.186275  ...     0.771084      0.915663     14\n",
      "3  0.098746    0.028390  0.186275  ...     0.771084      0.915663     14\n",
      "4  0.098571    0.028611  0.479675  ...     0.771084      0.915663     14\n",
      "5  0.098285    0.028668  0.191919  ...     0.771084      0.915663     14\n",
      "6  0.098995    0.028616  0.288889  ...     0.771084      0.915663     14\n",
      "7  0.098354    0.040132  0.195876  ...     0.771084      0.915663     14\n",
      "8  0.099553    0.029087  0.255814  ...     0.771084      0.915663     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098182    0.028477  0.480000  ...     0.939759      0.915663      9\n",
      "1  0.098540    0.028777  0.187500  ...     0.939759      0.915663      9\n",
      "2  0.098696    0.028600  0.512500  ...     0.939759      0.915663      9\n",
      "3  0.101307    0.028457  0.486842  ...     0.939759      0.915663      9\n",
      "4  0.098061    0.028439  0.355372  ...     0.939759      0.915663      9\n",
      "5  0.097884    0.028460  0.277778  ...     0.939759      0.915663      9\n",
      "6  0.097788    0.028346  0.113636  ...     0.939759      0.915663      9\n",
      "7  0.098518    0.028343  0.404580  ...     0.939759      0.915663      9\n",
      "8  0.097962    0.028253  0.303571  ...     0.939759      0.915663      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099795    0.028593  0.224299  ...     0.710843      0.915663     18\n",
      "1  0.098332    0.028829  0.228571  ...     0.710843      0.915663     18\n",
      "2  0.101372    0.028303  0.224299  ...     0.710843      0.915663     18\n",
      "3  0.098573    0.028974  0.224299  ...     0.710843      0.915663     18\n",
      "4  0.098365    0.028561  0.226415  ...     0.710843      0.915663     18\n",
      "5  0.098195    0.028281  0.224299  ...     0.710843      0.915663     18\n",
      "6  0.098426    0.028286  0.224299  ...     0.710843      0.915663     18\n",
      "7  0.098793    0.028325  0.224299  ...     0.710843      0.915663     18\n",
      "8  0.098231    0.029412  0.226415  ...     0.710843      0.915663     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.242595    0.339309  0.127907  ...     0.903614      0.915663      2\n",
      "1  1.238328    0.335820  0.089888  ...     0.903614      0.915663      2\n",
      "2  1.238144    0.337775  0.094118  ...     0.903614      0.915663      2\n",
      "3  1.241039    0.336572  0.087912  ...     0.903614      0.915663      2\n",
      "4  1.237661    0.339103  0.087912  ...     0.903614      0.915663      2\n",
      "5  1.232500    0.337901  0.089888  ...     0.903614      0.915663      2\n",
      "6  1.242438    0.343770  0.285714  ...     0.903614      0.915663      2\n",
      "7  1.239090    0.346148  0.096386  ...     0.903614      0.915663      2\n",
      "8  1.235791    0.338938  0.147727  ...     0.903614      0.915663      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.189050    0.052556  0.347368  ...      0.60241      0.915663     15\n",
      "1  0.097403    0.028641  0.326733  ...      0.60241      0.915663     15\n",
      "2  0.097473    0.028957  0.358696  ...      0.60241      0.915663     15\n",
      "3  0.100667    0.027878  0.284483  ...      0.60241      0.915663     15\n",
      "4  0.097165    0.028033  0.284483  ...      0.60241      0.915663     15\n",
      "5  0.097463    0.028544  0.317308  ...      0.60241      0.915663     15\n",
      "6  0.097728    0.028546  0.294643  ...      0.60241      0.915663     15\n",
      "7  0.097572    0.028807  0.297297  ...      0.60241      0.915663     15\n",
      "8  0.097509    0.028558  0.425287  ...      0.60241      0.915663     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098484    0.028665  0.506173  ...     0.963855      0.915663     12\n",
      "1  0.097006    0.029006  0.496855  ...     0.963855      0.915663     12\n",
      "2  0.096912    0.028093  0.111111  ...     0.963855      0.915663     12\n",
      "3  0.096588    0.027885  0.402985  ...     0.963855      0.915663     12\n",
      "4  0.097258    0.028574  0.509202  ...     0.963855      0.915663     12\n",
      "5  0.096719    0.027944  0.473684  ...     0.963855      0.915663     12\n",
      "6  0.096469    0.027992  0.500000  ...     0.963855      0.915663     12\n",
      "7  0.097425    0.028095  0.424460  ...     0.963855      0.915663     12\n",
      "8  0.096878    0.028432  0.496855  ...     0.963855      0.915663     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100003    0.028707  0.493421  ...     0.927711      0.915663      9\n",
      "1  0.099256    0.028512  0.318584  ...     0.927711      0.915663      9\n",
      "2  0.098352    0.028442  0.472603  ...     0.927711      0.915663      9\n",
      "3  0.098429    0.028388  0.493421  ...     0.927711      0.915663      9\n",
      "4  0.098141    0.028179  0.347458  ...     0.927711      0.915663      9\n",
      "5  0.097745    0.028190  0.330435  ...     0.927711      0.915663      9\n",
      "6  0.097959    0.028367  0.094118  ...     0.927711      0.915663      9\n",
      "7  0.098211    0.028289  0.407692  ...     0.927711      0.915663      9\n",
      "8  0.099787    0.028331  0.347458  ...     0.927711      0.915663      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099694    0.028499  0.209524  ...      0.73494      0.915663     18\n",
      "1  0.099072    0.028924  0.211538  ...      0.73494      0.915663     18\n",
      "2  0.098219    0.028518  0.209524  ...      0.73494      0.915663     18\n",
      "3  0.098731    0.028725  0.209524  ...      0.73494      0.915663     18\n",
      "4  0.098467    0.028469  0.209524  ...      0.73494      0.915663     18\n",
      "5  0.098847    0.028383  0.209524  ...      0.73494      0.915663     18\n",
      "6  0.099334    0.028498  0.209524  ...      0.73494      0.915663     18\n",
      "7  0.098761    0.041308  0.209524  ...      0.73494      0.915663     18\n",
      "8  0.099856    0.028974  0.213592  ...      0.73494      0.915663     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098375    0.028467  0.252252  ...     0.662651      0.915663     18\n",
      "1  0.098236    0.028666  0.254545  ...     0.662651      0.915663     18\n",
      "2  0.098363    0.028333  0.252252  ...     0.662651      0.915663     18\n",
      "3  0.097793    0.028490  0.252252  ...     0.662651      0.915663     18\n",
      "4  0.097482    0.028583  0.252252  ...     0.662651      0.915663     18\n",
      "5  0.097923    0.028105  0.252252  ...     0.662651      0.915663     18\n",
      "6  0.097753    0.028276  0.252252  ...     0.662651      0.915663     18\n",
      "7  0.098036    0.028476  0.252252  ...     0.662651      0.915663     18\n",
      "8  0.097896    0.028644  0.254545  ...     0.662651      0.915663     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099596    0.028851  0.127907  ...     0.903614      0.915663      2\n",
      "1  0.098256    0.028862  0.094118  ...     0.903614      0.915663      2\n",
      "2  0.098474    0.028888  0.127907  ...     0.903614      0.915663      2\n",
      "3  0.098303    0.028368  0.087912  ...     0.903614      0.915663      2\n",
      "4  0.098225    0.028603  0.088889  ...     0.903614      0.915663      2\n",
      "5  0.098002    0.028845  0.094118  ...     0.903614      0.915663      2\n",
      "6  0.097965    0.028608  0.358974  ...     0.903614      0.915663      2\n",
      "7  0.099349    0.028866  0.127907  ...     0.903614      0.915663      2\n",
      "8  0.098294    0.028715  0.193548  ...     0.903614      0.915663      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.247752    0.337788  0.247312  ...     0.722892      0.915663     15\n",
      "1  1.238601    0.337625  0.250000  ...     0.722892      0.915663     15\n",
      "2  1.237781    0.338418  0.302326  ...     0.722892      0.915663     15\n",
      "3  1.235829    0.338165  0.216981  ...     0.722892      0.915663     15\n",
      "4  1.236424    0.335907  0.216981  ...     0.722892      0.915663     15\n",
      "5  1.234664    0.338712  0.250000  ...     0.722892      0.915663     15\n",
      "6  1.242637    0.338614  0.225490  ...     0.722892      0.915663     15\n",
      "7  1.234828    0.339962  0.221154  ...     0.722892      0.915663     15\n",
      "8  1.232874    0.337902  0.333333  ...     0.722892      0.915663     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.181442    0.053366  0.175824  ...     0.807229      0.915663      2\n",
      "1  0.098304    0.028482  0.166667  ...     0.807229      0.915663      2\n",
      "2  0.097826    0.028746  0.179775  ...     0.807229      0.915663      2\n",
      "3  0.096855    0.027996  0.161616  ...     0.807229      0.915663      2\n",
      "4  0.097018    0.028144  0.163265  ...     0.807229      0.915663      2\n",
      "5  0.096918    0.028465  0.166667  ...     0.807229      0.915663      2\n",
      "6  0.097426    0.028674  0.279570  ...     0.807229      0.915663      2\n",
      "7  0.097668    0.028588  0.168421  ...     0.807229      0.915663      2\n",
      "8  0.097418    0.028655  0.177778  ...     0.807229      0.915663      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099868    0.029801  0.082353  ...     0.915663      0.915663      0\n",
      "1  0.098134    0.028609  0.262136  ...     0.915663      0.915663      0\n",
      "2  0.097872    0.028272  0.077778  ...     0.915663      0.915663      0\n",
      "3  0.097514    0.028368  0.077778  ...     0.915663      0.915663      0\n",
      "4  0.097978    0.028954  0.081395  ...     0.915663      0.915663      0\n",
      "5  0.098419    0.028897  0.350427  ...     0.915663      0.915663      0\n",
      "6  0.097621    0.028662  0.082353  ...     0.915663      0.915663      0\n",
      "7  0.098404    0.028764  0.084337  ...     0.915663      0.915663      0\n",
      "8  0.097938    0.028782  0.191489  ...     0.915663      0.915663      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.102159    0.029862  0.195652  ...     0.783133      0.915663     14\n",
      "1  0.101006    0.029707  0.195652  ...     0.783133      0.915663     14\n",
      "2  0.099930    0.028996  0.178218  ...     0.783133      0.915663     14\n",
      "3  0.099809    0.029400  0.180000  ...     0.783133      0.915663     14\n",
      "4  0.099803    0.029060  0.458333  ...     0.783133      0.915663     14\n",
      "5  0.099908    0.029315  0.180000  ...     0.783133      0.915663     14\n",
      "6  0.099369    0.029217  0.189474  ...     0.783133      0.915663     14\n",
      "7  0.099767    0.028949  0.178218  ...     0.783133      0.915663     14\n",
      "8  0.105941    0.030387  0.181818  ...     0.783133      0.915663     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100532    0.029323  0.196078  ...     0.759036      0.915663     23\n",
      "1  0.100257    0.029347  0.196078  ...     0.759036      0.915663     23\n",
      "2  0.099944    0.029126  0.466102  ...     0.759036      0.915663     23\n",
      "3  0.100023    0.028751  0.194175  ...     0.759036      0.915663     23\n",
      "4  0.099637    0.028888  0.194175  ...     0.759036      0.915663     23\n",
      "5  0.099684    0.029413  0.198020  ...     0.759036      0.915663     23\n",
      "6  0.100469    0.029191  0.343750  ...     0.759036      0.915663     23\n",
      "7  0.109877    0.030990  0.196078  ...     0.759036      0.915663     23\n",
      "8  0.100474    0.030504  0.215054  ...     0.759036      0.915663     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100889    0.038673  0.198020  ...     0.759036      0.915663     23\n",
      "1  0.100338    0.029865  0.200000  ...     0.759036      0.915663     23\n",
      "2  0.099954    0.031162  0.487805  ...     0.759036      0.915663     23\n",
      "3  0.099753    0.029135  0.194175  ...     0.759036      0.915663     23\n",
      "4  0.103136    0.029271  0.194175  ...     0.759036      0.915663     23\n",
      "5  0.101863    0.029362  0.194175  ...     0.759036      0.915663     23\n",
      "6  0.100376    0.029759  0.322581  ...     0.759036      0.915663     23\n",
      "7  0.100503    0.029604  0.196078  ...     0.759036      0.915663     23\n",
      "8  0.100226    0.029444  0.206186  ...     0.759036      0.915663     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099544    0.029489  0.309735  ...     0.578313      0.915663     15\n",
      "1  0.100064    0.029732  0.307018  ...     0.578313      0.915663     15\n",
      "2  0.099282    0.029325  0.380435  ...     0.578313      0.915663     15\n",
      "3  0.099379    0.028859  0.296610  ...     0.578313      0.915663     15\n",
      "4  0.099793    0.028996  0.296610  ...     0.578313      0.915663     15\n",
      "5  0.099494    0.029506  0.321101  ...     0.578313      0.915663     15\n",
      "6  0.099108    0.029229  0.299145  ...     0.578313      0.915663     15\n",
      "7  0.099837    0.029340  0.299145  ...     0.578313      0.915663     15\n",
      "8  0.099595    0.029507  0.336538  ...     0.578313      0.915663     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.268346    0.349685  0.382353  ...     0.759036      0.915663      4\n",
      "1  1.260571    0.349921  0.284091  ...     0.759036      0.915663      4\n",
      "2  1.259389    0.351007  0.240964  ...     0.759036      0.915663      4\n",
      "3  1.261194    0.346570  0.196078  ...     0.759036      0.915663      4\n",
      "4  1.260180    0.348178  0.200000  ...     0.759036      0.915663      4\n",
      "5  1.256566    0.349231  0.284091  ...     0.759036      0.915663      4\n",
      "6  1.264402    0.349566  0.258824  ...     0.759036      0.915663      4\n",
      "7  1.261649    0.350361  0.496000  ...     0.759036      0.915663      4\n",
      "8  1.256726    0.352542  0.204082  ...     0.759036      0.915663      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099041    0.028789  0.487179  ...     0.963855      0.915663     12\n",
      "1  0.098203    0.028921  0.483871  ...     0.963855      0.915663     12\n",
      "2  0.098465    0.029036  0.036145  ...     0.963855      0.915663     12\n",
      "3  0.098418    0.028932  0.354839  ...     0.963855      0.915663     12\n",
      "4  0.098590    0.028851  0.509202  ...     0.963855      0.915663     12\n",
      "5  0.098920    0.028825  0.436620  ...     0.963855      0.915663     12\n",
      "6  0.098830    0.028693  0.496855  ...     0.963855      0.915663     12\n",
      "7  0.099352    0.028961  0.349593  ...     0.963855      0.915663     12\n",
      "8  0.098603    0.028882  0.487179  ...     0.963855      0.915663     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099557    0.029401  0.205882  ...     0.746988      0.915663     14\n",
      "1  0.099367    0.028983  0.201923  ...     0.746988      0.915663     14\n",
      "2  0.099001    0.029048  0.201923  ...     0.746988      0.915663     14\n",
      "3  0.099430    0.028947  0.201923  ...     0.746988      0.915663     14\n",
      "4  0.099348    0.029408  0.478992  ...     0.746988      0.915663     14\n",
      "5  0.098777    0.028986  0.201923  ...     0.746988      0.915663     14\n",
      "6  0.098852    0.029374  0.218750  ...     0.746988      0.915663     14\n",
      "7  0.099835    0.028932  0.201923  ...     0.746988      0.915663     14\n",
      "8  0.099290    0.029669  0.210000  ...     0.746988      0.915663     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.101695    0.028926  0.245455  ...     0.674699      0.915663     23\n",
      "1  0.100462    0.029017  0.247706  ...     0.674699      0.915663     23\n",
      "2  0.099283    0.029072  0.504425  ...     0.674699      0.915663     23\n",
      "3  0.099402    0.028622  0.245455  ...     0.674699      0.915663     23\n",
      "4  0.099250    0.028348  0.245455  ...     0.674699      0.915663     23\n",
      "5  0.099030    0.028269  0.245455  ...     0.674699      0.915663     23\n",
      "6  0.098878    0.028684  0.284211  ...     0.674699      0.915663     23\n",
      "7  0.101522    0.028197  0.245455  ...     0.674699      0.915663     23\n",
      "8  0.099399    0.029797  0.247706  ...     0.674699      0.915663     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.108524    0.028901  0.250000  ...      0.73494      0.915663     15\n",
      "1  0.099404    0.029070  0.244444  ...      0.73494      0.915663     15\n",
      "2  0.098109    0.028570  0.265060  ...      0.73494      0.915663     15\n",
      "3  0.099124    0.028587  0.213592  ...      0.73494      0.915663     15\n",
      "4  0.098550    0.028061  0.209524  ...      0.73494      0.915663     15\n",
      "5  0.098236    0.028612  0.236559  ...      0.73494      0.915663     15\n",
      "6  0.100444    0.028791  0.234043  ...      0.73494      0.915663     15\n",
      "7  0.099018    0.028774  0.234043  ...      0.73494      0.915663     15\n",
      "8  0.098753    0.028820  0.440367  ...      0.73494      0.915663     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.107353    0.029189  0.313953  ...     0.674699      0.915663     15\n",
      "1  0.099295    0.028791  0.341176  ...     0.674699      0.915663     15\n",
      "2  0.099230    0.028792  0.384615  ...     0.674699      0.915663     15\n",
      "3  0.100752    0.028275  0.245455  ...     0.674699      0.915663     15\n",
      "4  0.099529    0.028545  0.245455  ...     0.674699      0.915663     15\n",
      "5  0.099580    0.029287  0.272727  ...     0.674699      0.915663     15\n",
      "6  0.105950    0.029094  0.262136  ...     0.674699      0.915663     15\n",
      "7  0.099914    0.028977  0.262136  ...     0.674699      0.915663     15\n",
      "8  0.099900    0.029006  0.370787  ...     0.674699      0.915663     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099210    0.028817  0.365385  ...     0.795181      0.915663      4\n",
      "1  0.100127    0.029156  0.312500  ...     0.795181      0.915663      4\n",
      "2  0.098642    0.029024  0.297872  ...     0.795181      0.915663      4\n",
      "3  0.097925    0.029200  0.173469  ...     0.795181      0.915663      4\n",
      "4  0.098370    0.028716  0.177083  ...     0.795181      0.915663      4\n",
      "5  0.098192    0.028698  0.274725  ...     0.795181      0.915663      4\n",
      "6  0.097923    0.028604  0.250000  ...     0.795181      0.915663      4\n",
      "7  0.098771    0.028528  0.488372  ...     0.795181      0.915663      4\n",
      "8  0.098706    0.029360  0.191011  ...     0.795181      0.915663      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.251534    0.335276  0.496933  ...     0.987952      0.915663     12\n",
      "1  1.240349    0.333571  0.481013  ...     0.987952      0.915663     12\n",
      "2  1.241075    0.345548  0.035294  ...     0.987952      0.915663     12\n",
      "3  1.240290    0.337312  0.378788  ...     0.987952      0.915663     12\n",
      "4  1.240302    0.336871  0.503030  ...     0.987952      0.915663     12\n",
      "5  1.238923    0.337688  0.460526  ...     0.987952      0.915663     12\n",
      "6  1.236073    0.338614  0.490683  ...     0.987952      0.915663     12\n",
      "7  1.241307    0.336043  0.310924  ...     0.987952      0.915663     12\n",
      "8  1.234989    0.337545  0.490683  ...     0.987952      0.915663     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098603    0.029253  0.153846  ...     0.831325      0.915663     14\n",
      "1  0.098186    0.028749  0.150538  ...     0.831325      0.915663     14\n",
      "2  0.098640    0.028681  0.144330  ...     0.831325      0.915663     14\n",
      "3  0.098243    0.028618  0.144330  ...     0.831325      0.915663     14\n",
      "4  0.098857    0.028827  0.473282  ...     0.831325      0.915663     14\n",
      "5  0.097943    0.028860  0.147368  ...     0.831325      0.915663     14\n",
      "6  0.098736    0.029112  0.162791  ...     0.831325      0.915663     14\n",
      "7  0.100791    0.028825  0.145833  ...     0.831325      0.915663     14\n",
      "8  0.097871    0.029076  0.147368  ...     0.831325      0.915663     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098145    0.028733  0.330097  ...     0.831325      0.915663     18\n",
      "1  0.097981    0.028772  0.166667  ...     0.831325      0.915663     18\n",
      "2  0.097890    0.028672  0.188235  ...     0.831325      0.915663     18\n",
      "3  0.097909    0.028692  0.162791  ...     0.831325      0.915663     18\n",
      "4  0.097579    0.028684  0.162791  ...     0.831325      0.915663     18\n",
      "5  0.097838    0.028533  0.147368  ...     0.831325      0.915663     18\n",
      "6  0.097361    0.028339  0.144330  ...     0.831325      0.915663     18\n",
      "7  0.098283    0.028583  0.145833  ...     0.831325      0.915663     18\n",
      "8  0.097574    0.028648  0.160920  ...     0.831325      0.915663     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.101073    0.029287  0.127660  ...     0.855422      0.915663      0\n",
      "1  0.101488    0.030540  0.137931  ...     0.855422      0.915663      0\n",
      "2  0.099403    0.028908  0.126316  ...     0.855422      0.915663      0\n",
      "3  0.100581    0.029050  0.126316  ...     0.855422      0.915663      0\n",
      "4  0.100344    0.028847  0.130435  ...     0.855422      0.915663      0\n",
      "5  0.099499    0.028697  0.228261  ...     0.855422      0.915663      0\n",
      "6  0.113558    0.029027  0.131868  ...     0.855422      0.915663      0\n",
      "7  0.099435    0.028601  0.129032  ...     0.855422      0.915663      0\n",
      "8  0.105444    0.029746  0.127660  ...     0.855422      0.915663      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098796    0.028397  0.506410  ...     0.927711      0.915663      9\n",
      "1  0.097972    0.028157  0.336207  ...     0.927711      0.915663      9\n",
      "2  0.098208    0.028291  0.496732  ...     0.927711      0.915663      9\n",
      "3  0.098147    0.028431  0.479730  ...     0.927711      0.915663      9\n",
      "4  0.098156    0.028320  0.144444  ...     0.927711      0.915663      9\n",
      "5  0.097915    0.028351  0.421053  ...     0.927711      0.915663      9\n",
      "6  0.098171    0.028339  0.189474  ...     0.927711      0.915663      9\n",
      "7  0.098959    0.028426  0.442029  ...     0.927711      0.915663      9\n",
      "8  0.097884    0.028430  0.398438  ...     0.927711      0.915663      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098581    0.028879  0.210000  ...     0.746988      0.915663     14\n",
      "1  0.098627    0.028838  0.214286  ...     0.746988      0.915663     14\n",
      "2  0.098044    0.028151  0.201923  ...     0.746988      0.915663     14\n",
      "3  0.101031    0.029052  0.212121  ...     0.746988      0.915663     14\n",
      "4  0.098161    0.028731  0.403846  ...     0.746988      0.915663     14\n",
      "5  0.099291    0.028771  0.201923  ...     0.746988      0.915663     14\n",
      "6  0.098562    0.028769  0.210000  ...     0.746988      0.915663     14\n",
      "7  0.098939    0.028694  0.203883  ...     0.746988      0.915663     14\n",
      "8  0.098282    0.028808  0.210000  ...     0.746988      0.915663     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098588    0.028733  0.284483  ...      0.60241      0.915663     23\n",
      "1  0.098604    0.028941  0.286957  ...      0.60241      0.915663     23\n",
      "2  0.099853    0.028922  0.537037  ...      0.60241      0.915663     23\n",
      "3  0.098677    0.028384  0.284483  ...      0.60241      0.915663     23\n",
      "4  0.104918    0.028900  0.286957  ...      0.60241      0.915663     23\n",
      "5  0.098164    0.028483  0.284483  ...      0.60241      0.915663     23\n",
      "6  0.098051    0.032457  0.305556  ...      0.60241      0.915663     23\n",
      "7  0.099098    0.028292  0.284483  ...      0.60241      0.915663     23\n",
      "8  0.100044    0.028980  0.286957  ...      0.60241      0.915663     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.253642    0.340372  0.278846  ...     0.650602      0.915663     10\n",
      "1  1.251915    0.339812  0.273585  ...     0.650602      0.915663     10\n",
      "2  1.266082    0.344215  0.425532  ...     0.650602      0.915663     10\n",
      "3  1.245202    0.342710  0.276190  ...     0.650602      0.915663     10\n",
      "4  1.256478    0.345805  0.263636  ...     0.650602      0.915663     10\n",
      "5  1.252858    0.341226  0.325843  ...     0.650602      0.915663     10\n",
      "6  1.245739    0.362645  0.298969  ...     0.650602      0.915663     10\n",
      "7  1.240861    0.346390  0.298969  ...     0.650602      0.915663     10\n",
      "8  1.237178    0.339401  0.322222  ...     0.650602      0.915663     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.106782    0.042599  0.130952  ...      0.86747      0.915663      0\n",
      "1  0.100760    0.029624  0.132530  ...      0.86747      0.915663      0\n",
      "2  0.097675    0.029113  0.118280  ...      0.86747      0.915663      0\n",
      "3  0.097564    0.029187  0.120879  ...      0.86747      0.915663      0\n",
      "4  0.097918    0.029045  0.132530  ...      0.86747      0.915663      0\n",
      "5  0.097772    0.028868  0.272727  ...      0.86747      0.915663      0\n",
      "6  0.097852    0.028899  0.120879  ...      0.86747      0.915663      0\n",
      "7  0.098802    0.028753  0.117021  ...      0.86747      0.915663      0\n",
      "8  0.098213    0.029085  0.132530  ...      0.86747      0.915663      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.106851    0.028925  0.512500  ...     0.939759      0.915663      9\n",
      "1  0.097948    0.028806  0.442857  ...     0.939759      0.915663      9\n",
      "2  0.102665    0.029324  0.515528  ...     0.939759      0.915663      9\n",
      "3  0.098020    0.036825  0.483444  ...     0.939759      0.915663      9\n",
      "4  0.098354    0.028661  0.442857  ...     0.939759      0.915663      9\n",
      "5  0.097957    0.028579  0.469388  ...     0.939759      0.915663      9\n",
      "6  0.097810    0.028739  0.309735  ...     0.939759      0.915663      9\n",
      "7  0.098844    0.028556  0.458333  ...     0.939759      0.915663      9\n",
      "8  0.098509    0.028706  0.426471  ...     0.939759      0.915663      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100706    0.030016  0.209524  ...      0.73494      0.915663     18\n",
      "1  0.099295    0.028882  0.211538  ...      0.73494      0.915663     18\n",
      "2  0.099052    0.028473  0.209524  ...      0.73494      0.915663     18\n",
      "3  0.099838    0.028715  0.209524  ...      0.73494      0.915663     18\n",
      "4  0.099586    0.028609  0.217822  ...      0.73494      0.915663     18\n",
      "5  0.098444    0.028033  0.209524  ...      0.73494      0.915663     18\n",
      "6  0.098536    0.028012  0.209524  ...      0.73494      0.915663     18\n",
      "7  0.099721    0.028313  0.209524  ...      0.73494      0.915663     18\n",
      "8  0.099582    0.029688  0.213592  ...      0.73494      0.915663     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099324    0.029011  0.183673  ...     0.783133      0.915663      2\n",
      "1  0.099109    0.028319  0.178218  ...     0.783133      0.915663      2\n",
      "2  0.098980    0.029043  0.183673  ...     0.783133      0.915663      2\n",
      "3  0.098696    0.028362  0.178218  ...     0.783133      0.915663      2\n",
      "4  0.098819    0.028291  0.178218  ...     0.783133      0.915663      2\n",
      "5  0.098884    0.028854  0.180000  ...     0.783133      0.915663      2\n",
      "6  0.103886    0.029616  0.277778  ...     0.783133      0.915663      2\n",
      "7  0.098800    0.029266  0.191489  ...     0.783133      0.915663      2\n",
      "8  0.099761    0.029212  0.189474  ...     0.783133      0.915663      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.106039    0.029484  0.141176  ...     0.855422      0.915663      2\n",
      "1  0.099087    0.029092  0.131868  ...     0.855422      0.915663      2\n",
      "2  0.098868    0.029106  0.137931  ...     0.855422      0.915663      2\n",
      "3  0.099527    0.028813  0.126316  ...     0.855422      0.915663      2\n",
      "4  0.100816    0.028601  0.126316  ...     0.855422      0.915663      2\n",
      "5  0.099581    0.029042  0.127660  ...     0.855422      0.915663      2\n",
      "6  0.099256    0.029016  0.275510  ...     0.855422      0.915663      2\n",
      "7  0.099667    0.029186  0.139535  ...     0.855422      0.915663      2\n",
      "8  0.099391    0.028937  0.144578  ...     0.855422      0.915663      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098625    0.028784  0.283019  ...     0.638554      0.915663     10\n",
      "1  0.098151    0.028888  0.283019  ...     0.638554      0.915663     10\n",
      "2  0.098217    0.028852  0.453608  ...     0.638554      0.915663     10\n",
      "3  0.098070    0.028721  0.277778  ...     0.638554      0.915663     10\n",
      "4  0.098881    0.028886  0.267857  ...     0.638554      0.915663     10\n",
      "5  0.098247    0.028878  0.322581  ...     0.638554      0.915663     10\n",
      "6  0.097910    0.028706  0.280374  ...     0.638554      0.915663     10\n",
      "7  0.098720    0.028932  0.337079  ...     0.638554      0.915663     10\n",
      "8  0.098028    0.028689  0.303030  ...     0.638554      0.915663     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.250061    0.341185  0.161616  ...     0.807229      0.915663      0\n",
      "1  1.243695    0.338943  0.163265  ...     0.807229      0.915663      0\n",
      "2  1.242987    0.347640  0.161616  ...     0.807229      0.915663      0\n",
      "3  1.240520    0.338034  0.161616  ...     0.807229      0.915663      0\n",
      "4  1.241499    0.339610  0.161616  ...     0.807229      0.915663      0\n",
      "5  1.238423    0.339999  0.186047  ...     0.807229      0.915663      0\n",
      "6  1.244501    0.340023  0.161616  ...     0.807229      0.915663      0\n",
      "7  1.245227    0.337427  0.161616  ...     0.807229      0.915663      0\n",
      "8  1.238645    0.339166  0.164948  ...     0.807229      0.915663      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.097750    0.028570  0.518750  ...     0.927711      0.915663      9\n",
      "1  0.097518    0.028333  0.483221  ...     0.927711      0.915663      9\n",
      "2  0.097719    0.028358  0.518750  ...     0.927711      0.915663      9\n",
      "3  0.097481    0.028471  0.486667  ...     0.927711      0.915663      9\n",
      "4  0.097350    0.028397  0.493421  ...     0.927711      0.915663      9\n",
      "5  0.097447    0.028529  0.421053  ...     0.927711      0.915663      9\n",
      "6  0.097624    0.028151  0.363636  ...     0.927711      0.915663      9\n",
      "7  0.098222    0.028471  0.425373  ...     0.927711      0.915663      9\n",
      "8  0.097542    0.028547  0.468966  ...     0.927711      0.915663      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.097992    0.028708  0.203883  ...     0.746988      0.915663     23\n",
      "1  0.097737    0.028693  0.205882  ...     0.746988      0.915663     23\n",
      "2  0.097677    0.028509  0.483333  ...     0.746988      0.915663     23\n",
      "3  0.097873    0.028506  0.201923  ...     0.746988      0.915663     23\n",
      "4  0.097654    0.028209  0.201923  ...     0.746988      0.915663     23\n",
      "5  0.097628    0.028639  0.203883  ...     0.746988      0.915663     23\n",
      "6  0.097842    0.028933  0.241379  ...     0.746988      0.915663     23\n",
      "7  0.098304    0.028723  0.203883  ...     0.746988      0.915663     23\n",
      "8  0.097991    0.029010  0.214286  ...     0.746988      0.915663     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100501    0.028795  0.153061  ...     0.819277      0.915663      0\n",
      "1  0.099755    0.028566  0.153061  ...     0.819277      0.915663      0\n",
      "2  0.098717    0.028471  0.153061  ...     0.819277      0.915663      0\n",
      "3  0.099130    0.028796  0.154639  ...     0.819277      0.915663      0\n",
      "4  0.098306    0.028170  0.153061  ...     0.819277      0.915663      0\n",
      "5  0.098171    0.028491  0.252747  ...     0.819277      0.915663      0\n",
      "6  0.098408    0.028451  0.154639  ...     0.819277      0.915663      0\n",
      "7  0.098863    0.028865  0.154639  ...     0.819277      0.915663      0\n",
      "8  0.098194    0.034609  0.166667  ...     0.819277      0.915663      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.118562    0.028694  0.081395  ...     0.915663      0.915663      0\n",
      "1  0.097505    0.028599  0.116279  ...     0.915663      0.915663      0\n",
      "2  0.097242    0.028384  0.081395  ...     0.915663      0.915663      0\n",
      "3  0.097666    0.028424  0.078652  ...     0.915663      0.915663      0\n",
      "4  0.097266    0.028090  0.077778  ...     0.915663      0.915663      0\n",
      "5  0.097345    0.029500  0.191489  ...     0.915663      0.915663      0\n",
      "6  0.097534    0.028567  0.079545  ...     0.915663      0.915663      0\n",
      "7  0.097455    0.028668  0.078652  ...     0.915663      0.915663      0\n",
      "8  0.097905    0.028818  0.084337  ...     0.915663      0.915663      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.240410    0.341330  0.191489  ...     0.783133      0.915663     14\n",
      "1  1.238818    0.337376  0.185567  ...     0.783133      0.915663     14\n",
      "2  1.245263    0.338455  0.178218  ...     0.783133      0.915663     14\n",
      "3  1.235589    0.339409  0.178218  ...     0.783133      0.915663     14\n",
      "4  1.236521    0.340783  0.488189  ...     0.783133      0.915663     14\n",
      "5  1.234320    0.335609  0.185567  ...     0.783133      0.915663     14\n",
      "6  1.236249    0.338041  0.195652  ...     0.783133      0.915663     14\n",
      "7  1.239601    0.339826  0.180000  ...     0.783133      0.915663     14\n",
      "8  1.239179    0.340856  0.189474  ...     0.783133      0.915663     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.259003    0.341929  0.198020  ...     0.759036      0.915663     14\n",
      "1  1.241427    0.337068  0.194175  ...     0.759036      0.915663     14\n",
      "2  1.241784    0.342802  0.194175  ...     0.759036      0.915663     14\n",
      "3  1.238472    0.337378  0.194175  ...     0.759036      0.915663     14\n",
      "4  1.241412    0.341100  0.447368  ...     0.759036      0.915663     14\n",
      "5  1.248169    0.337219  0.194175  ...     0.759036      0.915663     14\n",
      "6  1.239288    0.338763  0.198020  ...     0.759036      0.915663     14\n",
      "7  1.243867    0.338302  0.194175  ...     0.759036      0.915663     14\n",
      "8  1.235455    0.340070  0.194175  ...     0.759036      0.915663     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098876    0.028247  0.224299  ...     0.710843      0.915663     23\n",
      "1  0.097785    0.028625  0.226415  ...     0.710843      0.915663     23\n",
      "2  0.097472    0.028446  0.473214  ...     0.710843      0.915663     23\n",
      "3  0.097916    0.028247  0.224299  ...     0.710843      0.915663     23\n",
      "4  0.097557    0.028208  0.224299  ...     0.710843      0.915663     23\n",
      "5  0.097549    0.028135  0.224299  ...     0.710843      0.915663     23\n",
      "6  0.097107    0.028585  0.263736  ...     0.710843      0.915663     23\n",
      "7  0.098301    0.028298  0.224299  ...     0.710843      0.915663     23\n",
      "8  0.098898    0.028745  0.233010  ...     0.710843      0.915663     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098973    0.029295  0.126437  ...     0.915663      0.915663      2\n",
      "1  0.098199    0.028806  0.079545  ...     0.915663      0.915663      2\n",
      "2  0.097368    0.028528  0.082353  ...     0.915663      0.915663      2\n",
      "3  0.097802    0.028605  0.077778  ...     0.915663      0.915663      2\n",
      "4  0.097703    0.028006  0.077778  ...     0.915663      0.915663      2\n",
      "5  0.098549    0.028145  0.077778  ...     0.915663      0.915663      2\n",
      "6  0.097669    0.028745  0.232323  ...     0.915663      0.915663      2\n",
      "7  0.098072    0.029003  0.105882  ...     0.915663      0.915663      2\n",
      "8  0.097757    0.028664  0.146067  ...     0.915663      0.915663      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.102845    0.028919  0.252252  ...     0.662651      0.915663     18\n",
      "1  0.099410    0.028996  0.254545  ...     0.662651      0.915663     18\n",
      "2  0.098551    0.028575  0.252252  ...     0.662651      0.915663     18\n",
      "3  0.100391    0.028548  0.252252  ...     0.662651      0.915663     18\n",
      "4  0.098497    0.028307  0.252252  ...     0.662651      0.915663     18\n",
      "5  0.098539    0.028029  0.252252  ...     0.662651      0.915663     18\n",
      "6  0.097871    0.028113  0.252252  ...     0.662651      0.915663     18\n",
      "7  0.098454    0.028453  0.252252  ...     0.662651      0.915663     18\n",
      "8  0.097860    0.031491  0.252252  ...     0.662651      0.915663     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.098957    0.028410  0.284483  ...      0.60241      0.915663     23\n",
      "1  0.098196    0.028999  0.286957  ...      0.60241      0.915663     23\n",
      "2  0.098189    0.029042  0.565217  ...      0.60241      0.915663     23\n",
      "3  0.097955    0.028400  0.284483  ...      0.60241      0.915663     23\n",
      "4  0.097892    0.028758  0.286957  ...      0.60241      0.915663     23\n",
      "5  0.098361    0.028376  0.284483  ...      0.60241      0.915663     23\n",
      "6  0.098021    0.028782  0.311321  ...      0.60241      0.915663     23\n",
      "7  0.098103    0.033806  0.289474  ...      0.60241      0.915663     23\n",
      "8  0.100357    0.029315  0.292035  ...      0.60241      0.915663     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099218    0.028486  0.186275  ...     0.771084      0.915663     23\n",
      "1  0.097998    0.029244  0.197917  ...     0.771084      0.915663     23\n",
      "2  0.097667    0.028714  0.390476  ...     0.771084      0.915663     23\n",
      "3  0.097756    0.028111  0.186275  ...     0.771084      0.915663     23\n",
      "4  0.097911    0.028388  0.186275  ...     0.771084      0.915663     23\n",
      "5  0.098096    0.028541  0.186275  ...     0.771084      0.915663     23\n",
      "6  0.098508    0.028937  0.200000  ...     0.771084      0.915663     23\n",
      "7  0.098648    0.028444  0.186275  ...     0.771084      0.915663     23\n",
      "8  0.098234    0.028934  0.190000  ...     0.771084      0.915663     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.252925    0.343270  0.159091  ...     0.831325      0.915663      2\n",
      "1  1.245083    0.339615  0.147368  ...     0.831325      0.915663      2\n",
      "2  1.257453    0.338478  0.152174  ...     0.831325      0.915663      2\n",
      "3  1.248658    0.342989  0.144330  ...     0.831325      0.915663      2\n",
      "4  1.242880    0.337856  0.144330  ...     0.831325      0.915663      2\n",
      "5  1.236627    0.338265  0.145833  ...     0.831325      0.915663      2\n",
      "6  1.235086    0.339748  0.316832  ...     0.831325      0.915663      2\n",
      "7  1.242624    0.340503  0.155556  ...     0.831325      0.915663      2\n",
      "8  1.231966    0.337821  0.164706  ...     0.831325      0.915663      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.238523    0.338742  0.304762  ...     0.614458      0.915663     15\n",
      "1  1.230634    0.338420  0.288288  ...     0.614458      0.915663     15\n",
      "2  1.238470    0.339555  0.320000  ...     0.614458      0.915663     15\n",
      "3  1.231248    0.333625  0.278261  ...     0.614458      0.915663     15\n",
      "4  1.228974    0.333407  0.278261  ...     0.614458      0.915663     15\n",
      "5  1.230499    0.337375  0.288288  ...     0.614458      0.915663     15\n",
      "6  1.230994    0.336167  0.285714  ...     0.614458      0.915663     15\n",
      "7  1.235725    0.339689  0.280702  ...     0.614458      0.915663     15\n",
      "8  1.229330    0.338495  0.355556  ...     0.614458      0.915663     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.097932    0.028821  0.117647  ...     0.903614      0.915663      2\n",
      "1  0.097792    0.028947  0.094118  ...     0.903614      0.915663      2\n",
      "2  0.098569    0.029003  0.147727  ...     0.903614      0.915663      2\n",
      "3  0.097349    0.028286  0.087912  ...     0.903614      0.915663      2\n",
      "4  0.097701    0.028702  0.089888  ...     0.903614      0.915663      2\n",
      "5  0.097329    0.028834  0.127907  ...     0.903614      0.915663      2\n",
      "6  0.098593    0.028777  0.299065  ...     0.903614      0.915663      2\n",
      "7  0.097730    0.029123  0.096386  ...     0.903614      0.915663      2\n",
      "8  0.097537    0.029892  0.193548  ...     0.903614      0.915663      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.097073    0.028515  0.341176  ...     0.650602      0.915663     10\n",
      "1  0.096909    0.028556  0.329545  ...     0.650602      0.915663     10\n",
      "2  0.096862    0.028475  0.490566  ...     0.650602      0.915663     10\n",
      "3  0.097203    0.028753  0.292929  ...     0.650602      0.915663     10\n",
      "4  0.096617    0.028809  0.273585  ...     0.650602      0.915663     10\n",
      "5  0.096696    0.028419  0.379310  ...     0.650602      0.915663     10\n",
      "6  0.096867    0.028431  0.329545  ...     0.650602      0.915663     10\n",
      "7  0.097281    0.028594  0.341176  ...     0.650602      0.915663     10\n",
      "8  0.097643    0.028510  0.345238  ...     0.650602      0.915663     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099811    0.029124  0.378641  ...     0.771084      0.915663      4\n",
      "1  0.099198    0.028776  0.311828  ...     0.771084      0.915663      4\n",
      "2  0.098708    0.028797  0.280899  ...     0.771084      0.915663      4\n",
      "3  0.098980    0.028428  0.188119  ...     0.771084      0.915663      4\n",
      "4  0.098120    0.028465  0.200000  ...     0.771084      0.915663      4\n",
      "5  0.098342    0.028588  0.213483  ...     0.771084      0.915663      4\n",
      "6  0.098167    0.028361  0.280899  ...     0.771084      0.915663      4\n",
      "7  0.101582    0.028183  0.466667  ...     0.771084      0.915663      4\n",
      "8  0.098662    0.028649  0.220930  ...     0.771084      0.915663      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100638    0.028838  0.119565  ...      0.86747      0.915663      0\n",
      "1  0.099675    0.029049  0.125000  ...      0.86747      0.915663      0\n",
      "2  0.099115    0.028491  0.117021  ...      0.86747      0.915663      0\n",
      "3  0.098758    0.028724  0.118280  ...      0.86747      0.915663      0\n",
      "4  0.098892    0.028939  0.120879  ...      0.86747      0.915663      0\n",
      "5  0.099088    0.028904  0.191011  ...      0.86747      0.915663      0\n",
      "6  0.099336    0.028840  0.208791  ...      0.86747      0.915663      0\n",
      "7  0.099359    0.037363  0.117021  ...      0.86747      0.915663      0\n",
      "8  0.099873    0.030653  0.127907  ...      0.86747      0.915663      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099736    0.028858  0.240385  ...     0.698795      0.915663     14\n",
      "1  0.099710    0.028903  0.252525  ...     0.698795      0.915663     14\n",
      "2  0.098405    0.028398  0.231481  ...     0.698795      0.915663     14\n",
      "3  0.098716    0.028341  0.231481  ...     0.698795      0.915663     14\n",
      "4  0.098979    0.028912  0.395833  ...     0.698795      0.915663     14\n",
      "5  0.098875    0.028694  0.235849  ...     0.698795      0.915663     14\n",
      "6  0.098586    0.028701  0.233645  ...     0.698795      0.915663     14\n",
      "7  0.099768    0.028640  0.231481  ...     0.698795      0.915663     14\n",
      "8  0.098214    0.028547  0.233645  ...     0.698795      0.915663     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.240954    0.334237  0.512658  ...     0.927711      0.915663      9\n",
      "1  1.236429    0.336545  0.468966  ...     0.927711      0.915663      9\n",
      "2  1.250238    0.338448  0.518750  ...     0.927711      0.915663      9\n",
      "3  1.237031    0.339120  0.500000  ...     0.927711      0.915663      9\n",
      "4  1.245393    0.337919  0.472603  ...     0.927711      0.915663      9\n",
      "5  1.236164    0.336632  0.442029  ...     0.927711      0.915663      9\n",
      "6  1.233408    0.334699  0.363636  ...     0.927711      0.915663      9\n",
      "7  1.242671    0.341389  0.429630  ...     0.927711      0.915663      9\n",
      "8  1.234162    0.334516  0.483221  ...     0.927711      0.915663      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.252710    0.333131  0.500000  ...     0.927711      0.915663      9\n",
      "1  1.239391    0.331657  0.237624  ...     0.927711      0.915663      9\n",
      "2  1.239629    0.341691  0.515723  ...     0.927711      0.915663      9\n",
      "3  1.247969    0.333108  0.468966  ...     0.927711      0.915663      9\n",
      "4  1.239146    0.335388  0.245098  ...     0.927711      0.915663      9\n",
      "5  1.240185    0.331622  0.388889  ...     0.927711      0.915663      9\n",
      "6  1.242512    0.337222  0.104651  ...     0.927711      0.915663      9\n",
      "7  1.242970    0.339260  0.425373  ...     0.927711      0.915663      9\n",
      "8  1.246697    0.335458  0.368852  ...     0.927711      0.915663      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.102069    0.029408  0.138298  ...     0.843373      0.915663     18\n",
      "1  0.099532    0.028990  0.144444  ...     0.843373      0.915663     18\n",
      "2  0.099137    0.028863  0.141304  ...     0.843373      0.915663     18\n",
      "3  0.099129    0.028720  0.135417  ...     0.843373      0.915663     18\n",
      "4  0.099384    0.028646  0.139785  ...     0.843373      0.915663     18\n",
      "5  0.098439    0.028391  0.136842  ...     0.843373      0.915663     18\n",
      "6  0.101240    0.028255  0.135417  ...     0.843373      0.915663     18\n",
      "7  0.099135    0.028269  0.135417  ...     0.843373      0.915663     18\n",
      "8  0.098405    0.028782  0.141304  ...     0.843373      0.915663     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.107575    0.028872  0.351064  ...      0.73494      0.915663     15\n",
      "1  0.098943    0.028738  0.298851  ...      0.73494      0.915663     15\n",
      "2  0.097605    0.028435  0.440367  ...      0.73494      0.915663     15\n",
      "3  0.097835    0.028690  0.211538  ...      0.73494      0.915663     15\n",
      "4  0.098011    0.028104  0.209524  ...      0.73494      0.915663     15\n",
      "5  0.097988    0.028397  0.314607  ...      0.73494      0.915663     15\n",
      "6  0.097579    0.028602  0.247191  ...      0.73494      0.915663     15\n",
      "7  0.098216    0.028817  0.234043  ...      0.73494      0.915663     15\n",
      "8  0.098283    0.028891  0.401961  ...      0.73494      0.915663     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.103281    0.029308  0.144578  ...     0.855422      0.915663      2\n",
      "1  0.099950    0.028878  0.131868  ...     0.855422      0.915663      2\n",
      "2  0.099547    0.031681  0.164706  ...     0.855422      0.915663      2\n",
      "3  0.098880    0.028555  0.126316  ...     0.855422      0.915663      2\n",
      "4  0.099066    0.028815  0.127660  ...     0.855422      0.915663      2\n",
      "5  0.098690    0.028610  0.130435  ...     0.855422      0.915663      2\n",
      "6  0.098462    0.028421  0.317308  ...     0.855422      0.915663      2\n",
      "7  0.098792    0.028752  0.183908  ...     0.855422      0.915663      2\n",
      "8  0.099389    0.034140  0.183908  ...     0.855422      0.915663      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099809    0.028877  0.348315  ...     0.626506      0.915663     10\n",
      "1  0.099319    0.028961  0.333333  ...     0.626506      0.915663     10\n",
      "2  0.099319    0.028861  0.535714  ...     0.626506      0.915663     10\n",
      "3  0.098980    0.029029  0.295238  ...     0.626506      0.915663     10\n",
      "4  0.098925    0.028940  0.287037  ...     0.626506      0.915663     10\n",
      "5  0.098670    0.029202  0.440860  ...     0.626506      0.915663     10\n",
      "6  0.098341    0.028626  0.380952  ...     0.626506      0.915663     10\n",
      "7  0.112444    0.031831  0.463918  ...     0.626506      0.915663     10\n",
      "8  0.099319    0.028691  0.415730  ...     0.626506      0.915663     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099159    0.037970  0.276190  ...     0.650602      0.915663     10\n",
      "1  0.098742    0.028814  0.287129  ...     0.650602      0.915663     10\n",
      "2  0.098477    0.028787  0.357143  ...     0.650602      0.915663     10\n",
      "3  0.099086    0.028947  0.273585  ...     0.650602      0.915663     10\n",
      "4  0.097640    0.029783  0.273585  ...     0.650602      0.915663     10\n",
      "5  0.098357    0.028516  0.318681  ...     0.650602      0.915663     10\n",
      "6  0.098290    0.028610  0.302083  ...     0.650602      0.915663     10\n",
      "7  0.099671    0.028483  0.315217  ...     0.650602      0.915663     10\n",
      "8  0.097880    0.028442  0.329545  ...     0.650602      0.915663     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.238586    0.337034  0.376238  ...     0.759036      0.915663      4\n",
      "1  1.234085    0.339000  0.267442  ...     0.759036      0.915663      4\n",
      "2  1.236609    0.336638  0.235294  ...     0.759036      0.915663      4\n",
      "3  1.232443    0.338201  0.196078  ...     0.759036      0.915663      4\n",
      "4  1.235432    0.337611  0.198020  ...     0.759036      0.915663      4\n",
      "5  1.231598    0.337050  0.240964  ...     0.759036      0.915663      4\n",
      "6  1.233125    0.337263  0.250000  ...     0.759036      0.915663      4\n",
      "7  1.236354    0.340824  0.491935  ...     0.759036      0.915663      4\n",
      "8  1.234571    0.337780  0.204082  ...     0.759036      0.915663      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  1.245385    0.341517  0.149425  ...     0.843373      0.915663      2\n",
      "1  1.235210    0.336769  0.136842  ...     0.843373      0.915663      2\n",
      "2  1.241336    0.338695  0.142857  ...     0.843373      0.915663      2\n",
      "3  1.236368    0.334595  0.135417  ...     0.843373      0.915663      2\n",
      "4  1.233598    0.335593  0.135417  ...     0.843373      0.915663      2\n",
      "5  1.237021    0.335548  0.135417  ...     0.843373      0.915663      2\n",
      "6  1.237136    0.341209  0.247312  ...     0.843373      0.915663      2\n",
      "7  1.243004    0.341039  0.142857  ...     0.843373      0.915663      2\n",
      "8  1.238069    0.337584  0.144444  ...     0.843373      0.915663      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.104383    0.028471  0.400000  ...     0.831325      0.915663     15\n",
      "1  0.097074    0.028170  0.410256  ...     0.831325      0.915663     15\n",
      "2  0.096689    0.028040  0.420168  ...     0.831325      0.915663     15\n",
      "3  0.097132    0.027927  0.144330  ...     0.831325      0.915663     15\n",
      "4  0.097063    0.028097  0.144330  ...     0.831325      0.915663     15\n",
      "5  0.097627    0.028663  0.378378  ...     0.831325      0.915663     15\n",
      "6  0.097980    0.031782  0.258065  ...     0.831325      0.915663     15\n",
      "7  0.098883    0.028480  0.155556  ...     0.831325      0.915663     15\n",
      "8  0.098286    0.028800  0.425000  ...     0.831325      0.915663     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.105323    0.030762  0.447368  ...     0.759036      0.915663      4\n",
      "1  0.098402    0.028818  0.300000  ...     0.759036      0.915663      4\n",
      "2  0.097789    0.028814  0.370000  ...     0.759036      0.915663      4\n",
      "3  0.098425    0.028713  0.198020  ...     0.759036      0.915663      4\n",
      "4  0.097906    0.028777  0.202020  ...     0.759036      0.915663      4\n",
      "5  0.097698    0.028899  0.222222  ...     0.759036      0.915663      4\n",
      "6  0.098435    0.028729  0.357143  ...     0.759036      0.915663      4\n",
      "7  0.098709    0.028751  0.466102  ...     0.759036      0.915663      4\n",
      "8  0.098515    0.029128  0.224719  ...     0.759036      0.915663      4\n",
      "\n",
      "[9 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "fig = utils_plot_acc_eer_dist(test_df_plot, \"Test Accuracy\")\n",
    "IFfig = utils_plot_acc_eer_dist(test_df_plot, \"Test EER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean: 0.7645\n",
      "Overall mean: 0.2780\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 985.14x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 985.14x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = utils_plot_acc_eer_dist(test_df_plot, \"Test Accuracy\")\n",
    "IFfig = utils_plot_acc_eer_dist(test_df_plot, \"Test EER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean: 0.7363\n",
      "Overall mean: 0.2943\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 985.14x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 985.14x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = utils_plot_acc_eer_dist(test_df_plot, \"Test Accuracy\")\n",
    "IFfig = utils_plot_acc_eer_dist(test_df_plot, \"Test EER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean: 0.7663\n",
      "Overall mean: 0.2832\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 985.14x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 985.14x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = utils_plot_acc_eer_dist(test_df_plot, \"Test Accuracy\")\n",
    "IFfig = utils_plot_acc_eer_dist(test_df_plot, \"Test EER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean: 0.7699\n",
      "Overall mean: 0.2794\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 985.14x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 985.14x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test_df_plot[test_df_plot[\"Owner\"]==\"9\"]\n",
    "fig = utils_plot_acc_eer_dist(test_df_plot, \"Test Accuracy\")\n",
    "IFfig = utils_plot_acc_eer_dist(test_df_plot, \"Test EER\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean: 0.7608\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 985.14x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = utils_plot_acc_eer_dist(test_df_plot, \"Test Accuracy\")\n",
    "plt.savefig(f'WACA-IF-Accuray-win_size={P.window_size}-step_width={P.IF_step_width}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean: 0.2846\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 985.14x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "IFfig = utils_plot_acc_eer_dist(test_df_plot, \"Test EER\")\n",
    "plt.savefig(f'WACA-IF-EER-win_size={P.window_size}-step_width={P.IF_step_width}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>VALID-ROBUST-IF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frequency</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_subjects</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_test_subjects</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_ids</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_sample_points_per_exp</th>\n",
       "      <td>21000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp_begin_cutoff_idx</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp_end_cutoff_idx</th>\n",
       "      <td>-500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seconds_per_subject_train</th>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seconds_per_subject_test</th>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>window_size</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IF_step_width</th>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler</th>\n",
       "      <td>RobustScaler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler_scope</th>\n",
       "      <td>subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler_global</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IF_kernel</th>\n",
       "      <td>rbf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IF_nu</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IF_gamma</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_cols</th>\n",
       "      <td>[EMA_x_a, EMA_y_a, EMA_z_a, EMA_x_g, EMA_y_g, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exclude_subjects</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       Value\n",
       "name                                                         VALID-ROBUST-IF\n",
       "frequency                                                                100\n",
       "max_subjects                                                              29\n",
       "max_test_subjects                                                         10\n",
       "user_ids                   [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 2...\n",
       "num_sample_points_per_exp                                              21000\n",
       "exp_begin_cutoff_idx                                                     500\n",
       "exp_end_cutoff_idx                                                      -500\n",
       "seconds_per_subject_train                                                210\n",
       "seconds_per_subject_test                                                 210\n",
       "window_size                                                              500\n",
       "IF_step_width                                                            250\n",
       "scaler                                                          RobustScaler\n",
       "scaler_scope                                                         subject\n",
       "scaler_global                                                          False\n",
       "IF_kernel                                                                rbf\n",
       "IF_nu                                                                   None\n",
       "IF_gamma                                                                None\n",
       "feature_cols               [EMA_x_a, EMA_y_a, EMA_z_a, EMA_x_g, EMA_y_g, ...\n",
       "exclude_subjects                                                          []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.117446    0.029487  0.252525  ...     0.698795      0.891566     14\n",
      "1  0.102563    0.029366  0.238095  ...     0.698795      0.891566     14\n",
      "2  0.100275    0.029358  0.231481  ...     0.698795      0.891566     14\n",
      "3  0.099910    0.028982  0.231481  ...     0.698795      0.891566     14\n",
      "4  0.099543    0.028715  0.425743  ...     0.698795      0.891566     14\n",
      "5  0.099458    0.028923  0.240385  ...     0.698795      0.891566     14\n",
      "6  0.099875    0.029133  0.240385  ...     0.698795      0.891566     14\n",
      "7  0.104013    0.029261  0.231481  ...     0.698795      0.891566     14\n",
      "8  0.101777    0.030089  0.233645  ...     0.698795      0.891566     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100016    0.028985  0.161616  ...     0.807229      0.891566      0\n",
      "1  0.099698    0.029148  0.163265  ...     0.807229      0.891566      0\n",
      "2  0.098358    0.028646  0.161616  ...     0.807229      0.891566      0\n",
      "3  0.099607    0.029236  0.161616  ...     0.807229      0.891566      0\n",
      "4  0.097849    0.028574  0.163265  ...     0.807229      0.891566      0\n",
      "5  0.097991    0.028878  0.190476  ...     0.807229      0.891566      0\n",
      "6  0.101252    0.029107  0.161616  ...     0.807229      0.891566      0\n",
      "7  0.104863    0.029979  0.161616  ...     0.807229      0.891566      0\n",
      "8  0.104760    0.028905  0.164948  ...     0.807229      0.891566      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.104785    0.030580  0.484615  ...     0.240964      0.891566     12\n",
      "1  0.104137    0.029486  0.504000  ...     0.240964      0.891566     12\n",
      "2  0.105564    0.030347  0.431507  ...     0.240964      0.891566     12\n",
      "3  0.105131    0.030309  0.437500  ...     0.240964      0.891566     12\n",
      "4  0.100249    0.029670  0.793814  ...     0.240964      0.891566     12\n",
      "5  0.101118    0.029294  0.450000  ...     0.240964      0.891566     12\n",
      "6  0.099851    0.029145  0.692308  ...     0.240964      0.891566     12\n",
      "7  0.101261    0.029141  0.431507  ...     0.240964      0.891566     12\n",
      "8  0.106488    0.036521  0.588785  ...     0.240964      0.891566     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.103577    0.030894  0.447761  ...     0.891566      0.891566      9\n",
      "1  0.103414    0.028939  0.237113  ...     0.891566      0.891566      9\n",
      "2  0.101891    0.031480  0.463768  ...     0.891566      0.891566      9\n",
      "3  0.109296    0.032643  0.475177  ...     0.891566      0.891566      9\n",
      "4  0.110697    0.034070  0.412698  ...     0.891566      0.891566      9\n",
      "5  0.101032    0.029398  0.350877  ...     0.891566      0.891566      9\n",
      "6  0.100146    0.029414  0.186813  ...     0.891566      0.891566      9\n",
      "7  0.103752    0.029796  0.417323  ...     0.891566      0.891566      9\n",
      "8  0.113540    0.031701  0.274510  ...     0.891566      0.891566      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.104590    0.029778  0.228916  ...     0.771084      0.891566     15\n",
      "1  0.100433    0.029153  0.215909  ...     0.771084      0.891566     15\n",
      "2  0.108908    0.041050  0.304348  ...     0.771084      0.891566     15\n",
      "3  0.100468    0.028921  0.186275  ...     0.771084      0.891566     15\n",
      "4  0.100784    0.029047  0.186275  ...     0.771084      0.891566     15\n",
      "5  0.100964    0.028649  0.296703  ...     0.771084      0.891566     15\n",
      "6  0.099177    0.028910  0.228916  ...     0.771084      0.891566     15\n",
      "7  0.098920    0.028887  0.206522  ...     0.771084      0.891566     15\n",
      "8  0.103619    0.030048  0.390476  ...     0.771084      0.891566     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099943    0.029170  0.178218  ...     0.783133      0.891566      0\n",
      "1  0.102113    0.029954  0.183673  ...     0.783133      0.891566      0\n",
      "2  0.099205    0.028738  0.178218  ...     0.783133      0.891566      0\n",
      "3  0.098931    0.028575  0.178218  ...     0.783133      0.891566      0\n",
      "4  0.098775    0.029088  0.180000  ...     0.783133      0.891566      0\n",
      "5  0.100068    0.029318  0.191489  ...     0.783133      0.891566      0\n",
      "6  0.098819    0.029244  0.183673  ...     0.783133      0.891566      0\n",
      "7  0.098331    0.028674  0.178218  ...     0.783133      0.891566      0\n",
      "8  0.098408    0.029099  0.191489  ...     0.783133      0.891566      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099929    0.029471  0.385542  ...     0.614458      0.891566      4\n",
      "1  0.099319    0.029210  0.329897  ...     0.614458      0.891566      4\n",
      "2  0.098872    0.029309  0.310680  ...     0.614458      0.891566      4\n",
      "3  0.098617    0.029166  0.280702  ...     0.614458      0.891566      4\n",
      "4  0.098601    0.028845  0.285714  ...     0.614458      0.891566      4\n",
      "5  0.098832    0.029112  0.293578  ...     0.614458      0.891566      4\n",
      "6  0.097919    0.029087  0.344086  ...     0.614458      0.891566      4\n",
      "7  0.098359    0.029027  0.445652  ...     0.614458      0.891566      4\n",
      "8  0.103022    0.030369  0.285714  ...     0.614458      0.891566      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.105992    0.032144  0.507937  ...     0.228916      0.891566     12\n",
      "1  0.101668    0.029696  0.646465  ...     0.228916      0.891566     12\n",
      "2  0.101509    0.028805  0.435374  ...     0.228916      0.891566     12\n",
      "3  0.100143    0.029504  0.441379  ...     0.228916      0.891566     12\n",
      "4  0.099514    0.029360  0.744186  ...     0.228916      0.891566     12\n",
      "5  0.107411    0.029170  0.447552  ...     0.228916      0.891566     12\n",
      "6  0.108925    0.030043  0.609524  ...     0.228916      0.891566     12\n",
      "7  0.107395    0.029006  0.435374  ...     0.228916      0.891566     12\n",
      "8  0.101040    0.029605  0.640000  ...     0.228916      0.891566     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.101335    0.029342  0.283186  ...     0.614458      0.891566     10\n",
      "1  0.100099    0.028677  0.278261  ...     0.614458      0.891566     10\n",
      "2  0.099408    0.028972  0.347826  ...     0.614458      0.891566     10\n",
      "3  0.098815    0.029056  0.299065  ...     0.614458      0.891566     10\n",
      "4  0.098414    0.028445  0.278261  ...     0.614458      0.891566     10\n",
      "5  0.099185    0.029016  0.285714  ...     0.614458      0.891566     10\n",
      "6  0.110335    0.029823  0.347826  ...     0.614458      0.891566     10\n",
      "7  0.105963    0.031561  0.296296  ...     0.614458      0.891566     10\n",
      "8  0.115289    0.034672  0.290909  ...     0.614458      0.891566     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.109678    0.031885  0.342593  ...     0.855422      0.891566      9\n",
      "1  0.107181    0.031124  0.136364  ...     0.855422      0.891566      9\n",
      "2  0.101441    0.028984  0.398305  ...     0.855422      0.891566      9\n",
      "3  0.098757    0.028829  0.366071  ...     0.855422      0.891566      9\n",
      "4  0.098990    0.038197  0.141176  ...     0.855422      0.891566      9\n",
      "5  0.111482    0.036054  0.183908  ...     0.855422      0.891566      9\n",
      "6  0.100628    0.029190  0.129032  ...     0.855422      0.891566      9\n",
      "7  0.099162    0.028971  0.408333  ...     0.855422      0.891566      9\n",
      "8  0.099121    0.028908  0.236559  ...     0.855422      0.891566      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.186566    0.044199  0.266055  ...     0.650602      0.891566     14\n",
      "1  0.107015    0.031546  0.261261  ...     0.650602      0.891566     14\n",
      "2  0.103248    0.029633  0.258929  ...     0.650602      0.891566     14\n",
      "3  0.103417    0.029588  0.258929  ...     0.650602      0.891566     14\n",
      "4  0.101603    0.029304  0.460000  ...     0.650602      0.891566     14\n",
      "5  0.102799    0.030342  0.268519  ...     0.650602      0.891566     14\n",
      "6  0.104512    0.029868  0.261261  ...     0.650602      0.891566     14\n",
      "7  0.109212    0.030425  0.258929  ...     0.650602      0.891566     14\n",
      "8  0.104023    0.030901  0.263636  ...     0.650602      0.891566     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.132591    0.035531  0.258824  ...      0.73494      0.891566     14\n",
      "1  0.111015    0.029030  0.222222  ...      0.73494      0.891566     14\n",
      "2  0.101904    0.033171  0.209524  ...      0.73494      0.891566     14\n",
      "3  0.110138    0.030823  0.215686  ...      0.73494      0.891566     14\n",
      "4  0.100928    0.029177  0.500000  ...      0.73494      0.891566     14\n",
      "5  0.106836    0.032820  0.215686  ...      0.73494      0.891566     14\n",
      "6  0.106940    0.029878  0.234043  ...      0.73494      0.891566     14\n",
      "7  0.105730    0.037761  0.220000  ...      0.73494      0.891566     14\n",
      "8  0.102005    0.029962  0.261905  ...      0.73494      0.891566     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.112428    0.031687  0.535714  ...     0.277108      0.891566     12\n",
      "1  0.099258    0.030584  0.447761  ...     0.277108      0.891566     12\n",
      "2  0.107337    0.028785  0.419580  ...     0.277108      0.891566     12\n",
      "3  0.100592    0.028412  0.437956  ...     0.277108      0.891566     12\n",
      "4  0.098931    0.028854  0.689655  ...     0.277108      0.891566     12\n",
      "5  0.098653    0.028599  0.431655  ...     0.277108      0.891566     12\n",
      "6  0.105041    0.028602  0.566038  ...     0.277108      0.891566     12\n",
      "7  0.100635    0.028213  0.419580  ...     0.277108      0.891566     12\n",
      "8  0.103700    0.029390  0.535714  ...     0.277108      0.891566     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.112535    0.034537  0.136364  ...     0.855422      0.891566      2\n",
      "1  0.104278    0.030994  0.129032  ...     0.855422      0.891566      2\n",
      "2  0.105339    0.029768  0.137931  ...     0.855422      0.891566      2\n",
      "3  0.100234    0.029231  0.126316  ...     0.855422      0.891566      2\n",
      "4  0.099306    0.029348  0.126316  ...     0.855422      0.891566      2\n",
      "5  0.099058    0.029377  0.133333  ...     0.855422      0.891566      2\n",
      "6  0.097960    0.029112  0.393162  ...     0.855422      0.891566      2\n",
      "7  0.097470    0.029182  0.139535  ...     0.855422      0.891566      2\n",
      "8  0.098130    0.029401  0.183908  ...     0.855422      0.891566      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.102487    0.029350  0.144330  ...     0.831325      0.891566      0\n",
      "1  0.099892    0.028951  0.144330  ...     0.831325      0.891566      0\n",
      "2  0.098472    0.028898  0.144330  ...     0.831325      0.891566      0\n",
      "3  0.098674    0.028787  0.144330  ...     0.831325      0.891566      0\n",
      "4  0.098151    0.028821  0.145833  ...     0.831325      0.891566      0\n",
      "5  0.098404    0.029110  0.145833  ...     0.831325      0.891566      0\n",
      "6  0.098466    0.029130  0.145833  ...     0.831325      0.891566      0\n",
      "7  0.099164    0.028651  0.144330  ...     0.831325      0.891566      0\n",
      "8  0.098419    0.029443  0.144330  ...     0.831325      0.891566      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.106008    0.028482  0.224299  ...     0.710843      0.891566     18\n",
      "1  0.106937    0.033192  0.224299  ...     0.710843      0.891566     18\n",
      "2  0.106282    0.028238  0.224299  ...     0.710843      0.891566     18\n",
      "3  0.098460    0.028534  0.224299  ...     0.710843      0.891566     18\n",
      "4  0.098106    0.028420  0.224299  ...     0.710843      0.891566     18\n",
      "5  0.106712    0.034511  0.224299  ...     0.710843      0.891566     18\n",
      "6  0.118796    0.032013  0.224299  ...     0.710843      0.891566     18\n",
      "7  0.112164    0.033158  0.224299  ...     0.710843      0.891566     18\n",
      "8  0.105529    0.029545  0.224299  ...     0.710843      0.891566     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100734    0.028866  0.252252  ...     0.662651      0.891566     14\n",
      "1  0.099660    0.028755  0.252252  ...     0.662651      0.891566     14\n",
      "2  0.099297    0.028627  0.252252  ...     0.662651      0.891566     14\n",
      "3  0.098159    0.028403  0.252252  ...     0.662651      0.891566     14\n",
      "4  0.097990    0.028905  0.427083  ...     0.662651      0.891566     14\n",
      "5  0.098856    0.028745  0.254545  ...     0.662651      0.891566     14\n",
      "6  0.098431    0.028996  0.256881  ...     0.662651      0.891566     14\n",
      "7  0.099048    0.028472  0.252252  ...     0.662651      0.891566     14\n",
      "8  0.097879    0.029257  0.252252  ...     0.662651      0.891566     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.128402    0.029527  0.290598  ...     0.590361      0.891566     23\n",
      "1  0.100388    0.029612  0.295652  ...     0.590361      0.891566     23\n",
      "2  0.099675    0.029514  0.510000  ...     0.590361      0.891566     23\n",
      "3  0.100136    0.029455  0.290598  ...     0.590361      0.891566     23\n",
      "4  0.099909    0.029116  0.290598  ...     0.590361      0.891566     23\n",
      "5  0.100399    0.039944  0.290598  ...     0.590361      0.891566     23\n",
      "6  0.104798    0.029747  0.326923  ...     0.590361      0.891566     23\n",
      "7  0.100026    0.029081  0.290598  ...     0.590361      0.891566     23\n",
      "8  0.099798    0.029394  0.306306  ...     0.590361      0.891566     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100562    0.029117  0.517730  ...     0.819277      0.891566      9\n",
      "1  0.099803    0.029032  0.174419  ...     0.819277      0.891566      9\n",
      "2  0.099099    0.028860  0.517730  ...     0.819277      0.891566      9\n",
      "3  0.098996    0.028663  0.413793  ...     0.819277      0.891566      9\n",
      "4  0.098401    0.028676  0.339806  ...     0.819277      0.891566      9\n",
      "5  0.098571    0.028562  0.227273  ...     0.819277      0.891566      9\n",
      "6  0.098156    0.028546  0.178571  ...     0.819277      0.891566      9\n",
      "7  0.098561    0.028422  0.381818  ...     0.819277      0.891566      9\n",
      "8  0.101112    0.028487  0.320000  ...     0.819277      0.891566      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.101662    0.029155  0.329670  ...      0.73494      0.891566     15\n",
      "1  0.099613    0.029092  0.265060  ...      0.73494      0.891566     15\n",
      "2  0.099285    0.029035  0.419048  ...      0.73494      0.891566     15\n",
      "3  0.099382    0.028783  0.209524  ...      0.73494      0.891566     15\n",
      "4  0.099681    0.028888  0.209524  ...      0.73494      0.891566     15\n",
      "5  0.113008    0.032229  0.290698  ...      0.73494      0.891566     15\n",
      "6  0.104049    0.033485  0.255814  ...      0.73494      0.891566     15\n",
      "7  0.102670    0.029600  0.250000  ...      0.73494      0.891566     15\n",
      "8  0.104476    0.031919  0.464912  ...      0.73494      0.891566     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.100363    0.028924  0.271930  ...     0.626506      0.891566     23\n",
      "1  0.099866    0.029168  0.274336  ...     0.626506      0.891566     23\n",
      "2  0.098711    0.029095  0.474747  ...     0.626506      0.891566     23\n",
      "3  0.100128    0.028541  0.271930  ...     0.626506      0.891566     23\n",
      "4  0.107246    0.030525  0.271930  ...     0.626506      0.891566     23\n",
      "5  0.099854    0.028645  0.271930  ...     0.626506      0.891566     23\n",
      "6  0.099308    0.028827  0.287037  ...     0.626506      0.891566     23\n",
      "7  0.101846    0.028605  0.271930  ...     0.626506      0.891566     23\n",
      "8  0.099630    0.028722  0.274336  ...     0.626506      0.891566     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099353    0.030877  0.305085  ...     0.566265      0.891566     10\n",
      "1  0.109743    0.029982  0.318584  ...     0.566265      0.891566     10\n",
      "2  0.104922    0.029014  0.333333  ...     0.566265      0.891566     10\n",
      "3  0.098120    0.028955  0.313043  ...     0.566265      0.891566     10\n",
      "4  0.098083    0.028861  0.307692  ...     0.566265      0.891566     10\n",
      "5  0.122384    0.035111  0.310345  ...     0.566265      0.891566     10\n",
      "6  0.104822    0.029463  0.305085  ...     0.566265      0.891566     10\n",
      "7  0.100005    0.029427  0.321429  ...     0.566265      0.891566     10\n",
      "8  0.099663    0.029320  0.349515  ...     0.566265      0.891566     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.102604    0.029631  0.201923  ...     0.746988      0.891566     18\n",
      "1  0.121717    0.031811  0.201923  ...     0.746988      0.891566     18\n",
      "2  0.101149    0.029844  0.201923  ...     0.746988      0.891566     18\n",
      "3  0.099885    0.029599  0.201923  ...     0.746988      0.891566     18\n",
      "4  0.100637    0.029204  0.201923  ...     0.746988      0.891566     18\n",
      "5  0.099402    0.029124  0.201923  ...     0.746988      0.891566     18\n",
      "6  0.099752    0.029132  0.201923  ...     0.746988      0.891566     18\n",
      "7  0.099462    0.029323  0.201923  ...     0.746988      0.891566     18\n",
      "8  0.101201    0.029478  0.201923  ...     0.746988      0.891566     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.108721    0.035069  0.341176  ...     0.674699      0.891566      4\n",
      "1  0.105992    0.030060  0.303371  ...     0.674699      0.891566      4\n",
      "2  0.104364    0.033679  0.287234  ...     0.674699      0.891566      4\n",
      "3  0.107635    0.030123  0.247706  ...     0.674699      0.891566      4\n",
      "4  0.100612    0.029407  0.245455  ...     0.674699      0.891566      4\n",
      "5  0.117275    0.034896  0.247706  ...     0.674699      0.891566      4\n",
      "6  0.102124    0.031597  0.270000  ...     0.674699      0.891566      4\n",
      "7  0.104801    0.030360  0.410526  ...     0.674699      0.891566      4\n",
      "8  0.108641    0.030584  0.259615  ...     0.674699      0.891566      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.108422    0.030885  0.345238  ...     0.650602      0.891566      4\n",
      "1  0.103845    0.030630  0.290000  ...     0.650602      0.891566      4\n",
      "2  0.100343    0.029587  0.325843  ...     0.650602      0.891566      4\n",
      "3  0.099375    0.029265  0.258929  ...     0.650602      0.891566      4\n",
      "4  0.099340    0.029262  0.261261  ...     0.650602      0.891566      4\n",
      "5  0.099163    0.029469  0.287129  ...     0.650602      0.891566      4\n",
      "6  0.099090    0.029111  0.276190  ...     0.650602      0.891566      4\n",
      "7  0.099596    0.029228  0.406593  ...     0.650602      0.891566      4\n",
      "8  0.103707    0.031236  0.268519  ...     0.650602      0.891566      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.105706    0.029769  0.553571  ...     0.253012      0.891566     12\n",
      "1  0.104009    0.030117  0.750000  ...     0.253012      0.891566     12\n",
      "2  0.100661    0.028622  0.427586  ...     0.253012      0.891566     12\n",
      "3  0.099015    0.028710  0.436620  ...     0.253012      0.891566     12\n",
      "4  0.105920    0.032888  0.771739  ...     0.253012      0.891566     12\n",
      "5  0.102896    0.028796  0.484375  ...     0.253012      0.891566     12\n",
      "6  0.099074    0.035335  0.613861  ...     0.253012      0.891566     12\n",
      "7  0.108916    0.031493  0.439716  ...     0.253012      0.891566     12\n",
      "8  0.108342    0.031517  0.601942  ...     0.253012      0.891566     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.104974    0.029922  0.202020  ...     0.759036      0.891566      2\n",
      "1  0.100986    0.028831  0.194175  ...     0.759036      0.891566      2\n",
      "2  0.099716    0.029314  0.194175  ...     0.759036      0.891566      2\n",
      "3  0.100097    0.028816  0.194175  ...     0.759036      0.891566      2\n",
      "4  0.099958    0.029042  0.194175  ...     0.759036      0.891566      2\n",
      "5  0.100903    0.029545  0.198020  ...     0.759036      0.891566      2\n",
      "6  0.099621    0.029451  0.232558  ...     0.759036      0.891566      2\n",
      "7  0.100058    0.029308  0.198020  ...     0.759036      0.891566      2\n",
      "8  0.105025    0.030453  0.196078  ...     0.759036      0.891566      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.110007    0.030526  0.144330  ...     0.831325      0.891566      0\n",
      "1  0.109689    0.032223  0.148936  ...     0.831325      0.891566      0\n",
      "2  0.110141    0.029758  0.144330  ...     0.831325      0.891566      0\n",
      "3  0.110333    0.030834  0.144330  ...     0.831325      0.891566      0\n",
      "4  0.102478    0.033151  0.144330  ...     0.831325      0.891566      0\n",
      "5  0.115948    0.032759  0.178571  ...     0.831325      0.891566      0\n",
      "6  0.108947    0.033274  0.147368  ...     0.831325      0.891566      0\n",
      "7  0.102764    0.030465  0.144330  ...     0.831325      0.891566      0\n",
      "8  0.102758    0.032283  0.145833  ...     0.831325      0.891566      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 0 train_idx range: 0, 82\n",
      "owner: 0 valid_idx range: 83, 165\n",
      "imposter: 1 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.104358    0.029768  0.127907  ...      0.86747      0.891566      0\n",
      "1  0.100746    0.029090  0.208791  ...      0.86747      0.891566      0\n",
      "2  0.098882    0.028941  0.117021  ...      0.86747      0.891566      0\n",
      "3  0.098711    0.029198  0.118280  ...      0.86747      0.891566      0\n",
      "4  0.142088    0.031951  0.126437  ...      0.86747      0.891566      0\n",
      "5  0.117144    0.032787  0.384615  ...      0.86747      0.891566      0\n",
      "6  0.104652    0.030443  0.127907  ...      0.86747      0.891566      0\n",
      "7  0.100319    0.032521  0.117021  ...      0.86747      0.891566      0\n",
      "8  0.102506    0.031169  0.125000  ...      0.86747      0.891566      0\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099926    0.028644  0.258929  ...     0.650602      0.891566     23\n",
      "1  0.099175    0.028679  0.258929  ...     0.650602      0.891566     23\n",
      "2  0.101065    0.029624  0.318681  ...     0.650602      0.891566     23\n",
      "3  0.098909    0.028727  0.258929  ...     0.650602      0.891566     23\n",
      "4  0.100158    0.028602  0.258929  ...     0.650602      0.891566     23\n",
      "5  0.101891    0.032187  0.258929  ...     0.650602      0.891566     23\n",
      "6  0.104553    0.031276  0.287129  ...     0.650602      0.891566     23\n",
      "7  0.106077    0.029136  0.258929  ...     0.650602      0.891566     23\n",
      "8  0.103495    0.032089  0.258929  ...     0.650602      0.891566     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.113439    0.031571  0.290598  ...     0.590361      0.891566     23\n",
      "1  0.106264    0.029608  0.293103  ...     0.590361      0.891566     23\n",
      "2  0.098057    0.030352  0.546296  ...     0.590361      0.891566     23\n",
      "3  0.097838    0.028872  0.290598  ...     0.590361      0.891566     23\n",
      "4  0.104554    0.029290  0.290598  ...     0.590361      0.891566     23\n",
      "5  0.102363    0.029035  0.290598  ...     0.590361      0.891566     23\n",
      "6  0.100689    0.029542  0.369565  ...     0.590361      0.891566     23\n",
      "7  0.099430    0.028692  0.290598  ...     0.590361      0.891566     23\n",
      "8  0.097682    0.029010  0.320755  ...     0.590361      0.891566     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 1 train_idx range: 0, 82\n",
      "owner: 1 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 2 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.099931    0.030322  0.390805  ...     0.638554      0.891566     12\n",
      "1  0.098702    0.028747  0.518182  ...     0.638554      0.891566     12\n",
      "2  0.097703    0.028720  0.272727  ...     0.638554      0.891566     12\n",
      "3  0.097842    0.028548  0.291262  ...     0.638554      0.891566     12\n",
      "4  0.098074    0.028617  0.579365  ...     0.638554      0.891566     12\n",
      "5  0.098339    0.028780  0.340909  ...     0.638554      0.891566     12\n",
      "6  0.097511    0.028236  0.500000  ...     0.638554      0.891566     12\n",
      "7  0.097564    0.028314  0.283019  ...     0.638554      0.891566     12\n",
      "8  0.097176    0.029457  0.436170  ...     0.638554      0.891566     12\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.106354    0.029488  0.201923  ...     0.746988      0.891566     18\n",
      "1  0.107389    0.029851  0.203883  ...     0.746988      0.891566     18\n",
      "2  0.103339    0.029091  0.201923  ...     0.746988      0.891566     18\n",
      "3  0.102618    0.029917  0.201923  ...     0.746988      0.891566     18\n",
      "4  0.101701    0.029670  0.205882  ...     0.746988      0.891566     18\n",
      "5  0.104245    0.029272  0.201923  ...     0.746988      0.891566     18\n",
      "6  0.101860    0.028956  0.201923  ...     0.746988      0.891566     18\n",
      "7  0.099562    0.028660  0.201923  ...     0.746988      0.891566     18\n",
      "8  0.098773    0.029157  0.205882  ...     0.746988      0.891566     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.105992    0.032482  0.265487  ...     0.638554      0.891566     18\n",
      "1  0.108943    0.032875  0.267857  ...     0.638554      0.891566     18\n",
      "2  0.109727    0.029374  0.265487  ...     0.638554      0.891566     18\n",
      "3  0.109533    0.032617  0.265487  ...     0.638554      0.891566     18\n",
      "4  0.101526    0.031937  0.265487  ...     0.638554      0.891566     18\n",
      "5  0.100704    0.028808  0.265487  ...     0.638554      0.891566     18\n",
      "6  0.098130    0.028593  0.265487  ...     0.638554      0.891566     18\n",
      "7  0.098623    0.028695  0.265487  ...     0.638554      0.891566     18\n",
      "8  0.097828    0.028445  0.265487  ...     0.638554      0.891566     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.116645    0.031116  0.381443  ...     0.554217      0.891566     10\n",
      "1  0.105501    0.030181  0.308333  ...     0.554217      0.891566     10\n",
      "2  0.105338    0.032713  0.471264  ...     0.554217      0.891566     10\n",
      "3  0.105608    0.030971  0.327434  ...     0.554217      0.891566     10\n",
      "4  0.104824    0.029393  0.318966  ...     0.554217      0.891566     10\n",
      "5  0.103297    0.030205  0.373737  ...     0.554217      0.891566     10\n",
      "6  0.099665    0.029328  0.349057  ...     0.554217      0.891566     10\n",
      "7  0.100318    0.030203  0.349057  ...     0.554217      0.891566     10\n",
      "8  0.101892    0.029593  0.333333  ...     0.554217      0.891566     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 2 train_idx range: 0, 82\n",
      "owner: 2 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 3 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.104830    0.030158  0.170000  ...     0.795181      0.891566     14\n",
      "1  0.103256    0.030049  0.170000  ...     0.795181      0.891566     14\n",
      "2  0.108938    0.032166  0.170000  ...     0.795181      0.891566     14\n",
      "3  0.112522    0.030688  0.170000  ...     0.795181      0.891566     14\n",
      "4  0.112779    0.030598  0.388889  ...     0.795181      0.891566     14\n",
      "5  0.105714    0.030844  0.173469  ...     0.795181      0.891566     14\n",
      "6  0.118316    0.035649  0.177083  ...     0.795181      0.891566     14\n",
      "7  0.106009    0.031859  0.173469  ...     0.795181      0.891566     14\n",
      "8  0.111659    0.033174  0.170000  ...     0.795181      0.891566     14\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.109700    0.032391  0.280899  ...     0.698795      0.891566     15\n",
      "1  0.105459    0.030081  0.263158  ...     0.698795      0.891566     15\n",
      "2  0.102371    0.029829  0.457944  ...     0.698795      0.891566     15\n",
      "3  0.108901    0.029635  0.231481  ...     0.698795      0.891566     15\n",
      "4  0.102397    0.029431  0.231481  ...     0.698795      0.891566     15\n",
      "5  0.104398    0.029627  0.402062  ...     0.698795      0.891566     15\n",
      "6  0.110764    0.029685  0.250000  ...     0.698795      0.891566     15\n",
      "7  0.100800    0.029794  0.255102  ...     0.698795      0.891566     15\n",
      "8  0.113440    0.032583  0.355556  ...     0.698795      0.891566     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.107206    0.031391  0.457143  ...     0.915663      0.891566      9\n",
      "1  0.107289    0.031912  0.232323  ...     0.915663      0.891566      9\n",
      "2  0.101724    0.031729  0.518987  ...     0.915663      0.891566      9\n",
      "3  0.113045    0.033458  0.441176  ...     0.915663      0.891566      9\n",
      "4  0.121764    0.034905  0.392000  ...     0.915663      0.891566      9\n",
      "5  0.109062    0.031580  0.432836  ...     0.915663      0.891566      9\n",
      "6  0.109860    0.033470  0.262136  ...     0.915663      0.891566      9\n",
      "7  0.113163    0.032666  0.419847  ...     0.915663      0.891566      9\n",
      "8  0.102812    0.029495  0.382114  ...     0.915663      0.891566      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 3 train_idx range: 0, 82\n",
      "owner: 3 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 4 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.107428    0.033036  0.382114  ...     0.915663      0.891566      9\n",
      "1  0.106036    0.029401  0.182796  ...     0.915663      0.891566      9\n",
      "2  0.100699    0.029055  0.468531  ...     0.915663      0.891566      9\n",
      "3  0.098991    0.029173  0.460993  ...     0.915663      0.891566      9\n",
      "4  0.115617    0.033707  0.333333  ...     0.915663      0.891566      9\n",
      "5  0.101916    0.029913  0.155556  ...     0.915663      0.891566      9\n",
      "6  0.100942    0.029284  0.080460  ...     0.915663      0.891566      9\n",
      "7  0.101702    0.029621  0.410853  ...     0.915663      0.891566      9\n",
      "8  0.110735    0.028731  0.344828  ...     0.915663      0.891566      9\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.101949    0.028896  0.324561  ...     0.554217      0.891566     10\n",
      "1  0.105382    0.030203  0.308333  ...     0.554217      0.891566     10\n",
      "2  0.104611    0.030577  0.330357  ...     0.554217      0.891566     10\n",
      "3  0.097973    0.029024  0.316239  ...     0.554217      0.891566     10\n",
      "4  0.098036    0.029014  0.313559  ...     0.554217      0.891566     10\n",
      "5  0.100244    0.029161  0.366337  ...     0.554217      0.891566     10\n",
      "6  0.098404    0.029052  0.349057  ...     0.554217      0.891566     10\n",
      "7  0.099242    0.028987  0.349057  ...     0.554217      0.891566     10\n",
      "8  0.098687    0.029052  0.345794  ...     0.554217      0.891566     10\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.103686    0.029820  0.168421  ...     0.807229      0.891566      2\n",
      "1  0.112739    0.029268  0.163265  ...     0.807229      0.891566      2\n",
      "2  0.142254    0.029362  0.175824  ...     0.807229      0.891566      2\n",
      "3  0.099180    0.028849  0.161616  ...     0.807229      0.891566      2\n",
      "4  0.099034    0.028846  0.161616  ...     0.807229      0.891566      2\n",
      "5  0.098590    0.030830  0.164948  ...     0.807229      0.891566      2\n",
      "6  0.101682    0.032235  0.279570  ...     0.807229      0.891566      2\n",
      "7  0.102390    0.030591  0.179775  ...     0.807229      0.891566      2\n",
      "8  0.104461    0.029396  0.172043  ...     0.807229      0.891566      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 5 train_idx range: 0, 82\n",
      "owner: 5 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.106840    0.029356  0.209524  ...      0.73494      0.891566     23\n",
      "1  0.102814    0.029199  0.224490  ...      0.73494      0.891566     23\n",
      "2  0.099594    0.028980  0.469565  ...      0.73494      0.891566     23\n",
      "3  0.098700    0.028346  0.209524  ...      0.73494      0.891566     23\n",
      "4  0.098034    0.028485  0.209524  ...      0.73494      0.891566     23\n",
      "5  0.098837    0.028602  0.211538  ...      0.73494      0.891566     23\n",
      "6  0.098677    0.028884  0.258824  ...      0.73494      0.891566     23\n",
      "7  0.098661    0.028331  0.209524  ...      0.73494      0.891566     23\n",
      "8  0.105666    0.030913  0.211538  ...      0.73494      0.891566     23\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.112574    0.032111  0.207921  ...     0.746988      0.891566      2\n",
      "1  0.106478    0.029853  0.203883  ...     0.746988      0.891566      2\n",
      "2  0.106467    0.031926  0.214286  ...     0.746988      0.891566      2\n",
      "3  0.111332    0.031219  0.201923  ...     0.746988      0.891566      2\n",
      "4  0.107771    0.029145  0.201923  ...     0.746988      0.891566      2\n",
      "5  0.102684    0.028725  0.201923  ...     0.746988      0.891566      2\n",
      "6  0.099064    0.028988  0.318681  ...     0.746988      0.891566      2\n",
      "7  0.100840    0.030109  0.214286  ...     0.746988      0.891566      2\n",
      "8  0.098049    0.029632  0.212121  ...     0.746988      0.891566      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.108690    0.030946  0.284314  ...     0.650602      0.891566     15\n",
      "1  0.101773    0.033091  0.315217  ...     0.650602      0.891566     15\n",
      "2  0.111573    0.029697  0.425532  ...     0.650602      0.891566     15\n",
      "3  0.110498    0.032049  0.258929  ...     0.650602      0.891566     15\n",
      "4  0.099623    0.029490  0.258929  ...     0.650602      0.891566     15\n",
      "5  0.102195    0.034602  0.287129  ...     0.650602      0.891566     15\n",
      "6  0.106951    0.032730  0.284314  ...     0.650602      0.891566     15\n",
      "7  0.110936    0.032128  0.278846  ...     0.650602      0.891566     15\n",
      "8  0.104060    0.029061  0.372093  ...     0.650602      0.891566     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 4 train_idx range: 0, 82\n",
      "owner: 4 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 5 valid_idx range: 498, 580\n",
      "imposter: 6 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.175737    0.028981  0.238532  ...     0.686747      0.891566     18\n",
      "1  0.099324    0.029244  0.240741  ...     0.686747      0.891566     18\n",
      "2  0.098445    0.028656  0.238532  ...     0.686747      0.891566     18\n",
      "3  0.098425    0.028702  0.238532  ...     0.686747      0.891566     18\n",
      "4  0.097835    0.028511  0.238532  ...     0.686747      0.891566     18\n",
      "5  0.098232    0.028330  0.238532  ...     0.686747      0.891566     18\n",
      "6  0.098398    0.028364  0.238532  ...     0.686747      0.891566     18\n",
      "7  0.098135    0.028424  0.238532  ...     0.686747      0.891566     18\n",
      "8  0.103977    0.029547  0.238532  ...     0.686747      0.891566     18\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.102947    0.036178  0.276596  ...     0.686747      0.891566      4\n",
      "1  0.104386    0.031913  0.273684  ...     0.686747      0.891566      4\n",
      "2  0.108362    0.034621  0.242991  ...     0.686747      0.891566      4\n",
      "3  0.101657    0.029729  0.240741  ...     0.686747      0.891566      4\n",
      "4  0.102516    0.032357  0.240741  ...     0.686747      0.891566      4\n",
      "5  0.100041    0.029771  0.298851  ...     0.686747      0.891566      4\n",
      "6  0.099506    0.029320  0.245283  ...     0.686747      0.891566      4\n",
      "7  0.101380    0.029227  0.516949  ...     0.686747      0.891566      4\n",
      "8  0.101557    0.029350  0.250000  ...     0.686747      0.891566      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 9 train_idx range: 0, 82\n",
      "owner: 9 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 8 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.186017    0.047285  0.372093  ...     0.650602      0.891566      4\n",
      "1  0.098286    0.029012  0.287129  ...     0.650602      0.891566      4\n",
      "2  0.098150    0.028917  0.325843  ...     0.650602      0.891566      4\n",
      "3  0.099049    0.028507  0.258929  ...     0.650602      0.891566      4\n",
      "4  0.098178    0.028981  0.258929  ...     0.650602      0.891566      4\n",
      "5  0.099779    0.029035  0.298969  ...     0.650602      0.891566      4\n",
      "6  0.097855    0.028995  0.273585  ...     0.650602      0.891566      4\n",
      "7  0.098627    0.028827  0.495327  ...     0.650602      0.891566      4\n",
      "8  0.097646    0.029030  0.268519  ...     0.650602      0.891566      4\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 6 train_idx range: 0, 82\n",
      "owner: 6 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 7 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.108824    0.031556  0.149425  ...     0.843373      0.891566      2\n",
      "1  0.104758    0.029441  0.136842  ...     0.843373      0.891566      2\n",
      "2  0.107120    0.030330  0.151163  ...     0.843373      0.891566      2\n",
      "3  0.112166    0.029226  0.135417  ...     0.843373      0.891566      2\n",
      "4  0.101176    0.029457  0.135417  ...     0.843373      0.891566      2\n",
      "5  0.099604    0.029332  0.136842  ...     0.843373      0.891566      2\n",
      "6  0.098361    0.028792  0.247312  ...     0.843373      0.891566      2\n",
      "7  0.098651    0.029109  0.142857  ...     0.843373      0.891566      2\n",
      "8  0.099920    0.029259  0.144444  ...     0.843373      0.891566      2\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 7 train_idx range: 0, 82\n",
      "owner: 7 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 8 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.102105    0.033811  0.341880  ...     0.518072      0.891566     15\n",
      "1  0.106570    0.030691  0.327869  ...     0.518072      0.891566     15\n",
      "2  0.100522    0.029810  0.363636  ...     0.518072      0.891566     15\n",
      "3  0.103658    0.029794  0.325203  ...     0.518072      0.891566     15\n",
      "4  0.099953    0.029176  0.325203  ...     0.518072      0.891566     15\n",
      "5  0.121327    0.032783  0.350877  ...     0.518072      0.891566     15\n",
      "6  0.104480    0.029752  0.336134  ...     0.518072      0.891566     15\n",
      "7  0.110937    0.030272  0.325203  ...     0.518072      0.891566     15\n",
      "8  0.106826    0.032037  0.353982  ...     0.518072      0.891566     15\n",
      "\n",
      "[9 rows x 11 columns]\n",
      "owner: 8 train_idx range: 0, 82\n",
      "owner: 8 valid_idx range: 83, 165\n",
      "imposter: 0 valid_idx range: 166, 248\n",
      "imposter: 1 valid_idx range: 249, 331\n",
      "imposter: 2 valid_idx range: 332, 414\n",
      "imposter: 3 valid_idx range: 415, 497\n",
      "imposter: 4 valid_idx range: 498, 580\n",
      "imposter: 5 valid_idx range: 581, 663\n",
      "imposter: 6 valid_idx range: 664, 746\n",
      "imposter: 7 valid_idx range: 747, 829\n",
      "imposter: 9 valid_idx range: 830, 912\n",
      "   fit_time  score_time  test_eer  ...  test_recall  train_recall  owner\n",
      "0  0.178451    0.052723  0.265487  ...     0.638554      0.891566     10\n",
      "1  0.157993    0.029802  0.265487  ...     0.638554      0.891566     10\n",
      "2  0.100297    0.029724  0.357143  ...     0.638554      0.891566     10\n",
      "3  0.100005    0.030094  0.267857  ...     0.638554      0.891566     10\n",
      "4  0.103230    0.029271  0.265487  ...     0.638554      0.891566     10\n",
      "5  0.099713    0.029916  0.275229  ...     0.638554      0.891566     10\n",
      "6  0.099748    0.029640  0.267857  ...     0.638554      0.891566     10\n",
      "7  0.128619    0.032022  0.275229  ...     0.638554      0.891566     10\n",
      "8  0.108771    0.031579  0.267857  ...     0.638554      0.891566     10\n",
      "\n",
      "[9 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "utils_ppp(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T19:00:22.730993Z",
     "iopub.status.busy": "2022-04-30T19:00:22.730377Z",
     "iopub.status.idle": "2022-04-30T19:00:22.737903Z",
     "shell.execute_reply": "2022-04-30T19:00:22.736777Z",
     "shell.execute_reply.started": "2022-04-30T19:00:22.730946Z"
    }
   },
   "outputs": [],
   "source": [
    "IF_test_train_WACA_features_dic[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>VALID-ROBUST-OCSVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frequency</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_subjects</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_test_subjects</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_ids</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_sample_points_per_exp</th>\n",
       "      <td>21000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp_begin_cutoff_idx</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp_end_cutoff_idx</th>\n",
       "      <td>-500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seconds_per_subject_train</th>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seconds_per_subject_test</th>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>window_size</th>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocsvm_step_width</th>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler</th>\n",
       "      <td>RobustScaler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler_scope</th>\n",
       "      <td>subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaler_global</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocsvm_kernel</th>\n",
       "      <td>rbf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocsvm_nu</th>\n",
       "      <td>0.036822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocsvm_gamma</th>\n",
       "      <td>0.001326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_cols</th>\n",
       "      <td>[EMA_x_a, EMA_y_a, EMA_z_a, EMA_x_g, EMA_y_g, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exclude_subjects</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       Value\n",
       "name                                                      VALID-ROBUST-OCSVM\n",
       "frequency                                                                100\n",
       "max_subjects                                                              29\n",
       "max_test_subjects                                                         10\n",
       "user_ids                   [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 2...\n",
       "num_sample_points_per_exp                                              21000\n",
       "exp_begin_cutoff_idx                                                     500\n",
       "exp_end_cutoff_idx                                                      -500\n",
       "seconds_per_subject_train                                                210\n",
       "seconds_per_subject_test                                                 210\n",
       "window_size                                                              750\n",
       "ocsvm_step_width                                                         375\n",
       "scaler                                                          RobustScaler\n",
       "scaler_scope                                                         subject\n",
       "scaler_global                                                          False\n",
       "ocsvm_kernel                                                             rbf\n",
       "ocsvm_nu                                                            0.036822\n",
       "ocsvm_gamma                                                         0.001326\n",
       "feature_cols               [EMA_x_a, EMA_y_a, EMA_z_a, EMA_x_g, EMA_y_g, ...\n",
       "exclude_subjects                                                          []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils_ppp(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T18:09:42.461456Z",
     "iopub.status.busy": "2022-04-30T18:09:42.461166Z",
     "iopub.status.idle": "2022-04-30T18:09:42.487129Z",
     "shell.execute_reply": "2022-04-30T18:09:42.484901Z",
     "shell.execute_reply.started": "2022-04-30T18:09:42.461427Z"
    }
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "total = None\n",
    "for k in tqdm(test_pca_results):\n",
    "    total = list(test_pca_results[k][\"total\"].values())\n",
    "    data.append([k] + total)\n",
    "    \n",
    "# Create the pandas DataFrame\n",
    "columns = [\"pca_n_components\"] + list(test_pca_results[k][\"total\"].keys())\n",
    "df = pd.DataFrame(data, columns = columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-01T17:00:38.050161Z",
     "iopub.status.idle": "2022-04-01T17:00:38.050597Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(1,1, figsize=(8,8))\n",
    "\n",
    "ax.set_title('different pca_n_component values on test_set')\n",
    "g = sns.lineplot(x=df.pca_n_components, y=df.FAR, label = 'FAR', ax = ax)\n",
    "g = sns.lineplot(x=df.pca_n_components, y=df.FRR, label = 'FRR', ax = ax)\n",
    "g = sns.lineplot(x=df.pca_n_components, y=df.train_err_rate, label = 'train_err_rate', ax = ax)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# g.set_xticks(y_ticks)\n",
    "ax.set_xlabel('n_components')\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-03-29T07:31:16.23541Z",
     "iopub.status.idle": "2022-03-29T07:31:16.235848Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-03-29T07:31:16.237033Z",
     "iopub.status.idle": "2022-03-29T07:31:16.237464Z"
    }
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "total = None\n",
    "for k in tqdm(train_pca_results):\n",
    "    total = list(train_pca_results[k][\"total\"].values())\n",
    "    data.append([k] + total)\n",
    "    \n",
    "# Create the pandas DataFrame\n",
    "columns = [\"pca_n_components\"] + list(train_pca_results[k][\"total\"].keys())\n",
    "df = pd.DataFrame(data, columns = columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-03-29T07:31:16.238345Z",
     "iopub.status.idle": "2022-03-29T07:31:16.238785Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(1,1, figsize=(8,8))\n",
    "\n",
    "ax.set_title('different pca_n_component values on train_set')\n",
    "g = sns.lineplot(x=df.pca_n_components, y=df.FAR, label = 'FAR', ax = ax)\n",
    "g = sns.lineplot(x=df.pca_n_components, y=df.FRR, label = 'FRR', ax = ax)\n",
    "g = sns.lineplot(x=df.pca_n_components, y=df.train_err_rate, label = 'train_err_rate', ax = ax)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# g.set_xticks(y_ticks)\n",
    "ax.set_xlabel('n_components')\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
