{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Dataset [link](https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fdrive.google.com%2Fdrive%2Ffolders%2F1-6sd6E3cL8uWdNeO4xiXxLbTSYXSTiVr%3Fusp%3Dsharing&data=04%7C01%7Carash.gholami%40queensu.ca%7C92b180b5b67d475fb70e08d87b7f4305%7Cd61ecb3b38b142d582c4efb2838b925c%7C1%7C0%7C637395133804367020%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=1ieR3E8llMblC0QMBtx1%2FpHS71RQuCxzdNQLmH0HJVU%3D&reserved=0)\n\nExplanation: There are **49** users' data here. Naming is in the format of Username_ExperimentNumber. Please also note that it is unprocessed raw data, which includes a lot of noise etc. \n\nExperiment-1: **20** participants are involved in this task and the participants were asked to type **a story** from **a set** of short and simple stories from the American Literature https://americanliterature.com/100-great-short-stories for **four minutes**. The story was chosen randomly by the participants.\n\nExperiment-2: 20 participants are involved in this task and for this data set, all the participants were asked to type the **same text** https://en.wikipedia.org/wiki/The_Adventures_of_Tom_Sawyer for **four minutes**. For each participant, almost the same amount of data is collected as Experiment-1. This dataset is important to be able to measure the quality of the features. The collection of this dataset is quite realistic since in a real case the users could be also asked to type a given text during their CA sessions. \n\nExperiment-3: The participants are asked to type **10 different pangram sentences**. Pangrams sentences contain** all 26 letters** of the alphabet. This data set is used to test how the **letter distribution** of text affects the result.\n\nExperiment-4: **34** participants are involved in this task and the participants were instructed to imitate someone else's typing pattern by watching the prerecorded video of the other person. For these experiments, one of the participants was recorded on video while typing a short and simple sentence for **15 seconds** from a perspective that the hand motions, smartwatch, keyboard, and the screen could be seen. Although it was not required, the perspective allowed to infer what the victim was typing by watching. This dataset was primarily used to analyze the **attacking scenarios**. It is left to the participant to type the exact same text or imitate the hand movement of the victim.\n\n-----------------------\n\nI realized that you have mostly 4s and 10s as values on column A, but I also found 2s. What does column A represent? \nA represents the sensor type:\n \n**10 for the accelerometer\n4 for the gyroscope**\n2 for the magnetic field sensor\n\nYou can find more info about sensors and constant values that are used to define the sensor type in these links:\n- https://developer.android.com/guide/topics/sensors/sensors_overview\n- https://developer.android.com/reference/android/hardware/Sensor#TYPE_LINEAR_ACCELERATION\n\nNormally, I only collected and used accelerometer and gyroscope sensor data. Magnetometer data should not be in the dataset. If there is, that means it is an **outlier**.\n \nIs column B the time stamp, is it recorded at 100HZ?\n\nIn an android app, you can set the sampling rate \"SENSOR_DELAY_NORMAL, SENSOR_DELAY_UI, SENSOR_DELAY_GAME, or SENSOR_DELAY_FASTEST \" \n\nAs far as I remember I used **\"SENSOR_DELAY_FASTEST\" for all sensors**, but in practice, I had a lot of issues regarding this. They **don't give a constant sampling rate**. Every time I used they were giving different sampling rates. They were giving different sampling rates for two sessions of the same user, for different users, or for different sensors. Therefore, I would recommend using the one in the practice, which you can calculate using the timestamp and the sample count. \n\nOn the paper you said you have recorded both accelerometer and gyro data; but you seem to only have one of them at a time on columns C, D, and F. I am guessing 10 on column A represents accelerometer and 4 gyro, but I could be wrong. \nAlso, some rows have more columns, and start with 11 on column A. What do those represent?\nPlease see the explanation above. \n \n\nIn short, I would really appreciate it if you could let me know information I need to know for working with the data.\n\nAlso, as these sensors were giving me a lot of headaches because they are really not stable, I needed to perform a lot of preprocessing. I **cleaned the outliers** before further processing. I also applied some other filtering techniques such as **moving average** to obtain more smooth data. Moreover, I needed to **clean the part the users are not performing typing operation from the beginning and end**, which you can see if you plot the values, i.e., the **user stops meaning no value and then big deviations, where the user takes off the smartwatch**.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nCORES = -1\nSEED = 567\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-30T07:53:37.236372Z","iopub.execute_input":"2022-03-30T07:53:37.236923Z","iopub.status.idle":"2022-03-30T07:53:37.37419Z","shell.execute_reply.started":"2022-03-30T07:53:37.23689Z","shell.execute_reply":"2022-03-30T07:53:37.373223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade pip\n# !pip install python-docx\n# !pip install antropy\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport dataclasses\nimport math as math\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.model_selection import cross_validate, RandomizedSearchCV\nimport statsmodels.stats.api as sms\nfrom tqdm.auto import tqdm\nfrom dataclasses import asdict\nfrom sklearn import svm\nfrom tqdm import tqdm\nimport warnings\nimport random\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import roc_curve, accuracy_score, make_scorer, auc\nfrom scipy.optimize import brentq\nfrom scipy.interpolate import interp1d\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import VarianceThreshold # Feature selector\nfrom sklearn.model_selection import KFold\n\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import kurtosis\nfrom scipy.stats import skew\nfrom scipy.stats import iqr\nfrom scipy.stats import median_absolute_deviation\nfrom scipy.stats import mode\nfrom scipy.signal import find_peaks\nfrom scipy.signal import peak_widths\n# from scipy.special import entr\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, MaxAbsScaler, RobustScaler, PowerTransformer\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import auc\n# import antropy as ant\nimport time\n# import docx\n\n# Global utitlity functions are in separate notebook\n# import utility_functions as uf\n%run ../usr/lib/utility_functions/utility_functions.ipynb\n\nnp.random.seed(SEED)\nprint(f\"Seed was set to: {SEED}\")\n\nprint(\"Setup Complete\")","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:53:37.377517Z","iopub.execute_input":"2022-03-30T07:53:37.377864Z","iopub.status.idle":"2022-03-30T07:53:52.695539Z","shell.execute_reply.started":"2022-03-30T07:53:37.377829Z","shell.execute_reply":"2022-03-30T07:53:52.694393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclasses.dataclass\nclass ExperimentParameters:\n    \"\"\"Contains all relevant parameters to run an experiment.\"\"\"\n\n    name: str  # Name of Parameter set. Used as identifier for charts etc.\n    frequency: int\n    max_subjects: int\n    max_test_subjects: int\n    seconds_per_subject_train: float\n    seconds_per_subject_test: float\n    task_types: list  # Limit scenarios to [1, 3, 5] for sitting or [2, 4, 6] for walking, or don't limit (None)\n    window_size: int  # After resampling\n    step_width: int  # After resampling\n    scaler: str  # {\"std\", \"robust\", \"minmax\"}\n    scaler_scope: str  # {\"subject\", \"session\"}\n    scaler_global: bool  # fit transform scale on all data (True) or fit on training only (False)\n    ocsvm_nu: float  # Best value found in random search, used for final model\n    ocsvm_gamma: float  # Best value found in random search, used for final model\n    feature_cols: list  # Columns used as features\n    exclude_subjects: list  # Don't load data from those users\n        \n    # Calculated values\n    def __post_init__(self):\n        # HDF key of table:\n        self.table_name = f\"sensors_{self.frequency}hz\"\n\n        # Number of samples per _session_ used for training:\n        self.samples_per_subject_train = math.ceil(\n            (self.seconds_per_subject_train * 100)\n            / (100 / self.frequency)\n            / self.window_size\n        )\n\n        # Number of samples per _session_ used for testing:\n        self.samples_per_subject_test = math.ceil(\n            (self.seconds_per_subject_test * 100)\n            / (100 / self.frequency)\n            / self.window_size\n        )\n\n        \n\n# INSTANCES\n# ===========================================================\n\n# NAIVE_APPROACH\n# -----------------------------------------------------------\nNAIVE_MINMAX_OCSVM = ExperimentParameters(\n    name=\"NAIVE-MINMAX_OCSVM\",\n    frequency=100,\n    max_subjects=90,\n    max_test_subjects=30,\n    seconds_per_subject_train=67.5,\n    seconds_per_subject_test=67.5,    \n    task_types=None,\n    window_size=50,\n    step_width=50,\n    scaler=\"minmax\",\n    scaler_scope=\"subject\",\n    scaler_global=True,\n    ocsvm_nu=0.086,\n    ocsvm_gamma=0.091,\n    feature_cols=[\n        \"EMA_x_a\",\n        \"EMA_y_a\",\n        \"EMA_z_a\",\n        \"EMA_x_g\",\n        \"EMA_y_g\",\n        \"EMA_z_g\",\n    ],\n    exclude_subjects=[],\n)\n\n# VALID_APPROACH\n# -----------------------------------------------------------\nVALID_MINMAX_OCSVM = dataclasses.replace(\n    NAIVE_MINMAX_OCSVM,\n    name=\"VALID-MINMAX-OCSVM\",\n    scaler_global=False,\n    ocsvm_nu=0.165,\n    ocsvm_gamma=0.039,\n)\n\n# NAIVE_ROBUST_APPROACH\n# -----------------------------------------------------------\nNAIVE_ROBUST_OCSVM = dataclasses.replace(\n    NAIVE_MINMAX_OCSVM,\n    name=\"NAIVE-ROBUST-OCSVM\",\n    scaler=\"robust\",\n    scaler_global=True,\n    ocsvm_nu=0.153,\n    ocsvm_gamma=0.091,  # below median, selected by chart\n)\n\n# ROBUST_APPROACH (VALID)\n# -----------------------------------------------------------\nVALID_ROBUST_OCSVM = dataclasses.replace(\n    NAIVE_MINMAX_OCSVM,\n    name=\"VALID-ROBUST-OCSVM\",\n    scaler=\"robust\",\n    scaler_global=False,\n    ocsvm_nu=0.098,\n    ocsvm_gamma=0.003,\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:53:52.706251Z","iopub.execute_input":"2022-03-30T07:53:52.706623Z","iopub.status.idle":"2022-03-30T07:53:52.729218Z","shell.execute_reply.started":"2022-03-30T07:53:52.706589Z","shell.execute_reply":"2022-03-30T07:53:52.727872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# utils_plot_randomsearch_results(df_results, n_top=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:53:52.73405Z","iopub.execute_input":"2022-03-30T07:53:52.734852Z","iopub.status.idle":"2022-03-30T07:53:52.750037Z","shell.execute_reply.started":"2022-03-30T07:53:52.734791Z","shell.execute_reply":"2022-03-30T07:53:52.7486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"P = VALID_ROBUST_OCSVM","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:53:52.752872Z","iopub.execute_input":"2022-03-30T07:53:52.753408Z","iopub.status.idle":"2022-03-30T07:53:52.76644Z","shell.execute_reply.started":"2022-03-30T07:53:52.753362Z","shell.execute_reply":"2022-03-30T07:53:52.765483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"utils_ppp(P)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:53:52.767721Z","iopub.execute_input":"2022-03-30T07:53:52.768372Z","iopub.status.idle":"2022-03-30T07:53:52.810876Z","shell.execute_reply.started":"2022-03-30T07:53:52.768331Z","shell.execute_reply":"2022-03-30T07:53:52.809958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"utils_eer_scorer","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:53:52.812491Z","iopub.execute_input":"2022-03-30T07:53:52.812979Z","iopub.status.idle":"2022-03-30T07:53:52.820712Z","shell.execute_reply.started":"2022-03-30T07:53:52.812927Z","shell.execute_reply":"2022-03-30T07:53:52.819443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split Dataset for Valid/Test  \nIn two splits: one used during hyperparameter optimization, and one used during testing.\n\nThe split is done along the subjects: All sessions of a single subject will either be in the validation split or in the testing split, never in both.\n\nThey did a 30 60 split.","metadata":{}},{"cell_type":"markdown","source":"# Reshaping Raw Features.\nWe have our own function of windows for this. Do this for both training and testing.\n\n# Extracting time and frequency based features.\nAgain, we have a function for this. Do this for both training and testing.","metadata":{}},{"cell_type":"markdown","source":"# Hyperparameter Optimization \n\nI do not find any reaqsonable explaination how to use a cross-validation as we are talking about anomaly detection.\n\nI am using the experiment 1 data as train, and experiment 2 data as validation.","metadata":{}},{"cell_type":"markdown","source":"# Using SVM in a real-world Scenario with multiple genuine users and intruders\nSource: https://datascience.stackexchange.com/questions/23623/what-is-the-best-way-to-classify-data-not-belonging-to-set-of-classes\n\nStage 1: \n    Use one-class SVM to assign those images that do not belong to the set of predefined classes as the 9-th class.\n\nStage 2:\n    For those images that passes through your filter, let the multi-class SVM assign them to one of the 8 classes.","metadata":{}},{"cell_type":"markdown","source":"Loading data:","metadata":{}},{"cell_type":"code","source":"user_ids = [1, 2, 3, 4, 5, 6, 7, 8, 19, 21, 22, 26, 27, 28, 29, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49]\nbegin_idx=500\nend_idx=-500\nmin_len = 21000\n\ndf_exps_dict = load_data_frames(user_ids, begin_idx, end_idx, min_len)\ndfList_exp1, dfList_exp2 = df_exps_dict['dfList_exp1'], df_exps_dict['dfList_exp2']\n# print(\"Loading exp1 data:\")\n# dfList_exp1 = []\n# for i in r:\n#     dic = getDataStats1(i, begin_idx=begin_idx, end_idx=end_idx)\n    \n#     if(dic['accel']<min_len):\n#         raise Exception(\"The Stream is shorter than {}\".format(min_len))\n    \n#     dfList_exp1 = dfList_exp1 + [dic['df'].reset_index(drop=True)]\n\n    \n# print(\"Loading exp2 data:\")\n# dfList_exp2 = []\n# for i in r:\n#     dic = getDataStats2(i, begin_idx=begin_idx, end_idx=end_idx)\n    \n#     if(dic['accel']<min_len):\n#         raise Exception(\"The Stream is shorter than {}\".format(min_len))\n    \n#     dfList_exp2 = dfList_exp2 + [dic['df'].reset_index(drop=True)]\n# #     dfList = dfList + [dic['df']]","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:53:52.822557Z","iopub.execute_input":"2022-03-30T07:53:52.823207Z","iopub.status.idle":"2022-03-30T07:54:11.51279Z","shell.execute_reply.started":"2022-03-30T07:53:52.823158Z","shell.execute_reply":"2022-03-30T07:54:11.51143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.choice(range(5), 5, replace = False)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:54:11.514303Z","iopub.execute_input":"2022-03-30T07:54:11.514735Z","iopub.status.idle":"2022-03-30T07:54:11.521644Z","shell.execute_reply.started":"2022-03-30T07:54:11.514699Z","shell.execute_reply":"2022-03-30T07:54:11.520692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"randomized_data_idx = list(range(len(user_ids)))\nrandom.shuffle(randomized_data_idx)\nsplit_idx = 2 * (len(randomized_data_idx)//3) + 1\ntrain_set = randomized_data_idx[: split_idx]\ntest_set = randomized_data_idx[split_idx: ]\nprint(f\"train_set: {train_set}\\ntest_set: {test_set}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:55:15.664551Z","iopub.execute_input":"2022-03-30T07:55:15.665213Z","iopub.status.idle":"2022-03-30T07:55:15.672524Z","shell.execute_reply.started":"2022-03-30T07:55:15.665177Z","shell.execute_reply":"2022-03-30T07:55:15.671467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preparing train data\n# train_set = r\ndfList_exp1_train, dfList_exp2_train = [dfList_exp1[i] for i in train_set], [dfList_exp2[i] for i in train_set]\nprint(f\"len(dfList_exp1_train): {len(dfList_exp1_train)}\")\nprint(f\"len(dfList_exp2_train): {len(dfList_exp2_train)}\")\nXExpTrainDict = MakeXExpDic(dfList_exp1_train, dfList_exp2_train, window_size = 250, step = 250, numSamplePoints= 21000)\nX_exp1_train_dic, X_exp2_train_dic = XExpTrainDict[\"X_exp1_dic\"], XExpTrainDict[\"X_exp2_dic\"]\n\n# preparing test data\ndfList_exp1_test, dfList_exp2_test = [dfList_exp1[i] for i in test_set], [dfList_exp2[i] for i in test_set]\nprint(f\"len(dfList_exp1_test): {len(dfList_exp1_test)}\")\nprint(f\"len(dfList_exp2_test): {len(dfList_exp2_test)}\")\nXExpTestDict = MakeXExpDic(dfList_exp1_test, dfList_exp2_test, window_size = 250, step = 250, numSamplePoints= 21000)\nX_exp1_test_dic, X_exp2_test_dic = XExpTestDict[\"X_exp1_dic\"], XExpTestDict[\"X_exp2_dic\"]","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:55:17.597434Z","iopub.execute_input":"2022-03-30T07:55:17.597826Z","iopub.status.idle":"2022-03-30T07:56:15.015627Z","shell.execute_reply.started":"2022-03-30T07:55:17.597795Z","shell.execute_reply":"2022-03-30T07:56:15.014545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"train_set: {train_set}\")\nprint(f\"X_exp1_train_dic: {X_exp1_train_dic.keys()}\")\nprint(f\"X_exp2_train_dic: {X_exp2_train_dic.keys()}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:56:15.018064Z","iopub.execute_input":"2022-03-30T07:56:15.018436Z","iopub.status.idle":"2022-03-30T07:56:15.023986Z","shell.execute_reply.started":"2022-03-30T07:56:15.018403Z","shell.execute_reply":"2022-03-30T07:56:15.02302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"test_set: {test_set}\")\nprint(f\"X_exp1_test_dic: {X_exp1_test_dic.keys()}\")\nprint(f\"X_exp2_test_dic: {X_exp2_test_dic.keys()}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:56:15.025651Z","iopub.execute_input":"2022-03-30T07:56:15.025976Z","iopub.status.idle":"2022-03-30T07:56:15.039043Z","shell.execute_reply.started":"2022-03-30T07:56:15.025945Z","shell.execute_reply":"2022-03-30T07:56:15.038089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_dist = {\n#     \"model__gamma\": np.logspace(-3, 3), \n    \"model__contamination\": np.linspace(0.0001, 0.3),\n#     'scaler': [StandardScaler(), MinMaxScaler(),\n#         Normalizer(), MaxAbsScaler()],\n#     \"selector__threshold\": np.linspace(0, 2, num=1000),\n\n}\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndf_results = None  # Will be filled with randomsearch scores\n\n\n# Don't need to loop over to find the best PCA.\nfor run in tqdm(range(1)):\n#     df_results[run] = {}\n    \n    for owner_idx in tqdm(\n        range(len(train_set)),\n        desc=\"Owner\",\n    ):\n        run_seed = SEED + run\n        train_dic, valid_test_dic = X_exp2_train_dic, X_exp1_train_dic\n        X_vals_owner_idx = utils_create_cv_splits(owner_idx, train_dic, valid_test_dic, seed=run_seed)\n#         X_train = X_vals_owner_idx['X_train']\n#          = X.reshape(X.shape[-3], -1)  # flatten windows\n        X_test_regular = X_vals_owner_idx['X_test_regular'].copy()\n        X_test_anomalous = X_vals_owner_idx['X_test_anomalous'].copy()\n        \n#         print(X_vals_owner_idx['cv_splits'])\n        cv_splits = X_vals_owner_idx['cv_splits']\n        \n        pca = PCA(n_components = run)\n#         X_train = pca.fit_transform(X_train)\n#         X_test_regular = pca.transform(X_test_regular)\n#         X_test_anomalous = pca.transform(X_test_anomalous)\n        \n#         pca_fs.add_user_pca(owner_idx, pca)\n        \n        clf = IsolationForest(random_state=run_seed)\n        \n        pipeline = Pipeline([\n#                             ('scaler', StandardScaler()), \n                            ('scaler', Normalizer()),\n#                              ('pca', pca), \n#                              ('selector', VarianceThreshold()), \n                             ('model', clf)\n                            ])\n        \n        # random serach usees 5-folld cv by default. Probably need to remove it.\n        random_search = RandomizedSearchCV(\n            pipeline,\n            param_distributions=param_dist,\n            cv=cv_splits,\n            n_iter=80,\n            n_jobs=CORES,\n            refit=False,\n            scoring={\"eer\": utils_eer_scorer, \"accuracy\": \"accuracy\"},\n            verbose=0,\n            return_train_score=False,\n            iid=False,\n            error_score=\"raise\",\n            random_state=run_seed\n        )\n#         print(X_train.shape)\n#         print(X_test_anomalous.shape)\n\n        \n        random_search.fit(X_vals_owner_idx['X_train'], X_vals_owner_idx['y_train'])\n        impostors = [idx for idx in range(len(train_set)) if idx != owner_idx]\n\n        df_report = utils_cv_report(random_search, owner_idx, impostors)\n        df_report[\"run\"] = run\n        df_results = pd.concat([df_results, df_report], sort=False)\n        \n\n\ntrain_pca_results = df_results","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:59:51.658158Z","iopub.execute_input":"2022-03-30T07:59:51.658705Z","iopub.status.idle":"2022-03-30T08:17:20.719566Z","shell.execute_reply.started":"2022-03-30T07:59:51.658669Z","shell.execute_reply":"2022-03-30T08:17:20.71669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_results[df_results[\"rank_test_eer\"] == 1])","metadata":{"execution":{"iopub.status.busy":"2022-03-30T08:17:27.380142Z","iopub.execute_input":"2022-03-30T08:17:27.38056Z","iopub.status.idle":"2022-03-30T08:17:27.391883Z","shell.execute_reply.started":"2022-03-30T08:17:27.380522Z","shell.execute_reply":"2022-03-30T08:17:27.390488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_results[df_results[\"owner\"] == 14])","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:54:11.642593Z","iopub.status.idle":"2022-03-30T07:54:11.643037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(\n    df_results[df_results[\"rank_test_eer\"] == 1]\n    .sort_values(\"mean_test_eer\")\n    .head(100)\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T08:17:30.520582Z","iopub.execute_input":"2022-03-30T08:17:30.521154Z","iopub.status.idle":"2022-03-30T08:17:30.560608Z","shell.execute_reply.started":"2022-03-30T08:17:30.5211Z","shell.execute_reply":"2022-03-30T08:17:30.559537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(\n    df_results[df_results[\"rank_test_eer\"] == 1][\n        [\n            \"mean_fit_time\",\n            \"param_model__nu\",\n            \"param_model__gamma\",\n            \"mean_test_accuracy\",\n            \"std_test_accuracy\",\n            \"mean_test_eer\",\n            \"std_test_eer\",\n        ]\n    ].describe()\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:54:11.645341Z","iopub.status.idle":"2022-03-30T07:54:11.645771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = utils_plot_randomsearch_results(df_results, n_top=1)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:54:11.646594Z","iopub.status.idle":"2022-03-30T07:54:11.647047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# P.ocsvm_nu, P.ocsvm_gamma = 0.098, 0.002\nP.ocsvm_nu, P.ocsvm_gamma = 0.031, 0.080","metadata":{"execution":{"iopub.status.busy":"2022-03-30T04:44:46.142703Z","iopub.execute_input":"2022-03-30T04:44:46.143074Z","iopub.status.idle":"2022-03-30T04:44:46.147835Z","shell.execute_reply.started":"2022-03-30T04:44:46.143043Z","shell.execute_reply":"2022-03-30T04:44:46.146793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"utils_ppp(P)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T04:44:47.333014Z","iopub.execute_input":"2022-03-30T04:44:47.333377Z","iopub.status.idle":"2022-03-30T04:44:47.348907Z","shell.execute_reply.started":"2022-03-30T04:44:47.333347Z","shell.execute_reply":"2022-03-30T04:44:47.347992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# utils_plot_acc_eer_dist(df_plot, \"Test Accuracy\")","metadata":{"execution":{"iopub.status.busy":"2022-03-29T07:31:16.187319Z","iopub.status.idle":"2022-03-29T07:31:16.187721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# utils_plot_acc_eer_dist(df_results, \"Test EER\")","metadata":{"execution":{"iopub.status.busy":"2022-03-29T07:31:16.188496Z","iopub.status.idle":"2022-03-29T07:31:16.18888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"contamination = 0.0245816\ntest_df_results = None  # Will be filled with randomsearch scores\nfor run in tqdm(range(5)):\n\n\n    for owner_idx in tqdm(\n        range(len(test_set)),\n        desc=\"Owner\",\n    ):\n\n        \n        run_seed = SEED + run\n        train_dic, valid_test_dic = X_exp2_test_dic, X_exp1_test_dic\n        X_vals_owner_idx = utils_create_cv_splits(owner_idx, train_dic, valid_test_dic, seed=run_seed)\n#         X_train = X_vals_owner_idx['X_train']\n#          = X.reshape(X.shape[-3], -1)  # flatten windows\n        X_test_regular = X_vals_owner_idx['X_test_regular']\n        X_test_anomalous = X_vals_owner_idx['X_test_anomalous']\n#         print(X_vals_owner_idx)\n#         break\n#         np.random.shuffle(X_train)\n#         np.random.shuffle(X_test_regular)\n#         np.random.shuffle(X_test_anomalous)\n#         print(X_vals_owner_idx['cv_splits'])\n        train_test_cv_splits = X_vals_owner_idx['cv_splits']\n        \n        \n#         pca = PCA(n_components = run)\n#         X_train = pca.fit_transform(X_train)\n#         X_test_regular = pca.transform(X_test_regular)\n#         X_test_anomalous = pca.transform(X_test_anomalous)\n        \n        clf = IsolationForest(contamination=contamination, random_state=run_seed)\n        pipeline = Pipeline([\n#             Normalizer, MaxAbsScaler,\n#                             ('scaler', StandardScaler()), \n#                              ('scaler', RobustScaler()),\n#                              ('scaler', MaxAbsScaler()),\n                            ('scaler', Normalizer()),#best result\n#                             ('scaler', PowerTransformer()),\n#                              ('pca', pca), \n#                              ('selector', VarianceThreshold()), \n                             ('model', clf)\n                            ])\n        \n        scores = cross_validate(\n            pipeline,\n            X_vals_owner_idx['X_train'],\n            X_vals_owner_idx['y_train'],\n            cv=train_test_cv_splits,\n            scoring={\n                \"eer\": utils_eer_scorer,\n                \"accuracy\": \"accuracy\",\n                \"precision\": \"precision\",\n                \"recall\": \"recall\",\n            },\n            n_jobs=CORES,\n            verbose=0,\n            return_train_score=True,\n        )\n        \n        df_score = pd.DataFrame(scores)\n        df_score[\"owner\"] = owner_idx\n        df_score[\"train_eer\"] = df_score[\"train_eer\"].abs()  # Revert scorer's signflip\n        df_score[\"test_eer\"] = df_score[\"test_eer\"].abs()\n        test_df_results = pd.concat([test_df_results, df_score], axis=0)\n        \n\ntest_df_results.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T08:19:35.95836Z","iopub.execute_input":"2022-03-30T08:19:35.958805Z","iopub.status.idle":"2022-03-30T08:21:05.817266Z","shell.execute_reply.started":"2022-03-30T08:19:35.958772Z","shell.execute_reply":"2022-03-30T08:21:05.815815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_plot = test_df_results.rename(\n    columns={\"test_accuracy\": \"Test Accuracy\", \"test_eer\": \"Test EER\", \"owner\": \"Owner\"}\n).astype({\"Owner\": str})","metadata":{"execution":{"iopub.status.busy":"2022-03-30T08:21:14.111007Z","iopub.execute_input":"2022-03-30T08:21:14.111774Z","iopub.status.idle":"2022-03-30T08:21:14.12175Z","shell.execute_reply.started":"2022-03-30T08:21:14.111732Z","shell.execute_reply":"2022-03-30T08:21:14.120891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAGENTA = (202/255, 18/255, 125/255)\n# Define a style I use a lot for boxplots:\nutils_boxplot_style = dict(\n    color=\"tab:blue\",\n    linewidth=0.5,\n    saturation=1,\n    width=0.7,\n    flierprops=dict(\n        marker=\"o\", markersize=2, markerfacecolor=\"none\", markeredgewidth=0.5\n    ),\n)\n\n# Define a style I use a lot for lineplots:\nutils_lineplot_style = dict(\n    color=\"tab:blue\", linewidth=0.5, marker=\"o\", markersize=3, markeredgewidth=0.5\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T08:21:15.751693Z","iopub.execute_input":"2022-03-30T08:21:15.752068Z","iopub.status.idle":"2022-03-30T08:21:15.759967Z","shell.execute_reply.started":"2022-03-30T08:21:15.752038Z","shell.execute_reply":"2022-03-30T08:21:15.758859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_df_plot[test_df_plot[\"Owner\"]==\"9\"]","metadata":{"execution":{"iopub.status.busy":"2022-03-30T08:21:16.527723Z","iopub.execute_input":"2022-03-30T08:21:16.528253Z","iopub.status.idle":"2022-03-30T08:21:16.531692Z","shell.execute_reply.started":"2022-03-30T08:21:16.52822Z","shell.execute_reply":"2022-03-30T08:21:16.530971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = utils_plot_acc_eer_dist(test_df_plot, \"Test Accuracy\")","metadata":{"execution":{"iopub.status.busy":"2022-03-30T08:21:17.318868Z","iopub.execute_input":"2022-03-30T08:21:17.319406Z","iopub.status.idle":"2022-03-30T08:21:17.711513Z","shell.execute_reply.started":"2022-03-30T08:21:17.319372Z","shell.execute_reply":"2022-03-30T08:21:17.710453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = utils_plot_acc_eer_dist(test_df_plot, \"Test EER\")\n","metadata":{"execution":{"iopub.status.busy":"2022-03-30T08:21:17.71363Z","iopub.execute_input":"2022-03-30T08:21:17.713946Z","iopub.status.idle":"2022-03-30T08:21:18.027677Z","shell.execute_reply.started":"2022-03-30T08:21:17.713916Z","shell.execute_reply":"2022-03-30T08:21:18.026833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = []\ntotal = None\nfor k in tqdm(test_pca_results):\n    total = list(test_pca_results[k][\"total\"].values())\n    data.append([k] + total)\n    \n# Create the pandas DataFrame\ncolumns = [\"pca_n_components\"] + list(test_pca_results[k][\"total\"].keys())\ndf = pd.DataFrame(data, columns = columns)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-03-29T07:31:16.232613Z","iopub.status.idle":"2022-03-29T07:31:16.233016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax =plt.subplots(1,1, figsize=(8,8))\n\nax.set_title('different pca_n_component values on test_set')\ng = sns.lineplot(x=df.pca_n_components, y=df.FAR, label = 'FAR', ax = ax)\ng = sns.lineplot(x=df.pca_n_components, y=df.FRR, label = 'FRR', ax = ax)\ng = sns.lineplot(x=df.pca_n_components, y=df.train_err_rate, label = 'train_err_rate', ax = ax)\n\n\n\n\n# g.set_xticks(y_ticks)\nax.set_xlabel('n_components')\n\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T07:31:16.234008Z","iopub.status.idle":"2022-03-29T07:31:16.234436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_results[0]","metadata":{"execution":{"iopub.status.busy":"2022-03-29T07:31:16.23541Z","iopub.status.idle":"2022-03-29T07:31:16.235848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = []\ntotal = None\nfor k in tqdm(train_pca_results):\n    total = list(train_pca_results[k][\"total\"].values())\n    data.append([k] + total)\n    \n# Create the pandas DataFrame\ncolumns = [\"pca_n_components\"] + list(train_pca_results[k][\"total\"].keys())\ndf = pd.DataFrame(data, columns = columns)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-03-29T07:31:16.237033Z","iopub.status.idle":"2022-03-29T07:31:16.237464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax =plt.subplots(1,1, figsize=(8,8))\n\nax.set_title('different pca_n_component values on train_set')\ng = sns.lineplot(x=df.pca_n_components, y=df.FAR, label = 'FAR', ax = ax)\ng = sns.lineplot(x=df.pca_n_components, y=df.FRR, label = 'FRR', ax = ax)\ng = sns.lineplot(x=df.pca_n_components, y=df.train_err_rate, label = 'train_err_rate', ax = ax)\n\n\n\n\n# g.set_xticks(y_ticks)\nax.set_xlabel('n_components')\n\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T07:31:16.238345Z","iopub.status.idle":"2022-03-29T07:31:16.238785Z"},"trusted":true},"execution_count":null,"outputs":[]}]}